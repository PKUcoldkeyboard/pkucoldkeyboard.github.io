<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cuterwrite's Blog</title><link>https://cuterwrite.top/</link><description>Recent content on Cuterwrite's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>cuterwrite</copyright><lastBuildDate>Tue, 13 Aug 2024 22:44:00 +0000</lastBuildDate><atom:link href="https://cuterwrite.top/index.xml" rel="self" type="application/rss+xml"/><item><title>Arm 矩阵加速：可伸缩矩阵扩展 SME</title><link>https://cuterwrite.top/p/arm-sme-for-performance/</link><pubDate>Tue, 13 Aug 2024 22:44:00 +0000</pubDate><guid>https://cuterwrite.top/p/arm-sme-for-performance/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-14_117622407_p0_master1200.webp" alt="Featured image of post Arm 矩阵加速：可伸缩矩阵扩展 SME" />&lt;h1 id="arm-矩阵加速可伸缩矩阵扩展-sme">Arm 矩阵加速：可伸缩矩阵扩展 SME&lt;/h1>
&lt;h2 id="1-sme-简介">1. SME 简介&lt;/h2>
&lt;p>可伸缩矩阵扩展 SME (Scalable Matrix Extension) SME 是在可伸缩向量扩展（Scalable Vector Extensions， SVE 和 SVE2）的基础上建立的，并增加了有效处理矩阵的能力，主要功能包括：&lt;/p>
&lt;ul>
&lt;li>计算 SVE 向量的外积（Outer product）&lt;/li>
&lt;li>矩阵块（tile） 存储&lt;/li>
&lt;li>tile 向量的加载、存储、插入和提取（包括动态转置）&lt;/li>
&lt;li>Streaming SVE 模式&lt;/li>
&lt;/ul>
&lt;p>下表总结了 SME、SVE 和 SVE2 的主要功能：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>SME&lt;/th>
&lt;th>SVE&lt;/th>
&lt;th>SVE2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Streaming SVE 模式&lt;/td>
&lt;td>NEON DSP++&lt;/td>
&lt;td>可伸缩向量&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>动态矩阵转置&lt;/td>
&lt;td>多精度算术&lt;/td>
&lt;td>per-lane predication&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>向量外积&lt;/td>
&lt;td>匹配检测和直方图&lt;/td>
&lt;td>Gather-load 与 Scatter-store&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>加载、存储、插入和提取矩阵向量&lt;/td>
&lt;td>非时间性 scatter/gather&lt;/td>
&lt;td>预测向量化&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>按位置换（bitwise permute）&lt;/td>
&lt;td>ML 扩展（FP16 + DOT）&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>AE、SHA3、SM4、Crypto&lt;/td>
&lt;td>V8.6 BF16, FP 与 Int8 支持&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>SME 定义了以下新功能：&lt;/p>
&lt;ul>
&lt;li>新的架构状态，可以用来存储二维矩阵 tile&lt;/li>
&lt;li>Streaming SVE 模式，支持执行向量长度与 tile 长度匹配的 SVE2 指令。&lt;/li>
&lt;li>将两个向量的外积累加（或累减）到一个矩阵 tile 中的新指令。&lt;/li>
&lt;li>新的加载、存储和移动指令：可以将向量写入到矩阵 tile 的一行或一列，也可以将矩阵 tile 的一行或一列读取到向量。&lt;/li>
&lt;/ul>
&lt;p>与 SVE2 类似，SME 也是一种支持可伸缩向量长度的扩展，可实现向量长度无关性 (VLA)、per-lane predication、predication 驱动的循环控制和管理功能。&lt;/p>
&lt;h2 id="2-streaming-sve-模式">2. Streaming SVE 模式&lt;/h2>
&lt;p>SME 引入了 Streaming SVE 模式，该模式实现了 SVE2 指令集的一个子集，并增加了新的 SME 专用指令。&lt;/p>
&lt;p>Streaming SVE 模式支持对大型数据集进行高吞吐量地流式数据处理，流式数据通常具有简单的循环控制流和有限的条件性。&lt;/p>
&lt;p>在 Non-streaming SVE 模式下，支持完整的 SVE2 指令集，通常处理复杂的数据结构和复杂的判断。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-14_3443.webp"
alt="Streaming SVE 模式与 Non-streaming SVE 模式" width="80%" loading="lazy">&lt;figcaption>
&lt;h4>Streaming SVE 模式与 Non-streaming SVE 模式&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>大多数 SME 指令仅在 Streaming SVE 模式下可用。Streaming SVE 模式下的流向量长度（SVL）可能与非流向量长度（NSVL）不同。&lt;/p>
&lt;p>预期是：SVL 要比 NSVL 更长或是相同，也就是 SVL &amp;gt;= NSVL。例如，NSVL 的长度可以为 128-bit , 而 SVL 的长度可以为 512-bit 。&lt;/p>
&lt;p>SME 的 SVL 可以是 128-bit , 256-bit , 512-bit, 1024-bit 或是 2048-bit 。SVL 需要是 2 的次幂，而 NSVL 需要是 128 的整数倍。&lt;/p>
&lt;p>与 SVE2 类似，软件可以控制 &lt;code>SMCR_ELx.LEN&lt;/code> 寄存器位来设置 EL1, EL2, EL3 想用的有效 SVL 长度（可以设置为比硬件支持的 SVL 更短）。&lt;/p>
&lt;p>有关 Streaming SVE 模式的更多信息，请参阅《Arm 架构参考手册》第 B1.4.6 节（A-profile 架构）。&lt;/p>
&lt;h2 id="3-切换-non-streaming-和-streaming-sve-模式">3. 切换 Non-streaming 和 Streaming SVE 模式&lt;/h2>
&lt;p>如果 CPU 硬件实现既支持 Streaming SVE 模式的 SME ，又支持 Non-streaming SVE 模式的 SVE2 ，应用程序可以根据自己的需求动态切换这两个操作模式。&lt;/p>
&lt;p>为 SME 提供一个独立的操作模式，使 CPU 硬件实现可以为同一应用提供不同的向量长度。比如 CPU 硬件实现可以选择支持一个更长的 Streaming SVE 模式向量长度，并针对适用于高吞吐量的流操作对硬件进行优化。&lt;/p>
&lt;p>应用程序很容易在 Streaming SVE 模式和 Non-streaming SVE 模式之间动态切换。SME 引入的 &lt;code>PSTATE.{SM, ZA}&lt;/code> 位可以可启用和禁用 Streaming SVE 模式和 SME ZA 存储：&lt;/p>
&lt;ul>
&lt;li>SM: 启用与禁用 Streaming SVE 模式&lt;/li>
&lt;li>ZA：启用和禁用 ZA 存储访问&lt;/li>
&lt;/ul>
&lt;p>可以通过 &lt;code>MSR/MRS&lt;/code> 指令操作 Streaming Vector Control Register (SVCR) 来设置和读取 &lt;code>PSTATE.{SM, ZA}&lt;/code> 位，具体操作如下：&lt;/p>
&lt;ul>
&lt;li>&lt;code>MSR SVCRSM, #&amp;lt;imm&amp;gt; MSR SVCRSM，#&lt;/code>&lt;/li>
&lt;li>&lt;code>MSR SVCRZA, #&amp;lt;imm&amp;gt;&lt;/code>&lt;/li>
&lt;li>&lt;code>MSR SVCRSMZA, #&amp;lt;imm&amp;gt;&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>SMSTART 指令是设置 &lt;code>PSTATE.SM&lt;/code> 和 &lt;code>PSTATE.ZA&lt;/code> 的 &lt;code>MSR&lt;/code> 指令的别名&lt;/p>
&lt;ul>
&lt;li>&lt;code>SMSTART&lt;/code>：同时启用 Streaming SVE 模式和 ZA 存储访问&lt;/li>
&lt;li>&lt;code>SMSTART SM&lt;/code>：启用 Streaming SVE 模式&lt;/li>
&lt;li>&lt;code>SMSTART ZA&lt;/code>：启用 ZA 存储访问&lt;/li>
&lt;/ul>
&lt;p>SMSTOP 指令则是清除 &lt;code>PSTATE.SM&lt;/code> 和 &lt;code>PSTATE.ZA&lt;/code> 的 &lt;code>MSR&lt;/code> 指令的别名。&lt;/p>
&lt;ul>
&lt;li>&lt;code>SMSTOP&lt;/code>：同时禁用 Streaming SVE 模式和 ZA 存储访问&lt;/li>
&lt;li>&lt;code>SMSTOP SM&lt;/code>：禁用 Streaming SVE 模式&lt;/li>
&lt;li>&lt;code>SMSTOP ZA&lt;/code>：禁用 ZA 存储访问&lt;/li>
&lt;/ul>
&lt;p>下图展示了应用程序是如何在 Streaming SVE 模式和 Non-streaming SVE 模式之间切换的：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_Scalable_Matrix_p1.webp"
alt="应用程序切换 Streaming SVE 模式和 Non-streaming SVE 模式" width="50%" loading="lazy">&lt;figcaption>
&lt;h4>应用程序切换 Streaming SVE 模式和 Non-streaming SVE 模式&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>有关使用 SMSTART 和 SMSTOP 在 Streaming SVE 模式和 Non-Streaming SVE 模式之间切换的更多信息，请参阅《Arm 架构参考手册》中有关 A-profile 架构的 C6.2.327 和 C6.2.328 节。&lt;/p>
&lt;h2 id="4-sme-架构状态">4. SME 架构状态&lt;/h2>
&lt;p>与 SVE2 类似，在 Streaming SVE 模式，它有 &lt;code>Z0-Z31&lt;/code> 向量寄存器，和 &lt;code>P0-P15&lt;/code> Predicate 寄存器。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_4130_ARM2799_3_Scalable_Matrix_p1.webp"
alt="Streaming mode registers" width="70%" loading="lazy">
&lt;/figure>
&lt;p>SVE 向量寄存器的最低编号位 &lt;code>Zn&lt;/code> 也保存着固定长度的 &lt;code>Vn、Qn、Dn、Sn、Hn&lt;/code> 和 &lt;code>Bn&lt;/code> 寄存器。&lt;/p>
&lt;p>进入 Streaming SVE 模式（ &lt;code>PSTATE.SM&lt;/code> 由 0 变为 1）或退出 Streaming SVE 模式（ &lt;code>PSTATE.SM&lt;/code> 由 1 变为 0）时，所有这些寄存器都将置零。&lt;/p>
&lt;p>大多数 Non-streaming SVE2 指令可用于 Streaming SVE 模式，但&lt;strong>可能使用不同的向量长度&lt;/strong>（流模式使用 VSL 长度，非流模式使用 NVSL 长度）。可以使用 &lt;code>RDSVL&lt;/code> 指令读取当前的有效向量长度 VL。&lt;/p>
&lt;pre>&lt;code class="language-armasm">//Read multiple of Streaming SVE vector register size to Xd
RDSVL &amp;lt;Xd&amp;gt;, #&amp;lt;imm&amp;gt;
&lt;/code>&lt;/pre>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>因为 SME 支持 Vector Length Agnostic (VLA) ，在 Streaming SVE 模式下，软件很少需要明确读 SVL 向量长度。在 Non-streaming SVE 模式下，通常使用 RDSVL 指令来确定 SVL 的值。&lt;/p>&lt;/div>
&lt;h2 id="5-za-array">5. ZA array&lt;/h2>
&lt;p>SME 新引入的 ZA (Z Array, ZA Storage) 是一个二维（2D）正方形数组，大小是 SVL x SVL。之所以叫 Z Array，也是因为它行与列的长度与 Streaming SVE 模式下的 Zn 寄存器一致。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_4314_ARM2799_4_Scalable_Matrix_p1.webp"
alt="ZA array" width="50%" loading="lazy">&lt;figcaption>
&lt;h4>ZA array&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>例如：如果 Streaming SVE 模式下的向量长度为 256-bit，即 Zn 寄存器的长度为 256-bit，那么 ZA 的大小为 256-bit x 256-bit。&lt;/p>
&lt;p>ZA array 可以通过以下方式访问：&lt;/p>
&lt;ul>
&lt;li>ZA array vector 访问&lt;/li>
&lt;li>ZA tiles&lt;/li>
&lt;li>ZA tile slices&lt;/li>
&lt;/ul>
&lt;h3 id="51-za-array-vector-访问">5.1 ZA array vector 访问&lt;/h3>
&lt;p>ZA array 的一行可以当成一个 SVL 长度的向量来访问，这个向量可以放数据类型长度为 8-bit, 16-bit, 32-bit, 64-bit 或 128-bit 的元素，比如 32-bit 的 fp32 浮点数。&lt;/p>
&lt;pre>&lt;code class="language-c">ZA.B[N], ZA.H[N], ZA.S[N], ZA.D[N], ZA.Q[N]
&lt;/code>&lt;/pre>
&lt;p>其中 &lt;code>B，H，S，D，Q&lt;/code> 分别表示 8-bit , 16-bit , 32-bit , 64-bit , 128-bit。&lt;/p>
&lt;p>ZA array vector 的数量与 SVL 中的字节数相同，例如，如果 SLV 是 256-bit ，那么 ZA array vector 的数量是 32 个，N 的范围是 0 到 31。&lt;/p>
&lt;p>为了支持上下文切换，SME 引入了新的 &lt;code>LDR&lt;/code> 和 &lt;code>STR&lt;/code> 指令，用于从内存加载和存储一个 ZA array vector。&lt;/p>
&lt;pre>&lt;code class="language-armasm">LDR ZA[&amp;lt;Wv&amp;gt;, &amp;lt;imm&amp;gt;], [&amp;lt;Xn|SP&amp;gt;{, #&amp;lt;imm&amp;gt;, MUL VL}]
STR ZA[&amp;lt;Wv&amp;gt;, &amp;lt;imm&amp;gt;], [&amp;lt;Xn|SP&amp;gt;{, #&amp;lt;imm&amp;gt;, MUL VL}]
&lt;/code>&lt;/pre>
&lt;h3 id="52-za-tiles">5.2 ZA tiles&lt;/h3>
&lt;p>ZA tile 是在 ZA 中的正方形的二维子矩阵。ZA tile 的宽度始终是 SVL，与 ZA array 的宽度相同。&lt;/p>
&lt;p>ZA 可以分成多少个可用的 ZA tile 是由元素的数据类型大小决定的：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>元素数据类型大小&lt;/th>
&lt;th>tile 数量&lt;/th>
&lt;th>tile 名称&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>8-bit&lt;/td>
&lt;td>1&lt;/td>
&lt;td>ZA0.B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>16-bit&lt;/td>
&lt;td>2&lt;/td>
&lt;td>ZA0.H-ZA1.H&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>32-bit&lt;/td>
&lt;td>4&lt;/td>
&lt;td>ZA0.S-ZA3.S&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>64-bit&lt;/td>
&lt;td>8&lt;/td>
&lt;td>ZA0.D-ZA7.D&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>128-bit&lt;/td>
&lt;td>16&lt;/td>
&lt;td>ZA0.Q-ZA15.Q&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>当元素数据类型为 8-bit 时，ZA 只能作为一个 ZA tile (ZA0.B) 被访问。&lt;/li>
&lt;li>当元素数据类型为 16-bit 时，ZA 可以作为 2 个 ZA tile (ZA0.H 和 ZA1.H) 被访问。&lt;/li>
&lt;li>当元素数据类型为 32-bit 时，ZA 可以作为 4 个 ZA tile (ZA0.S 到 ZA3.S) 被访问。&lt;/li>
&lt;li>当元素数据类型为 64-bit 时，ZA 可以作为 8 个 ZA tile (ZA0.D 到 ZA7.D) 被访问。&lt;/li>
&lt;li>当元素数据类型为 128-bit 时，ZA 可以作为 16 个 ZA tile (ZA0.Q 到 ZA15.Q) 被访问。&lt;/li>
&lt;/ul>
&lt;p>例如，如果 SVL 为 256-bit，元素数据类型大小为 8-bit，则 ZA 可以视为 ZA0.B，也可视为 32 个向量（32 行，每行大小为 32 x 8-bit，即每行 32 个元素）。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_ZA0B.webp"
alt="ZA0.B" width="50%" loading="lazy">
&lt;/figure>
&lt;p>如果 SVL 为 256-bit，元素数据类型大小为 16-bit，则 ZA 可以视为 2 个 ZA tile (ZA0.H 和 ZA1.H)，每个 tile 视为 16 个向量（16 行，每行大小为 16 x 16-bit，即每行 16 个元素）。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_ZA0H_ZA1H.webp"
alt="ZA0.H 和 ZA1.H" width="40%" loading="lazy">
&lt;/figure>
&lt;p>这样做的好处是充分利用了 ZA storage，在实际应用中，比如说当 SVL 为 256-bit，元素数据类型大小为 32-bit，ZA 的大小为 256-bit x 256-bit 时，&lt;strong>要对两个 Z 寄存器里的向量做外积运算&lt;/strong>，计算得到的外积结果是 8 x 8 的二维浮点数数组，这个外积只需要 ZA 的 1/4 的存储空间。将 ZA 分成 4 个 ZA tile，这样就可以充分利用 ZA storage。&lt;/p>
&lt;h3 id="53-za-tile-slices">5.3 ZA tile slices&lt;/h3>
&lt;p>一个 ZA tile 可以作为一个整体来访问，也可以以一个个 ZA tile slice 的方式访问。&lt;/p>
&lt;p>当作为一个整体访问时，指令可以使用 tile 的名字访问：&lt;/p>
&lt;pre>&lt;code class="language-text">ZA0.B, ZA0.H-ZA1.H, ZA0.S-ZA3.S, ZA0.D-ZA7.D or ZA0.Q-ZA15.Q
&lt;/code>&lt;/pre>
&lt;p>一个 ZA tile slice 是由其 ZA tile 中&lt;strong>水平方向或是垂直方向的连续元素组成的一维数组&lt;/strong>，即在 ZA tile 中的一行或是一列。&lt;/p>
&lt;p>对一个 ZA tile 的向量访问即是读写一个 ZA tile slice ：&lt;/p>
&lt;ul>
&lt;li>水平或垂直方向的 ZA tile slice 访问，由 ZA tile 名字后的 &lt;code>H&lt;/code> 或 &lt;code>V&lt;/code> 后缀来表示。&lt;/li>
&lt;li>具体的 ZA tile slice 由一个索引来表示，由 ZA tile 名字后的切片索引 &lt;code>[N]&lt;/code> 来表示。&lt;/li>
&lt;/ul>
&lt;p>例如，如果 SVL 为 128 位，元素数据类型大小为 8-bit，那么其水平的和垂直的 ZA tile slice 可由下图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_6724_ARM2799_7_Scalable_Matrix_p1.webp"
alt="ZA tile slices" width="50%" loading="lazy">
&lt;/figure>
&lt;p>再例如，如果 SVL 为 128 位，元素数据类型大小为 16-bit，那么其水平的和垂直的 ZA tile slice 可由下图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_6888_ARM2799_8_Scalable_Matrix_p1.webp"
alt="ZA tile slices" width="50%" loading="lazy">
&lt;/figure>
&lt;p>为了提高硬件访问 ZA tile 和 ZA tile slices 的效率，ZA tile 的 ZA tile slices 是交错排列的。&lt;/p>
&lt;p>下图显示了这种交错排列的示例。在此示例中，SVL 为 256 位，元素数据类型大小为 16 位。这意味着，ZA 可被视为两个 ZA tile（ZA0H 和 ZA1H），并具有交错的水平 tile slices ：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_4885_SME_interleave.webp"
alt="ZA tile slices" width="auto" loading="lazy">
&lt;/figure>
&lt;p>下图展示了不同的元素数据类型大小的水平和垂直方向 ZA tile slice 的混合视图:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_7673_SME_V_H.webp"
alt="ZA tile slices" width="auto" loading="lazy">
&lt;/figure>
&lt;p>左侧各栏显示了 ZA 存储器每一行的不同处理方式。&lt;/p>
&lt;p>设 SIZE 为向量元素的大小，其中 SIZE 为 1、2、4、8、16，分别代表数据类型 B、H、S、D 或 Q。&lt;/p>
&lt;p>设 NUM_OF_ELEMENTS 为向量中的元素个数，即 bytes_of(SVL)/SIZE。&lt;/p>
&lt;p>水平 tile slice， &lt;code>ZAnH.&amp;lt;B|H|S|D|Q&amp;gt;[m]&lt;/code> 访问一个向量，该向量包含 ZA storage 中的整行（m x SIZE + n）。该向量包含数据类型为 B、H、S、D 或 Q 的元素。&lt;/p>
&lt;p>垂直 tile slice，&lt;code>ZAnV.&amp;lt;B|H|S|D|Q&amp;gt;[m] &lt;/code> 访问一个向量，该向量包含 ZA storage 中的整列（m x SIZE）。该向量包含数据类型为 B、H、S、D 或 Q 的元素。&lt;/p>
&lt;p>&lt;code>ZAnV.[m] &lt;/code> 访问一个包含列（m x SIZE）和行元素（i x SIZE + n）的向量，其中 i 为 0 ~ NUM_OF_ELEMENTS-1。该向量包含数据类型为 B、H、S、D 或 Q 的元素。&lt;/p>
&lt;p>使用混合元素数据类型大小以及水平和垂直 tile slice 的应用应小心处理重叠。&lt;/p>
&lt;p>有关 ZA Array、ZA array vectors、tile 和 tile slices 的更多信息，请参阅《Arm 架构参考手册》中有关 A-profile 架构的 B1.4.8 至 B1.4.12 节。&lt;/p>
&lt;h2 id="6-steaming-sve-模式下支持的指令">6. Steaming SVE 模式下支持的指令&lt;/h2>
&lt;p>某些指令在 Streaming SVE 模式下有限制：&lt;/p>
&lt;ul>
&lt;li>一些 SVE/SVE2 指令变为非法执行
&lt;ul>
&lt;li>Gathed-load 和 Scatter-store 指令&lt;/li>
&lt;li>使用 First Fault 寄存器的 SVE2 指令&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>大多的 NEON 指令变为 UNDEFINED&lt;/li>
&lt;/ul>
&lt;p>有关受 Streaming SVE 模式影响的指令的更多信息，请参阅文档 《Arm 架构参考手册》。&lt;/p>
&lt;p>SME 增加了几条新指令，其中包括：&lt;/p>
&lt;ul>
&lt;li>矩阵外积和累加或减法指令，包括 FMOPA、UMOPA 和 BFMOPA。
&lt;ul>
&lt;li>SVE2 向量寄存器（Z0-Z31）作为外积运算的行和列输入。&lt;/li>
&lt;li>ZA storage 保存二维矩阵 tile 的输出结果。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>将 SVE2 Z 向量与 ZA 的行或列做加法运算的指令&lt;/li>
&lt;li>对 ZA tiles 的清零操作指令&lt;/li>
&lt;li>增加了一些在 Streaming 和 Non-streaming 模式下都能使用的指令&lt;/li>
&lt;/ul>
&lt;h2 id="7-sme-指令">7. SME 指令&lt;/h2>
&lt;p>操作 ZA storage 的 SME 指令主要包括：&lt;/p>
&lt;ul>
&lt;li>计算两个向量的外积，并累加或累减，然后将结果放入一个 ZA tile 的指令&lt;/li>
&lt;li>将 SVE 向量（Z 寄存器）存入或取出 ZA tile 的行或列的指令&lt;/li>
&lt;li>水平或垂直方向上，一个 SVE 向量与 ZA tile 的加法指令&lt;/li>
&lt;li>给一个标量寄存器加上 Streaming SVE 模式下向量长度的倍数的指令&lt;/li>
&lt;/ul>
&lt;h3 id="71-外积并累加或累减指令">7.1 外积并累加或累减指令&lt;/h3>
&lt;p>为了帮助理解外积并累加或累减指令，让我们看看如何使用外积操作来做矩阵乘法。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_2313_Picture1_png-1280x960.webp"
alt="Outer product" width="auto" loading="lazy">
&lt;/figure>
&lt;p>计算两个向量 a 和 b 的外积会得到一个包含外积的结果矩阵 C：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_1665_Picture2_png-1280x960.webp"
alt="Outer product" width="auto" loading="lazy">
&lt;/figure>
&lt;p>现在考虑两个矩阵 a 和 b 的矩阵乘运算：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_8117_Picture3_png-1280x960.webp"
alt="Matrix multiplication" width="auto" loading="lazy">
&lt;/figure>
&lt;p>这个矩阵乘可以通过计算两次外积操作和两个结果矩阵的累加来实现（就是常用的手写计算的方法），如下图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_3731_Picture4_png-1280x960.webp"
alt="Matrix multiplication with outer product" width="auto" loading="lazy">
&lt;/figure>
&lt;p>SME 为以下数据类型引入了高效的外积并累加或减法指令：&lt;/p>
&lt;ul>
&lt;li>8-bit, 16-bit 整数&lt;/li>
&lt;li>FP16, BF16, FP32 和 FP64 浮点数&lt;/li>
&lt;/ul>
&lt;p>这些指令计算两个 Z 向量寄存器（Zn 和 Zm）中两个向量的外积，将结果数组与一个 ZA tile（ZAda）中已有数据进行累加或累减，并将结果存入同一 ZA tile（ZAda）中。每个源向量由相应的控制 predicate 寄存器（Pn 和 Pm）独立地 predicate。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>输出数组&lt;/th>
&lt;th>输入向量&lt;/th>
&lt;th>描述&lt;/th>
&lt;th>示例&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>INT32&lt;/td>
&lt;td>INT8, INT8&lt;/td>
&lt;td>将四个 INT8 外积之和存入每个 INT32 元素&lt;/td>
&lt;td>SMOPA 或 SMOPS 或 UMOPA 或 UMOPS：带符号或无符号整数外积和，并累加或累减。例如： &lt;code>UMOPS &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.B, &amp;lt;Zm&amp;gt;.B&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>INT32&lt;/td>
&lt;td>INT16, INT16&lt;/td>
&lt;td>将两个 INT16 外积之和存入每个 INT32 元素&lt;/td>
&lt;td>SMOPA 或 SMOPS 或 UMOPA 或 UMOPS：带符号或无符号整数外积和，并累加或累减。例如： &lt;code>UMOPS &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.H, &amp;lt;Zm&amp;gt;.H&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>INT64&lt;/td>
&lt;td>INT16, INT16&lt;/td>
&lt;td>如果实现了 FEAT_SME_I16I64，则将四个 INT16 外积之和存入每个 INT64 元素&lt;/td>
&lt;td>SMOPA 或 SMOPS 或 UMOPA 或 UMOPS：带符号或无符号整数外积和，并累加或累减。例如： &lt;code>UMOPS &amp;lt;ZAda&amp;gt;.D, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.H, &amp;lt;Zm&amp;gt;.H&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP32&lt;/td>
&lt;td>BF16, BF16&lt;/td>
&lt;td>将两个 BF16 外积之和存入每个 FP32 元素&lt;/td>
&lt;td>BFMOPA 或 BFMOPS：BFloat16 外积和，并累加或累减。例如： &lt;code>BFMOPS &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.H, &amp;lt;Zm&amp;gt;.H&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP32&lt;/td>
&lt;td>FP16, FP16&lt;/td>
&lt;td>将两个 FP16 外积之和存入每个 FP32 元素&lt;/td>
&lt;td>FMOPA 或 FMOPS：半精度浮点外积和，并累加或累减。例如： &lt;code>FMOPS &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.H, &amp;lt;Zm&amp;gt;.H&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP32&lt;/td>
&lt;td>FP32, FP32&lt;/td>
&lt;td>简单的 FP32 外积&lt;/td>
&lt;td>FMOPA 或 FMOPS：浮点外积和，并累加或累减。例如： &lt;code>FMOPS &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.S, &amp;lt;Zm&amp;gt;.S&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP64&lt;/td>
&lt;td>FP64, FP64&lt;/td>
&lt;td>如果实现了 FEAT_SME_F64F64，则进行简单的 FP64 外积&lt;/td>
&lt;td>FMOPA 或 FMOPS：浮点外积和，并累加或累减。例如： &lt;code>FMOPS &amp;lt;ZAda&amp;gt;.D, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.D, &amp;lt;Zm&amp;gt;.D&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="711-fp32-fp64-外积并累加或累减指令">7.1.1 FP32, FP64 外积并累加或累减指令&lt;/h4>
&lt;p>那些输入向量和输出数组有同样数据类型（FP32， FP64）的指令相对简单。&lt;/p>
&lt;p>下例展示了 FP32 类型的外积并累加或累减指令。&lt;/p>
&lt;pre>&lt;code class="language-armasm">FMOPA &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.S, &amp;lt;Zm&amp;gt;.S
FMOPS &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.S, &amp;lt;Zm&amp;gt;.S
&lt;/code>&lt;/pre>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_3613751670-667e5f923c64.webp"
alt="FMOPA and FMOPS" width="auto" loading="lazy">
&lt;/figure>
&lt;p>这个例子中，假设 SVL 向量长度为 128，&lt;code>Zn.S&lt;/code> 和 &lt;code>Zm.S&lt;/code> 中存放了 4 个 FP32 数组成的向量，此指令计算 &lt;code>Zn.S&lt;/code> 和 &lt;code>Zm.S&lt;/code> 的外积，外积结果为图中灰色的矩阵，然后将此外积结果累加或累减 &lt;code>ZAda.S&lt;/code> 这个 ZA tile 中原有的值，将结果存入同一 ZA tile。&lt;/p>
&lt;h4 id="712-fp16-bf16-int16-int8-i16i64-类型的外积并累加或累减指令">7.1.2 FP16, BF16, INT16, INT8, I16I64 类型的外积并累加或累减指令&lt;/h4>
&lt;p>由于这些指令会扩大计算结果数据类型，因此这些操作不像前面 FP32，FP64 类型指令那么简单明了。&lt;/p>
&lt;ul>
&lt;li>BF16 指令计算两个 BF16 的外积的和，扩大结果类型为 FP32, 然后将结果与目标 tile 进行破坏性相加或相减。&lt;/li>
&lt;li>INT8 指令计算四个 INT8 的外积的和，扩大结果类型为 INT32，然后将结果与目标 tile 进行破坏性相加或相减。&lt;/li>
&lt;li>INT16 指令计算两个 INT16 的外积的和，扩大结果类型为 INT32，然后将结果与目标 tile 进行破坏性相加或相减。&lt;/li>
&lt;li>FP16 指令计算两个 FP16 的外积的和，扩大结果类型为 FP32，然后将结果与目标 tile 进行破坏性相加或相减。&lt;/li>
&lt;li>如果实现了 FEAT_SME_I16I64，I16I64 指令计算四个 INT16 的外积的和，扩大结果类型为 INT64, 然后将结果与目标 tile 进行破坏性相加或相减。&lt;/li>
&lt;/ul>
&lt;p>以下例子展示了 SVL 向量长度为 128 的 INT8 UMOPA 指令进行的操作：&lt;/p>
&lt;pre>&lt;code class="language-armasm">UMOPA &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.B, &amp;lt;Zm&amp;gt;.B
&lt;/code>&lt;/pre>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_1030_Picture6_png-1280x960.webp"
alt="INT8 UMOPA" width="auto" loading="lazy">
&lt;/figure>
&lt;p>每个输入寄存器（&lt;code>Zn.B&lt;/code>、&lt;code>Zm.B&lt;/code>）都被视为一个包含 4x4 元素的矩阵，可以看作是 4 个连续元素组成的块（如图中红线所标）被转置了。&lt;/p>
&lt;p>在这个例子中，因为 SVL 向量长度为 128-bit：&lt;/p>
&lt;ul>
&lt;li>第一源向量 &lt;code>Zn.B&lt;/code> ，包含一个无符号 8-bit 整数的 4x4 子矩阵。&lt;/li>
&lt;li>第二源向量 &lt;code>Zm.B&lt;/code> ，包含一个无符号 8-bit 整数的 4x4 子矩阵。&lt;/li>
&lt;li>UMOPA 指令计算出 4x4 扩大了的 32-bit 整数外积的和，然后破坏性地累加上目标 tile（ZAda）中的整数。&lt;/li>
&lt;/ul>
&lt;p>更笼统地说，UMOPA 指令是将第一个源向量中的子矩阵与第二个源向量中的子矩阵相乘。每个源向量包含一个(SVL/32) x 4 的无符号 8-bit 整数的子矩阵。然后将得到的 (SVL/32) x (SVL/32)扩大了的 32-bit 整数外积和破坏性地加上一个 32-bit 整数目标 tile。&lt;/p>
&lt;p>下面的例子展示了 SVL 为 128-bit 的 BF16 BFMOPA 进行的操作：&lt;/p>
&lt;pre>&lt;code class="language-armasm">BFMOPA &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.H, &amp;lt;Zm&amp;gt;.H
&lt;/code>&lt;/pre>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_6545_Picture7_png-1280x960.webp"
alt="BF16 BFMOPA" width="auto" loading="lazy">
&lt;/figure>
&lt;p>在这个例子中，因为 SVL 向量长度为 128-bit：&lt;/p>
&lt;ul>
&lt;li>第一源向量 &lt;code>Zn.H&lt;/code> ，包含一个 BF16 整数的 4x2 子矩阵，它被扩大成单精度浮点数。&lt;/li>
&lt;li>第二源向量 &lt;code>Zm.H&lt;/code> ，包含一个 BF16 整数的 2x4 子矩阵，它被扩大成单精度浮点数。&lt;/li>
&lt;li>BMOPA 指令计算出 4x4 单精度外积的和，然后破坏性地累加上目标 tile（ZAda）中的单精度浮点数。&lt;/li>
&lt;/ul>
&lt;p>更笼统地说，BFMOPA 指令扩大了存放在第一源向量里的(SVL/32) x2 BF16 子矩阵的类型为单精度，扩大了存放在第二源向量里的 2x (SVL/32) BF16 子矩阵的类型为单精度，将这两个子矩阵相乘。然后将得到的 (SVL/32) x (SVL/32)单精度外积和破坏性地加上一个单精度目标 tile。&lt;/p>
&lt;p>以下表格显示了几种数据类型和 SVL 长度的一条外积并累加或累减指令所做的对应数据类型的 MAC(乘累加)数量：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>128-bit&lt;/th>
&lt;th>256-bit&lt;/th>
&lt;th>512-bit&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>FP32&lt;/td>
&lt;td>16&lt;/td>
&lt;td>64&lt;/td>
&lt;td>256&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP64&lt;/td>
&lt;td>4&lt;/td>
&lt;td>16&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>INT8&lt;/td>
&lt;td>64&lt;/td>
&lt;td>256&lt;/td>
&lt;td>1024&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>INT16&lt;/td>
&lt;td>32&lt;/td>
&lt;td>128&lt;/td>
&lt;td>512&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BF16&lt;/td>
&lt;td>32&lt;/td>
&lt;td>128&lt;/td>
&lt;td>512&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP16&lt;/td>
&lt;td>32&lt;/td>
&lt;td>128&lt;/td>
&lt;td>512&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="72-带-predication-的-sme-指令">7.2 带 Predication 的 SME 指令&lt;/h3>
&lt;p>每个源向量都可以被其相应的控制 predicate 寄存器独立地 predicate:&lt;/p>
&lt;ul>
&lt;li>外积并累加或累减指令使用 Pn/M 和 Pn/M (没有/Z 形式)：Inactive 的源元素被当成具有 0 值。&lt;/li>
&lt;li>Slice move 指令使用 Pg/M: 目标 slice 中 Inactive 的元素保持不变。&lt;/li>
&lt;li>Tile slice load 指令使用 Pg/Z: 目标 tile slice 中的 Inactive 元素被设置为 0。&lt;/li>
&lt;li>Tile slice store 指令使用 Pg: Inactive 的元素不会写入内存。&lt;/li>
&lt;/ul>
&lt;p>Predication 让矩阵的维数不是 SVL 的倍数的情况更容易处理。&lt;/p>
&lt;p>例如下图的指令：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_2656_Picture12_png-600x0.webp"
alt="SME predication" width="auto" loading="lazy">
&lt;/figure>
&lt;p>输入向量 &lt;code>Z0&lt;/code> 被 &lt;code>P0&lt;/code> predicate，&lt;code>Z1&lt;/code> 被 &lt;code>P1&lt;/code> predicate。&lt;/p>
&lt;p>在这个例子中：&lt;/p>
&lt;ul>
&lt;li>SVL 向量长度为 512-bit。&lt;/li>
&lt;li>Z 寄存器中包含 16 个 FP32 数组成的向量。&lt;/li>
&lt;li>&lt;code>P0&lt;/code> 中最后两个元素是 inactive 的。&lt;/li>
&lt;li>&lt;code>P1&lt;/code> 中最后一个元素是 inactive 的。&lt;/li>
&lt;/ul>
&lt;p>这条指令更新 &lt;code>ZA0.S&lt;/code> 中 (16-2) x (16-1) 个 FP32 元素，因为使用了 &lt;code>Pn/M&lt;/code> , &lt;code>ZA0.S&lt;/code> 中剩下的元素保持不变。&lt;/p>
&lt;p>下图展示了更多的 predicated 外积并累加或累减的例子。图中被划线的文字表示被 inactive predicate 元素影响的计算部分。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_2072_Picture14_png-1280x960.webp"
alt="SME predication FMOPA" width="auto" loading="lazy">
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_3513_Picture16_png-1280x960.webp"
alt="SME predication UMOPA" width="auto" loading="lazy">
&lt;/figure>
&lt;h3 id="73-za-tile-与一个-z-向量的加运算">7.3 ZA tile 与一个 Z 向量的加运算&lt;/h3>
&lt;p>SME 包括 ZA tile 的行或列都加上一个向量的指令，这些指令也有 predication 的支持。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>指令&lt;/th>
&lt;th>说明&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ADDHA&lt;/td>
&lt;td>将源向量添加到 ZA tile 的每个水平 slice 上&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ADDVA&lt;/td>
&lt;td>将源向量添加到 ZA tile 的每个垂直 slice 上&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>例如：&lt;/p>
&lt;pre>&lt;code class="language-armasm">ADDHA ZA0.S, P0/M, P1/M, Z1.S
&lt;/code>&lt;/pre>
&lt;p>将执行以下操作：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_ARM2799_9_Scalable_Matrix_p2_png-1200x0.webp"
alt="SME ADDHA" width="auto" loading="lazy">
&lt;/figure>
&lt;p>这个 ADDHA 指令将源向量 Z1 中的每个元素加上 ZA0.S tile 每一水平 slice 的相应 active 元素。&lt;/p>
&lt;p>Tile 中元素被一对 governing predicate 进行 predicate。 一个水平 slice 中的一个元素在下面情况下可以认为是 active：&lt;/p>
&lt;ul>
&lt;li>它在第二 governing predicate 对应的元素是 TRUE, 并且&lt;/li>
&lt;li>它在第一 governing predicate 对应的水平 slice 行号也为 TRUE,目标 tile 中 inactive 元素保持不变。&lt;/li>
&lt;/ul>
&lt;h3 id="74-tile-load-store-move-指令">7.4 Tile load, store, move 指令&lt;/h3>
&lt;p>SME tile load, store, move 指令可以：&lt;/p>
&lt;ul>
&lt;li>从内存读取数据，放入 ZA tile 的行或列&lt;/li>
&lt;li>将 ZA tile 的行或列写入内存&lt;/li>
&lt;li>将 ZA tile 的行移动到 SVE Z 向量寄存器&lt;/li>
&lt;li>将 SVE Z 向量寄存器移动到 ZA tile 行或列&lt;/li>
&lt;/ul>
&lt;h4 id="741-tile-slice-load-和-store-指令">7.4.1 Tile slice load 和 store 指令&lt;/h4>
&lt;p>LD1B、LD1H、LD1S、LD1D 和 LD1Q 指令分别将连续内存值加载到具有 8-bit、16-bit、32-bit、64-bit 或 128-bit 元素的 ZA tile slice 中。&lt;/p>
&lt;p>ST1B、ST1H、ST1S、ST1D 和 ST1Q 指令分别将包含 8-bit、16-bit、32-bit、64-bit 或 128-bit 元素的 ZA tile slice 存储到连续内存中。&lt;/p>
&lt;p>这些指令也支持 predication ，例如：&lt;/p>
&lt;pre>&lt;code class="language-armasm">LD1B ZA0H.B[W0, #imm], P0/Z, [X1, X2]
&lt;/code>&lt;/pre>
&lt;p>此 LD1B 指令执行 predicated 的连续 byte 读取，它从地址为(X1+X2)的内存读取数据到 ZA0 中行号为（W0+imm）的这个水平 tile slice 中。目标 tile slice 中 Inactive 的元素被设置为 0。&lt;/p>
&lt;pre>&lt;code class="language-armasm">ST1H ZA1V.H[W0, #imm], P2, [X1, X2, LSL #1]
&lt;/code>&lt;/pre>
&lt;p>此 ST1H 指令执行 predicated 连续 halfword 的存操作，它将 ZA1 中列号为（W0+imm）的垂直 tile slice 存到地址为（X1+X2*2）的内存， tile slice 中 Inactive 的元素不写入内存。&lt;/p>
&lt;h4 id="742-tile-slice-move-指令">7.4.2 Tile slice move 指令&lt;/h4>
&lt;p>MOV 指令（MOVA 指令的别名）将一个 Z 向量寄存器的值移动到一个 ZA tile slice，或将一个 ZA tile slice 中的值移动到一个 Z 向量寄存器。这条指令操作带指定元素大小的 ZA tile 的单个水平或垂直 tile slice。 Slice 的行号/列号由 slice 的检索寄存器加上立即数偏移指定。目标 slice 中 Inactive 的元素保持不变。&lt;/p>
&lt;p>例如：&lt;/p>
&lt;pre>&lt;code class="language-armasm">MOV ZA0H.B[W0, #imm], P0/M, Z0.B
&lt;/code>&lt;/pre>
&lt;p>或&lt;/p>
&lt;pre>&lt;code class="language-armasm">MOVA ZA0H.B[W0, #imm], P0/M, Z0.B
&lt;/code>&lt;/pre>
&lt;p>此指令将向量寄存器 &lt;code>Z0.B&lt;/code> 中的值移动到 &lt;code>ZA0H.B[W0,#imm]&lt;/code> 这个水平 ZA tile slice 中，使用 &lt;code>P0&lt;/code> 作为 predication 寄存器。目标 tile slice 中 Inactive 的元素保持不变。&lt;/p>
&lt;h3 id="75-za-array-vector-loadstore-指令">7.5 ZA array vector load/store 指令&lt;/h3>
&lt;p>SME LDR 指令从内存读取数据到一个 ZA array 向量，SME STR 指令将一个 ZA array 向量中的值存入内存。
这些指令是不带 predication 功能的。它们主要是为了软件的 context switching 时对 ZA storage 进行 save/restore。SME LDR/STR 指令也可以在 Non-streaming SVE 模式下，当 PSTATE.ZA 使能的情况下使用。
例如，下面的 STR 指令的 ZA array 向量是由一个向量选择寄存器 Wv（标量寄存器 W）加上可选的立即数（Wv+Imm）指定。访问内存的地址为：一个标量寄存器作为 base，加上相同的可选立即数偏移乘以当前向量长度 byte 数。&lt;/p>
&lt;pre>&lt;code class="language-armasm">STR ZA[&amp;lt;Wv&amp;gt;, &amp;lt;imm&amp;gt;], [&amp;lt;Xn|SP&amp;gt;{, #&amp;lt;imm&amp;gt;, MUL VL}]
&lt;/code>&lt;/pre>
&lt;h3 id="76-za-tile-清零指令">7.6 ZA tile 清零指令&lt;/h3>
&lt;p>SME ZERO 指令可以清零一组 64-bit ZA tile:&lt;/p>
&lt;pre>&lt;code class="language-armasm">ZERO { &amp;lt;mask&amp;gt;}
&lt;/code>&lt;/pre>
&lt;p>ZERO 指令可以清零多到 8 个名为 &lt;code>ZA0.D&lt;/code> 到 &lt;code>ZA8.D&lt;/code> 的 ZA tile，那些 tile 要清零由指令中的 mask 指定，剩下的其他 tile 保持不变。&lt;/p>
&lt;p>这条指令也可以在 Non-streaming SVE 模式，当 &lt;code>PSTATE.ZA&lt;/code> 开启的情况下使用。&lt;/p>
&lt;p>如果要清零整个 ZA array, 可以使用一个指令别名，&lt;code>ZERO {ZA}&lt;/code> 。&lt;/p>
&lt;h3 id="77-新的-sve2-指令">7.7 新的 SVE2 指令&lt;/h3>
&lt;p>SME 构架扩展加入了一些新的 SVE2 指令，这些指令也可以在 PE 实现了 SVE2, 处于 Non-streaming SVE 模式时使用。这些指令包括：&lt;/p>
&lt;ul>
&lt;li>选择一个 predicate 寄存器或是 all-false 的 Predicate select 指令&lt;/li>
&lt;li>翻转（Reverse）64-bit double word 元素的指令&lt;/li>
&lt;li>有符号/无符号钳位为更小/更大值向量的指令&lt;/li>
&lt;/ul>
&lt;p>下面介绍以下 Predicate select 指令。&lt;/p>
&lt;h4 id="771-psel-指令">7.7.1 PSEL 指令&lt;/h4>
&lt;p>PSEL 指令选择一个 predicate 寄存器或是 all-false 到目标 predicate 寄存器，如下所示：&lt;/p>
&lt;pre>&lt;code class="language-armasm">PSEL &amp;lt;Pd&amp;gt;, &amp;lt;Pn&amp;gt;, &amp;lt;Pm&amp;gt;.&amp;lt;T&amp;gt;[&amp;lt;Wv&amp;gt;, &amp;lt;imm&amp;gt;]
&lt;/code>&lt;/pre>
&lt;p>如果指令中第二源 predicate 寄存器（Pm）中指定的元素为 True, 这条指令将第一源 predicate 寄存器(Pn)的内容放到目标 predicate 寄存器(Pd), 否者设置目标 predicate 寄存器的值全部为 false。
例如以下指令，假设 W12 的值为 0：&lt;/p>
&lt;pre>&lt;code class="language-armasm">PSEL P0, P1, P2.B[W12, #0]
&lt;/code>&lt;/pre>
&lt;p>第二源 predicate 寄存器的[W12+0]即[0]个元素为 False, 因此目标寄存器 P0 被设置为全 0（all-false），如下图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_4401_Picture10_png-1280x960.webp"
alt="SME PSEL" width="auto" loading="lazy">
&lt;/figure>
&lt;p>现在看看如下指令，仍然假设 W12 的值为 0，但这次立即数偏移为 1：&lt;/p>
&lt;pre>&lt;code class="language-armasm">PSEL P0, P1, P2.B[W12, #1]
&lt;/code>&lt;/pre>
&lt;p>第二源 predicate 寄存器的[W12+1]即[1]个元素为 True, 因此选择第一源 predicate 寄存器的值到目标寄存器 P0，如下图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_0116_Picture11_png-1280x960.webp"
alt="SME PSEL" width="auto" loading="lazy">
&lt;/figure>
&lt;h2 id="参考文献">参考文献&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://community.arm.com/arm-community-blogs/b/architectures-and-processors-blog/posts/arm-scalable-matrix-extension-introduction" target="_blank" rel="noopener" >Arm Scalable Matrix Extension (SME) Introduction
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;li>&lt;a class="link" href="https://community.arm.com/arm-community-blogs/b/architectures-and-processors-blog/posts/arm-scalable-matrix-extension-introduction-p2" target="_blank" rel="noopener" >Arm Scalable Matrix Extension (SME) Instructions
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;/ul></description></item><item><title>Arm 性能优化：可伸缩向量扩展 SVE</title><link>https://cuterwrite.top/p/arm-sve-for-performance/</link><pubDate>Sun, 11 Aug 2024 02:13:00 +0000</pubDate><guid>https://cuterwrite.top/p/arm-sve-for-performance/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_117622464_p0_master1200.webp" alt="Featured image of post Arm 性能优化：可伸缩向量扩展 SVE" />&lt;h1 id="arm-性能优化可伸缩向量扩展-sve">Arm 性能优化：可伸缩向量扩展 SVE&lt;/h1>
&lt;h2 id="1-sve-介绍">1. SVE 介绍&lt;/h2>
&lt;p>继固定 128 位向量长度指令集的 Neon 架构扩展之后，Arm 设计了可伸缩向量扩展 (SVE) 作为 AArch64 的下一代 SIMD 扩展。SVE 引入可伸缩概念，允许灵活的向量长度实现，并在 CPU 实现中提供一系列可能的值。向量长度可以从最小 128 位到最大 2048 位不等，以 128 位为增量。&lt;strong>SVE 设计保证相同的应用程序可以在支持 SVE 的不同实现上运行，而无需重新编译代码&lt;/strong>。SVE 提高了该架构对高性能计算 (HPC) 和机器学习 (ML) 应用程序的适用性，这些应用程序需要非常大量的数据处理。SVE2 是 SVE 和 Neon 的超集。SVE2 允许在数据级并行中使用更多功能域。SVE2 继承了 SVE 的概念、向量寄存器和操作原理。SVE 和 SVE2 定义了 32 个可伸缩向量寄存器。芯片合作伙伴可以选择合适的向量长度设计实现，硬件可在 128 位到 2048 位之间（以 128 位为增量）变化。SVE 和 SVE2 的优势在于，只有一个向量指令集使用可伸缩变量。&lt;/p>
&lt;p>SVE 设计理念使开发人员能够编写和构建一次软件，然后在具有各种 SVE 向量长度实现的不同 AArch64 硬件上运行相同的二进制文件。二进制文件的可移植性意味着开发人员不必知道其系统的向量长度实现。消除了重建二进制文件的需求，使软件更容易移植。除了可伸缩向量之外，SVE 和 SVE2 还包括：&lt;/p>
&lt;ul>
&lt;li>per-lane predication&lt;/li>
&lt;li>Gather Load/Scatter Store&lt;/li>
&lt;li>推测性向量化&lt;/li>
&lt;/ul>
&lt;p>这些特性有助于在处理大型数据集时对循环进行向量化和优化。&lt;/p>
&lt;p>SVE2 和 SVE 的主要区别在于指令集的功能覆盖范围。SVE 专为 HPC 和 ML 应用而设计。SVE2 扩展了 SVE 指令集，使其能够加速 HPC 和 ML 以外领域的数据处理。SVE2 指令集还可以加速以下应用中使用的常见算法：&lt;/p>
&lt;ul>
&lt;li>计算机视觉&lt;/li>
&lt;li>多媒体&lt;/li>
&lt;li>LTE 基处理&lt;/li>
&lt;li>基因组学&lt;/li>
&lt;li>内存数据库&lt;/li>
&lt;li>Web 服务&lt;/li>
&lt;li>通用软件&lt;/li>
&lt;/ul>
&lt;p>SVE 和 SVE2 都支持收集和处理大量数据。SVE 和 SVE2 不是 Neon 指令集的扩展。相反，SVE 和 SVE2 经过重新设计，以提供比 Neon 更好的数据并行性。但是，SVE 和 SVE2 的硬件逻辑覆盖了 Neon 硬件的实现。当微架构支持 SVE 或 SVE2 时，它也支持 Neon。要使用 SVE 和 SVE2，在该微架构上运行的软件必须首先支持 Neon。&lt;/p>
&lt;h2 id="2-sve-架构基础">2. SVE 架构基础&lt;/h2>
&lt;p>本节介绍 SVE 和 SVE2 共享的基本架构特性。与 SVE 一样，SVE2 也基于可扩展向量。除了 Neon 提供的现有寄存器库之外，SVE 和 SVE2 还添加了以下寄存器：&lt;/p>
&lt;ul>
&lt;li>32 个可伸缩向量寄存器，&lt;code>Z0-Z31&lt;/code>&lt;/li>
&lt;li>16 个可伸缩 Predicate 寄存器，&lt;code>P0-P15&lt;/code>
&lt;ul>
&lt;li>1 个 首故障 Predicate 寄存器，&lt;code>FFR&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>可伸缩向量系统控制寄存器, &lt;code>ZCR_ELx&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="21-可伸缩向量寄存器">2.1 可伸缩向量寄存器&lt;/h3>
&lt;p>可伸缩向量寄存器 &lt;code>Z0-Z31&lt;/code> 可以在微架构中实现为 128-2048 位。最低的 128 位与 Neon 的固定 128 位向量 &lt;code>V0-V31&lt;/code> 共享。&lt;/p>
&lt;p>下图显示了可伸缩向量寄存器 &lt;code>Z0-Z31&lt;/code>：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_Z-register.webp"
alt="Z 寄存器-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>可伸缩向量寄存器 Z0-Z31&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>可伸缩向量：&lt;/p>
&lt;ul>
&lt;li>可以容纳 64、32、16 和 8 位元素&lt;/li>
&lt;li>支持整数、双精度、单精度和半精度浮点元素&lt;/li>
&lt;li>可以针对每个异常级别（EL）配置向量长度&lt;/li>
&lt;/ul>
&lt;h3 id="22-可伸缩-predicate-寄存器">2.2 可伸缩 Predicate 寄存器&lt;/h3>
&lt;p>为了控制哪些活动元素参与运算，Predicate 寄存器（简称为 P 寄存器）在许多 SVE 指令中用作掩码，这也为向量运算提供了灵活性。下图显示了可伸缩 Predicate 寄存器 &lt;code>P0-P15&lt;/code> ：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_Predicate-register.webp"
alt="P 寄存器-2024-08-12" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>可伸缩 Predicate 寄存器 P0-P15&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>P 寄存器通常用作数据操作的位掩码：&lt;/p>
&lt;ul>
&lt;li>每个 P 寄存器是 Z 寄存器长度的 1/8&lt;/li>
&lt;li>&lt;code>P0-P7&lt;/code> 用于加载、存储和算术运算&lt;/li>
&lt;li>&lt;code>P8-P15&lt;/code> 用于循环管理&lt;/li>
&lt;li>FFR 是一个特殊的 P 寄存器，由 first-fault vector load 指令和 store 指令设置，用于指示每个元素的加载和存储操作的成功情况。FFR 旨在支持推测性内存访问，这使得在许多情况下向量化更容易和更安全。&lt;/li>
&lt;/ul>
&lt;h3 id="23-可伸缩向量系统控制寄存器">2.3 可伸缩向量系统控制寄存器&lt;/h3>
&lt;p>下图展示了可伸缩向量系统控制寄存器 &lt;code>ZCR_ELx&lt;/code> ：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_ZCR_Elx.webp"
alt="ZCR_Elx-2024-08-12" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>可伸缩向量系统控制寄存器 ZCR_Elx&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>可伸缩向量系统控制寄存器指示 SVE 实现特性：&lt;/p>
&lt;ul>
&lt;li>&lt;code>ZCR_Elx.LEN&lt;/code> 字段用于当前和较低异常级别的向量长度。&lt;/li>
&lt;li>大多数位当前保留供将来使用。&lt;/li>
&lt;/ul>
&lt;h3 id="24-sve-汇编语法">2.4 SVE 汇编语法&lt;/h3>
&lt;p>SVE 汇编语法格式由操作码、目标寄存器、P 寄存器（如果指令支持 Predicate 掩码）和输入操作数组成。以下指令示例将详细说明此格式。&lt;/p>
&lt;p>示例 1:&lt;/p>
&lt;pre>&lt;code class="language-armasm">LDFF1D {&amp;lt;Zt&amp;gt;.D}, &amp;lt;Pg&amp;gt;/Z, [&amp;lt;Xn|SP&amp;gt;, &amp;lt;Zm&amp;gt;.D, LSL #3]
&lt;/code>&lt;/pre>
&lt;p>其中：&lt;/p>
&lt;ul>
&lt;li>&lt;code>&amp;lt;Zt&amp;gt;&lt;/code> 是 Z 寄存器, &lt;code>Z0-Z31&lt;/code>&lt;/li>
&lt;li>&lt;code>&amp;lt;Zt&amp;gt;&lt;/code>.D 和 &lt;code>&amp;lt;Zm&amp;gt;.D&lt;/code> 指定目标和操作数向量的元素类型，不需要指定元素的数量。&lt;/li>
&lt;li>&lt;code>&amp;lt;Pg&amp;gt;&lt;/code> 是 P 寄存器, &lt;code>P0-P15&lt;/code>&lt;/li>
&lt;li>&lt;code>&amp;lt;Pg&amp;gt;/Z&lt;/code> 是对 P 寄存器归零。&lt;/li>
&lt;li>&lt;code>&amp;lt;Zm&amp;gt;&lt;/code> 指定 Gather Load 地址模式的偏移量。&lt;/li>
&lt;/ul>
&lt;p>示例 2:&lt;/p>
&lt;pre>&lt;code class="language-armasm">ADD &amp;lt;Zdn&amp;gt;.&amp;lt;T&amp;gt;, &amp;lt;Pg&amp;gt;/M, &amp;lt;Zdn&amp;gt;.&amp;lt;T&amp;gt;, &amp;lt;Zm&amp;gt;.&amp;lt;T&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>其中：&lt;/p>
&lt;ul>
&lt;li>&lt;code>&amp;lt;Pg&amp;gt;/M&lt;/code> 是合并 P 寄存器。&lt;/li>
&lt;li>&lt;code>&amp;lt;Zdn&amp;gt;&lt;/code> 既是目标寄存器，也是输入操作数之一。指令语法在两个位置都显示 &lt;code>&amp;lt;Zdn&amp;gt;&lt;/code> ，是为了方便起见。在汇编编码中，为了简化，它们只被编码一次。&lt;/li>
&lt;/ul>
&lt;p>示例 3:&lt;/p>
&lt;pre>&lt;code class="language-armasm">ORRS &amp;lt;Pd&amp;gt;.B, &amp;lt;Pg&amp;gt;.Z, &amp;lt;Pn&amp;gt;.B, &amp;lt;Pm&amp;gt;.B
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>S&lt;/code> 是 P 寄存器条件标志 &lt;code>NZCV&lt;/code> 的新解释。&lt;/li>
&lt;li>&lt;code>&amp;lt;Pg&amp;gt;&lt;/code> 控制 P 寄存器在示例操作中充当位掩码。&lt;/li>
&lt;/ul>
&lt;h3 id="25-sve-架构特性">2.5 SVE 架构特性&lt;/h3>
&lt;p>SVE 包括以下关键架构特性：&lt;/p>
&lt;ul>
&lt;li>per-lane predication&lt;/li>
&lt;/ul>
&lt;p>为了允许对所选元素进行灵活的操作，SVE 引入了 16 个 P 寄存器， &lt;code>P0-P15&lt;/code> ，用于指示对向量活动通道的有效操作。例如：&lt;/p>
&lt;pre>&lt;code class="language-armasm">ADD Z0.D, P0/M, Z0.D, Z1.D
&lt;/code>&lt;/pre>
&lt;p>活动元素 &lt;code>Z0&lt;/code> 和 &lt;code>Z1&lt;/code> 相加并将结果放入 &lt;code>Z0&lt;/code> 中，&lt;code>P0&lt;/code> 指示操作数的哪些元素是活动的和非活动的。&lt;code>P0&lt;/code> 后面的 &lt;strong>M&lt;/strong> 表示 Merging ，表示将非活动元素合并，因此 &lt;code>Z0&lt;/code> 的非活动元素在 &lt;code>ADD&lt;/code> 操作后将保持其初始值。如果 &lt;code>P0&lt;/code> 后面是 &lt;strong>Z&lt;/strong> ，则非活动元素将被清零，目标寄存器的非活动元素将在操作后归零。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_Per-lane_Predication.webp"
alt="Per-lane_Predication-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Per-lane predication merging&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>如果使用的是 &lt;strong>\Z&lt;/strong> ，则非活动元素将被清零，目标寄存器的非活动元素将在操作后归零。例如&lt;/p>
&lt;pre>&lt;code class="language-armasm">CPY Z0.B, P0/Z, #0xFF
&lt;/code>&lt;/pre>
&lt;p>表示将有符号整数 0xFF 复制到 &lt;code>Z0&lt;/code> 的活动通道中，而非活动通道将被清零。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_Per-lane_Predicate_Zeroing.webp"
alt="Per-lane_Predicate_Zeroing-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Per-lane predication zeroing&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>并非所有指令都具有 Predicate 选项。此外，并非所有 Predicate 操作都同时具有合并和清零选项。您必须参考 &lt;a class="link" href="https://developer.arm.com/documentation/ddi0487/latest/t" target="_blank" rel="noopener" >AArch64 SVE Supplement
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
以了解每个指令的规范细节。&lt;/p>&lt;/div>
&lt;ul>
&lt;li>Gather Load 和 Scatter Store&lt;/li>
&lt;/ul>
&lt;p>SVE 中的寻址模式允许将向量用作 Gather Load 和 Scatter Store 指令中的基地址和偏移量，这使得能够访问非连续的内存位置。例如：&lt;/p>
&lt;pre>&lt;code class="language-armasm">LD1SB Z0.S, P0/Z, [Z1.S] // 将有符号字节从由 32 位向量基地址 Z1 生成的内存地址 Gather Load 到 Z0 的活动 32 位元素中。
LD1SB Z0.D, P0/Z, [X0, Z1.D] // 将有符号字节从由 64 位标量基地址 X0 加上 Z1.D 中的向量索引生成的内存地址 Gather Load 到 Z0 的活动元素中。
&lt;/code>&lt;/pre>
&lt;p>以下示例显示了加载操作 &lt;code>LD1SB Z0.S, P0/Z, [Z1.S]&lt;/code> ，其中 &lt;code>P0&lt;/code> 包含所有真元素，&lt;code>Z1&lt;/code> 包含分散的地址。加载后，&lt;code>Z0.S&lt;/code> 的每个元素的低位字节将用从分散内存位置获取的数据更新。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_gather-load_and_scatter_store_example.webp"
alt="gather-load_and_scatter_store_example-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Gather-load 与 Scatter-store 示例&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>P 寄存器驱动的循环控制和管理&lt;/li>
&lt;/ul>
&lt;p>作为 SVE 的一项关键特性，P 寄存器不仅可以灵活地控制向量运算的各个元素，还可以实现 P 寄存器驱动的循环控制。P 寄存器驱动的循环控制和管理使循环控制高效且灵活。此功能通过在 P 寄存器中注册活动和非活动元素索引，消除了处理部分向量的额外循环头和尾的开销。P 寄存器驱动的循环控制和管理意味着，在接下来的循环迭代中，只有活动元素才会执行预期的操作。例如：&lt;/p>
&lt;pre>&lt;code class="language-armasm">WHILEL0 P0.S, x8, x9 // 在 P0 中生成一个谓词，从最低编号的元素开始，当第一个无符号标量操作数 X8 的递增值小于第二个标量操作数 X9 时为真，之后为假，直到最高编号的元素。
B.FIRST Loop_start // B.FIRST（等效于 B.MI）或 B.NFRST（等效于 B.PL）通常用于根据上述指令测试结果进行分支，判断 P0 的第一个元素是真还是假，作为循环的结束或继续条件。
&lt;/code>&lt;/pre>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_Predicate-driver_loop_control_and_management_example.webp"
alt="Predicate-driver_loop_control_and_management_example-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>P 寄存器驱动的循环控制和管理示例&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>用于软件管理推测的向量分区&lt;/li>
&lt;/ul>
&lt;p>推测性加载可能会给传统向量的内存读取带来挑战，&lt;strong>如果在读取过程中某些元素发生错误，则难以逆转加载操作并跟踪哪些元素加载失败&lt;/strong>。Neon 不允许推测性加载。为了允许对向量进行推测性加载（例如 LDRFF），SVE 引入了 first-fault vector load 指令。为了允许向量访问跨越无效页面，SVE 还引入了 FFR 寄存器。&lt;strong>使用 first-fault vector load 指令加载到 SVE 向量时，FFR 寄存器会更新每个元素的加载成功或失败结果&lt;/strong>。当发生加载错误时，FFR 会立即注册相应的元素，将其余元素注册为 0 或 false，并且不会触发异常。通常，RDFFR 指令用于读取 FFR 状态。当第一个元素为假时，RDFFR 指令结束迭代。如果第一个元素为真，RDFFR 指令继续迭代。FFR 的长度与 P 向量相同。可以使用 SETFFR 指令初始化该值。以下示例使用 LDFF1D 从内存中读取数据，FFR 会相应地更新：&lt;/p>
&lt;pre>&lt;code class="language-armasm">LDFF1D Z0.D, P0/Z, [Z1.D, #0] // 使用首个故障行为将双字从由向量基地址 Z1 加 0 生成的内存地址收集加载到 Z0 的活动元素中。非活动元素不会读取设备内存或发出故障信号，并在目标向量中设置为零。从有效内存成功加载将 FFR 中的对应元素设置为真。首个故障加载将 FFR 中的对应元素和其余元素设置为假或 0。
&lt;/code>&lt;/pre>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_Vector-partioning-for-software-managed-speculation-example.webp"
alt="Vector-partioning-for-software-managed-speculation-example-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>用于软件管理推测的向量分区示例&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>扩展的浮点和水平规约&lt;/li>
&lt;/ul>
&lt;p>为了允许在向量中进行高效的归约操作，并满足对精度的不同要求，SVE 增强了浮点和水平归约操作。这些指令可能具有顺序（从低到高）或基于树（成对）的浮点归约顺序，其中操作顺序可能会导致不同的舍入结果。这些操作需要在可重复性和性能之间进行权衡。例如：&lt;/p>
&lt;pre>&lt;code class="language-armasm">FADDA D0, P0/M, D1, Z2.D // 从源头向量的低位到高位元素进行浮点加严格顺序归约，将结果累积到 SIMD&amp;amp;FP 标量寄存器中。该示例指令将 D1 与 Z2.D 的所有活动元素相加，并将结果存储到标量寄存器 D0 中。向量元素按从低到高的顺序严格处理，标量源 D1 提供初始值。源向量中的非活动元素将被忽略。而 FADDV 将执行递归成对归约，并将结果存储到标量寄存器中。
&lt;/code>&lt;/pre>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_Extended_Floating-poing-and-horizontal-reductions-example.webp"
alt="Extended_Floating-poing-and-horizontal-reductions-example-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>扩展的浮点和水平规约示例&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h2 id="3-sve2-新增特性">3. SVE2 新增特性&lt;/h2>
&lt;p>本节介绍 SVE2 为 Arm AArch64 架构新增的特性。为了实现可伸缩的性能，SVE2 基于 SVE 构建，允许向量实现高达 2048 位。&lt;/p>
&lt;p>在 SVE2 中，添加了许多复制 Neon 中现有指令的指令，包括：&lt;/p>
&lt;ul>
&lt;li>转换后的 Neon 整数运算，例如，带符号绝对差累加 (SAB) 和带符号减半加法 (SHADD)。&lt;/li>
&lt;li>转换后的 Neon 扩展、缩小和成对运算，例如，无符号长加法 - 底部 (UADDLB) 和无符号长加法 - 顶部 (UADDLT)。&lt;/li>
&lt;/ul>
&lt;p>元素处理顺序发生了变化。SVE2 对交错的偶数和奇数元素进行处理，而 Neon 对窄或宽操作的低半部分和高半部分元素进行处理。下图说明了 Neon 和 SVE2 处理之间的区别：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_transformed_neon_widen_narraow_pairwise_operations.webp"
alt="transformed_neon_widen_narraow_pairwise_operations-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>转换后的 Neon 窄或宽操作对比&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>复数操作，例如带旋转的复整数乘加 (CMLA)。&lt;/li>
&lt;li>多精度运算，用于大整数运算和密码学，例如，带进位长加法 - 底部 (ADCLB)、带进位长加法 - 顶部 (ADCLT) 以及 SM4 加密和解密 (SM4E)。&lt;/li>
&lt;/ul>
&lt;p>为了向后兼容，最新架构中需要 Neon 和 VFP。虽然 SVE2 包含 SVE 和 Neon 的一些功能，但 SVE2 并不排除 Neon 在芯片上的存在。&lt;/p>
&lt;p>SVE2 支持针对 HPC 市场以外的新兴应用进行优化，例如，在机器学习 (ML)（UDOT 指令）、计算机视觉（TBL 和 TBX 指令）、基带网络（CADD 和 CMLA 指令）、基因组学（BDEP 和 BEXT 指令）和服务器（MATCH 和 NMATCH 指令）中。&lt;/p>
&lt;p>SVE2 增强了通用处理器大量数据操作的整体性能，而无需其他片外加速器。&lt;/p>
&lt;h2 id="4-使用-sve-编程">4. 使用 SVE 编程&lt;/h2>
&lt;p>本节介绍支持 SVE2 应用程序开发的软件工具和库。本节还介绍了如何为支持 SVE2 的目标开发应用程序，在支持 SVE2 的硬件上运行该应用程序，以及在任何 Armv8-A 硬件上模拟该应用程序。&lt;/p>
&lt;h3 id="41-软件和库支持">4.1 软件和库支持&lt;/h3>
&lt;p>要构建 SVE 或 SVE2 应用程序，你必须选择支持 SVE 和 SVE2 功能的编译器。&lt;/p>
&lt;ul>
&lt;li>GNU 工具 8.0+ 版本支持 SVE。&lt;/li>
&lt;li>&lt;a class="link" href="https://developer.arm.com/tools-and-software/server-and-hpc/compile/arm-compiler-for-linux" target="_blank" rel="noopener" >Arm Compiler for Linux
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
18.0+ 版本支持 SVE，20.0+ 版本支持 SVE 和 SVE2。&lt;/li>
&lt;li>GNU 和 Arm Compiler for Linux 编译器都支持优化 C/C++/Fortran 代码。&lt;/li>
&lt;li>LLVM（开源 Clang）5 及更高版本包括对 SVE 的支持，9 及更高版本包括对 SVE2 的支持。要了解 LLVM 工具的每个版本支持哪些 SVE 或 SVE2 功能，请参阅 &lt;a class="link" href="https://developer.arm.com/tools-and-software/open-source-software/developer-tools/llvm-toolchain/sve-support" target="_blank" rel="noopener" >LLVM 工具链 SVE 支持页面
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/li>
&lt;/ul>
&lt;p>&lt;a class="link" href="https://developer.arm.com/Tools%20and%20Software/Arm%20Performance%20Libraries" target="_blank" rel="noopener" >Arm Performance Libraries
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
针对数学例程进行了高度优化，可以链接到你的应用程序。Arm Performance Libraries 19.3+ 版本支持 SVE 的数学库。&lt;/p>
&lt;p>Arm Compiler for Linux 是 Arm Allinea Studio 的一部分，包含 Arm C/C++ 编译器、Arm Fortran 编译器和 Arm Performance Libraries。&lt;/p>
&lt;h3 id="42-如何使用-sve2-编程">4.2 如何使用 SVE2 编程&lt;/h3>
&lt;p>编写或生成 SVE 和 SVE2 代码的方法有多种。在本小节中，我们将探讨其中的一些方法。&lt;/p>
&lt;p>要编写或生成 SVE 和 SVE2 代码，你可以：&lt;/p>
&lt;ul>
&lt;li>编写 SVE 汇编代码&lt;/li>
&lt;li>使用 SVE 内部函数编程&lt;/li>
&lt;li>自动向量化&lt;/li>
&lt;li>使用 SVE 优化库&lt;/li>
&lt;/ul>
&lt;p>让我们更详细地了解这四种选择。&lt;/p>
&lt;h4 id="421-编写-sve-汇编代码">4.2.1 编写 SVE 汇编代码&lt;/h4>
&lt;p>你可以将 SVE 指令作为内联汇编编写到 C/C++ 代码中，或者作为完整的函数编写到汇编源代码中。例如：&lt;/p>
&lt;pre>&lt;code class="language-armasm"> .globl subtract_arrays // -- Begin function
.p2align 2
.type subtract_arrays, @function
subtract_arrays: // @subtract_arrays
.cfi_startproc
// %bb.0:
orr w9, wzr, #0x400
mov x8, xzr
whilelo p0.s, xzr, x9
.LBB0_1: // =&amp;gt;This Inner Loop Header: Depth=1
ld1w { z0.s }, p0/z, [x1, x8, lsl #2]
ld1w { z1.s }, p0/z, [x2, x8, lsl #2]
sub z0.s, z0.s, z1.s
st1w { z0.s }, p0, [x0, x8, lsl #2]
incw x8
whilelo p0.s, x8, x9
b.mi .LBB0_1
// %bb.2:
ret
.Lfunc_end0:
.size subtract_arrays, .Lfunc_end0-subtract_arrays
.cfi_endproc
&lt;/code>&lt;/pre>
&lt;p>如果你混合使用高级语言和汇编语言编写的函数，则必须熟悉针对 SVE 更新的&lt;a class="link" href="https://developer.arm.com/documentation/ihi0036/latest/" target="_blank" rel="noopener" >应用程序二进制接口 (ABI)
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
标准。&lt;a class="link" href="https://developer.arm.com/documentation/ihi0055/latest" target="_blank" rel="noopener" >Arm 架构过程调用标准 (AAPCS)
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
指定了数据类型和寄存器分配，并且与汇编编程最相关。AAPCS 要求：&lt;/p>
&lt;ul>
&lt;li>&lt;code>Z0-Z7&lt;/code> 和 &lt;code>P0-P3 &lt;/code>用于传递可伸缩向量参数和结果。&lt;/li>
&lt;li>&lt;code>Z8-Z15&lt;/code> 和 &lt;code>P4-P15&lt;/code> 是被调用者保存的。&lt;/li>
&lt;li>所有其他向量寄存器（&lt;code>Z16-Z31&lt;/code>）都可能被被调用函数破坏，调用函数负责在需要时备份和恢复它们。&lt;/li>
&lt;/ul>
&lt;h4 id="422-使用-sve-instruction-函数intrinsics">4.2.2 使用 SVE instruction 函数（Intrinsics）&lt;/h4>
&lt;p>SVE 内部函数是由编译器支持的函数，可以替换为相应的指令。程序员可以直接在 C 和 C++ 等高级语言中调用指令函数。SVE 的 ACLE（Arm C 语言扩展）定义了哪些 SVE 指令函数可用、它们的参数以及它们的功能。支持 ACLE 的编译器可以在编译期间将内部函数替换为映射的 SVE 指令。要使用 ACLE 内部函数，你必须包含头文件 &lt;code>arm_sve.h&lt;/code>，其中包含可在 C/C++ 中使用的向量类型和指令函数（针对 SVE）列表。每种数据类型都描述了向量中元素的大小和数据类型：&lt;/p>
&lt;ul>
&lt;li>&lt;code>svint8_t svuint8_t&lt;/code>&lt;/li>
&lt;li>&lt;code>svint16_t svuint16_t svfloat16_t&lt;/code>&lt;/li>
&lt;li>&lt;code>svint32_t svuint32_t svfloat32_t&lt;/code>&lt;/li>
&lt;li>&lt;code>svint64_t svuint64_t svfloat64_t&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>例如，&lt;code>svint64_t&lt;/code> 表示 64 位有符号整数向量，&lt;code>svfloat16_t&lt;/code> 表示半精度浮点数向量。&lt;/p>
&lt;p>以下示例 C 代码已使用 SVE 内部函数进行了手动优化：&lt;/p>
&lt;pre>&lt;code class="language-c">// intrinsic_example.c
#include &amp;lt;arm_sve.h&amp;gt;
svuint64_t uaddlb_array(svuint32_t Zs1, svuint32_t Zs2)
{
// widening add of even elements
svuint64_t result = svaddlb(Zs1, Zs2);
return result;
}
&lt;/code>&lt;/pre>
&lt;p>包含 &lt;code>arm_sve.h&lt;/code> 头文件的源代码可以使用 SVE 向量类型，就像数据类型可以用于变量声明和函数参数一样。要使用 Arm C/C++ 编译器编译代码并以支持 SVE 的 Armv8-A 架构为目标，请使用：&lt;/p>
&lt;pre>&lt;code class="language-bash">armclang -O3 -S -march=armv8-a+sve2 -o intrinsic_example.s intrinsic_example.c
&lt;/code>&lt;/pre>
&lt;p>此命令生成以下汇编代码：&lt;/p>
&lt;pre>&lt;code class="language-armasm">// instrinsic_example.s
uaddlb_array: // @uaddlb_array
.cfi_startproc
// %bb.0:
uaddlb z0.d, z0.s, z1.s
ret
&lt;/code>&lt;/pre>
&lt;h4 id="423-自动向量化">4.2.3 自动向量化&lt;/h4>
&lt;p>C/C++/Fortran 编译器（例如，适用于 Arm 平台的原生 &lt;a class="link" href="https://developer.arm.com/tools-and-software/server-and-hpc/compile/arm-compiler-for-linux" target="_blank" rel="noopener" >Arm Compiler for Linux
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
和 GNU 编译器）支持使用 SVE 或 SVE2 指令对 C、C++ 和 Fortran 循环进行向量化。要生成 SVE 或 SVE2 代码，请选择适当的编译器选项。例如，使用 armclang 启用 SVE2 优化的一个选项是 &lt;code>-march=armv8-a+sve2&lt;/code> 。如果要使用 SVE 版本的库，请将 &lt;code>-march=armv8-a+sve2&lt;/code> 与 &lt;code>-armpl=sve&lt;/code> 结合使用。&lt;/p>
&lt;h4 id="424-使用-svesve2-优化库">4.2.4 使用 SVE/SVE2 优化库&lt;/h4>
&lt;p>使用针对 SVE/SVE2 高度优化的库，例如 &lt;a class="link" href="https://developer.arm.com/Tools%20and%20Software/Arm%20Performance%20Libraries" target="_blank" rel="noopener" >Arm Performance Libraries
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
和 Arm Compute Libraries。Arm Performance Libraries 包含针对 BLAS、LAPACK、FFT、稀疏线性代数和 libamath 优化的数学函数的高度优化实现。要能够链接任何 Arm Performance Libraries 函数，您必须安装 Arm Allinea Studio 并在代码中包含 armpl.h。要使用 Arm Compiler for Linux 和 Arm Performance Libraries 构建应用程序，您必须在命令行中指定 &lt;code>-armpl=&amp;lt;arg&amp;gt;&lt;/code> 。如果您使用 GNU 工具，则必须使用 &lt;code>-L&amp;lt;armpl_install_dir&amp;gt;/lib&lt;/code> 将 Arm Performance Libraries 安装路径包含在链接器命令行中，并指定与 Arm Compiler for Linux &lt;code>-armpl=&amp;lt;arg&amp;gt;&lt;/code> 选项等效的 GNU 选项，即 &lt;code>-larmpl_lp64&lt;/code> 。有关更多信息，请参阅 Arm Performance Libraries 入门指南。&lt;/p>
&lt;h3 id="43-如何运行-svesve2-程序">4.3 如何运行 SVE/SVE2 程序&lt;/h3>
&lt;p>如果您无法访问 SVE 硬件，则可以使用模型或仿真器来运行代码。你可以选择以下几种模型和仿真器：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>QEMU&lt;/strong>： 交叉编译和原生模型，支持在具有 SVE 的 Arm AArch64 平台上进行建模。&lt;/li>
&lt;li>&lt;strong>Fast Models&lt;/strong>： 跨平台模型，支持在基于 x86 的主机上运行的具有 SVE 的 Arm AArch64 平台进行建模。支持 SVE2 的 架构包络模型 AEM 只对主要合作伙伴可用。&lt;/li>
&lt;li>&lt;strong>Arm Instruction Emulator (ArmIE)&lt;/strong>： 直接在 Arm 平台上运行。支持 SVE，并从 19.2+ 版本开始支持 SVE2。&lt;/li>
&lt;/ul>
&lt;h2 id="5-acle-intrinsics">5. ACLE Intrinsics&lt;/h2>
&lt;h3 id="51-acle-简介">5.1 ACLE 简介&lt;/h3>
&lt;p>ACLE (Arm C 语言扩展) 是在 C 和 C++ 代码中利用内部函数和其他特性来支持 Arm 的功能。&lt;/p>
&lt;ul>
&lt;li>ACLE (ARM C 语言扩展) 通过特定于 Arm 的特性扩展了 C/C++ 语言。
&lt;ul>
&lt;li>预定义宏：&lt;code>__ARM_ARCH_ISA_A64&lt;/code> 、 &lt;code>__ARM_BIG_ENDIAN&lt;/code> 等。&lt;/li>
&lt;li>内部函数：&lt;code>__clz(uint32_t x)&lt;/code> 、 &lt;code>__cls(uint32_t x)&lt;/code> 等。&lt;/li>
&lt;li>数据类型：SVE、NEON 和 FP16 数据类型。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>用于 SVE 的 ACLE 支持使用 ACLE 进行可变长度向量 (VLA) 编程。
&lt;ul>
&lt;li>几乎每个 SVE 指令都有一个对应的内部函数。&lt;/li>
&lt;li>数据类型用于表示 SVE 内部函数所使用的无大小向量。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>适用于以下用户的场景：
&lt;ul>
&lt;li>希望手动调整 SVE 代码的用户。&lt;/li>
&lt;li>希望适配或手动优化应用程序和库的用户。&lt;/li>
&lt;li>需要对 Arm 目标进行底层访问的用户。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="52-如何使用-acle">5.2 如何使用 ACLE&lt;/h3>
&lt;ul>
&lt;li>引入头文件
&lt;ul>
&lt;li>&lt;code>arm_acle.h&lt;/code> ：核心 ACLE&lt;/li>
&lt;li>&lt;code>arm_fp16.h&lt;/code> ：添加 FP16 数据类型。
&lt;ul>
&lt;li>目标平台需支持 FP16，即 &lt;code>march=armv8-a+fp16&lt;/code>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>arm_neon.h&lt;/code> ：添加 NEON Intrinsics 和数据类型。
&lt;ul>
&lt;li>目标平台需支持 NEON，即 &lt;code>march=armv8-a+simd&lt;/code>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>arm_sve.h&lt;/code> ：添加 SVE Intrinsics 和数据类型。
&lt;ul>
&lt;li>目标平台需支持 SVE，即 &lt;code>march=armv8-a+sve&lt;/code>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="53-sve-acle">5.3 SVE ACLE&lt;/h3>
&lt;ul>
&lt;li>首先需要做的是引入头文件&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-c">#include &amp;lt;arm_sve.h&amp;gt;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>VLA 数据类型
&lt;ul>
&lt;li>&lt;code>svfloat64_t&lt;/code>, &lt;code>svfloat16_t&lt;/code>, &lt;code>svuint32_t&lt;/code> 等。&lt;/li>
&lt;li>命名规则：&lt;code>sv&amp;lt;datatype&amp;gt;&amp;lt;datasize&amp;gt;_t&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Predication
&lt;ul>
&lt;li>合并：&lt;code>_m&lt;/code>&lt;/li>
&lt;li>置零：&lt;code>_z&lt;/code>&lt;/li>
&lt;li>不确定：&lt;code>_x&lt;/code>&lt;/li>
&lt;li>P 寄存器的数据类型：&lt;code>svbool_t&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>使用泛型做函数重载，比如函数 &lt;code>svadd&lt;/code> 会根据参数类型自动选择对应的函数。&lt;/li>
&lt;li>函数命名规则：&lt;code>svbase[disambiguator][type0][type1]...[predication]&lt;/code>
&lt;ul>
&lt;li>base 指的是基本操作，比如 &lt;code>add&lt;/code>、&lt;code>mul&lt;/code>、&lt;code>sub&lt;/code> 等。&lt;/li>
&lt;li>disambiguator 用于区分相同基本操作的不同变体。&lt;/li>
&lt;li>typeN 指定了向量和 P 寄存器的类型。&lt;/li>
&lt;li>predication 指定了非活动元素的处理方式。&lt;/li>
&lt;li>例如： &lt;code>svfloat64_t svld1_f64&lt;/code>, &lt;code>svbool_t svwhilelt_b8&lt;/code>, &lt;code>svuint32_t svmla_u32_z&lt;/code>, &lt;code>svuint32_t svmla_u32_m&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="54-sve-常用-intrinsics">5.4 SVE 常用 Intrinsics&lt;/h3>
&lt;ul>
&lt;li>Predicate
&lt;ul>
&lt;li>Predicate 是一个 bool 类型的向量，用于控制计算过程中向量中对应位置是否参与运算&lt;/li>
&lt;li>&lt;code>svbool_t pg = svwhilelt_b32(i, num)&lt;/code> 产生 (i, i + 1, i + 2, &amp;hellip;, i + vl - 1) &amp;lt; num 的 predicate&lt;/li>
&lt;li>&lt;code>svbool_t pg = svptrue_b32()&lt;/code> 产生一个全为 true 的 predicate&lt;/li>
&lt;li>其中，b32 对应处理 32 位数据（int/float），此外还有 b8, b16, b64 对应的 intrinsic&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>内存数据存取
&lt;ul>
&lt;li>&lt;code>svld1(pg, *base)&lt;/code>： 从地址 base 中加载连续向量。&lt;/li>
&lt;li>&lt;code>svst1(pg, *base, vec)&lt;/code>： 将向量 vec 存储到地址 base 中。&lt;/li>
&lt;li>&lt;code>svld1_gather_index(pg, *base, vec_index)&lt;/code>： 从地址 base 中加载向量索引对应的数据。&lt;/li>
&lt;li>&lt;code>svst1_scatter_index(pg, *base, vec_index, vec)&lt;/code>： 将向量 vec 中数据存储到向量索引对应的位置。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>基础计算
&lt;ul>
&lt;li>&lt;code>svadd_z(pg, sv_vec1, sv_vec2)&lt;/code>&lt;/li>
&lt;li>&lt;code>svadd_m(pg, sv_vec1, sv_vec2)&lt;/code>&lt;/li>
&lt;li>&lt;code>svadd_x(pg, sv_vec1, sv_vec2)&lt;/code>&lt;/li>
&lt;li>&lt;code>svadd_x(pg, sv_vec1, x)&lt;/code>&lt;/li>
&lt;li>其中，&lt;code>_z&lt;/code> 表示将 pg 为 false 的位置置零，&lt;code>_m&lt;/code> 表示保留原值，&lt;code>_x&lt;/code> 表示不确定（什么值都有可能）。&lt;/li>
&lt;li>第二个操作数可以为标量数据。&lt;/li>
&lt;li>&lt;code>svmul&lt;/code>, &lt;code>svsub&lt;/code>, &lt;code>svsubr&lt;/code>, &lt;code>svdiv&lt;/code>, &lt;code>svdivr&lt;/code>：其中，&lt;code>svsubr&lt;/code> 相比 &lt;code>svsub&lt;/code> 交换了减数与被减数的位置。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>其它
&lt;ul>
&lt;li>&lt;code>svdup_f64(double x)&lt;/code>： 生成一个所有元素都为 x 的向量。&lt;/li>
&lt;li>&lt;code>svcntd()&lt;/code>：返回 64-bit 数据的向量长度：&lt;code>svcntb&lt;/code> 对应 8 位， &lt;code>svcnth&lt;/code> 对应 16 位，&lt;code>svcntw&lt;/code> 对应 32 位。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="55-sve-结构体-intrinsics">5.5 SVE 结构体 Intrinsics&lt;/h3>
&lt;p>对应结构体数据，SVE 提供了一些特殊的 Intrinsics，比如：&lt;code>svld3&lt;/code>, &lt;code>svget3&lt;/code>, &lt;code>svset3&lt;/code>, &lt;code>svst3&lt;/code> 等。这些 Intrinsics 用于处理结构体数据。&lt;/p>
&lt;p>例如，对于粒子结构体：&lt;/p>
&lt;pre>&lt;code class="language-c">typedef struct {
float x;
float y;
float z;
} Particle;
&lt;/code>&lt;/pre>
&lt;p>可以使用 &lt;code>svld3&lt;/code> 加载结构体中全部的数据为 3 个向量的组，然后使用 &lt;code>svget3&lt;/code> 从 3 个向量的组中提取一个向量, index 的值为 0, 1, 2 分别对应 x, y, z。&lt;/p>
&lt;pre>&lt;code class="language-c">Particle *ps;
float factor = 2.2;
// 初始化部分省略
for (int i = 0; i &amp;lt; num; i += svcntw()) {
svbool_t pg = svwhilelt_b32(i, num);
svfloat32x3_t sv_ps = svld3(pg, (float32_t *)&amp;amp;ps[i]);
svfloat32_t sv_ps_x = svget3(sv_ps, 0);
svfloat32_t sv_ps_y = svget3(sv_ps, 1);
// 执行计算
sv_ps_x = svmul_x(pg, sv_ps_x, factor);
sv_ps_y = svmul_x(pg, sv_ps_y, factor);
//保存结果
sv_ps = svset3(sv_ps, 0, sv_ps_x);
sv_ps = svset3(sv_ps, 1, sv_ps_y);
svst3(pg, (float32_t *)&amp;amp;ps[i], sv_ps);
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>svld3(pg, *base)&lt;/code>： 加载结构体中全部的数据为 3 个向量的组；其中，base 是 3 个元素结构体数组的地址。&lt;/li>
&lt;li>&lt;code>svget3(tuple, index)&lt;/code>： 从 3 个向量的组中提取一个向量；index 的值为 0、1 或 2。&lt;/li>
&lt;li>&lt;code>svset3(tuple, index, vec)&lt;/code>： 设置 3 个向量的组中的一个向量；index 的值为 0、1 或 2。&lt;/li>
&lt;li>&lt;code>svst3(pg, *base, vec)&lt;/code>： 将 3 个向量的组存储到结构体中；其中，base 是 3 个元素结构体数组的地址。&lt;/li>
&lt;/ul>
&lt;h3 id="56-sve-条件选择">5.6 SVE 条件选择&lt;/h3>
&lt;p>SVE 中提供了 &lt;code>svcmplt&lt;/code>、&lt;code>svcompact&lt;/code>、&lt;code>svcntp_b32&lt;/code> 等方法，可以根据条件选择保留向量中的元素。&lt;/p>
&lt;p>例如，对于无向量化的代码：&lt;/p>
&lt;pre>&lt;code class="language-c">for (int i = 0; i &amp;lt; num; i++) {
float tmp = provided[i];
if (tmp &amp;lt; mark) {
selected[count++] = tmp;
if (count &amp;gt;= maxSize) {
break;
}
}
}
&lt;/code>&lt;/pre>
&lt;p>该代码的作用是从 provided 数组中选择小于 mark 的元素，存储到 selected 数组中，直到 selected 数组满。&lt;/p>
&lt;p>用 SVE Intrinsic 改写：&lt;/p>
&lt;pre>&lt;code class="language-c">for (int i = 0; i &amp;lt; num; i += svcntw()) {
svbool_t pg = svwhilelt_b32(i, num);
svfloat32_t sv_tmp = svld1(pg, &amp;amp;provided[i]);
svbool_t pg_sel = svcmplt(pg, sv_tmp, mark);
sv_tmp = svcompact(pg_sel, sv_tmp);
svst1(pg, &amp;amp;selected[count], sv_tmp);
count += svcntp_b32(pg, pg_sel);
if (count &amp;gt;= maxSize) {
break;
}
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>svcmplt(pg, vec1, vec2)&lt;/code> ：比较两个向量的大小，返回一个 predicate，表示 vec1 中小于 vec2 的位置。&lt;/li>
&lt;li>&lt;code>svcompact(pg, sv_tmp)&lt;/code> ：压缩向量，将 pg 为 active 的数据按序移动到向量低位，其余位置置零。&lt;/li>
&lt;li>&lt;code>svcntp_b32(pg, pg2)&lt;/code> ：返回 pg2 中 active 的元素个数&lt;/li>
&lt;li>这段代码先将 provided 数组中的数据加载到 sv_tmp 中，然后使用 &lt;code>svcmplt&lt;/code> 生成一个 predicate，表示小于 mark 的位置。接着使用 &lt;code>svcompact&lt;/code> 压缩 sv_tmp，得到小于 mark 的数据，再通过 &lt;code>svst1&lt;/code> 存储到 selected 数组中。最后，使用 &lt;code>svcntp_b32&lt;/code> 统计 active 的元素个数，更新 count。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_compact.webp"
alt="compact-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>svcompact 示意图（256-bit 向量）&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>由于进行了 compact 操作，所以 selected 数组从 count 位置连续存储新的小于 mark 的数据，剩下的位置被置零。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_svst1.webp"
alt="svst1-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>svst1 示意图（256-bit 向量）&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="57-sve-向量化循环交织">5.7 SVE 向量化循环交织&lt;/h3>
&lt;p>SVE Intrinsic 实现的向量化循环交织，相比编译器自动向量化能大大减少读取向量的次数。&lt;/p>
&lt;p>例如，对于无向量化的代码：&lt;/p>
&lt;pre>&lt;code class="language-c">for (int j = offset; j &amp;lt; outerLen - offset; j++) {
int m2index = (j - offset) * innerLen;
int m1index = m2index + innerLen;
int m0index = m1index + innerLen;
int p1index = m0index + innerLen;
int p2index = p1index + innerLen;
for (int i = 0; i &amp;lt; innerLen; i++) {
res[m0index + i] = m2factor * field[m2index + i] +
m1factor * field[m1index + i] +
m0factor * field[m0index + i] +
p1factor * field[p1index + i] +
p2factor * field[p2index + i];
}
}
&lt;/code>&lt;/pre>
&lt;p>编译器对该代码进行自动向量化后，每次迭代需读取五次不同向量的数据，效率较低。&lt;/p>
&lt;p>用 SVE Intrinsic 改写：&lt;/p>
&lt;pre>&lt;code class="language-c">for (int i = 0; i &amp;lt; innerLen; i += svcntd()) {
svbool_t pg = svwhilelt_b32(i, innerLen);
int dataIndex = i;
svfloat64_t jm2Field = svld1(pg, &amp;amp;field[dataIndex]);
dataIndex += innerLen;
svfloat64_t jm1Field = svld1(pg, &amp;amp;field[dataIndex]);
dataIndex += innerLen;
svfloat64_t jm0Field = svld1(pg, &amp;amp;field[dataIndex]);
dataIndex += innerLen;
svfloat64_t jp1Field = svld1(pg, &amp;amp;field[dataIndex]);
for (int j = offset; j &amp;lt; outerLen - offset; j += 1) {
svfloat64_t jp2Field = svld1(pg, &amp;amp;field[(j + offset) * innerLen + i]);
svfloat64_t svRes = svmul_x(pg, jm2Field, m2factor);
svRes = svmad_x(pg, jm1Field, m1factor, svRes);
svRes = svmad_x(pg, jm0Field, m0factor, svRes);
svRes = svmad_x(pg, jp1Field, p1factor, svRes);
svRes = svmad_x(pg, jp2Field, p2factor, svRes);
svst1(pg, &amp;amp;res[j * innerLen + 1], svRes);
jm2Field = jm1Field;
jm1Field = jm0Field;
jm0Field = jp1Field;
jp1Field = jp2Field;
}
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>svmad_x(pg, vec1, vec2, vec3)&lt;/code> ：计算 vec1 * vec2 + vec3，返回一个向量。&lt;/li>
&lt;li>这段代码每次迭代只需读取一个向量，大大减少向量读取的次数。&lt;/li>
&lt;/ul>
&lt;h2 id="参考文献">参考文献&lt;/h2>
&lt;ol>
&lt;li>&lt;a class="link" href="https://developer.arm.com/-/media/Arm%20Developer%20Community/PDF/102340_0001_02_en_introduction-to-sve2.pdf?revision=b208e56b-6569-4ae2-b6f3-cd7d5d1ecac3" target="_blank" rel="noopener" >Introduction to SVE2
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;li>&lt;a class="link" href="https://www.stonybrook.edu/commcms/ookami/support/_docs/5%20-%20Advanced%20SVE.pdf" target="_blank" rel="noopener" >SVE Deep Dive
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;li>&lt;a class="link" href="https://arm-software.github.io/acle/main/acle.html" target="_blank" rel="noopener" >Arm C Language Extensions
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;/ol></description></item><item><title>LLM 生态介绍：从模型微调到应用落地</title><link>https://cuterwrite.top/p/llm-ecosystem/</link><pubDate>Fri, 05 Jul 2024 22:46:00 +0000</pubDate><guid>https://cuterwrite.top/p/llm-ecosystem/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_119269220_p0_master1200.webp" alt="Featured image of post LLM 生态介绍：从模型微调到应用落地" />&lt;h1 id="llm-生态介绍从模型微调到应用落地">LLM 生态介绍：从模型微调到应用落地&lt;/h1>
&lt;h2 id="模型微调">模型微调&lt;/h2>
&lt;p>预训练的 LLM 通常具备广泛的知识，但要使其在特定任务上表现出色，微调是必不可少的。以下是一些常用的 LLM 微调工具：&lt;/p>
&lt;h3 id="axolotl">Axolotl&lt;/h3>
&lt;a href="https://github.com/OpenAccess-AI-Collective/axolotl" target="_blank" class="card-github fetch-waiting no-styling"
repo="OpenAccess-AI-Collective/axolotl" id="repo-SMx3z8qZDXTcA2qj-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-SMx3z8qZDXTcA2qj-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">OpenAccess-AI-Collective&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">axolotl&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-SMx3z8qZDXTcA2qj-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-SMx3z8qZDXTcA2qj-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-SMx3z8qZDXTcA2qj-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-SMx3z8qZDXTcA2qj-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-SMx3z8qZDXTcA2qj-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-SMx3z8qZDXTcA2qj-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/OpenAccess-AI-Collective\/axolotl', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-SMx3z8qZDXTcA2qj-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-SMx3z8qZDXTcA2qj-language').innerText = data.language;
document.getElementById('repo-SMx3z8qZDXTcA2qj-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-SMx3z8qZDXTcA2qj-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-SMx3z8qZDXTcA2qj-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-SMx3z8qZDXTcA2qj-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-SMx3z8qZDXTcA2qj-license').classList.add = "no-license"
};
document.getElementById('repo-SMx3z8qZDXTcA2qj-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for OpenAccess-AI-Collective\/axolotl.")
}).catch(err => {
const c = document.getElementById('repo-SMx3z8qZDXTcA2qj-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for OpenAccess-AI-Collective\/axolotl.")
})
&lt;/script>
&lt;p>Axolotl 是一款旨在简化各种人工智能模型微调的工具，支持多种配置和架构。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>训练各种 Huggingface 模型，如 llama、pythia、falcon、mpt&lt;/li>
&lt;li>支持 fullfinetune、lora、qlora、relora 和 gptq&lt;/li>
&lt;li>使用简单的 yaml 文件或 CLI 重写功能自定义配置&lt;/li>
&lt;li>加载不同的数据集格式，使用自定义格式，或自带标记化数据集&lt;/li>
&lt;li>与 xformer、闪存关注、绳索缩放和多重包装集成&lt;/li>
&lt;li>可通过 FSDP 或 Deepspeed 与单 GPU 或多 GPU 协同工作&lt;/li>
&lt;li>使用 Docker 在本地或云端轻松运行&lt;/li>
&lt;li>将结果和可选的检查点记录到 wandb 或 mlflow 中&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>快速入门：&lt;/strong>
要求： Python &amp;gt;=3.10 和 Pytorch &amp;gt;=2.1.1&lt;/p>
&lt;pre>&lt;code class="language-bash">git clone https://github.com/OpenAccess-AI-Collective/axolotl
cd axolotl
pip3 install packaging ninja
pip3 install -e '.[flash-attn,deepspeed]'
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>使用方法：&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash"># preprocess datasets - optional but recommended
CUDA_VISIBLE_DEVICES=&amp;quot;&amp;quot; python -m axolotl.cli.preprocess examples/openllama-3b/lora.yml
# finetune lora
accelerate launch -m axolotl.cli.train examples/openllama-3b/lora.yml
# inference
accelerate launch -m axolotl.cli.inference examples/openllama-3b/lora.yml \
--lora_model_dir=&amp;quot;./outputs/lora-out&amp;quot;
# gradio
accelerate launch -m axolotl.cli.inference examples/openllama-3b/lora.yml \
--lora_model_dir=&amp;quot;./outputs/lora-out&amp;quot; --gradio
# remote yaml files - the yaml config can be hosted on a public URL
# Note: the yaml config must directly link to the **raw** yaml
accelerate launch -m axolotl.cli.train https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/examples/openllama-3b/lora.yml
&lt;/code>&lt;/pre>
&lt;p>其它详细信息，请访问 &lt;a class="link" href="https://github.com/OpenAccess-AI-Collective/axolotl" target="_blank" rel="noopener" >Axolotl
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
项目主页。&lt;/p>
&lt;h3 id="llama-factory">Llama-Factory&lt;/h3>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-07-07_llama-factory-logo.webp"
alt="Llama Factory Logo" width="80%" loading="lazy">
&lt;/figure>
&lt;a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" class="card-github fetch-waiting no-styling"
repo="hiyouga/LLaMA-Factory" id="repo-uJn89qJ55TGCkvxj-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-uJn89qJ55TGCkvxj-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">hiyouga&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">LLaMA-Factory&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-uJn89qJ55TGCkvxj-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-uJn89qJ55TGCkvxj-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-uJn89qJ55TGCkvxj-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-uJn89qJ55TGCkvxj-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-uJn89qJ55TGCkvxj-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-uJn89qJ55TGCkvxj-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/hiyouga\/LLaMA-Factory', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-uJn89qJ55TGCkvxj-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-uJn89qJ55TGCkvxj-language').innerText = data.language;
document.getElementById('repo-uJn89qJ55TGCkvxj-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-uJn89qJ55TGCkvxj-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-uJn89qJ55TGCkvxj-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-uJn89qJ55TGCkvxj-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-uJn89qJ55TGCkvxj-license').classList.add = "no-license"
};
document.getElementById('repo-uJn89qJ55TGCkvxj-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for hiyouga\/LLaMA-Factory.")
}).catch(err => {
const c = document.getElementById('repo-uJn89qJ55TGCkvxj-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for hiyouga\/LLaMA-Factory.")
})
&lt;/script>
&lt;p>Llama-Factory 是 Meta 推出的，专注于 Llama 模型微调的框架。它构建于 PyTorch 生态之上，并提供高效的训练和评估工具。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>多种模型&lt;/strong>：LLaMA、LLaVA、Mistral、Mixtral-MoE、Qwen、Yi、Gemma、Baichuan、ChatGLM、Phi 等等。&lt;/li>
&lt;li>&lt;strong>集成方法&lt;/strong>：（增量）预训练、（多模态）指令监督微调、奖励模型训练、PPO 训练、DPO 训练、KTO 训练、ORPO 训练等等。&lt;/li>
&lt;li>&lt;strong>多种精度&lt;/strong>：16 比特全参数微调、冻结微调、LoRA 微调和基于 AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ 的 2/3/4/5/6/8 比特 QLoRA 微调。&lt;/li>
&lt;li>&lt;strong>先进算法&lt;/strong>：GaLore、BAdam、DoRA、LongLoRA、LLaMA Pro、Mixture-of-Depths、LoRA+、LoftQ、PiSSA 和 Agent 微调。&lt;/li>
&lt;li>&lt;strong>实用技巧&lt;/strong>：FlashAttention-2、Unsloth、RoPE scaling、NEFTune 和 rsLoRA。&lt;/li>
&lt;li>&lt;strong>实验监控&lt;/strong>：LlamaBoard、TensorBoard、Wandb、MLflow 等等。&lt;/li>
&lt;li>&lt;strong>极速推理&lt;/strong>：基于 vLLM 的 OpenAI 风格 API、浏览器界面和命令行接口。&lt;/li>
&lt;/ul>
&lt;link href="https://web.sdk.qcloud.com/player/tcplayer/release/v4.2.1/tcplayer.min.css" rel="stylesheet">
&lt;script src="https://web.sdk.qcloud.com/player/tcplayer/release/v4.2.1/libs/hls.min.0.13.2m.js">&lt;/script>
&lt;script src="https://web.sdk.qcloud.com/player/tcplayer/release/v4.2.1/tcplayer.v4.2.1.min.js">&lt;/script>
&lt;style>
.tcplayer {
position: absolute;
width: 100%;
height: 100%;
left: 0;
top: 0;
border: 0;
}
&lt;/style>
&lt;div class="video-wrapper">
&lt;video
id="player-container-id"
preload="auto"
width="100%"
height="100%"
playsinline
webkit-playsinline>
&lt;/video>
&lt;/div>
&lt;script>
var tcplayer = TCPlayer("player-container-id", {
reportable: false,
poster: "",
});
tcplayer.src('https:\/\/cuterwrite-1302252842.file.myqcloud.com\/img\/2024-07-07_309488092-ec36a9dd-37f4-4f72-81bd-d76c6d0a6594_output.mp4');
&lt;/script>
&lt;p>&lt;strong>性能指标&lt;/strong>&lt;/p>
&lt;p>与 ChatGLM 官方的 &lt;a class="link" href="https://github.com/THUDM/ChatGLM2-6B/tree/main/ptuning" target="_blank" rel="noopener" >P-Tuning
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
微调相比，LLaMA Factory 的 LoRA 微调提供了 &lt;strong>3.7 倍&lt;/strong>的加速比，同时在广告文案生成任务上取得了更高的 Rouge 分数。结合 4 比特量化技术，LLaMA Factory 的 QLoRA 微调进一步降低了 GPU 显存消耗。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-07-07_benchmark.webp"
alt="Performance" width="auto" loading="lazy">
&lt;/figure>
&lt;details>
&lt;summary>变量定义&lt;/summary>
&lt;ul>
&lt;li>&lt;strong>Training Speed&lt;/strong>: 训练阶段每秒处理的样本数量。（批处理大小=4，截断长度=1024）&lt;/li>
&lt;li>&lt;strong>Rouge Score&lt;/strong>: &lt;a class="link" href="https://aclanthology.org/D19-1321.pdf" target="_blank" rel="noopener" >广告文案生成
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
任务验证集上的 Rouge-2 分数。（批处理大小=4，截断长度=1024）&lt;/li>
&lt;li>&lt;strong>GPU Memory&lt;/strong>: 4 比特量化训练的 GPU 显存峰值。（批处理大小=1，截断长度=1024）&lt;/li>
&lt;li>我们在 ChatGLM 的 P-Tuning 中采用 &lt;code>pre_seq_len=128&lt;/code>，在 LLaMA Factory 的 LoRA 微调中采用 &lt;code>lora_rank=32&lt;/code>。&lt;/li>
&lt;/ul>
&lt;/details>
&lt;p>&lt;strong>快速入门&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash">git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e &amp;quot;.[torch,metrics]&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>可选的额外依赖项：torch、torch-npu、metrics、deepspeed、bitsandbytes、hqq、eetq、gptq、awq、aqlm、vllm、galore、badam、qwen、modelscope、quality&lt;/p>
&lt;div class="notice notice-tip" >
&lt;div class="notice-title">&lt;svg t="1705945832245" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="19409" width="200" height="200">&lt;path d="M512 74.666667C270.933333 74.666667 74.666667 270.933333 74.666667 512S270.933333 949.333333 512 949.333333 949.333333 753.066667 949.333333 512 753.066667 74.666667 512 74.666667z m238.933333 349.866666l-2.133333 2.133334-277.333333 277.333333c-10.666667 10.666667-29.866667 12.8-42.666667 2.133333L426.666667 704l-149.333334-149.333333c-12.8-12.8-12.8-32 0-44.8 10.666667-10.666667 29.866667-12.8 42.666667-2.133334l2.133333 2.133334 125.866667 125.866666 253.866667-253.866666c10.666667-10.666667 29.866667-12.8 42.666666-2.133334l2.133334 2.133334c12.8 12.8 12.8 32 4.266666 42.666666z" fill="#ffffff" p-id="19410">&lt;/path>&lt;/svg>&lt;/div>&lt;p>遇到包冲突时，可使用 pip install &amp;ndash;no-deps -e . 解决。&lt;/p>&lt;/div>
&lt;details>
&lt;summary>Windows 用户指南&lt;/summary>
&lt;p>如果要在 Windows 平台上开启量化 LoRA（QLoRA），需要安装预编译的 &lt;code>bitsandbytes&lt;/code> 库, 支持 CUDA 11.1 到 12.2, 请根据您的 CUDA 版本情况选择适合的&lt;a class="link" href="https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels" target="_blank" rel="noopener" >发布版本
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl
&lt;/code>&lt;/pre>
&lt;p>如果要在 Windows 平台上开启 FlashAttention-2，需要安装预编译的 &lt;code>flash-attn&lt;/code> 库，支持 CUDA 12.1 到 12.2，请根据需求到 &lt;a class="link" href="https://github.com/bdashore3/flash-attention/releases" target="_blank" rel="noopener" >flash-attention
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
下载对应版本安装。&lt;/p>
&lt;/details>
&lt;details>
&lt;summary>昇腾 NPU 用户指南&lt;/summary>
&lt;p>在昇腾 NPU 设备上安装 LLaMA Factory 时，需要指定额外依赖项，使用 &lt;code>pip install -e &amp;quot;.[torch-npu,metrics]&amp;quot;&lt;/code> 命令安装。此外，还需要安装 &lt;strong>&lt;a class="link" href="https://www.hiascend.com/developer/download/community/result?module=cann" target="_blank" rel="noopener" >Ascend CANN Toolkit 与 Kernels
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/strong>，安装方法请参考&lt;a class="link" href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/80RC2alpha002/quickstart/quickstart/quickstart_18_0004.html" target="_blank" rel="noopener" >安装教程
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
或使用以下命令：&lt;/p>
&lt;pre>&lt;code class="language-bash"># 请替换 URL 为 CANN 版本和设备型号对应的 URL
# 安装 CANN Toolkit
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-toolkit_8.0.RC1.alpha001_linux-&amp;quot;$(uname -i)&amp;quot;.run
bash Ascend-cann-toolkit_8.0.RC1.alpha001_linux-&amp;quot;$(uname -i)&amp;quot;.run --install
# 安装 CANN Kernels
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run
bash Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run --install
# 设置环境变量
source /usr/local/Ascend/ascend-toolkit/set_env.sh
&lt;/code>&lt;/pre>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>依赖项&lt;/th>
&lt;th>至少&lt;/th>
&lt;th>推荐&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>CANN&lt;/td>
&lt;td>8.0.RC1&lt;/td>
&lt;td>8.0.RC1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>torch&lt;/td>
&lt;td>2.1.0&lt;/td>
&lt;td>2.1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>torch-npu&lt;/td>
&lt;td>2.1.0&lt;/td>
&lt;td>2.1.0.post3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>deepspeed&lt;/td>
&lt;td>0.13.2&lt;/td>
&lt;td>0.13.2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>请使用 &lt;code>ASCEND_RT_VISIBLE_DEVICES&lt;/code> 而非 &lt;code>CUDA_VISIBLE_DEVICES&lt;/code> 来指定运算设备。&lt;/p>
&lt;p>如果遇到无法正常推理的情况，请尝试设置 &lt;code>do_sample: false&lt;/code>。&lt;/p>
&lt;p>下载预构建 Docker 镜像：&lt;a class="link" href="http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html" target="_blank" rel="noopener" >32GB
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
| &lt;a class="link" href="http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html" target="_blank" rel="noopener" >64GB
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;/details>
&lt;p>下面三行命令分别对 Llama3-8B-Instruct 模型进行 LoRA 微调、推理和合并。&lt;/p>
&lt;pre>&lt;code class="language-bash">llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml
&lt;/code>&lt;/pre>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" rel="noopener" >Llama-Factory
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
项目主页。&lt;/p>
&lt;h3 id="firefly">Firefly&lt;/h3>
&lt;a href="https://github.com/yangjianxin1/Firefly" target="_blank" class="card-github fetch-waiting no-styling"
repo="yangjianxin1/Firefly" id="repo-KjMULgXFgUwuRl46-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-KjMULgXFgUwuRl46-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">yangjianxin1&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">Firefly&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-KjMULgXFgUwuRl46-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-KjMULgXFgUwuRl46-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-KjMULgXFgUwuRl46-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-KjMULgXFgUwuRl46-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-KjMULgXFgUwuRl46-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-KjMULgXFgUwuRl46-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/yangjianxin1\/Firefly', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-KjMULgXFgUwuRl46-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-KjMULgXFgUwuRl46-language').innerText = data.language;
document.getElementById('repo-KjMULgXFgUwuRl46-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-KjMULgXFgUwuRl46-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-KjMULgXFgUwuRl46-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-KjMULgXFgUwuRl46-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-KjMULgXFgUwuRl46-license').classList.add = "no-license"
};
document.getElementById('repo-KjMULgXFgUwuRl46-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for yangjianxin1\/Firefly.")
}).catch(err => {
const c = document.getElementById('repo-KjMULgXFgUwuRl46-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for yangjianxin1\/Firefly.")
})
&lt;/script>
&lt;p>&lt;strong>Firefly&lt;/strong> 是一个开源的大模型训练项目，支持对主流的大模型进行预训练、指令微调和 DPO，包括但不限于 Qwen2、Yi-1.5、Llama3、Gemma、Qwen1.5、MiniCPM、Llama、InternLM、Baichuan、ChatGLM、Yi、Deepseek、Qwen、Orion、Ziya、Xverse、Mistral、Mixtral-8x7B、Zephyr、Vicuna、Bloom 等。
本项目支持&lt;strong>全量参数训练、LoRA、QLoRA 高效训练&lt;/strong>，支持&lt;strong>预训练、SFT、DPO&lt;/strong>。 如果你的训练资源有限，我们极力推荐使用 QLoRA 进行指令微调，因为我们在 Open LLM Leaderboard 上验证了该方法的有效性，并且取得了非常不错的成绩。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>📗 支持预训练、指令微调、DPO，支持全量参数训练、LoRA、QLoRA 高效训练。通过配置文件的方式训练不同的模型，小白亦可快速上手训练模型。&lt;/li>
&lt;li>📗 支持使用&lt;a class="link" href="https://github.com/yangjianxin1/unsloth" target="_blank" rel="noopener" >Unsloth
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
加速训练，并且节省显存。&lt;/li>
&lt;li>📗 支持绝大部分主流的开源大模型，如 Llama3、Gemma、MiniCPM、Llama、InternLM、Baichuan、ChatGLM、Yi、Deepseek、Qwen、Orion、Ziya、Xverse、Mistral、Mixtral-8x7B、Zephyr、Vicuna、Bloom，训练时与各个官方的 chat 模型的 template 对齐。&lt;/li>
&lt;li>📗 整理并开源指令微调数据集：firefly-train-1.1M 、moss-003-sft-data、ultrachat、 WizardLM_evol_instruct_V2_143k、school_math_0.25M。&lt;/li>
&lt;li>📗 开源&lt;a class="link" href="https://huggingface.co/YeungNLP" target="_blank" rel="noopener" >Firefly 系列指令微调模型权重
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/li>
&lt;li>📗 在 Open LLM Leaderboard 上验证了 QLoRA 训练流程的有效性。&lt;/li>
&lt;/ul>
&lt;p>该项目的 README 中包含了详细的使用说明，包括如何安装、如何训练、如何微调、如何评估等。请访问 &lt;a class="link" href="https://github.com/yangjianxin1/Firefly" target="_blank" rel="noopener" >Firefly
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
项目主页。&lt;/p>
&lt;h3 id="xtuner">XTuner&lt;/h3>
&lt;a href="https://github.com/InternLM/xtuner" target="_blank" class="card-github fetch-waiting no-styling"
repo="InternLM/xtuner" id="repo-WyedWkInUNah7cj8-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-WyedWkInUNah7cj8-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">InternLM&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">xtuner&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-WyedWkInUNah7cj8-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-WyedWkInUNah7cj8-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-WyedWkInUNah7cj8-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-WyedWkInUNah7cj8-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-WyedWkInUNah7cj8-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-WyedWkInUNah7cj8-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/InternLM\/xtuner', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-WyedWkInUNah7cj8-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-WyedWkInUNah7cj8-language').innerText = data.language;
document.getElementById('repo-WyedWkInUNah7cj8-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-WyedWkInUNah7cj8-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-WyedWkInUNah7cj8-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-WyedWkInUNah7cj8-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-WyedWkInUNah7cj8-license').classList.add = "no-license"
};
document.getElementById('repo-WyedWkInUNah7cj8-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for InternLM\/xtuner.")
}).catch(err => {
const c = document.getElementById('repo-WyedWkInUNah7cj8-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for InternLM\/xtuner.")
})
&lt;/script>
&lt;p>XTuner 是一个高效、灵活、全能的轻量化大模型微调工具库。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>高效&lt;/strong>
&lt;ul>
&lt;li>支持大语言模型 LLM、多模态图文模型 VLM 的预训练及轻量级微调。XTuner 支持在 8GB 显存下微调 7B 模型，同时也支持多节点跨设备微调更大尺度模型（70B+）。&lt;/li>
&lt;li>自动分发高性能算子（如 FlashAttention、Triton kernels 等）以加速训练吞吐。&lt;/li>
&lt;li>兼容 &lt;a class="link" href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener" >DeepSpeed
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
🚀，轻松应用各种 ZeRO 训练优化策略。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>灵活&lt;/strong>
&lt;ul>
&lt;li>支持多种大语言模型，包括但不限于 &lt;a class="link" href="https://huggingface.co/internlm" target="_blank" rel="noopener" >InternLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="https://huggingface.co/mistralai" target="_blank" rel="noopener" >Mixtral-8x7B
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="https://huggingface.co/meta-llama" target="_blank" rel="noopener" >Llama 2
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="https://huggingface.co/THUDM" target="_blank" rel="noopener" >ChatGLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="https://huggingface.co/Qwen" target="_blank" rel="noopener" >Qwen
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="https://huggingface.co/baichuan-inc" target="_blank" rel="noopener" >Baichuan
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/li>
&lt;li>支持多模态图文模型 LLaVA 的预训练与微调。利用 XTuner 训得模型 &lt;a class="link" href="https://huggingface.co/xtuner/llava-internlm2-20b" target="_blank" rel="noopener" >LLaVA-InternLM2-20B
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
表现优异。&lt;/li>
&lt;li>精心设计的数据管道，兼容任意数据格式，开源数据或自定义数据皆可快速上手。&lt;/li>
&lt;li>支持 &lt;a class="link" href="http://arxiv.org/abs/2305.14314" target="_blank" rel="noopener" >QLoRA
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="http://arxiv.org/abs/2106.09685" target="_blank" rel="noopener" >LoRA
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、全量参数微调等多种微调算法，支撑用户根据具体需求作出最优选择。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>全能&lt;/strong>
&lt;ul>
&lt;li>支持增量预训练、指令微调与 Agent 微调。&lt;/li>
&lt;li>预定义众多开源对话模版，支持与开源或训练所得模型进行对话。&lt;/li>
&lt;li>训练所得模型可无缝接入部署工具库 &lt;a class="link" href="https://github.com/InternLM/lmdeploy" target="_blank" rel="noopener" >LMDeploy
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、大规模评测工具库 &lt;a class="link" href="https://github.com/open-compass/opencompass" target="_blank" rel="noopener" >OpenCompass
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
及 &lt;a class="link" href="https://github.com/open-compass/VLMEvalKit" target="_blank" rel="noopener" >VLMEvalKit
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>快速上手：&lt;/strong>
&lt;details>
&lt;summary>安装&lt;/summary>
&lt;ul>
&lt;li>
&lt;p>推荐使用 conda 先构建一个 Python-3.10 的虚拟环境&lt;/p>
&lt;pre>&lt;code class="language-bash">conda create --name xtuner-env python=3.10 -y
conda activate xtuner-env
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>通过 pip 安装 XTuner：&lt;/p>
&lt;pre>&lt;code class="language-shell">pip install -U xtuner
&lt;/code>&lt;/pre>
&lt;p>亦可集成 DeepSpeed 安装：&lt;/p>
&lt;pre>&lt;code class="language-shell">pip install -U 'xtuner[deepspeed]'
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>从源码安装 XTuner：&lt;/p>
&lt;pre>&lt;code class="language-shell">git clone https://github.com/InternLM/xtuner.git
cd xtuner
pip install -e '.[all]'
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul>
&lt;/details>
&lt;/p>
&lt;details>
&lt;summary>微调&lt;/summary>
&lt;p>XTuner 支持微调大语言模型。数据集预处理指南请查阅&lt;a class="link" href="./docs/zh_cn/user_guides/dataset_prepare.md" >文档
&lt;/a>
。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>步骤 0&lt;/strong>，准备配置文件。XTuner 提供多个开箱即用的配置文件，用户可以通过下列命令查看：&lt;/p>
&lt;pre>&lt;code class="language-shell">xtuner list-cfg
&lt;/code>&lt;/pre>
&lt;p>或者，如果所提供的配置文件不能满足使用需求，请导出所提供的配置文件并进行相应更改：&lt;/p>
&lt;pre>&lt;code class="language-shell">xtuner copy-cfg ${CONFIG_NAME} ${SAVE_PATH}
vi ${SAVE_PATH}/${CONFIG_NAME}_copy.py
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>步骤 1&lt;/strong>，开始微调。&lt;/p>
&lt;pre>&lt;code class="language-shell">xtuner train ${CONFIG_NAME_OR_PATH}
&lt;/code>&lt;/pre>
&lt;p>例如，我们可以利用 QLoRA 算法在 oasst1 数据集上微调 InternLM2.5-Chat-7B：&lt;/p>
&lt;pre>&lt;code class="language-shell"># 单卡
xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2
# 多卡
(DIST) NPROC_PER_NODE=${GPU_NUM} xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2
(SLURM) srun ${SRUN_ARGS} xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --launcher slurm --deepspeed deepspeed_zero2
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>
&lt;p>&lt;code>--deepspeed&lt;/code> 表示使用 &lt;a class="link" href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener" >DeepSpeed
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
🚀 来优化训练过程。XTuner 内置了多种策略，包括 ZeRO-1、ZeRO-2、ZeRO-3 等。如果用户期望关闭此功能，请直接移除此参数。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>更多示例，请查阅&lt;a class="link" href="./docs/zh_cn/user_guides/finetune.md" >文档
&lt;/a>
。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>步骤 2&lt;/strong>，将保存的 PTH 模型（如果使用的 DeepSpeed，则将会是一个文件夹）转换为 HuggingFace 模型：&lt;/p>
&lt;pre>&lt;code class="language-shell">xtuner convert pth_to_hf ${CONFIG_NAME_OR_PATH} ${PTH} ${SAVE_PATH}
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul>
&lt;/details>
&lt;p>其它详细信息，请访问 &lt;a class="link" href="https://github.com/InternLM/xtuner" target="_blank" rel="noopener" >XTuner
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h2 id="模型量化">模型量化&lt;/h2>
&lt;p>LLM 通常体积庞大，对计算资源要求高。模型量化技术可以压缩模型大小，提高运行效率，使其更易于部署：&lt;/p>
&lt;h3 id="autogptq">AutoGPTQ&lt;/h3>
&lt;a href="https://github.com/PanQiWei/AutoGPTQ" target="_blank" class="card-github fetch-waiting no-styling"
repo="PanQiWei/AutoGPTQ" id="repo-2l1IgDfyGDkflGVF-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-2l1IgDfyGDkflGVF-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">PanQiWei&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">AutoGPTQ&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-2l1IgDfyGDkflGVF-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-2l1IgDfyGDkflGVF-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-2l1IgDfyGDkflGVF-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-2l1IgDfyGDkflGVF-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-2l1IgDfyGDkflGVF-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-2l1IgDfyGDkflGVF-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/PanQiWei\/AutoGPTQ', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-2l1IgDfyGDkflGVF-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-2l1IgDfyGDkflGVF-language').innerText = data.language;
document.getElementById('repo-2l1IgDfyGDkflGVF-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-2l1IgDfyGDkflGVF-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-2l1IgDfyGDkflGVF-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-2l1IgDfyGDkflGVF-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-2l1IgDfyGDkflGVF-license').classList.add = "no-license"
};
document.getElementById('repo-2l1IgDfyGDkflGVF-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for PanQiWei\/AutoGPTQ.")
}).catch(err => {
const c = document.getElementById('repo-2l1IgDfyGDkflGVF-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for PanQiWei\/AutoGPTQ.")
})
&lt;/script>
&lt;p>AutoGPTQ 一个基于 GPTQ 算法，简单易用且拥有用户友好型接口的大语言模型量化工具包。&lt;/p>
&lt;p>&lt;strong>快速安装&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>对于 CUDA 11.7：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>对于 CUDA 11.8：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>对于 RoCm 5.4.2：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/rocm542/
&lt;/code>&lt;/pre>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/AutoGPTQ/AutoGPTQ/" target="_blank" rel="noopener" >AutoGPTQ
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="autoawq">AutoAWQ&lt;/h3>
&lt;a href="https://github.com/casper-hansen/AutoAWQ" target="_blank" class="card-github fetch-waiting no-styling"
repo="casper-hansen/AutoAWQ" id="repo-AGfiGPg7ELIg9AGO-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-AGfiGPg7ELIg9AGO-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">casper-hansen&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">AutoAWQ&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-AGfiGPg7ELIg9AGO-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-AGfiGPg7ELIg9AGO-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-AGfiGPg7ELIg9AGO-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-AGfiGPg7ELIg9AGO-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-AGfiGPg7ELIg9AGO-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-AGfiGPg7ELIg9AGO-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/casper-hansen\/AutoAWQ', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-AGfiGPg7ELIg9AGO-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-AGfiGPg7ELIg9AGO-language').innerText = data.language;
document.getElementById('repo-AGfiGPg7ELIg9AGO-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-AGfiGPg7ELIg9AGO-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-AGfiGPg7ELIg9AGO-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-AGfiGPg7ELIg9AGO-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-AGfiGPg7ELIg9AGO-license').classList.add = "no-license"
};
document.getElementById('repo-AGfiGPg7ELIg9AGO-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for casper-hansen\/AutoAWQ.")
}).catch(err => {
const c = document.getElementById('repo-AGfiGPg7ELIg9AGO-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for casper-hansen\/AutoAWQ.")
})
&lt;/script>
&lt;p>AutoAWQ 是另一款自动化模型量化工具，支持多种量化精度，并提供灵活的配置选项，可以根据不同的硬件平台和性能需求进行调整。&lt;/p>
&lt;p>AutoAWQ 是一个易于使用的 4 位量化模型软件包。与 FP16 相比，AutoAWQ 可将模型速度提高 3 倍，内存需求减少 3 倍。AutoAWQ 实现了用于量化 LLMs 的激活感知权重量化（AWQ）算法。AutoAWQ 是在 MIT 的原始工作 &lt;a class="link" href="https://github.com/mit-han-lab/llm-awq" target="_blank" rel="noopener" >AWQ
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
基础上创建和改进的。&lt;/p>
&lt;p>&lt;strong>安装方法：&lt;/strong>&lt;/p>
&lt;p>安装前，确保安装了 CUDA &amp;gt;= 12.1（注意：以下只是最快捷的安装方法）&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install autoawq
&lt;/code>&lt;/pre>
&lt;p>更多详细信息和示例，请访问 &lt;a class="link" href="https://github.com/casper-hansen/AutoAWQ" target="_blank" rel="noopener" >AutoAWQ
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="neural-compressor">Neural Compressor&lt;/h3>
&lt;a href="https://github.com/intel/neural-compressor" target="_blank" class="card-github fetch-waiting no-styling"
repo="intel/neural-compressor" id="repo-v91Se5KlW4xZbPi6-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-v91Se5KlW4xZbPi6-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">intel&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">neural-compressor&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-v91Se5KlW4xZbPi6-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-v91Se5KlW4xZbPi6-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-v91Se5KlW4xZbPi6-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-v91Se5KlW4xZbPi6-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-v91Se5KlW4xZbPi6-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-v91Se5KlW4xZbPi6-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/intel\/neural-compressor', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-v91Se5KlW4xZbPi6-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-v91Se5KlW4xZbPi6-language').innerText = data.language;
document.getElementById('repo-v91Se5KlW4xZbPi6-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-v91Se5KlW4xZbPi6-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-v91Se5KlW4xZbPi6-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-v91Se5KlW4xZbPi6-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-v91Se5KlW4xZbPi6-license').classList.add = "no-license"
};
document.getElementById('repo-v91Se5KlW4xZbPi6-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for intel\/neural-compressor.")
}).catch(err => {
const c = document.getElementById('repo-v91Se5KlW4xZbPi6-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for intel\/neural-compressor.")
})
&lt;/script>
&lt;p>Neural Compressor 是英特尔开发的模型压缩工具包，支持所有主流深度学习框架（TensorFlow、PyTorch、ONNX Runtime 和 MXNet）上流行的模型压缩技术。&lt;/p>
&lt;p>&lt;strong>安装方法：&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install &amp;quot;neural-compressor&amp;gt;=2.3&amp;quot; &amp;quot;transformers&amp;gt;=4.34.0&amp;quot; torch torchvision
&lt;/code>&lt;/pre>
&lt;p>更多详细信息和示例，请访问 &lt;a class="link" href="https://github.com/intel/neural-compressor" target="_blank" rel="noopener" >Neural Compressor
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h2 id="模型部署">模型部署&lt;/h2>
&lt;p>将训练好的 LLM 部署到生产环境至关重要。以下是一些常用的 LLM 部署工具：&lt;/p>
&lt;h3 id="vllm">vLLM&lt;/h3>
&lt;a href="https://github.com/vllm-project/vllm" target="_blank" class="card-github fetch-waiting no-styling"
repo="vllm-project/vllm" id="repo-adqPLrtdwijYxSpE-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-adqPLrtdwijYxSpE-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">vllm-project&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">vllm&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-adqPLrtdwijYxSpE-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-adqPLrtdwijYxSpE-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-adqPLrtdwijYxSpE-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-adqPLrtdwijYxSpE-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-adqPLrtdwijYxSpE-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-adqPLrtdwijYxSpE-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/vllm-project\/vllm', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-adqPLrtdwijYxSpE-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-adqPLrtdwijYxSpE-language').innerText = data.language;
document.getElementById('repo-adqPLrtdwijYxSpE-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-adqPLrtdwijYxSpE-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-adqPLrtdwijYxSpE-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-adqPLrtdwijYxSpE-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-adqPLrtdwijYxSpE-license').classList.add = "no-license"
};
document.getElementById('repo-adqPLrtdwijYxSpE-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for vllm-project\/vllm.")
}).catch(err => {
const c = document.getElementById('repo-adqPLrtdwijYxSpE-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for vllm-project\/vllm.")
})
&lt;/script>
&lt;p>vLLM 是一个快速、易用的 LLM 推理服务库。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>快速&lt;/strong>
&lt;ul>
&lt;li>SOTA 服务吞吐量&lt;/li>
&lt;li>利用 PagedAttention 高效管理注意力键值内存&lt;/li>
&lt;li>持续批量处理收到的请求&lt;/li>
&lt;li>利用 CUDA/HIP 图进行加速&lt;/li>
&lt;li>量化：支持 GPTQ、AWQ、SqueezeLLM、FP8 KV 高速缓存&lt;/li>
&lt;li>优化的 CUDA 内核&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>灵活&lt;/strong>
&lt;ul>
&lt;li>与流行的 Hugging Face 模型无缝集成&lt;/li>
&lt;li>利用各种解码算法（包括并行采样、波束搜索等）提供高吞吐量服务&lt;/li>
&lt;li>为分布式推理提供张量并行支持&lt;/li>
&lt;li>流输出&lt;/li>
&lt;li>兼容 OpenAI 的应用程序接口服务器&lt;/li>
&lt;li>支持 NVIDIA GPU、AMD GPU、Intel CPU 和 GPU
-（实验性）支持前缀缓存
-（试验性）支持多种语言&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>无缝支持&lt;/strong>
&lt;ul>
&lt;li>基于 Transformer 的模型，例如 Llama&lt;/li>
&lt;li>基于 MoE 的模型，例如 Mixtral&lt;/li>
&lt;li>多模态模型，例如 LLaVA&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>快速安装：&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install vllm
&lt;/code>&lt;/pre>
&lt;p>更多详细信息，请查看 &lt;a class="link" href="https://vllm.readthedocs.io/en/latest/" target="_blank" rel="noopener" >vLLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
官方文档。&lt;/p>
&lt;h3 id="sgl">SGL&lt;/h3>
&lt;a href="https://github.com/sgl-project/sglang" target="_blank" class="card-github fetch-waiting no-styling"
repo="sgl-project/sglang" id="repo-Y7Ic1RgpSq4N1f9l-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-Y7Ic1RgpSq4N1f9l-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">sgl-project&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">sglang&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-Y7Ic1RgpSq4N1f9l-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-Y7Ic1RgpSq4N1f9l-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-Y7Ic1RgpSq4N1f9l-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-Y7Ic1RgpSq4N1f9l-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-Y7Ic1RgpSq4N1f9l-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-Y7Ic1RgpSq4N1f9l-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/sgl-project\/sglang', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-Y7Ic1RgpSq4N1f9l-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-Y7Ic1RgpSq4N1f9l-language').innerText = data.language;
document.getElementById('repo-Y7Ic1RgpSq4N1f9l-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-Y7Ic1RgpSq4N1f9l-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-Y7Ic1RgpSq4N1f9l-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-Y7Ic1RgpSq4N1f9l-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-Y7Ic1RgpSq4N1f9l-license').classList.add = "no-license"
};
document.getElementById('repo-Y7Ic1RgpSq4N1f9l-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for sgl-project\/sglang.")
}).catch(err => {
const c = document.getElementById('repo-Y7Ic1RgpSq4N1f9l-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for sgl-project\/sglang.")
})
&lt;/script>
&lt;p>SGLang 是一种结构化生成语言，专为大型语言模型（LLMs）而设计。它通过共同设计前端语言和运行系统，使你与 LLMs 的交互更快、更可控。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>灵活的前端语言&lt;/strong>：通过链式生成调用、高级提示、控制流、多种模式、并行性和外部交互，可轻松编写 LLM 应用程序。&lt;/li>
&lt;li>&lt;strong>高性能后端运行时&lt;/strong>：具有 RadixAttention 功能，可通过在多次调用中重复使用 KV 缓存来加速复杂的 LLM 程序。它还可以作为独立的推理引擎，实现所有常用技术（如连续批处理和张量并行）。&lt;/li>
&lt;/ul>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/sgl-project/" target="_blank" rel="noopener" >SGL
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="skypilot">SkyPilot&lt;/h3>
&lt;a href="https://github.com/skypilot-org/skypilot" target="_blank" class="card-github fetch-waiting no-styling"
repo="skypilot-org/skypilot" id="repo-hxbzn1QuePqAV5w4-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-hxbzn1QuePqAV5w4-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">skypilot-org&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">skypilot&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-hxbzn1QuePqAV5w4-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-hxbzn1QuePqAV5w4-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-hxbzn1QuePqAV5w4-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-hxbzn1QuePqAV5w4-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-hxbzn1QuePqAV5w4-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-hxbzn1QuePqAV5w4-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/skypilot-org\/skypilot', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-hxbzn1QuePqAV5w4-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-hxbzn1QuePqAV5w4-language').innerText = data.language;
document.getElementById('repo-hxbzn1QuePqAV5w4-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-hxbzn1QuePqAV5w4-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-hxbzn1QuePqAV5w4-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-hxbzn1QuePqAV5w4-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-hxbzn1QuePqAV5w4-license').classList.add = "no-license"
};
document.getElementById('repo-hxbzn1QuePqAV5w4-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for skypilot-org\/skypilot.")
}).catch(err => {
const c = document.getElementById('repo-hxbzn1QuePqAV5w4-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for skypilot-org\/skypilot.")
})
&lt;/script>
&lt;p>SkyPilot 是 UC Berkeley RISELab 推出的灵活的云端 LLM 部署工具，支持多种云平台和硬件加速器，可以自动选择最优的部署方案，并提供成本优化功能。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>多云支持:&lt;/strong> 支持 AWS, GCP, Azure 等多种云平台，方便用户选择合适的部署环境。&lt;/li>
&lt;li>&lt;strong>轻松扩展&lt;/strong>：排队和运行多个作业，自动管理&lt;/li>
&lt;li>&lt;strong>轻松接入对象存储&lt;/strong>：轻松访问对象存储（S3、GCS、R2）&lt;/li>
&lt;/ul>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/skypilot-org/skypilot" target="_blank" rel="noopener" >SkyPilot
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="tensorrt-llm">TensorRT-LLM&lt;/h3>
&lt;a href="https://github.com/NVIDIA/TensorRT-LLM" target="_blank" class="card-github fetch-waiting no-styling"
repo="NVIDIA/TensorRT-LLM" id="repo-zqE2EOHBn0VVWE2I-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-zqE2EOHBn0VVWE2I-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">NVIDIA&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">TensorRT-LLM&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-zqE2EOHBn0VVWE2I-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-zqE2EOHBn0VVWE2I-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-zqE2EOHBn0VVWE2I-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-zqE2EOHBn0VVWE2I-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-zqE2EOHBn0VVWE2I-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-zqE2EOHBn0VVWE2I-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/NVIDIA\/TensorRT-LLM', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-zqE2EOHBn0VVWE2I-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-zqE2EOHBn0VVWE2I-language').innerText = data.language;
document.getElementById('repo-zqE2EOHBn0VVWE2I-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-zqE2EOHBn0VVWE2I-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-zqE2EOHBn0VVWE2I-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-zqE2EOHBn0VVWE2I-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-zqE2EOHBn0VVWE2I-license').classList.add = "no-license"
};
document.getElementById('repo-zqE2EOHBn0VVWE2I-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for NVIDIA\/TensorRT-LLM.")
}).catch(err => {
const c = document.getElementById('repo-zqE2EOHBn0VVWE2I-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for NVIDIA\/TensorRT-LLM.")
})
&lt;/script>
&lt;p>TensorRT-LLM 是 NVIDIA 推出的高性能 LLM 推理引擎，能够充分利用 GPU 加速计算，并针对 Transformer 模型结构进行了优化，大幅提升推理速度。&lt;/p>
&lt;p>TensorRT-LLM 为用户提供了易于使用的 Python API，用于定义大型语言模型 (LLMs) 和构建 TensorRT 引擎，这些引擎包含最先进的优化技术，可在英伟达™（NVIDIA®）图形处理器上高效执行推理。TensorRT-LLM 还包含用于创建执行这些 TensorRT 引擎的 Python 和 C++ 运行时的组件。&lt;/p>
&lt;p>详细信息，请访问 &lt;a class="link" href="https://github.com/NVIDIA/TensorRT-LLM" target="_blank" rel="noopener" >TensorRT-LLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="openvino">OpenVino&lt;/h3>
&lt;a href="https://github.com/openvinotoolkit/openvino" target="_blank" class="card-github fetch-waiting no-styling"
repo="openvinotoolkit/openvino" id="repo-5JbcmAM4GKjx7Ksg-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-5JbcmAM4GKjx7Ksg-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">openvinotoolkit&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">openvino&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-5JbcmAM4GKjx7Ksg-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-5JbcmAM4GKjx7Ksg-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-5JbcmAM4GKjx7Ksg-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-5JbcmAM4GKjx7Ksg-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-5JbcmAM4GKjx7Ksg-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-5JbcmAM4GKjx7Ksg-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/openvinotoolkit\/openvino', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-5JbcmAM4GKjx7Ksg-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-5JbcmAM4GKjx7Ksg-language').innerText = data.language;
document.getElementById('repo-5JbcmAM4GKjx7Ksg-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-5JbcmAM4GKjx7Ksg-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-5JbcmAM4GKjx7Ksg-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-5JbcmAM4GKjx7Ksg-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-5JbcmAM4GKjx7Ksg-license').classList.add = "no-license"
};
document.getElementById('repo-5JbcmAM4GKjx7Ksg-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for openvinotoolkit\/openvino.")
}).catch(err => {
const c = document.getElementById('repo-5JbcmAM4GKjx7Ksg-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for openvinotoolkit\/openvino.")
})
&lt;/script>
&lt;p>OpenVINO™ 是用于优化和部署人工智能推理的开源工具包。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>推理优化&lt;/strong>：提升深度学习在计算机视觉、自动语音识别、生成式人工智能、使用大型和小型语言模型的自然语言处理以及许多其他常见任务中的性能。&lt;/li>
&lt;li>&lt;strong>灵活的模型支持&lt;/strong>：使用 TensorFlow、PyTorch、ONNX、Keras 和 PaddlePaddle 等流行框架训练的模型。无需原始框架即可转换和部署模型。&lt;/li>
&lt;li>&lt;strong>广泛的平台兼容性&lt;/strong>：减少资源需求，在从边缘到云的一系列平台上高效部署。OpenVINO™ 支持在 CPU（x86、ARM）、GPU（支持 OpenCL 的集成和独立 GPU）和 AI 加速器（英特尔 NPU）上进行推理。&lt;/li>
&lt;li>&lt;strong>社区和生态系统&lt;/strong>：加入一个活跃的社区，为提高各个领域的深度学习性能做出贡献。&lt;/li>
&lt;/ul>
&lt;p>详细信息，请访问 &lt;a class="link" href="https://github.com/openvinotoolkit/openvino" target="_blank" rel="noopener" >OpenVino
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="tgi">TGI&lt;/h3>
&lt;a href="https://github.com/huggingface/text-generation-inference" target="_blank" class="card-github fetch-waiting no-styling"
repo="huggingface/text-generation-inference" id="repo-xfJIPFVWfpLVUAM1-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-xfJIPFVWfpLVUAM1-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">huggingface&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">text-generation-inference&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-xfJIPFVWfpLVUAM1-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-xfJIPFVWfpLVUAM1-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-xfJIPFVWfpLVUAM1-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-xfJIPFVWfpLVUAM1-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-xfJIPFVWfpLVUAM1-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-xfJIPFVWfpLVUAM1-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/huggingface\/text-generation-inference', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-xfJIPFVWfpLVUAM1-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-xfJIPFVWfpLVUAM1-language').innerText = data.language;
document.getElementById('repo-xfJIPFVWfpLVUAM1-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-xfJIPFVWfpLVUAM1-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-xfJIPFVWfpLVUAM1-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-xfJIPFVWfpLVUAM1-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-xfJIPFVWfpLVUAM1-license').classList.add = "no-license"
};
document.getElementById('repo-xfJIPFVWfpLVUAM1-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for huggingface\/text-generation-inference.")
}).catch(err => {
const c = document.getElementById('repo-xfJIPFVWfpLVUAM1-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for huggingface\/text-generation-inference.")
})
&lt;/script>
&lt;p>文本生成推理（TGI）是一个用于部署和服务大型语言模型（LLMs）的工具包。TGI 可为最流行的开源 LLMs 实现高性能文本生成，包括 Llama、Falcon、StarCoder、BLOOM、GPT-NeoX 等。&lt;/p>
&lt;p>TGI 实现了许多功能，可以在 &lt;a class="link" href="https://github.com/huggingface/text-generation-inference" target="_blank" rel="noopener" >TGI
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页上找到详细信息。&lt;/p>
&lt;h2 id="本地运行">本地运行&lt;/h2>
&lt;p>得益于模型压缩和优化技术，我们也可以在个人设备上运行 LLM：&lt;/p>
&lt;h3 id="mlx">MLX&lt;/h3>
&lt;a href="https://github.com/ml-explore/mlx" target="_blank" class="card-github fetch-waiting no-styling"
repo="ml-explore/mlx" id="repo-2mmQ2SKcvFsLXZ8R-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-2mmQ2SKcvFsLXZ8R-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">ml-explore&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">mlx&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-2mmQ2SKcvFsLXZ8R-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-2mmQ2SKcvFsLXZ8R-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-2mmQ2SKcvFsLXZ8R-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-2mmQ2SKcvFsLXZ8R-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-2mmQ2SKcvFsLXZ8R-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-2mmQ2SKcvFsLXZ8R-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/ml-explore\/mlx', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-2mmQ2SKcvFsLXZ8R-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-2mmQ2SKcvFsLXZ8R-language').innerText = data.language;
document.getElementById('repo-2mmQ2SKcvFsLXZ8R-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-2mmQ2SKcvFsLXZ8R-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-2mmQ2SKcvFsLXZ8R-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-2mmQ2SKcvFsLXZ8R-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-2mmQ2SKcvFsLXZ8R-license').classList.add = "no-license"
};
document.getElementById('repo-2mmQ2SKcvFsLXZ8R-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for ml-explore\/mlx.")
}).catch(err => {
const c = document.getElementById('repo-2mmQ2SKcvFsLXZ8R-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for ml-explore\/mlx.")
})
&lt;/script>
&lt;p>MLX 是一个专门支持在 Apple 设备上运行 LLM 的框架，充分利用 Metal 加速计算，并提供简单易用的 API，方便开发者将 LLM 集成到 iOS 应用中.&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>相似的应用程序接口&lt;/strong>：MLX 的 Python API 与 NumPy 非常相似。MLX 还拥有功能齐全的 C++、C 和 Swift API，这些 API 与 Python API 非常相似。MLX 拥有更高级别的软件包，如 &lt;code>mlx.nn&lt;/code> 和 &lt;code>mlx.optimizers&lt;/code> ，其 API 与 PyTorch 非常接近，可简化更复杂模型的构建。&lt;/li>
&lt;li>&lt;strong>可组合函数变换&lt;/strong>：MLX 支持用于自动微分、自动矢量化和计算图优化的可组合函数变换。&lt;/li>
&lt;li>&lt;strong>懒计算&lt;/strong>：MLX 中的计算只有在需要时才将数组实体化。&lt;/li>
&lt;li>&lt;strong>动态图构建&lt;/strong>：MLX 中的计算图形是动态构建的。改变函数参数的形状不会导致编译速度变慢，而且调试简单直观。&lt;/li>
&lt;li>&lt;strong>多设备&lt;/strong>：操作可在任何支持的设备（目前是 CPU 和 GPU）上运行。&lt;/li>
&lt;li>&lt;strong>统一内存&lt;/strong>：统一内存模型是 MLX 与其他框架的一个显著区别。MLX 中的阵列位于共享内存中。对 MLX 数组的操作可在任何支持的设备类型上执行，而无需传输数据。&lt;/li>
&lt;/ul>
&lt;p>MLX 是机器学习研究人员为机器学习研究人员设计的。该框架旨在方便用户使用，但仍能高效地训练和部署模型。框架本身的设计概念也很简单。我们的目标是让研究人员能够轻松扩展和改进 MLX，从而快速探索新思路。更多详细信息，请访问 &lt;a class="link" href="https://github.com/ml-explore/mlx" target="_blank" rel="noopener" >MLX
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="llamacpp">Llama.cpp&lt;/h3>
&lt;p>Llama.cpp 是使用 C++ 实现的 Llama 模型推理引擎，可以在 CPU 上高效运行，并支持多种操作系统和硬件平台，方便开发者在资源受限的设备上运行 LLM。&lt;/p>
&lt;a href="https://github.com/ggerganov/llama.cpp" target="_blank" class="card-github fetch-waiting no-styling"
repo="ggerganov/llama.cpp" id="repo-YZUR9YhDT1aGTrXV-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-YZUR9YhDT1aGTrXV-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">ggerganov&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">llama.cpp&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-YZUR9YhDT1aGTrXV-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-YZUR9YhDT1aGTrXV-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-YZUR9YhDT1aGTrXV-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-YZUR9YhDT1aGTrXV-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-YZUR9YhDT1aGTrXV-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-YZUR9YhDT1aGTrXV-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/ggerganov\/llama.cpp', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-YZUR9YhDT1aGTrXV-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-YZUR9YhDT1aGTrXV-language').innerText = data.language;
document.getElementById('repo-YZUR9YhDT1aGTrXV-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-YZUR9YhDT1aGTrXV-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-YZUR9YhDT1aGTrXV-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-YZUR9YhDT1aGTrXV-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-YZUR9YhDT1aGTrXV-license').classList.add = "no-license"
};
document.getElementById('repo-YZUR9YhDT1aGTrXV-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for ggerganov\/llama.cpp.")
}).catch(err => {
const c = document.getElementById('repo-YZUR9YhDT1aGTrXV-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for ggerganov\/llama.cpp.")
})
&lt;/script>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>CPU 推理:&lt;/strong> 针对 CPU 平台进行优化，可以在没有 GPU 的设备上运行 LLM。&lt;/li>
&lt;li>&lt;strong>跨平台支持:&lt;/strong> 支持 Linux, macOS, Windows 等多种操作系统，方便用户在不同平台上使用。&lt;/li>
&lt;li>&lt;strong>轻量级部署:&lt;/strong> 编译后的二进制文件体积小，方便用户部署和使用.&lt;/li>
&lt;/ul>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener" >Llama.cpp
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="ollama">Ollama&lt;/h3>
&lt;a href="https://github.com/ollama/ollama" target="_blank" class="card-github fetch-waiting no-styling"
repo="ollama/ollama" id="repo-WKDQYlwM9T0ziljG-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-WKDQYlwM9T0ziljG-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">ollama&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">ollama&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-WKDQYlwM9T0ziljG-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-WKDQYlwM9T0ziljG-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-WKDQYlwM9T0ziljG-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-WKDQYlwM9T0ziljG-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-WKDQYlwM9T0ziljG-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-WKDQYlwM9T0ziljG-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/ollama\/ollama', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-WKDQYlwM9T0ziljG-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-WKDQYlwM9T0ziljG-language').innerText = data.language;
document.getElementById('repo-WKDQYlwM9T0ziljG-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-WKDQYlwM9T0ziljG-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-WKDQYlwM9T0ziljG-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-WKDQYlwM9T0ziljG-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-WKDQYlwM9T0ziljG-license').classList.add = "no-license"
};
document.getElementById('repo-WKDQYlwM9T0ziljG-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for ollama\/ollama.")
}).catch(err => {
const c = document.getElementById('repo-WKDQYlwM9T0ziljG-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for ollama\/ollama.")
})
&lt;/script>
&lt;p>在 &lt;a class="link" href="https://cuterwrite.top/p/ollama/" >【Ollama：从入门到进阶】
&lt;/a>
一文中介绍过，Ollama 是一个用于构建大型语言模型应用的工具，它提供了一个简洁易用的命令行界面和服务器，让你能够轻松下载、运行和管理各种开源 LLM。与需要复杂配置和强大硬件的传统 LLM 不同，Ollama 让你能够方便地像使用手机 App 一样体验 LLM 的强大功能。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>简单易用&lt;/strong>：Ollama 提供了一个简洁易用的命令行工具，方便用户下载、运行和管理 LLM。&lt;/li>
&lt;li>&lt;strong>多种模型&lt;/strong>：Ollama 支持多种开源 LLM，包括 Qwen2、Llama3、Mistral 等。&lt;/li>
&lt;li>&lt;strong>兼容 OpenAI 接口&lt;/strong>：Ollama 支持 OpenAI API 接口，便于切换原有应用到 Ollama 上。&lt;/li>
&lt;/ul>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/ollama/ollama" target="_blank" rel="noopener" >Ollama
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h2 id="agent-及-rag-框架">Agent 及 RAG 框架&lt;/h2>
&lt;p>将 LLM 与外部数据和工具结合，可以构建更强大的应用。以下是一些常用的 Agent 及 RAG 框架：&lt;/p>
&lt;h3 id="llamaindex">LlamaIndex&lt;/h3>
&lt;a href="https://github.com/run-llama/llama_index" target="_blank" class="card-github fetch-waiting no-styling"
repo="run-llama/llama_index" id="repo-srP8RmH8ywtZdxNW-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-srP8RmH8ywtZdxNW-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">run-llama&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">llama_index&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-srP8RmH8ywtZdxNW-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-srP8RmH8ywtZdxNW-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-srP8RmH8ywtZdxNW-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-srP8RmH8ywtZdxNW-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-srP8RmH8ywtZdxNW-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-srP8RmH8ywtZdxNW-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/run-llama\/llama_index', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-srP8RmH8ywtZdxNW-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-srP8RmH8ywtZdxNW-language').innerText = data.language;
document.getElementById('repo-srP8RmH8ywtZdxNW-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-srP8RmH8ywtZdxNW-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-srP8RmH8ywtZdxNW-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-srP8RmH8ywtZdxNW-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-srP8RmH8ywtZdxNW-license').classList.add = "no-license"
};
document.getElementById('repo-srP8RmH8ywtZdxNW-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for run-llama\/llama_index.")
}).catch(err => {
const c = document.getElementById('repo-srP8RmH8ywtZdxNW-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for run-llama\/llama_index.")
})
&lt;/script>
&lt;p>LlamaIndex（GPT 索引）是用于 LLM 应用程序的数据框架。使用 LlamaIndex 构建应用程序通常需要使用 LlamaIndex 核心和一组选定的集成（或插件）。在 Python 中使用 LlamaIndex 构建应用程序有两种方法：&lt;/p>
&lt;ul>
&lt;li>启动器： &lt;code>llama-index&lt;/code> ( &lt;a class="link" href="https://pypi.org/project/llama-index/" target="_blank" rel="noopener" >https://pypi.org/project/llama-index/
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
)。Python 入门包，包括核心 LlamaIndex 以及部分集成。&lt;/li>
&lt;li>定制化：&lt;code>llama-index-core&lt;/code> ( &lt;a class="link" href="https://pypi.org/project/llama-index-core/" target="_blank" rel="noopener" >https://pypi.org/project/llama-index-core/
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
)。安装核心 LlamaIndex，并在 LlamaHub 上添加应用程序所需的 LlamaIndex 集成包。目前有 300 多个 LlamaIndex 集成包可与核心无缝协作，让你可以使用自己喜欢的 LLM、嵌入和向量存储数据库进行构建&lt;/li>
&lt;/ul>
&lt;p>LlamaIndex Python 库是以名字命名的，因此包含 &lt;code>core&lt;/code> 的导入语句意味着使用的是核心包。相反，那些不含 &lt;code>core&lt;/code> 的语句则意味着使用的是集成包。&lt;/p>
&lt;pre>&lt;code class="language-python"># typical pattern
from llama_index.core.xxx import ClassABC # core submodule xxx
from llama_index.xxx.yyy import (
SubclassABC,
) # integration yyy for submodule xxx
# concrete example
from llama_index.core.llms import LLM
from llama_index.llms.openai import OpenAI
&lt;/code>&lt;/pre>
&lt;h3 id="crewai">CrewAI&lt;/h3>
&lt;a href="https://github.com/joaomdmoura/crewAI" target="_blank" class="card-github fetch-waiting no-styling"
repo="joaomdmoura/crewAI" id="repo-hkabr3t9ViOa8LXj-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-hkabr3t9ViOa8LXj-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">joaomdmoura&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">crewAI&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-hkabr3t9ViOa8LXj-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-hkabr3t9ViOa8LXj-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-hkabr3t9ViOa8LXj-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-hkabr3t9ViOa8LXj-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-hkabr3t9ViOa8LXj-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-hkabr3t9ViOa8LXj-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/joaomdmoura\/crewAI', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-hkabr3t9ViOa8LXj-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-hkabr3t9ViOa8LXj-language').innerText = data.language;
document.getElementById('repo-hkabr3t9ViOa8LXj-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-hkabr3t9ViOa8LXj-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-hkabr3t9ViOa8LXj-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-hkabr3t9ViOa8LXj-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-hkabr3t9ViOa8LXj-license').classList.add = "no-license"
};
document.getElementById('repo-hkabr3t9ViOa8LXj-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for joaomdmoura\/crewAI.")
}).catch(err => {
const c = document.getElementById('repo-hkabr3t9ViOa8LXj-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for joaomdmoura\/crewAI.")
})
&lt;/script>
&lt;p>CrewAI 是一个构建 AI Agent 的框架，可以将 LLM 与其他工具和 API 集成，实现更复杂的任务，例如自动执行网页操作、生成代码等。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>基于角色的智能体设计&lt;/strong>：你可以使用特定的角色、目标和工具来自定义智能体。&lt;/li>
&lt;li>&lt;strong>自主智能体间委托&lt;/strong>：智能体可以自主地将任务委托给其他智能体，并相互查询信息，从而提高解决问题的效率。&lt;/li>
&lt;li>&lt;strong>灵活的任务管理&lt;/strong>：可以使用可定制的工具来定义任务，并动态地将任务分配给智能体。&lt;/li>
&lt;li>&lt;strong>流程驱动&lt;/strong>：该系统以流程为中心，目前支持按顺序执行任务和分层流程。未来还会支持更复杂的流程，例如协商和自主流程。&lt;/li>
&lt;li>&lt;strong>保存输出为文件&lt;/strong>：可以将单个任务的输出保存为文件，以便以后使用。&lt;/li>
&lt;li>&lt;strong>将输出解析为 Pydantic 或 Json&lt;/strong>：可以将单个任务的输出解析为 Pydantic 模型或 Json 格式，以便于后续处理和分析。&lt;/li>
&lt;li>&lt;strong>支持开源模型&lt;/strong>：可以使用 OpenAI 或其他开源模型来运行您的智能体团队。更多关于配置智能体与模型连接的信息，包括如何连接到本地运行的模型，请参阅&lt;a class="link" href="https://docs.crewai.com/how-to/LLM-Connections/" target="_blank" rel="noopener" >将 crewAI 连接到大型语言模型
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/li>
&lt;/ul>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/joaomdmoura/crewAI" target="_blank" rel="noopener" >CrewAI
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="opendevin">OpenDevin&lt;/h3>
&lt;a href="https://github.com/opendevin/opendevin" target="_blank" class="card-github fetch-waiting no-styling"
repo="opendevin/opendevin" id="repo-ydrKWx46JI48vbFO-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-ydrKWx46JI48vbFO-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">opendevin&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">opendevin&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-ydrKWx46JI48vbFO-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-ydrKWx46JI48vbFO-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-ydrKWx46JI48vbFO-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-ydrKWx46JI48vbFO-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-ydrKWx46JI48vbFO-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-ydrKWx46JI48vbFO-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/opendevin\/opendevin', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-ydrKWx46JI48vbFO-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-ydrKWx46JI48vbFO-language').innerText = data.language;
document.getElementById('repo-ydrKWx46JI48vbFO-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-ydrKWx46JI48vbFO-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-ydrKWx46JI48vbFO-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-ydrKWx46JI48vbFO-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-ydrKWx46JI48vbFO-license').classList.add = "no-license"
};
document.getElementById('repo-ydrKWx46JI48vbFO-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for opendevin\/opendevin.")
}).catch(err => {
const c = document.getElementById('repo-ydrKWx46JI48vbFO-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for opendevin\/opendevin.")
})
&lt;/script>
&lt;p>OpenDevin 是一个由人工智能和 LLMs 驱动的自主软件工程师平台。&lt;/p>
&lt;p>OpenDevin 智能体与人类开发人员合作编写代码、修复错误和发布功能。&lt;/p>
&lt;p>详细信息，请访问 &lt;a class="link" href="https://github.com/opendevin/opendevin" target="_blank" rel="noopener" >OpenDevin
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h2 id="模型评测">模型评测&lt;/h2>
&lt;p>为了选择合适的 LLM 并评估其性能，我们需要进行模型评测：&lt;/p>
&lt;h3 id="lmsys">LMSys&lt;/h3>
&lt;p>LMSys Org 是由加州大学伯克利分校的学生和教师与加州大学圣地亚哥分校以及卡内基梅隆大学合作成立的开放式研究组织。&lt;/p>
&lt;p>目标是通过共同开发开放模型、数据集、系统和评估工具，使大型模型对每个人都可访问。训练大型语言模型并广泛提供它们的应用，同时也在开发分布式系统以加速它们的训练和推理过程。&lt;/p>
&lt;p>目前，LMSys Chatbot Area 是最被认可的大模型排行榜之一，受多家公司和研究机构的认可。&lt;/p>
&lt;p>排行榜地址：&lt;a class="link" href="https://arena.lmsys.org/" target="_blank" rel="noopener" >https://arena.lmsys.org/
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h3 id="opencompass">OpenCompass&lt;/h3>
&lt;p>OpenCompass 是一个 LLM 评估平台，支持 100 多个数据集上的各种模型（Llama3、Mistral、InternLM2、GPT-4、LLaMa2、Qwen、GLM、Claude 等）。&lt;/p>
&lt;a href="https://github.com/open-compass/opencompass" target="_blank" class="card-github fetch-waiting no-styling"
repo="open-compass/opencompass" id="repo-mECNiKK751yJ9B5A-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-mECNiKK751yJ9B5A-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">open-compass&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">opencompass&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-mECNiKK751yJ9B5A-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-mECNiKK751yJ9B5A-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-mECNiKK751yJ9B5A-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-mECNiKK751yJ9B5A-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-mECNiKK751yJ9B5A-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-mECNiKK751yJ9B5A-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/open-compass\/opencompass', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-mECNiKK751yJ9B5A-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-mECNiKK751yJ9B5A-language').innerText = data.language;
document.getElementById('repo-mECNiKK751yJ9B5A-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-mECNiKK751yJ9B5A-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-mECNiKK751yJ9B5A-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-mECNiKK751yJ9B5A-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-mECNiKK751yJ9B5A-license').classList.add = "no-license"
};
document.getElementById('repo-mECNiKK751yJ9B5A-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for open-compass\/opencompass.")
}).catch(err => {
const c = document.getElementById('repo-mECNiKK751yJ9B5A-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for open-compass\/opencompass.")
})
&lt;/script>
&lt;h3 id="open-llm-leaderboard">Open LLM Leaderboard&lt;/h3>
&lt;p>Open LLM Leaderboard 是一个持续更新的 LLM 排行榜，根据多个评测指标对不同模型进行排名，方便开发者了解最新的模型性能和发展趋势。&lt;/p>
&lt;p>排行榜地址：&lt;a class="link" href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard" target="_blank" rel="noopener" >https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>LLM 生态正在蓬勃发展，涵盖了从模型训练到应用落地的各个环节。相信随着技术的不断进步，LLM 将会在更多领域发挥重要作用，为我们带来更加智能的应用体验。&lt;/p></description></item><item><title>RDMA 之 Memory Window</title><link>https://cuterwrite.top/p/rdma-memory-window/</link><pubDate>Wed, 26 Jun 2024 23:55:00 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-memory-window/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116373724_p0_master1200.webp" alt="Featured image of post RDMA 之 Memory Window" />&lt;h1 id="rdma-之-memory-window">RDMA 之 Memory Window&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/353590347">&lt;cite>知乎专栏：14. RDMA 之 Memory Window&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>&lt;strong>本文是“RDMA 杂谈”专栏文章的第 14 篇，欢迎转载，转载请注明出处&lt;/strong>&lt;/p>
&lt;p>我们在 &lt;a class="link" href="https://cuterwrite.top/p/rdma-mr/" >【RDMA 之 Memory Region】
&lt;/a>
一文中介绍过 Memory Region，它是一片由用户注册的特殊的内存区域：一方面其中的内容不会被换页到硬盘中，另一方面 RDMA 网卡中记录了它的地址转换关系，使得硬件拿到用户指定在 WR 中的虚拟地址之后找到对应的物理地址。&lt;/p>
&lt;p>本文我们来讲解 Memory Window 的概念，它是一种基于 Memory Region 的、更灵活的内存管理单元。除了 MW 的概念之外，本文也会更详细的介绍一些 RDMA 领域的内存相关概念，比如 L_Key/R_Key 等。本文配合 &lt;a class="link" href="https://cuterwrite.top/p/rdma-mr/" >【RDMA 之 Memory Region】
&lt;/a>
阅读效果更佳，建议先读者温习一下。&lt;/p>
&lt;h2 id="memory-window-是什么">Memory Window 是什么&lt;/h2>
&lt;p>Memory Window 简称 MW，中文就翻译成内存窗口吧。是一种由用户申请的，用于让远端节点访问本端内存区域的 RDMA 资源。每个 MW 都会绑定（称为 bind）在一个已经注册的 MR 上，但是它相比于 MR 可以提供更灵活的权限控制。MW 可以粗略理解为是 MR 的子集，一个 MR 上可以划分出很多 MW，每个 MW 都可以设置自己的权限。MW 和 MR 的关系如下图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_1.webp"
alt="2024-06-28_12_1" width="30%" loading="lazy">&lt;figcaption>
&lt;h4>MR 与 MW 的关系&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h2 id="内存的访问权限控制">内存的访问权限控制&lt;/h2>
&lt;p>为了后文说明为何设计 MW，我们先来把 MR 和 MW 都涉及的权限控制讲解一下。&lt;/p>
&lt;h3 id="mrmw-的权限配置">MR/MW 的权限配置&lt;/h3>
&lt;p>这里的权限，指的是本端/对端节点，对于本端内存的读/写权限，它们两两组合形成了四种权限：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>本端&lt;/th>
&lt;th>对端&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>读&lt;/td>
&lt;td>Local Read&lt;/td>
&lt;td>Remote Read&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>写&lt;/td>
&lt;td>Local Write&lt;/td>
&lt;td>Remote Write&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>除了这四种权限之外，还有 Atomic 权限等，不在本文讨论范围内。&lt;/p>
&lt;p>上表中这四种权限中最低的是本地读（Local Read），是用户必须赋予 MR/MW 的权限，因为如果一块内存本地的用户都无法访问的话，那就失去意义了；另外还有个限制，如果某个 MR 需要配置远端写（Remote Write）或者还没介绍的远端原子操作权限（Remote Atomic），那么也一定要配置本地写（Local Write）权限。在此约束之下，每个 MR 或者 MW 都可以按需配置权限，比如我们注册的一个 MR 需要允许远端节点写入数据，而不允许读，那么我们就打开 Remote Write 权限，关闭 Remote Read 权限。这样 HCA（网卡）收到对端发起的对这个 MR 范围内的某个地址的 WRITE 请求之后，就可以予以放行；而 HCA 收到对端对这个 MR 的 READ 操作时，就会拒绝这个请求，并返回错误信息给对端。&lt;/p>
&lt;h3 id="memory-key">Memory Key&lt;/h3>
&lt;p>上述的访问权限配置，并不能杜绝恶意用户对于本地或者远端内存的访问。比如某个节点给了一块内存区域的 Remote Write 权限，那么岂不是任意远端节点（进程）只要传入了合法的地址信息，都可以对这片区域进行写入了？因此，IB 规范设计了 Memory Key，简单理解它就是访问 MR 的钥匙机制，只有持有正确的钥匙，才能打开 MR/MW 的大门。&lt;/p>
&lt;p>Key 是一串数字，由两部分组成：24bit 的 Index 以及 8bit 的 Key：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_2.webp"
alt="2024-06-28_12_2" width="60%" loading="lazy">&lt;figcaption>
&lt;h4>L_Key/R_Key 的组成&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>其中，Index 用于 HCA 快速索引到本地的虚拟-物理地址转换表等 MR 相关的信息，而 Key 用于校验整个字段的合法性，以防止未授权的用户任意传递 Index。&lt;/p>
&lt;p>Memory Key 按照用途分为两种，Local Key 和 Remote Key：&lt;/p>
&lt;h4 id="l_key">L_Key&lt;/h4>
&lt;p>即 Local Key，关联到一个 MR 上，用于 HCA 访问本端内存。当本端的某个进程试图使用一个已经注册的 MR 的内存时，HCA 会校验其传递的 L_Key。并且利用 L_Key 中的索引查找地址转换表，把虚拟地址翻译成物理地址然后访问内存。&lt;/p>
&lt;p>我们在 &lt;a class="link" href="https://cuterwrite.top/p/rdma-shared-receive-queue/" >【RDMA 之 Shared Receive Queue】
&lt;/a>
一文中描述过 sge，sge 由起始地址、长度和秘钥组成。用户在填写 WR 时，如果需要 HCA 访问本端内存，那么就需要通过一个 sge 的链表（sgl）来描述内存块，这里 sge 的秘钥填的就是 L_Key，也就是下图中的 key1 和 key3，他们分别是 MR1 的 L_Key 和 MR2 的 L_Key。如果没有 L_Key，那么任何一个本地用户进程都可以指挥硬件访问其他本地用户注册的 MR 的内容，硬件也难以高效的将虚拟地址翻译成物理地址。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_3.webp"
alt="2024-06-28_12_3" width="50%" loading="lazy">&lt;figcaption>
&lt;h4>L_Key 的作用&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h4 id="r_key">R_Key&lt;/h4>
&lt;p>即 Remote Key，关联到一个 MR 或者 MW 上，用于远端节点访问本端内存。当远端节点试图访问本端的内存时，一方面本端的 HCA 会校验 R_Key 是否合法，另一方面会利用 R_Key 中的索引查地址转换表，把虚拟地址翻译成物理地址然后访问内存。&lt;/p>
&lt;p>凡是 RDMA 操作（即 Write/Read/Atomic），用户都要在 WR 中携带远端内存区域的 R_Key。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_4.webp"
alt="2024-06-28_12_4" width="70%" loading="lazy">&lt;figcaption>
&lt;h4>R_Key 的作用&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>IB 规范通过上述两种机制，来确保 MR 可以按照用户的期望被正确且安全的访问。我们用一个比喻来总结下 MR/MW 权限控制相关的内容：&lt;/p>
&lt;p>A 给自己的房间（MR）配了两把钥匙（Memory Key），一把留作自用（L_Key），另一把钥匙（R_Key）邮寄（可以是任何通信方式）给了 B。B 可以在 A 不在家的时候（本端 CPU 不感知远端节点对本地内存的 RDMA 操作），通过钥匙（R_Key）打开门。打开门之后，可能 B 只能隔着玻璃查看房间的摆设（A 只给了这个 MR 远程读权限），或者进入房间内发现漆黑一片什么也看不到，但是可以向房间里放物品（A 只给了这个 MR 远程写权限），当然也有可能没有玻璃也开了灯（同时给了远程读写权限）。&lt;/p>
&lt;h2 id="为什么要有-mw">为什么要有 MW&lt;/h2>
&lt;p>简而言之，设计 MW 的目的就是想更灵活的控制内存的远程访问权限。&lt;/p>
&lt;p>&lt;a class="link" href="https://cuterwrite.top/p/rdma-mr/" >【RDMA 之 Memory Region】
&lt;/a>
一文中我们介绍过用户注册 MR 的过程，需要从用户态陷入内核态，调用内核提供的函数 pin 住内存（防止换页），然后制作虚拟-物理地址映射表并下发给硬件。&lt;/p>
&lt;p>因为 MR 是由内核管理的，如果用户想修改一个已经存在的 MR 的信息，比如我想收回某个 MR 的远端写权限，只保留远端读权限；或者想要使一个之前已经授权给远端节点的 R_Key 失效，那么用户需要通过重注册 MR（Reregister MR）接口来进行修改，该接口等价于先取消注册 MR（Deregister MR），然后注册 MR（Register MR）。&lt;strong>上述流程需要陷入内核态来完成，而这个过程是耗时较长的&lt;/strong>。&lt;/p>
&lt;p>不同于需要通过控制路径修改权限的 MR，&lt;strong>MW 在创建好之后，可以通过数据路径（即通过用户态直接下发 WR 到硬件的方式）动态的绑定到一个已经注册的 MR 上，并同时设置或者更改其访问权限，这个过程的速度远远超过重新注册 MR 的过程&lt;/strong>。&lt;/p>
&lt;p>那么现在为了使一片内存能够被远端节点进行 RDMA WRITE/READ 操作，我们就拥有了注册 MR 以及注册 MW 然后绑定到一个已注册的 MR 两种方式，它们都会产生一个 R_Key 来提供给远端节点。前一种方式准备阶段的步骤简单，但是不够灵活，一旦注册之后修改起来会比较麻烦；后一种方式相比前一种多了注册 MW 和绑定 MW 到 MR 两种操作，但是可以方便迅速的控制远端访问权限。&lt;/p>
&lt;h2 id="mw-和-mr-权限的关系">MW 和 MR 权限的关系&lt;/h2>
&lt;p>也许有的读者会想到，MR 申请时配置了自己的权限，MW 绑定到 MR 时也会配置自己的权限，这两者的权限是什么关系呢？IB 规范在 10.6.7.2.2 节有专门介绍：&lt;/p>
&lt;blockquote>
&lt;p>When binding a Memory Window, a Consumer can request any combination of remote access rights for the Window. However, if the associated Region does not have local write access enabled and the Consumer requests remote write or remote atomic access for the Window, the Channel Interface must return an error either at bind time or access time.&lt;/p>
&lt;/blockquote>
&lt;p>总结来说，&lt;strong>如果想要给 MW 配置远程写或者远程原子操作（Atomic）权限，那么它绑定到的 MR 必须有本地写权限，其他情况下两者权限互不干扰&lt;/strong>：远端用户用 MW，就要遵循 MW 的权限配置；远端用户用 MR，就要遵循 MR 的权限配置。&lt;/p>
&lt;h2 id="用户接口">用户接口&lt;/h2>
&lt;p>老样子，用户接口时我们按照控制路径和数据路径来分类：&lt;/p>
&lt;h3 id="控制路径">控制路径&lt;/h3>
&lt;p>MW 支持增、删和查，不能直接修改：&lt;/p>
&lt;h4 id="创建allocate-mw">创建——Allocate MW&lt;/h4>
&lt;p>申请 MW，主要是创建 MW 相关的软件结构和让硬件做好准备，用户需要指定后文中介绍的 MW 的类型。这个接口会产生一个 Memory Window 的句柄，用户以后可以用这个句柄指代这个 MW。&lt;/p>
&lt;p>注意此时 MW 没有绑定到 MR 上，处于不可从远端访问的状态。&lt;/p>
&lt;h4 id="删除deallocate-mw">删除——Deallocate MW&lt;/h4>
&lt;p>取消注册 MW。很好理解，就是销毁相关资源。&lt;/p>
&lt;h4 id="查询query-mw">查询——Query MW&lt;/h4>
&lt;p>查询 MW 的信息，包括 R_Key 及其状态、MW 类型以及 PD 等。&lt;/p>
&lt;p>需要再次强调的是，虽然这个 Verbs 在 IB 规范中有描述，但是并没有在 RDMA 软件栈中实现相关的 API。类似情况的 Verbs 接口还有不少，RDMA 软件栈以实用为原则，没有用户需求的接口一般都没有实现。&lt;/p>
&lt;h3 id="数据路径">数据路径&lt;/h3>
&lt;p>MW 在数据路径有一套独特的接口，分为 Bind 和 Invalidate 两类：&lt;/p>
&lt;h4 id="绑定bind">绑定——Bind&lt;/h4>
&lt;p>Bind(ing)意为“绑定”，指的是将一个 MW“关联”到一个已经注册的 MR 的指定范围上，并配置一定的读写权限。绑定的结果会产生一个 R_key，用户可以把这个 R_Key 传递给远端节点用于远程访问。注意一个 MW 可以被多次绑定，一个 MR 上也可以绑定多个 MW。如果一个 MR 还有被绑定的 MW，那么这个 MR 是不能被取消注册的。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_5.webp"
alt="2024-06-28_12_5" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>Bind 的软硬件交互&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>Bind 有两种方式，一种是调用 Post Send 接口下发 Bind MW WR，一种是调用 Bind MW 接口。&lt;/p>
&lt;ul>
&lt;li>Post Send Bind MW WR&lt;/li>
&lt;/ul>
&lt;p>前文我们讲过，相比于 MR，MW 最大的优势就是可以从数据路径快速的配置权限。Post Send Bind MW WR 操作，指的就是用户通过 post send 接口（比如 ibv_post_send()）下发一个 WR 到 SQ 中，这个 WR 的操作类型（比如 SEND/RDMA WRITE/RDMA READ）被指定为 BIND MW，此外 WR 中还携带有权限和要绑定到的 MR 的范围信息。与其他 WR 不同，下发 Bind MW 的 WR 之后，硬件并不会发送任何数据包，而是将 MW 绑定到了指定 MR 上。&lt;/p>
&lt;p>这种方式仅适用于后文介绍的 Type 2 的 MW。&lt;/p>
&lt;ul>
&lt;li>Bind MW&lt;/li>
&lt;/ul>
&lt;p>虽然这是一个独立的接口，但是实际是在 Post Send Bind MW WR 外面又封装了一层。用户传入 MW 绑定的相关信息，包括权限及要绑定的 MR 的信息，驱动程序负责组装和下发 WR 到硬件中。该接口成功后，会将新生成的 R_Key 返回给用户。&lt;/p>
&lt;p>这种方式仅适用于后文介绍的 Type 1 的 MW。&lt;/p>
&lt;p>上述两种操作的关系是这样的：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_6.webp"
alt="2024-06-28_12_6" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>两种 Bind 操作的关系&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h4 id="无效化invalidate">无效化——Invalidate&lt;/h4>
&lt;p>Invalidate 意为无效化，指的是用户通过下发一个带有 Invalidate 操作码的 WR 到硬件而使一个 R_Key 无效的操作。&lt;/p>
&lt;p>&lt;strong>需要强调的是，Invalidate 操作的对象是 R_Key 而不是 MW 本身，即 Invalidate 之后的效果是：远端用户无法再使用这个 R_Key 访问对应的 MW，而 MW 资源仍然存在，以后仍然可以生成新的 R_Key 给远端使用。&lt;/strong>&lt;/p>
&lt;p>Invalidate 操作只能用于下文介绍的 Type 2 的 MW。&lt;/p>
&lt;p>按照 Invalidate 操作的发起方不同，又可以进一步分成两种：&lt;/p>
&lt;ul>
&lt;li>Local Invalidate&lt;/li>
&lt;/ul>
&lt;p>本地无效操作。上层用户如果想在不回收 MW 资源的情况下，收回某个远端的用户的 R_Key 的权限。那么就可以下发一个 Local Invalidate 操作到 SQ 中，硬件收到之后会对相应的 MR 的配置进行修改。成功执行之后，如果持有这个 R_Key 的远端用户想要对 MW 进行 RDMA 操作，将会被本地的硬件拒绝并返回错误。&lt;/p>
&lt;p>因为是本地操作，所以硬件收到这个 WR 之后也不会发送消息到链路上。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_7.webp"
alt="2024-06-28_12_7" width="60%" loading="lazy">&lt;figcaption>
&lt;h4>Local Invalidate 操作的软硬件交互&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>Remote Invalidate&lt;/li>
&lt;/ul>
&lt;p>远端无效操作。当一个远端用户不再使用一个 R_Key 之后，可以主动发送消息，让本端回收这个 R_Key。远端用户下发一个带有此操作码的 WR 到 SQ 中，其硬件收到后，将会组装一个报文并发送到本端。本端硬件收到远端的 Remote Invalidate 操作之后，将会把对应的 R_Key 置为不可用状态。同 Local Invalidate 一样，此后对端将无法使用这个 R_Key 对对应的 MW 进行 RDMA 操作。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_8.webp"
alt="2024-06-28_12_8" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>Remote Invalidate 操作的软硬件交互&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h2 id="mw-的类型">MW 的类型&lt;/h2>
&lt;p>根据实现和应用场景的不同，IB 规范对 MW 进行了分类：&lt;/p>
&lt;h3 id="type-1">Type 1&lt;/h3>
&lt;p>Type 1 的 MW 通过 PD 和一个 QP 关联，不会绑定到一个 QP 上，所以也不会影响销毁同一个 PD 下的 QP。&lt;/p>
&lt;p>Type 1 的 MW 的 R_Key 的 key 域段由驱动和硬件掌握，这里“掌握”的意思是，由驱动和硬件分配 key，而不是上层用户。这也是前文中说 Type 1 的 MW 不能被执行 Invalidate 操作的原因。如果 Type 1 MW 的用户想要使一个 R_Key 失效，那么重新通过 Bind MW 接口绑定一次这个 MW，硬件或者驱动就回自动分配一个新的 R_Key 的 key 域段，原有的 R_Key 也就失效了。&lt;/p>
&lt;p>此外，如果用户暂时想要使一个 MW 不再绑定到任何 MR，但是又想保留相关的资源而不是销毁这个 MW，那么可以通过调用 Bind MW 接口，并将 MW 长度设置为 0 来实现。&lt;/p>
&lt;p>IB 规范允许多个 Type 1 MW 绑定到同一个 MR 上，范围可以相互覆盖。&lt;/p>
&lt;h3 id="type-2">Type 2&lt;/h3>
&lt;p>Type 2 的 MW 赋予了用户更大的自由度，其 R_Key 的 key 域段由用户掌握，即用户想怎么分配就怎么分配。前文已经讲过，用户通过 Post Send Bind MW WR 操作来进行绑定，这个过程并不会返回 R_Key。用户必须记住 Allocate MW 时的 index，并且和其选择的 8 bit key 组成 R_Key 并发送给对端。&lt;/p>
&lt;p>用户可以通过前文介绍过的 Invalidate 操作来使一个 R_Key 无效，如果想要分配一个新的 R_Key 到 MW 上，必须先通过 Invalidate 操作无效之前的 R_Key。&lt;/p>
&lt;p>与 Type 1 不同，Type 2 的 MW 不支持 0 长度的绑定。&lt;/p>
&lt;p>IB 规范同样也允许多个 Type 2 绑定到同一个 MR 上，范围可以相互覆盖。&lt;/p>
&lt;p>此外，根据绑定关系不同，Type 2 还可以分为两种实现方式，它们的差异仅在于和 QP 的绑定关系上。&lt;/p>
&lt;h4 id="type-2a">Type 2A&lt;/h4>
&lt;p>通过 QPN 和一个 QP 关联，也就是说远端访问这个 MW 范围内的内存时候，除了 R_Key 之外，还必须指定正确的 QPN。如果一个 QP 上还有绑定的 Type 2A 的 MW，那么这个 QP 不可以被销毁。&lt;/p>
&lt;h4 id="type-2b">Type 2B&lt;/h4>
&lt;p>通过 QPN 和 PD 与一个 QP 关联，比 Type 2A 多了个 PD 的校验，即远端通过 RDMA 操作访问 MW 的内存时，除了 QPN 要正确之外，其指定的本端 QP 的 PD 要与绑定这个 MW 时的 PD 相同。另外，与 Type 2A 不同，QP 如果还有 Type 2B MW 绑定关系时是可以被销毁的。&lt;/p>
&lt;p>这里 IB 规范中原有的介绍就比较分散，我们来简单总结一下几种 MW 的异同：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Type 1&lt;/th>
&lt;th>Type 2A&lt;/th>
&lt;th>Type 2B&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>关联关系&lt;/td>
&lt;td>PD&lt;/td>
&lt;td>QP&lt;/td>
&lt;td>PD + QP&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>R_Key 的 key 域归属&lt;/td>
&lt;td>驱动+硬件&lt;/td>
&lt;td>用户&lt;/td>
&lt;td>用户&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>绑定方式&lt;/td>
&lt;td>Bind MW 绑定后之前的 R_Key 自动失效&lt;/td>
&lt;td>Post Send Bind MWWR 绑定前需要先使之前的 R_Key 无效化&lt;/td>
&lt;td>Post Send Bind MWWR 绑定前需要先使之前的 R_Key 无效化&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>是否支持零长度&lt;/td>
&lt;td>是&lt;/td>
&lt;td>否&lt;/td>
&lt;td>否&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>是否支持 Invalidate&lt;/td>
&lt;td>否&lt;/td>
&lt;td>是&lt;/td>
&lt;td>是&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>关联的 QP 是否可以被销毁&lt;/td>
&lt;td>-&lt;/td>
&lt;td>否&lt;/td>
&lt;td>是&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>此外，IB 规范中对上述几种类型还有如下描述：HCA 必须实现 Type 1 的 MW，另外可以仅选择实现 Type 2A 和 2B 中的一种。Type 1 和 Type 2 的 MW 可以同时关联到同一个 MR 上。因为我了解到的应用程序中使用 MW 的情况不多，所以具体在什么场景下应该使用哪种 MW 也说不出所以然来，如果读者有对这方面的了解欢迎一起交流。&lt;/p>
&lt;p>好了，MW 就讲到这里，到此为止 RDMA 技术中常见的资源就都介绍完了。&lt;/p>
&lt;p>鉴于一般支持 RDMA 的设备都比较昂贵，下一篇我将介绍如何通过软件模拟设备的方式——即 Soft-RoCE 进行一些编程实验。&lt;/p>
&lt;h2 id="ib-规范相关章节">IB 规范相关章节&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>3.5.3 Memory Keys 介绍&lt;/p>
&lt;/li>
&lt;li>
&lt;p>9.4.1.1 Invalidate 操作&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.6.7 权限管理&lt;/p>
&lt;/li>
&lt;li>
&lt;p>11.2.10.9~12 相关 Verbs 介绍&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="参考文档">参考文档&lt;/h2>
&lt;p>[1] IB Specification Vol 1-Release-1.4&lt;/p>
&lt;p>[2] Linux Kernel Networking - Implementation and Theory. Chapter 13&lt;/p></description></item><item><title>RDMA 之 Shared Receive Queue</title><link>https://cuterwrite.top/p/rdma-shared-receive-queue/</link><pubDate>Wed, 26 Jun 2024 23:34:00 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-shared-receive-queue/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116373922_p0_master1200.webp" alt="Featured image of post RDMA 之 Shared Receive Queue" />&lt;h1 id="rdma-之-shared-receive-queue">RDMA 之 Shared Receive Queue&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/279904125">&lt;cite>知乎专栏：11. RDMA 之 Shared Receive Queue&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>我们曾在 &lt;a class="link" href="https://cuterwrite.top/p/rdma-element/" >【3. RDMA 基本元素】
&lt;/a>
中简单介绍了 SRQ 的概念，本文将带大家了解更多关于 SRQ 的细节。&lt;/p>
&lt;h2 id="基本概念">基本概念&lt;/h2>
&lt;h3 id="什么是-srq">什么是 SRQ&lt;/h3>
&lt;p>全称为 Shared Receive Queue，直译为共享接收队列。我们知道，RDMA 通信的基本单位是 QP，每个 QP 都由一个发送队列 SQ 和接收队列 RQ 组成。&lt;/p>
&lt;p>SRQ 是 IB 协议为了给接收端节省资源而设计的。我们可以把一个 RQ 共享给所有关联的 QP 使用，这个公用的 RQ 就称为 SRQ。当与其关联的 QP 想要下发接收 WQE 时，都填写到这个 SRQ 中。然后每当硬件接收到数据后，就根据 SRQ 中的下一个 WQE 的内容把数据存放到指定位置。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_11_1.webp"
alt="2024-06-28_11_1" width="80%" loading="lazy">
&lt;/figure>
&lt;h3 id="为什么要用-srq">为什么要用 SRQ&lt;/h3>
&lt;p>通常情况下，我们向 SQ 中下任务的数量要远远超过向 RQ 中下发任务的数量。为什么呢？请先回忆一下哪些操作类型会用到 SQ，哪些又会用到 RQ。&lt;/p>
&lt;p>SEND/WRITE/READ 都需要通信发起方向 SQ 中下发一个 WR，而只有和 SEND 配合的 RECV 操作才需要通信响应方下发 WR 到 RQ 中（带立即数的 Write 操作也会消耗 Receive WR，我们还没讲到）。而我们又知道，SEND-RECV 这一对操作通常都是用于传递控制信息，WRITE 和 READ 才是进行大量远端内存读写操作时的主角，所以自然 SQ 的使用率是远远高于 RQ 的。&lt;/p>
&lt;p>每个队列都是有实体的，占用着内存以及网卡的片上存储空间。在商用场景下，QP 的数量是可能达到十万级甚至更高的，对内存容量提出了很高的要求，内存都是白花花的银子买的，SRQ 就是 IB 协议为了节省用户的内存而设计的一种机制。&lt;/p>
&lt;p>来看一下协议中对为什么要使用 SRQ 的官方解释（10.2.9.1 章节）：&lt;/p>
&lt;blockquote>
&lt;p>Without SRQ, an RC, UC or UD Consumer must post the number of receive WRs necessary to handle incoming receives on a given QP. If the Consumer cannot predict the incoming rate on a given QP, because, for example, the connection has a bursty nature, the Consumer must either: post a sufficient number of RQ WRs to handle the highest incoming rate for each connection, or, for RC, let message flow control cause the remote sender to back off until local Consumer posts more WRs.&lt;/p>
&lt;p>• Posting sufficient WRs on each QP to hold the possible incoming rate, wastes WQEs, and the associated Data Segments, when the Receive Queue is inactive. Furthermore, the HCA doesn’t provide a way of reclaiming these WQEs for use on other connections.&lt;/p>
&lt;p>• Letting the RC message flow control cause the remote sender to back off can add unnecessary latencies, specially if the local Consumer is unaware that the RQ is starving.&lt;/p>
&lt;/blockquote>
&lt;p>简单来说，就是没有 SRQ 的情况下，因为 RC/UC/UD 的接收方不知道对端什么时候会发送过来多少数据，所以必须做好最坏的打算，做好突发性收到大量数据的准备，也就是向 RQ 中下发足量的接收 WQE；另外 RC 服务类型可以利用流控机制来反压发送方，也就是告诉对端”我这边 RQ WQE 不够了“，这样发送端就会暂时放缓或停止发送数据。&lt;/p>
&lt;p>但是正如我们前文所说，第一种方法由于是为最坏情况准备的，大部分时候有大量的 RQ WQE 处于空闲状态未被使用，这对内存是一种极大地浪费；第二种方法虽然不用下发那么多 RQ WQE 了，但是流控是有代价的，即会增加通信时延。&lt;/p>
&lt;p>而 SRQ 通过允许很多 QP 共享接收 WQE（以及用于存放数据的内存空间）来解决了上面的问题。当任何一个 QP 收到消息后，硬件会从 SRQ 中取出一个 WQE，根据其内容存放接收到的数据，然后硬件通过 Completion Queue 来返回接收任务的完成信息给对应的上层用户。&lt;/p>
&lt;p>我们来看一下使用 SRQ 比使用普通的 RQ 可以节省多少内存&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>：&lt;/p>
&lt;p>假设接受数据的节点上有 N 对 QP，并且每个 QP 都可能在随机的时间收到连续的 M 个消息（每个消息都需要消耗一个 RQ 中的 WQE），&lt;/p>
&lt;ul>
&lt;li>如果不使用 SRQ 的话，用户一共需要下发 N * M 个 RQ WQE。&lt;/li>
&lt;li>如果使用 SRQ 的话，用户只需要下发 K * M 个 RQ WQE，而 K 远小于 N。&lt;/li>
&lt;/ul>
&lt;p>这个 K 是可以由用户根据业务来配置的，如果存在大量的并发接收的情况，那么就把 K 设置大一点，否则 K 设置成个位数就足够应付一般的情况了。&lt;/p>
&lt;p>我们一共节省了 (N - K) * M 个 RQ WQE，RQ WQE 本身其实不是很大，大约在几个 KB 的样子，看起来好像占不了多少内存。但是如前文所说，实际上节省的还有用于&lt;strong>存放数据的内存空间&lt;/strong>，这可是很大一块内存了，我们用图来说明：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_11_2.webp"
alt="2024-06-28_11_2" width="80%" loading="lazy">
&lt;/figure>
&lt;p>上图中的 SRQ 中有两个 RQ WQE，我们看一下 RQ WQE 的内容，它们是由数个 sge（Scatter/Gather Element）组成的，每个 sge 由一个内存地址，长度和秘钥组成。有了起始地址和长度，sge 就可以指向一块连续的内存区域，那么多个 sge 就可以表示多个彼此离散的连续内存块，我们称多个 sge 为 sgl（Scatter/Gather List）。sge 在 IB 软件协议栈中随处可见（其实在整个 Linux 都很常见），可以用非常少的空间表示非常大的内存区域，IB 的用户都使用 sge 来指定发送和接收区域的。&lt;/p>
&lt;p>可以简单估算下每个 sge 可以指向多大的内存区域，length 是一个 32bit 的无符号整型，可以表示 4GB 的空间。假设一个 RQ WQE 最大可以存放 256 个 sge，那么一个 RQ WQE 一共就是 1TB。当然实际上不可能这么大，这里只是想直观的告诉读者 RQ WQE 背后可能占用着多大的内存空间。&lt;/p>
&lt;h3 id="srqc">SRQC&lt;/h3>
&lt;p>即 SRQ Context。同 QPC 一样，SRQC 是用来告知硬件跟 SRQ 有关的属性的，包括深度、WQE 大小等信息，本文不再赘述了。&lt;/p>
&lt;h3 id="srqn">SRQN&lt;/h3>
&lt;p>即 SRQ Number。同 QP 一样，每个节点中可能存在多个 SRQ，为了标识和区分这些 SRQ，每个 SRQ 都有一个序号，称为 SRQN。&lt;/p>
&lt;h3 id="srq-的-pd">SRQ 的 PD&lt;/h3>
&lt;p>我们在 &lt;a class="link" href="https://cuterwrite.top/p/rdma-protection-domain/" >【7. RDMA 之 Protection Domain】
&lt;/a>
中介绍过 Protection Domain 的概念，它用来隔离不同的 RDMA 资源。每个 SRQ 都必须指定一个自己的 PD，可以跟自己关联的 QP 的 PD 相同，也可以不同；SRQ 之间也可以使用相同的 PD。&lt;/p>
&lt;p>如果在使用 SRQ 的时候，收到了数据包，那么只有在要访问的 MR 和 SRQ 处于同一个 PD 下，才会正常接收这个数据包，否则会产生立即错误。&lt;/p>
&lt;h2 id="异步事件">异步事件&lt;/h2>
&lt;p>我们在 &lt;a class="link" href="https://cuterwrite.top/p/rdma-completion-queue/" >【10. RDMA 之 Completion Queue】
&lt;/a>
一文中介绍过，IB 协议根据错误的上报方式将错误类型分为立即错误，完成错误和异步错误。其中的异步错误类似于中断/事件，所以我们有时候也称其为异步事件。每个 HCA 都会注册一个事件处理函数专门用来处理异步事件，收到异步事件后，驱动程序会对其进行必要的处理和进一步上报给用户。&lt;/p>
&lt;p>关于 SRQ 有一个特殊的异步事件，用来及时通知上层用户 SRQ 的状态，即 SRQ Limit Reached 事件。&lt;/p>
&lt;h3 id="srq-limit">SRQ Limit&lt;/h3>
&lt;p>SRQ 可以设置一个水线/阈值，当队列中剩余的 WQE 数量小于水线时，这个 SRQ 会就上报一个异步事件。提醒用户“队列中的 WQE 快用完了，请下发更多 WQE 以防没有地方接收新的数据”。这个水线/阈值就被称为 SRQ Limit，这个上报的事件就被称为 SRQ Limit Reached。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_11_3.webp"
alt="2024-06-28_11_3" width="30%" loading="lazy">
&lt;/figure>
&lt;p>因为 SRQ 是多个 QP 共享的，所以如果深度比较小的情况下，很有可能突然里面的 WQE 就用完了。所以协议设计了这种机制，来保证用户能够及时干预 WQE 不够的情况。&lt;/p>
&lt;p>上报异步事件之后，SRQ Limit 的值会被硬件重新设置为 0（应该是为了防止一直上报异步事件给上层）。当然用户可以不使用这个机制，只需要将 SRQ Limit 的值设为 0 即可。&lt;/p>
&lt;h2 id="用户接口">用户接口&lt;/h2>
&lt;h3 id="控制面">控制面&lt;/h3>
&lt;p>还是老四样——“增、删、改、查”：&lt;/p>
&lt;ul>
&lt;li>创建——Create SRQ&lt;/li>
&lt;/ul>
&lt;p>创建 SRQ 的时候，跟 QP 一样会申请所有 SRQ 相关的软硬件资源，比如驱动程序会申请 SRQN，申请 SRQC 的空间并向其中填写配置。创建 SRQ 时还必须指定每个 SRQ 的深度（能存放多少 WQE）以及每个 WQE 的最大 sge 数量。&lt;/p>
&lt;ul>
&lt;li>销毁——Destroy SRQ&lt;/li>
&lt;/ul>
&lt;p>销毁 SRQ 的所有相关软硬件资源。&lt;/p>
&lt;ul>
&lt;li>修改——Modify SRQ&lt;/li>
&lt;/ul>
&lt;p>除了 SRQ 深度等属性外，SRQ Limit 的值也是通过这个接口设置的。因为每次产生 SRQ Limit Reached 事件之后，水线的值都会被清零，所以每次都需要用户调用 Modify SRQ 重新设置水线。&lt;/p>
&lt;ul>
&lt;li>查询——Query SRQ&lt;/li>
&lt;/ul>
&lt;p>通常是用来查询水线的配置的。&lt;/p>
&lt;h3 id="数据面">数据面&lt;/h3>
&lt;h3 id="post-srq-receive">Post SRQ Receive&lt;/h3>
&lt;p>跟 Post Receive 一样，就是向 SRQ 中下发接收 WQE，里面包含了作为接收缓冲区的内存块的信息。需要注意的是，&lt;strong>主语是 SRQ，与 QP 没有任何关系&lt;/strong>，现在用户是不关心这个 SRQ 被哪些 QP 关联的。&lt;/p>
&lt;h2 id="srq-和-rq-的区别">SRQ 和 RQ 的区别&lt;/h2>
&lt;p>从功能上来说，SRQ 和 RQ 一样都是用来储存接收任务书的，但是由于 SRQ 的共享性，所以其和 RQ 有一些差异。&lt;/p>
&lt;h3 id="状态机">状态机&lt;/h3>
&lt;p>我们在 &lt;a class="link" href="https://cuterwrite.top/p/rdma-queue-pair/" >【9. RDMA 之 Queue Pair】
&lt;/a>
中介绍过，QP 有着复杂的状态机，不同的状态下 QP 的收发能力存在差异。而 SRQ 只有非错误和错误两种状态：&lt;/p>
&lt;p>无论是哪种状态下，用户都可以向 SRQ 中下发 WQE，但是在错误状态下，相关联的 QP 不能从这个 SRQ 中获得收到的数据。另外在错误状态下，用户也无法查询和修改 SRQ 的属性。&lt;/p>
&lt;p>QP 处于错误状态时，可以通过 Modify QP 来使其回到 RESET 状态，但是对 SRQ 来说，只能通过销毁它来退出错误状态。&lt;/p>
&lt;h3 id="接收流程">接收流程&lt;/h3>
&lt;p>对于一个 QP 来说，RQ 和 SRQ 不能同时使用，两者需选其一，如果对一个已经关联 SRQ 的 QP 的 RQ 下发 WQE，那么会返回一个立即错误。&lt;/p>
&lt;p>下面我们来对比看一下 SRQ 和 RQ 的接收流程。本小结的内容是本文的重点，相信读者看过之后，就对 SRQ 的机制有比较完整的了解了。&lt;/p>
&lt;h3 id="rq-的接收流程">RQ 的接收流程&lt;/h3>
&lt;p>首先，我们重温一下普通 RQ 的接收流程（结合发送端的完整流程请阅读 &lt;a class="link" href="https://cuterwrite.top/p/rdma-op/" >【4. RDMA 操作类型】
&lt;/a>
）一文）：&lt;/p>
&lt;ol start="0">
&lt;li>
&lt;p>创建 QP。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>通过 Post Recv 接口，用户分别向 QP2 和 QP3 的 RQ 下发接收 WQE，WQE 中包含接收到数据后放到哪块内存区域的信息。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>硬件收到数据。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>硬件发现是发给 QP3 的，那么从 QP3 的 RQ 中取出 WQE1，将接收到的数据放到 WQE1 指定的内存区域。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>硬件完成数据存放后，向 QP3 的 RQ 关联的 CQ3 产生一个 CQE，上报任务完成信息。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>用户从 CQ3 中取出 WC（CQE），然后从指定内存区域取走数据。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>硬件收到数据。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>硬件发现是发送给 QP2 的，那么从 QP2 的 RQ 中取出 WQE1，将接收到的数据放到 WQE1 指定的内存区域。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>硬件完成数据存放后，向 QP2 的 RQ 关联的 CQ2 产生一个 CQE，上报任务完成信息。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>用户从 CQ2 中取出 WC（CQE），然后从指定内存区域取走数据。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_11_4.webp"
alt="2024-06-28_11_4" width="90%" loading="lazy">
&lt;/figure>
&lt;h3 id="srq-的接收流程">SRQ 的接收流程&lt;/h3>
&lt;p>而 SRQ 的接收流程有一些区别：&lt;/p>
&lt;ol start="0">
&lt;li>
&lt;p>创建 SRQ1，并创建 QP2 和 QP3，都关联到 SRQ1 上。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>通过 Post SRQ Recv 接口，用户向 SRQ1 中下发两个接收 WQE，WQE 中包含接收到数据后放到哪块内存区域的信息。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>硬件收到数据。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>硬件发现是发给 QP3 的，从 SRQ1 中取出第一个 WQE（现在是 WQE1），根据 WQE 内容存放收到的数据。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>SRQ 中的每个 WQE 是“无主的“，不关联到任何一个 QP，硬件按队列顺序依次取出 WQE 就把数据放到里面了。&lt;/p>
&lt;/blockquote>
&lt;ol start="4">
&lt;li>
&lt;p>硬件发现 QP3 的 RQ 关联的 CQ 是 CQ3，所以向其中产生一个 CQE。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>用户从 CQ3 中取出 CQE，从指定内存区域取走数据。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>细心地读者可能会问，用户下发 WR 时，每个 WR 都指定了一些未来用来存放数据的内存区域。但是 SRQ 是一个池子，里面每个 WQE 都指向了不同的若干段内存区域。用户收到某个 QP 对应的 CQ 中的 WC 后如何知道接收到的数据存放到哪里了呢？&lt;/p>
&lt;p>WC 中其实有 wr_id 信息，告知用户数据放到哪个 WR（WQE）指定的内存区域了，既然 WR 是用户下发的，用户自然知道其指向的具体位置。&lt;/p>
&lt;/blockquote>
&lt;ol start="6">
&lt;li>
&lt;p>硬件收到数据&lt;/p>
&lt;/li>
&lt;li>
&lt;p>硬件发现是发给 QP2 的，从 SRQ1 中取出第一个 WQE（现在是 WQE2），根据 WQE 内容存放收到的数据。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>硬件发现 QP2 的 RQ 关联的 CQ 是 CQ2，所以向其中产生一个 CQE。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>用户从 CQ2 中取出 CQE，从指定内存区域取走数据。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_11_5.webp"
alt="2024-06-28_11_5" width="90%" loading="lazy">
&lt;/figure>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>本文首先介绍了 SRQ 的基本概念，然后是其设计初衷、相关机制和用户接口，最后对比 RQ 描述了 SRQ 的接收流程。在实际业务中，SRQ 的使用率还是蛮高的，希望读者能够深入理解。&lt;/p>
&lt;p>就写到这里吧，感谢阅读。下一篇我将给大家介绍下 Memeory Window。&lt;/p>
&lt;h2 id="协议相关章节">协议相关章节&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>10.2.9 SRQ 的设计思想以及相关操作&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.2.3 SRQ 和 QP 的 PD&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.8.2 关联 SRQ 的 QP 和不使用 SRQ 的 QP 的关系&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.8.5 SRQ 相关的返回 WC&lt;/p>
&lt;/li>
&lt;li>
&lt;p>11.5.2.4 异步事件&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="其他参考资料">其他参考资料&lt;/h2>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>Linux Kernel Networking - Implement and Theory. Chapter 13. Shared Receive Queue&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>RDMA 之 Completion Queue</title><link>https://cuterwrite.top/p/rdma-completion-queue/</link><pubDate>Wed, 26 Jun 2024 23:11:00 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-completion-queue/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116903369_p0_master1200.webp" alt="Featured image of post RDMA 之 Completion Queue" />&lt;h1 id="rdma-之-completion-queue">RDMA 之 Completion Queue&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/259650980">&lt;cite>知乎专栏：10. RDMA 之 Completion Queue&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>我们曾经在前面的文章中简单介绍过 CQ，本文将更深入的讲解关于它的一些细节。阅读本文前，读者可以先温习一下这篇文章： &lt;a class="link" href="https://cuterwrite.top/p/rdma-element/" >【“3. RDMA 基本元素”】
&lt;/a>
。&lt;/p>
&lt;h2 id="基本概念">基本概念&lt;/h2>
&lt;p>我们先回顾下 CQ 的作用。CQ 意为完成队列，它的作用和 WQ（SQ 和 RQ）相反，硬件通过 CQ 中的 CQE/WC 来告诉软件某个 WQE/WR 的完成情况。再次提醒读者，对于上层用户来说一般用 WC，对于驱动程序来说，一般称为 CQE，本文不对两者进行区分。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_1.webp"
alt="2024-06-27_10_1" width="80%" loading="lazy">
&lt;/figure>
&lt;p>CQE 可以看作一份“报告”，其中写明了某个任务的执行情况，其中包括：&lt;/p>
&lt;ul>
&lt;li>本次完成了哪个 QP 的哪一个 WQE 指定的任务（QP Number 和 WR ID）&lt;/li>
&lt;li>本次任务执行了什么操作（Opcode 操作类型）&lt;/li>
&lt;li>本次任务执行成功/失败，失败原因是 XXX（Status 状态和错误码）&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>每当硬件处理完一个 WQE 之后，都会产生一个 CQE 放在 CQ 队列中。如果一个 WQE 对应的 CQE 没有产生，那么这个 WQE 就会一直被认为还未处理完，这意味着什么呢？&lt;/p>
&lt;ul>
&lt;li>涉及从内存中取数据的操作（SEND 和 WRITE）&lt;/li>
&lt;/ul>
&lt;p>在产生 CQE 之前，硬件可能还未发送消息，可能正在发送消息，可能对端有接收到正确的消息。由于内存区域是在发送前申请好的，所以上层软件收到对应的 CQE 之前，其必须认为这片内存区域仍在使用中，不能将所有相关的内存资源进行释放。&lt;/p>
&lt;ul>
&lt;li>涉及向内存中存放数据的操作（RECV 和 READ）&lt;/li>
&lt;/ul>
&lt;p>在产生 CQE 之前，有可能硬件还没有开始写入数据，有可能数据才写了一半，也有可能数据校验出错。所以上层软件在获得 CQE 之前，这段用于存放接收数据的内存区域中的内容是不可信的。&lt;/p>
&lt;p>总之，用户必须获取到 CQE 并确认其内容之后才能认为消息收发任务已经完成。&lt;/p>
&lt;h3 id="何时产生">何时产生&lt;/h3>
&lt;p>我们将按照服务类型（本篇只讲 RC 和 UD）和操作类型来分别说明，因为不同的情况产生 CQE 的时机和含义都不同，建议读者回顾第 4 篇&lt;a class="link" href="https://cuterwrite.top/p/rdma-op/" >【“4. RDMA 基本操作”】
&lt;/a>
和第 5 篇&lt;a class="link" href="https://cuterwrite.top/p/rdma-service-types/" >【“5. RDMA 基本服务类型”】
&lt;/a>
。&lt;/p>
&lt;ul>
&lt;li>可靠服务类型（RC）&lt;/li>
&lt;/ul>
&lt;p>前面的文章说过，&lt;strong>可靠意味着本端关心发出的消息能够被对端准确的接收&lt;/strong>，这是通过 ACK、校验和重传等机制保证的。&lt;/p>
&lt;ul>
&lt;li>SEND&lt;/li>
&lt;/ul>
&lt;p>SEND 操作需要硬件从内存中获取数据，然后组装成数据包通过物理链路发送到对端。对 SEND 来说，Client 端产生 CQE 表示&lt;strong>对端已准确无误的收到数据&lt;/strong>，对端硬件收到数据并校验之后，会回复 ACK 包给发送方。发送方收到这 ACK 之后才会产生 CQE，从而告诉用户这个任务成功执行了。如图所示，左侧 Client 端在红点的位置产生了本次任务的 CQE。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_2.webp"
alt="2024-06-27_10_2" width="50%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>RECV&lt;/li>
&lt;/ul>
&lt;p>RECV 操作需要硬件将收到的数据放到用户 WQE 中指定的内存区域，完成校验和数据存放动作后，硬件就会产生 CQE。如上图右侧 Server 端所示。&lt;/p>
&lt;ul>
&lt;li>WRITE&lt;/li>
&lt;/ul>
&lt;p>对于 Client 端来说，WRITE 操作和 SEND 操作是一样的，硬件会从内存中取出数据，并等待对端回复 ACK 后，才会产生 CQE。差别在于，因为 WRITE 是 RDMA 操作，对端 CPU 不感知，自然用户也不感知，所以上面的图变成了这样：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_3.webp"
alt="2024-06-27_10_3" width="50%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>READ&lt;/li>
&lt;/ul>
&lt;p>READ 和 RECV 有点像，Client 端发起 READ 操作后，对端会回复我们想读取的数据，然后本端校验没问题后，会把数据放到 WQE 中指定的位置。完成上述动作后，本端会产生 CQE。READ 同样是 RDMA 操作，对端用户不感知，自然也没有 CQE 产生。这种情况上图变成了这样：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_4.webp"
alt="2024-06-27_10_4" width="50%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>不可靠服务类型（UD）&lt;/li>
&lt;/ul>
&lt;p>因为不可靠的服务类型没有重传和确认机制，所以产生 CQE 表示硬件&lt;strong>已经将对应 WQE 指定的数据发送出去了&lt;/strong>。以前说过 UD 只支持 SEND-RECV 操作，不支持 RDMA 操作。所以对于 UD 服务的两端，CQE 产生时机如下图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_5.webp"
alt="2024-06-27_10_5" width="50%" loading="lazy">
&lt;/figure>
&lt;h3 id="wq-和-cq-的对应关系">WQ 和 CQ 的对应关系&lt;/h3>
&lt;p>&lt;strong>每个 WQ 都必须关联一个 CQ，而每个 CQ 可以关联多个 SQ 和 RQ。&lt;/strong>&lt;/p>
&lt;p>这里的所谓“关联”，指的是一个 WQ 的所有 WQE 对应的 CQE，都会被硬件放到绑定的 CQ 中，需要注意同属于一个 QP 的 SQ 和 RQ 可以各自关联不同的 CQ。如下图所示，QP1 的 SQ 和 RQ 都关联了 CQ1，QP2 的 RQ 关联到了 CQ1、SQ 关联到了 CQ2。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_6.webp"
alt="2024-06-27_10_6" width="auto" loading="lazy">
&lt;/figure>
&lt;p>因为每个 WQ 必须关联一个 CQ，所以用户创建 QP 前需要提前创建好 CQ，然后分别指定 SQ 和 RQ 将会使用的 CQ。&lt;/p>
&lt;p>&lt;strong>同一个 WQ 中的 WQE，其对应的 CQE 间是保序的&lt;/strong>&lt;/p>
&lt;p>硬件是按照“先进先出”的 FIFO 顺序从某一个 WQ（SQ 或者 RQ）中取出 WQE 并进行处理的，而向 WR 关联的 CQ 中存放 CQE 时，也是遵从这些 WQE 被放到 WQ 中的顺序的。简单来说，就是谁先被放到队列里，谁就先被完成。该过程如下图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_7.webp"
alt="2024-06-27_10_7" width="auto" loading="lazy">
&lt;/figure>
&lt;p>需要注意的是，使用 SRQ 的情况以及 RD 服务类型的 RQ 这两种情况是不保序的，本文中不展开讨论。&lt;/p>
&lt;p>&lt;strong>不同 WQ 中的 WQE，其对应的 CQE 间是不保序的&lt;/strong>&lt;/p>
&lt;p>前文中我们说过，一个 CQ 可能会被多个 WQ 共享。这种情况下，是不能保证这些 WQE 对应的 CQE 的产生顺序的。如下图所示（WQE 编号表示下发的次序，即 1 最先被下发，6 最后被下发）：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_8.webp"
alt="2024-06-27_10_8" width="auto" loading="lazy">
&lt;/figure>
&lt;p>上面的描述其实还包含了“同一个 QP 的 SQ 和 RQ 中的 WQE，其对应的 CQE 间是不保序的”的情况，这一点其实比较容易理解，SQ 和 RQ，一个负责主动发起的任务，一个负责被动接收的任务，它们本来就可以是认为是两条不同方向的通道，自然不应该相互影响。假设用户对同一个 QP 先下发了一个 Receive WQE，又下发一个 Send WQE，总不能对端不给本端发送消息，本端就不能发送消息给对端了吧？&lt;/p>
&lt;p>既然这种情况下 CQE 产生的顺序和获取 WQE 的顺序是不相关的，那么上层应用和驱动是如何知道收到的 CQE 关联的是哪个 WQE 呢？其实很简单，&lt;strong>CQE 中指明它所对应的 WQE 的编号&lt;/strong>就可以了。&lt;/p>
&lt;p>另外需要注意的是，即使在多个 WQ 共用一个 CQ 的情况下，“同一个 WQ 中的 WQE，其对应的 CQE 间是保序的”这一点也是一定能够保证的，即上图中的属于 WQ1 的 WQE 1、3、4 对应的 CQE 一定是按照顺序产生的，对于属于 WQ2 的 WQE 2、5、6 也是如此。&lt;/p>
&lt;h3 id="cqc">CQC&lt;/h3>
&lt;p>同 QP 一样，CQ 只是一段存放 CQE 的队列内存空间。硬件除了知道首地址以外，对于这片区域可以说是一无所知。所以需要提前跟软件约定好格式，然后驱动将申请内存，并按照格式把 CQ 的基本信息填写到这片内存中供硬件读取，这片内存就是 CQC。CQC 中包含了 CQ 的容量大小，当前处理的 CQE 的序号等等信息。所以把 QPC 的图稍微修改一下，就能表示出 CQC 和 CQ 的关系：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_9.webp"
alt="2024-06-27_10_9" width="auto" loading="lazy">
&lt;/figure>
&lt;h3 id="cqn">CQN&lt;/h3>
&lt;p>CQ Number，就是 CQ 的编号，用来区别不同的 CQ。CQ 没有像 QP0 和 QP1 一样的特殊保留编号，本文中不再赘述了。&lt;/p>
&lt;h2 id="完成错误">完成错误&lt;/h2>
&lt;p>IB 协议中有三种错误类型，立即错误（immediate error）、完成错误（Completion Error）以及异步错误（Asynchronous Errors)。&lt;/p>
&lt;p>立即错误的是“立即停止当前操作，并返回错误给上层用户”；完成错误指的是“通过 CQE 将错误信息返回给上层用户”；而异步错误指的是“通过中断事件的方式上报给上层用户”。可能还是有点抽象，我们来举个例子说明这两种错误都会在什么情况下产生：&lt;/p>
&lt;ul>
&lt;li>用户在 Post Send 时传入了非法的操作码，比如想在 UD 的时候使用 RDMA WRITE 操作。&lt;/li>
&lt;/ul>
&lt;p>结果：产生立即错误（有的厂商在这种情况会产生完成错误）&lt;/p>
&lt;p>一般这种情况下，驱动程序会直接退出 post send 流程，并返回错误码给上层用户。注意此时 WQE 还没有下发到硬件就返回了。&lt;/p>
&lt;ul>
&lt;li>用户下发了一个 WQE，操作类型为 SEND，但是长时间没有受到对方的 ACK。&lt;/li>
&lt;/ul>
&lt;p>结果：产生完成错误&lt;/p>
&lt;p>因为 WQE 已经到达了硬件，所以硬件会产生对应的 CQE，CQE 中包含超时未响应的错误详情。&lt;/p>
&lt;ul>
&lt;li>用户态下发了多个 WQE，所以硬件会产生多个 CQE，但是软件一直没有从 CQ 中取走 CQE，导致 CQ 溢出。
结果：产生异步错误&lt;/li>
&lt;/ul>
&lt;p>因为软件一直没取 CQE，所以自然不会从 CQE 中得到信息。此时 IB 框架会调用软件注册的事件处理函数，来通知用户处理当前的错误。&lt;/p>
&lt;p>由此可见，它们都是底层向上层用户报告错误的方式，只是产生的时机不一样而已。IB 协议中对不同情况的错误应该以哪种方式上报做了规定，比如下图中，对于 Modify QP 过程中修改非法的参数，应该返回立即错误。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_10.webp"
alt="2024-06-27_10_10" width="auto" loading="lazy">
&lt;/figure>
&lt;p>本文的重点在于 CQ，所以介绍完错误类型之后，我们着重来看一下完成错误。完成错误是硬件通过在 CQE 中填写错误码来实现上报的，一次通信过程需要发起端（Requester）和响应端（Responder）参与，具体的错误原因也分为本端和对端。我们先来看一下错误检测是在什么阶段进行的（下图对 IB 协议中 Figure 118 进行了重画）：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_11.webp"
alt="2024-06-27_10_11" width="auto" loading="lazy">
&lt;/figure>
&lt;p>Requester 的错误检测点有两个：&lt;/p>
&lt;ol>
&lt;li>本地错误检测&lt;/li>
&lt;/ol>
&lt;p>即对 SQ 中的 WQE 进行检查，如果检测到错误，就从本地错误检查模块直接产生 CQE 到 CQ，不会发送数据到响应端了；如果没有错误，则发送数据到对端。&lt;/p>
&lt;ol start="2">
&lt;li>远端错误检测&lt;/li>
&lt;/ol>
&lt;p>即检测响应端的 ACK 是否异常，ACK/NAK 是由对端的本地错误检测模块检测后产生的，里面包含了响应端是否有错误，以及具体的错误类型。无论远端错误检测的结果是否有问题，都会产生 CQE 到 CQ 中。&lt;/p>
&lt;p>Responder 的错误检测点只有一个：&lt;/p>
&lt;ol>
&lt;li>本地错误检测&lt;/li>
&lt;/ol>
&lt;p>实际上检测的是对端报文是否有问题，IB 协议也将其称为“本地”错误检测。如果检测到错误，则会体现在 ACK/NAK 报文中回复给对端，以及在本地产生一个 CQE。&lt;/p>
&lt;p>需要注意的是，上述的产生 ACK 和远端错误检测只对面向连接的服务类型有效，无连接的服务类型。比如 UD 类型并不关心对端是否收到，接收端也不会产生 ACK，所以在 Requester 的本地错误检测之后就一定会产生 CQE，无论是否有远端错误。&lt;/p>
&lt;p>然后我们简单介绍下几种常见的完成错误：&lt;/p>
&lt;ul>
&lt;li>RC 服务类型的 SQ 完成错误&lt;/li>
&lt;li>Local Protection Error
&lt;ul>
&lt;li>本地保护域错误。本地 WQE 中指定的数据内存地址的 MR 不合法，即用户试图使用一片未注册的内存中的数据。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Remote Access Error
&lt;ul>
&lt;li>远端权限错误。本端没有权限读/写指定的对端内存地址。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Transport Retry Counter Exceeded Error
&lt;ul>
&lt;li>重传超次错误。对端一直未回复正确的 ACK，导致本端多次重传，超过了预设的次数。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>RC 服务类型的 RQ 完成错误&lt;/li>
&lt;li>Local Access Error
&lt;ul>
&lt;li>本地访问错误。说明对端试图写入其没有权限写入的内存区域。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Local Length Error
&lt;ul>
&lt;li>本地长度错误。本地 RQ 没有足够的空间来接收对端发送的数据。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>完整的完成错误类型列表请参考 IB 协议的 10.10.3 节。&lt;/p>
&lt;h2 id="用户接口">用户接口&lt;/h2>
&lt;p>同 QP 一样，我们依然从通信准备阶段（控制面）和通信进行阶段（数据面）来介绍 IB 协议对上层提供的关于 CQ 的接口。&lt;/p>
&lt;h3 id="控制面">控制面&lt;/h3>
&lt;p>同 QP 一样，还是“增删改查”四种，但是可能因为对于 CQ 来说，上层用户是资源使用者而不是管理者，只能从 CQ 中读数据而不能写数据，所以对用户开放的可配的参数就只有“CQ 规格”一种。&lt;/p>
&lt;ul>
&lt;li>创建——Create CQ&lt;/li>
&lt;/ul>
&lt;p>创建的时候用户必须指定 CQ 的规格，即能够储存多少个 CQE，另外用户还可以填写一个 CQE 产生后的回调函数指针（下文会涉及）。内核态驱动会将其他相关的参数配置好，填写到跟硬件约定好的 CQC 中告知硬件。&lt;/p>
&lt;ul>
&lt;li>销毁——Destroy CQ&lt;/li>
&lt;/ul>
&lt;p>释放一个 CQ 软硬件资源，包含 CQ 本身及 CQC，另外 CQN 自然也将失效。&lt;/p>
&lt;ul>
&lt;li>修改——Resize CQ&lt;/li>
&lt;/ul>
&lt;p>这里名字稍微有点区别，因为 CQ 只允许用户修改规格大小，所以就用的 Resize 而不是 Modify。&lt;/p>
&lt;ul>
&lt;li>查询——Query CQ&lt;/li>
&lt;/ul>
&lt;p>查询 CQ 的当前规格，以及用于通知的回调函数指针。&lt;/p>
&lt;blockquote>
&lt;p>通过对比 RDMA 规范和软件协议栈，可以发现很多 verbs 接口并不是按照规范实现的。所以读者如果发现软件 API 和协议有差异时也无须感到疑惑，RDMA 技术本身一直还在演进，软件框架也处于活跃更新的状态。如果更关心编程实现，那么请以软件协议栈的 API 文档为准；如果更关心学术上的研究，那么请以 RDMA 规范为准。&lt;/p>
&lt;/blockquote>
&lt;h3 id="数据面">数据面&lt;/h3>
&lt;p>CQE 是硬件将信息传递给软件的媒介，虽然软件知道在什么情况下会产生 CQE，但是软件并不知道具体什么时候硬件会把 CQE 放到 CQ 中。在通信和计算机领域，我们把这种接收方不知道发送方什么时候发送的模式称为“异步”。我们先来举一个网卡的例子，再来说明用户如何通过数据面接口获取 CQE（WC）。&lt;/p>
&lt;p>网卡收到数据包后如何让 CPU 知道这件事，并进行数据包处理，有两种常见的模式：&lt;/p>
&lt;ul>
&lt;li>中断模式&lt;/li>
&lt;/ul>
&lt;p>当数据量较少，或者说偶发的数据交换较多时，适合采用中断模式——即 CPU 平常在做其他事情，当网卡收到数据包时，会上报中断打断 CPU 当前的任务，CPU 转而来处理数据包（比如 TCP/IP 协议栈的各层解析）。处理完数据之后，CPU 跳回到中断前的任务继续执行。&lt;/p>
&lt;p>每次中断都需要保护现场，也就是把当前各个寄存器的值、局部变量的值等等保存到栈中，回来之后再恢复现场（出栈），这本身是有开销的。如果业务负载较重，网卡一直都在接收数据包，那么 CPU 就会一直收到中断，CPU 将一直忙于中断切换，导致其他任务得不到调度。&lt;/p>
&lt;ul>
&lt;li>轮询模式&lt;/li>
&lt;/ul>
&lt;p>所以除了中断模式之外，网卡还有一种轮询模式，即收到数据包后都先放到缓冲区里，CPU 每隔一段时间会去检查网卡是否受到数据。如果有数据，就把缓冲区里的数据一波带走进行处理，没有的话就接着处理别的任务。&lt;/p>
&lt;p>通过对比中断模式我们可以发现，轮询模式虽然每隔一段时间需要 CPU 检查一次，带来了一定的开销，但是当业务繁忙的时候采用轮询模式能够极大的减少中断上下文的切换次数，反而减轻了 CPU 的负担。&lt;/p>
&lt;p>现在的网卡，一般都是中断+轮询的方式，也就是根据业务负载动态切换。&lt;/p>
&lt;p>在 RDMA 协议中，CQE 就相当于是网卡收到的数据包，RDMA 硬件把它传递给 CPU 去处理。RDMA 框架定义了两种对上层的接口，分别是 poll 和 notify，对应着轮询和中断模式。&lt;/p>
&lt;h3 id="poll-completion-queue">Poll completion queue&lt;/h3>
&lt;p>很直白，poll 就是轮询的意思。用户调用这个接口之后，CPU 就会定期去检查 CQ 里面是否有新鲜的 CQE，如果有的话，就取出这个 CQE（注意取出之后 CQE 就被“消耗”掉了），解析其中的信息并返回给上层用户。&lt;/p>
&lt;h3 id="request-completion-notification">Request completion notification&lt;/h3>
&lt;p>直译过来是请求完成通知，用户调用这个接口之后，相当于向系统注册了一个中断。这样当硬件将 CQE 放到 CQ 中后，会立即触发一个中断给 CPU，CPU 进而就会停止手上的工作取出 CQE，处理后返回给用户。&lt;/p>
&lt;p>同样的，这两种接口使用哪种，取决于用户对于实时性的要求，以及实际业务的繁忙程度。&lt;/p>
&lt;p>感谢阅读，CQ 就介绍到这里，下篇打算详细讲讲 SRQ。&lt;/p>
&lt;h2 id="协议相关章节">协议相关章节&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>9.9 CQ 错误检测和恢复&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.2.6 CQ 和 WQ 的关系&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.10 错误类型及其处理&lt;/p>
&lt;/li>
&lt;li>
&lt;p>11.2.8 CQ 相关控制面接口&lt;/p>
&lt;/li>
&lt;li>
&lt;p>11.4.2 CQ 相关数据面接口&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="其他参考资料">其他参考资料&lt;/h2>
&lt;p>[1] Linux Kernel Networking - Implement and Theory. Chapter 13. Completion Queue&lt;/p></description></item><item><title>RDMA 之 Queue Pair</title><link>https://cuterwrite.top/p/rdma-queue-pair/</link><pubDate>Tue, 25 Jun 2024 02:21:00 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-queue-pair/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116342820_p0_master1200.webp" alt="Featured image of post RDMA 之 Queue Pair" />&lt;h1 id="rdma-之-queue-pair">RDMA 之 Queue Pair&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/195757767">&lt;cite>知乎专栏：9. RDMA 基本服务类型&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;h2 id="queue-pair">Queue Pair&lt;/h2>
&lt;p>我们曾经在 &lt;a class="link" href="https://cuterwrite.top/p/rdma-element/" >【“3. RDMA 基本元素”】
&lt;/a>
一文中简单的介绍了 QP 的概念，本文将更深入的讲解一些关于 QP 的细节。&lt;/p>
&lt;h2 id="基本概念回顾">基本概念回顾&lt;/h2>
&lt;p>首先我们来简单回顾下关于 QP 的基础知识：&lt;/p>
&lt;p>根据 IB 协议中的描述，QP 是硬件和软件之间的一个虚拟接口。QP 是队列结构，按顺序存储着软件给硬件下发的任务（WQE），WQE 中包含从哪里取出多长的数据，并且发送给哪个目的地等等信息。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-26_9_1.webp"
alt="2024-06-26_9_1" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>QP 的概念&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>每个 QP 间都是独立的，彼此通过 PD 隔离，因此一个 QP 可以被视为某个用户独占的一种资源，一个用户也可以同时使用多个 QP。&lt;/p>
&lt;p>QP 有很多种服务类型，包括 RC、UD、RD 和 UC 等，所有的源 QP 和目的 QP 必须为同一种类型才能进行数据交互。&lt;/p>
&lt;p>虽然 IB 协议将 QP 称为“虚拟接口”，但是它是有实体的：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>硬件上，QP 是一段包含着若干个 WQE 的存储空间，IB 网卡会从这段空间中读取 WQE 的内容，并按照用户的期望去内存中存取数据。至于这个存储空间是内存空间还是 IB 网卡的片内存储空间，IB 协议并未做出限制，每个厂商有各自的实现&lt;/p>
&lt;/li>
&lt;li>
&lt;p>软件上，QP 是一个由 IB 网卡的驱动程序所维护的数据结构，其中包含 QP 的地址指针以及一些相关的软件属性。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="qpc">QPC&lt;/h3>
&lt;p>&lt;a class="link" href="https://cuterwrite.top/p/rdma-service-types/" >【“5. RDMA 基本服务类型”】
&lt;/a>
一文中，我们曾经提到过 QPC 全称是 Queue Pair Context，用于存储 QP 相关属性。驱动程序里面是有储存 QP 的软件属性的，既然我们可以在软件里储存 QP 的属性，为什么还要用使用 QPC 呢？&lt;/p>
&lt;p>这是因为&lt;strong>QPC 主要是给硬件看的，也会用来在软硬件之间同步 QP 的信息。&lt;/strong>&lt;/p>
&lt;p>我们说过 QP 在硬件上的实体只是一段存储空间而已，硬件除了知道这段空间的起始地址和大小之外一无所知，甚至连这个 QP 服务类型都不知道。还有很多其他的重要信息，比如某个 QP 中包含了若干个 WQE，硬件怎么知道有多少个，当前应该处理第几个呢？&lt;/p>
&lt;p>所有上述的这些信息，软件是可以设计一定的数据结构并为其申请内存空间的，但是软件看到的都是虚拟地址，这些内存空间在物理上是离散的，硬件并不知道这些数据存放到了哪里。所以就需要软件通过操作系统提前申请好一大片连续的空间，即 QPC 来承载这些信息给硬件看。网卡及其配套的驱动程序提前约定好了 QPC 中都有哪些内容，这些内容分别占据多少空间，按照什么顺序存放。这样驱动和硬件就可以通过通过 QPC 这段空间来读写 QP 的状态等等信息。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-26_9_2_QPC.webp"
alt="2024-06-26_9_2" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>QPC 的概念&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>如上图所示，硬件其实只需要知道 QPC 的地址 0x12350000 就可以了，因为它可以解析 QPC 的内容，从而得知 QP 的位置，QP 序号，QP 大小等等信息。进而就能找到 QP，知道应该取第几个 WQE 去处理。不同的厂商可能实现有些差异，但是大致的原理就是这样。&lt;/p>
&lt;p>IB 软件栈中还有很多 Context 的概念，除了 QPC 之外，还有 Device Context，SRQC，CQC，EQC（Event Queue Context，事件队列上下文）等，它们的作用与 QPC 类似，都是用来在记录和同步某种资源的相关属性。&lt;/p>
&lt;h3 id="qp-number">QP Number&lt;/h3>
&lt;p>简称为 QPN，就是每个 QP 的编号。IB 协议中规定用 $2^{24}$ 个 bit 来表示 QPN，即每个节点最大可以同时使用 $2^{24}$ 个 QP，这已经是一个很大的数量了，几乎不可能用完。每个节点都各自维护着 QPN 的集合，相互之间是独立的，即不同的节点上可以存在编号相同的 QP。&lt;/p>
&lt;p>QPN 的概念本身非常简单，但是有两个特殊的保留编号需要额外注意一下：&lt;/p>
&lt;h4 id="qp0">QP0&lt;/h4>
&lt;p>编号为 0 的 QP 用于子网管理接口 SMI（Subnet Management Interface），用于管理子网中的全部节点，说实话我也还没搞清楚这个接口的作用，暂且按下不表。&lt;/p>
&lt;h4 id="qp1">QP1&lt;/h4>
&lt;p>编号为 1 的 QP 用于通用服务接口 GSI（General Service Interface），GSI 是一组管理服务，其中最出名的就是 CM（Communication Management），是一种在通信双方节点正式建立连接之前用来交换必须信息的一种方式。其细节将在后面的文章中专门展开介绍。&lt;/p>
&lt;p>这也就是我们之前的文章画的关于 QP 的图中，没有出现过 QP0 和 QP1 的原因了。这两个 QP 之外的其他 QP 就都是普通 QP 了。用户在创建 QP 的时候，驱动或者硬件会给这个新 QP 分配一个 QPN，一般的 QPN 都是 2、3、4 这样按顺序分配的。当 QP 被销毁之后，它的 QPN 也会被重新回收，并在合适的时候分配给其他新创建的 QP。&lt;/p>
&lt;h2 id="用户接口">用户接口&lt;/h2>
&lt;p>我们从控制层面和数据层面来分类介绍用户接口，控制面即用户对某种资源进行某种设置，一般都是在正式收发数据之前进行；而数据面自然就是真正的数据收发过程中进行的操作。&lt;/p>
&lt;h3 id="控制面">控制面&lt;/h3>
&lt;p>接触过算法的读者应该都了解，链表的节点涉及到“增、删、改、查”四个操作，链表的节点是一片内存区域，是一种软件资源。&lt;/p>
&lt;p>“增”即向操作系统申请一片内存用来存放数据，系统将在内存中划分一块空间，并将其标记为“已被进程 XX 使用”，其他没有权限的进程将无法覆盖甚至读取这片内存空间。&lt;/p>
&lt;p>“删”即通知操作系统，这片空间我不使用了，可以标记成“未使用”并给其它进程使用了。&lt;/p>
&lt;p>“改”就是写，即修改这片内存区域的内容。&lt;/p>
&lt;p>&amp;ldquo;查&amp;quot;就是读，即获取这片内存区域的内容。&lt;/p>
&lt;p>QP 作为 RDMA 技术中最重要的一种资源，在生命周期上与链表并无二致：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>操作&lt;/th>
&lt;th>链表节点&lt;/th>
&lt;th>QP&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>增&lt;/td>
&lt;td>struct ListNode *node = malloc(sizeof(struct ListNode *));&lt;/td>
&lt;td>Create QP&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>删&lt;/td>
&lt;td>free(node);&lt;/td>
&lt;td>Destroy QP&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>改&lt;/td>
&lt;td>node-&amp;gt;val = xxx;&lt;/td>
&lt;td>Modify QP&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>查&lt;/td>
&lt;td>xxx = node-&amp;gt;val;&lt;/td>
&lt;td>Query QP&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>这四种操作，其实就是 Verbs（RDMA 对上层应用的 API）在控制面上对上层用户提供给用户的几个接口：&lt;/p>
&lt;h4 id="create-qp">Create QP&lt;/h4>
&lt;p>创建一个 QP 的软硬件资源，包含 QP 本身以及 QPC。用户创建时会写传入一系列的初始化属性，包含该 QP 的服务类型，可以储存的 WQE 数量等信息&lt;/p>
&lt;h4 id="destroy-qp">Destroy QP&lt;/h4>
&lt;p>释放一个 QP 的全部软硬件资源，包含 QP 本身及 QPC。销毁 QP 后，用户将无法通过 QPN 索引到这个 QP。&lt;/p>
&lt;h4 id="modify-qp">Modify QP&lt;/h4>
&lt;p>修改一个 QP 的某些属性，比如 QP 的状态，路径的 MTU 等等。这个修改过程既包括软件数据结构的修改，也包括对 QPC 的修改。&lt;/p>
&lt;h4 id="query-qp">Query QP&lt;/h4>
&lt;p>查询一个 QP 当前的状态和一些属性，查询到的数据来源于驱动以及 QPC 的内容。&lt;/p>
&lt;p>这四种操作都有配套的 Verbs 接口，类似于 &lt;code>ibv_create_qp()&lt;/code> 这种形式，我们编写 APP 时直接调用就可以了。更多关于对上层的 API 的细节，我们将在后面专门进行介绍。&lt;/p>
&lt;h2 id="数据面">数据面&lt;/h2>
&lt;p>数据面上，一个 QP 对上层的接口其实只有两种，分别用于向 QP 中填写发送和接收请求。&lt;strong>这里的“发送”和“接收”并不是指的发送和接收数据，而是指的是一次通信过程的“发起方”（Requestor）和“接收方”（Responser）&lt;/strong>。&lt;/p>
&lt;p>在行为上都是软件向 QP 中填写一个 WQE（对应用层来说叫 WR），请求硬件执行一个动作。所以这两种行为都叫做“Post XXX Request”的形式，即下发 XXX 请求。&lt;/p>
&lt;h3 id="post-send-request">Post Send Request&lt;/h3>
&lt;p>再强调一下，Post Send 本身不是指这个 WQE 的操作类型是 Send，而是表示这个 WQE 属于通信发起方。这个流程中填写到 QP 中的 WQE/WR 可以是 Send 操作，RDMA Write 操作以及 RDMA Read 操作等。&lt;/p>
&lt;p>用户需要提前准备好数据缓冲区、目的地址等信息，然后调用接口将 WR 传给驱动，驱动再把 WQE 填写到 QP 中。&lt;/p>
&lt;h3 id="post-receive-request">Post Receive Request&lt;/h3>
&lt;p>Post Recv 的使用场景就相对比较少了，一般只在 Send-Recv 操作的接收端执行，接收端需要提前准备好接收数据的缓冲区，并将缓冲区地址等信息以 WQE 的形式告知硬件。&lt;/p>
&lt;h2 id="qp-状态机">QP 状态机&lt;/h2>
&lt;p>说到 QP 的状态，就不得不祭出下面这张图（取自 IB 协议 10.3.1 节）：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-26_9_3.webp"
alt="2024-06-26_9_3" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>QP 状态机&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>所谓状态机，就是描述一个对象的不同状态，以及触发状态间跳转的条件。为一个对象设计状态机可以使这个对象的生命周期变得非常明确，实现上也会使得逻辑更加清晰。&lt;/p>
&lt;p>对于 QP 来说，IB 规范也为其设计了几种状态，处于不同状态的 QP 的功能是有差异的，比如只有进入到 Ready to Send 状态之后，QP 才能够进行 Post Send 数据操作。正常状态（绿色的）之间的状态转换都是由用户通过上文介绍的 Modify QP 的用户接口来主动触发的；而错误状态（红色的）往往是出错之后自动跳转的，当一个 QP 处于错误状态之后就无法执行正常的业务了，就需要上层通过 Modify QP 将其重新配置到正常状态上。&lt;/p>
&lt;p>上图中我们只关注 QP 的部分，EE（End-to-End Context）是专门给 RD 服务类型使用的一个概念，我们暂不涉及。我们通过 Create QP 接口来进入这个状态图，通过 Destroy QP 接口来离开这个状态图。&lt;/p>
&lt;p>QP 有以下几种状态，我们仅介绍一下比较重要的点：&lt;/p>
&lt;h3 id="rstreset">RST（Reset）&lt;/h3>
&lt;p>复位状态。当一个 QP 通过 Create QP 创建好之后就处于这个状态，相关的资源都已经申请好了，但是这个 QP 目前什么都做不了，其无法接收用户下发的 WQE，也无法接受对端某个 QP 的消息。&lt;/p>
&lt;h3 id="initinitialized">INIT（Initialized）&lt;/h3>
&lt;p>已初始化状态。这个状态下，用户可以通过 Post Receive 给这个 QP 下发 Receive WR，但是接收到的消息并不会被处理，会被静默丢弃；如果用户下发了一个 Post Send 的 WR，则会报错。&lt;/p>
&lt;h3 id="rtrready-to-receive">RTR（Ready to Receive）&lt;/h3>
&lt;p>准备接收状态。在 INIT 状态的基础上，RQ 可以正常工作，即对于接收到的消息，可以按照其中 WQE 的指示搬移数据到指定内存位置。此状态下 SQ 仍然不能工作。&lt;/p>
&lt;h3 id="rtsready-to-send">RTS（Ready to Send）&lt;/h3>
&lt;p>准备发送状态。在 RTR 基础上，SQ 可以正常工作，即用户可以进行 Post Send，并且硬件也会根据 SQ 的内容将数据发送出去。进入该状态前，QP 必须已于对端建立好链接。&lt;/p>
&lt;h3 id="sqdsend-queue-drain">SQD（Send Queue Drain）&lt;/h3>
&lt;p>SQ 排空状态。顾名思义，该状态会将 SQ 队列中现存的未处理的 WQE 全部处理掉，这个时候用户还可以下发新的 WQE 下来，但是这些 WQE 要等到旧的 WQE 全处理之后才会被处理。&lt;/p>
&lt;h3 id="sqersend-queue-error">SQEr（Send Queue Error）&lt;/h3>
&lt;p>SQ 错误状态。当某个 Send WR 发生完成错误（即硬件通过 CQE 告知驱动发生的错误）时，会导致 QP 进入此状态。&lt;/p>
&lt;h3 id="errerror">ERR（Error）&lt;/h3>
&lt;p>即错误状态。其他状态如果发生了错误，都可能进入该状态。Error 状态时，QP 会停止处理 WQE，已经处理到一半的 WQE 也会停止。上层需要在修复错误后再将 QP 重新切换到 RST 的初始状态。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>本文先回顾了 QP 的一些重要基本概念，然后讲解了 QPC、QPN 等 QP 强相关的概念，最后介绍了用户操作 QP 常用的接口以及 QP 状态机，相信本文过后读者一定对 QP 有了更深的了解。&lt;/p>
&lt;p>其实作为 RDMA 的核心概念，QP 的内容很多，本文难以全部囊括。我将在后面的文章中逐渐把相关的内容补全，比如 QKey 的概念将在后续专门介绍各种 Key 的文章中讲解。&lt;/p>
&lt;p>好了，本文就到这了，感谢阅读。预告下一篇文章将详细讲解 CQ。&lt;/p>
&lt;h2 id="协议相关章节">协议相关章节&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>3.5.1 10.2.4 QP 的基本概念&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.3 QP 状态机&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.2.5 QP 相关的软件接口&lt;/p>
&lt;/li>
&lt;li>
&lt;p>11.4 Post Send Post Recv&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>基于 Workbox 实现 Hugo 渐进式 Web 应用</title><link>https://cuterwrite.top/p/hugo-pwa/</link><pubDate>Tue, 18 Jun 2024 22:28:00 +0000</pubDate><guid>https://cuterwrite.top/p/hugo-pwa/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116406967_p0_master1200.webp" alt="Featured image of post 基于 Workbox 实现 Hugo 渐进式 Web 应用" />&lt;h1 id="基于-workbox-实现-hugo-pwa">基于 Workbox 实现 Hugo PWA&lt;/h1>
&lt;p>最近给基于 &lt;a class="link" href="https://gohugo.io/" target="_blank" rel="noopener" >Hugo
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
搭建的博客添加了 PWA 功能，显著提升了加载速度和用户体验，甚至实现了离线访问。至于如何实现，那么你需要了解 &lt;strong>Progressive Web Apps (PWA)&lt;/strong>。&lt;/p>
&lt;h2 id="什么是-pwa">什么是 PWA&lt;/h2>
&lt;p>渐进式 Web 应用（Progressive Web Apps，简称 PWA）利用现代 Web API 和传统的渐进式增强策略，打造出跨平台的 Web 应用程序。这些应用无处不在，功能丰富，为用户带来媲美原生应用的体验。&lt;/p>
&lt;p>&lt;strong>PWA 的优势：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>⚡️ &lt;strong>更快的加载速度&lt;/strong>: PWA 可以缓存重要资源，即使网络状况不佳也能快速加载。&lt;/li>
&lt;li>✈️ &lt;strong>离线访问&lt;/strong>: PWA 可以缓存内容，让用户即使离线也能访问内容。&lt;/li>
&lt;li>🔔 &lt;strong>推送通知&lt;/strong>: 像原生应用一样，PWA 可以向用户发送推送通知，提高用户参与度。&lt;/li>
&lt;li>📱 &lt;strong>安装到主屏幕&lt;/strong>: 用户可以将你的应用添加到电脑或手机桌面，像原生应用一样浏览你的 Web 应用。&lt;/li>
&lt;/ul>
&lt;p>PWA 的实现原理是 &lt;strong>Service Worker&lt;/strong>。&lt;strong>Service Worker 是一种特殊的 JavaScript 资源，在浏览器后台独立运行，充当着网络浏览器和 Web 服务器之间的代理。它可以拦截和处理网络请求、缓存资源以及推送通知&lt;/strong>。&lt;/p>
&lt;p>主流的前端框架 Vue、React、Angular 都提供了相应的 PWA 插件。而对于 Hugo 这样的静态网站生成器，我们可以通过手动添加 &lt;a class="link" href="https://developer.chrome.com/docs/workbox" target="_blank" rel="noopener" >Workbox
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
来实现 PWA 功能。&lt;/p>
&lt;h2 id="workbox">Workbox&lt;/h2>
&lt;p>&lt;a class="link" href="https://developer.chrome.com/docs/workbox" target="_blank" rel="noopener" >Workbox
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
是由 Google Chrome 团队开发的一套模块，旨在简化常见的 Service Worker 路由和缓存操作。每个模块都针对 Service Worker 开发的特定方面进行了优化。Workbox 的目标是尽可能简化 Service Worker 的使用，同时在需要时灵活地满足复杂应用的需求。&lt;/p>
&lt;p>如果没有 Workbox，我们需要手动编写 Service Worker 来监听 fetch 事件、缓存资源并实现离线访问等功能。而 Workbox 提供了一套工具，可以帮助我们自动生成 Service Worker，并且内置了一些常用的缓存策略，使我们能够更加专注于业务逻辑。&lt;/p>
&lt;h2 id="配置-pwa">配置 PWA&lt;/h2>
&lt;p>在上一节中，我们了解了 PWA 的概念和优势，以及 Workbox 如何简化 Service Worker 的开发。接下来将一步步地给 Hugo 博客配置 PWA 功能。&lt;/p>
&lt;h3 id="注册-service-worker">注册 Service Worker&lt;/h3>
&lt;p>首先，我们需要在页面中注册 Service Worker。将以下代码段添加到你的 Hugo 主题的 &lt;code>layouts/partials/footer/custom.html&lt;/code> 文件中（其他主题可能需要根据文件结构进行调整）：&lt;/p>
&lt;pre>&lt;code class="language-javascript">&amp;lt;script&amp;gt;
// Check that service workers are registered
if ('serviceWorker' in navigator) {
// Use the window load event to keep the page load performant
window.addEventListener('load', () =&amp;gt; {
navigator.serviceWorker.register('/sw.js').then(reg =&amp;gt; {
console.log('Service worker registered with scope: ', reg.scope);
}, err =&amp;gt; {
console.log('Service worker registration failed: ', err);
});
});
}
&amp;lt;/script&amp;gt;
&lt;/code>&lt;/pre>
&lt;div class="notice notice-tip" >
&lt;div class="notice-title">&lt;svg t="1705945832245" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="19409" width="200" height="200">&lt;path d="M512 74.666667C270.933333 74.666667 74.666667 270.933333 74.666667 512S270.933333 949.333333 512 949.333333 949.333333 753.066667 949.333333 512 753.066667 74.666667 512 74.666667z m238.933333 349.866666l-2.133333 2.133334-277.333333 277.333333c-10.666667 10.666667-29.866667 12.8-42.666667 2.133333L426.666667 704l-149.333334-149.333333c-12.8-12.8-12.8-32 0-44.8 10.666667-10.666667 29.866667-12.8 42.666667-2.133334l2.133333 2.133334 125.866667 125.866666 253.866667-253.866666c10.666667-10.666667 29.866667-12.8 42.666666-2.133334l2.133334 2.133334c12.8 12.8 12.8 32 4.266666 42.666666z" fill="#ffffff" p-id="19410">&lt;/path>&lt;/svg>&lt;/div>&lt;p>注意： 在注册 Service Worker 之前，你需要先创建 &lt;code>sw.js&lt;/code> 文件，我们将在下一小节中完成这一步骤。&lt;/p>&lt;/div>
&lt;p>完成注册后，你可以在浏览器的开发者工具 (F12) 中的 &lt;strong>&amp;ldquo;Application&amp;rdquo; -&amp;gt; &amp;ldquo;Service Workers&amp;rdquo;&lt;/strong> 面板中查看 Service Worker 的注册状态。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-19_service-worker.webp"
alt="Service Worker" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>Service Worker&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="导入-workbox">导入 Workbox&lt;/h3>
&lt;p>在你的 Hugo 网站根目录下的 &lt;code>static&lt;/code> 文件夹中创建 &lt;code>sw.js&lt;/code> 文件。然后，在 &lt;code>sw.js&lt;/code> 文件中添加以下代码，使用 CDN 导入 Workbox：&lt;/p>
&lt;pre>&lt;code class="language-javascript">importScripts('https://cdn.bootcdn.net/ajax/libs/workbox-sw/7.1.0/workbox-sw.js');
&lt;/code>&lt;/pre>
&lt;h3 id="缓存策略">缓存策略&lt;/h3>
&lt;p>Workbox 提供了一些常用的缓存策略，如 &lt;code>CacheFirst&lt;/code>、&lt;code>NetworkFirst&lt;/code>、&lt;code>StaleWhileRevalidate&lt;/code> 等。这里先介绍几种常用的策略。&lt;/p>
&lt;h4 id="cacheonly-仅缓存">CacheOnly 仅缓存&lt;/h4>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-19_cache-6850d07d742bf_1440.webp"
alt="CacheOnly" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>CacheOnly&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>强制响应来自缓存。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp(regex),
new workbox.strategies.CacheOnly()
);
&lt;/code>&lt;/pre>
&lt;h4 id="networkonly-仅网络">NetworkOnly 仅网络&lt;/h4>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-19_network-48f46158a5575_1440.webp"
alt="NetworkOnly" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>NetworkOnly&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>这种缓存策略强制要求所有请求都从网络获取最新数据，完全绕过缓存。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp(regex),
new workbox.strategies.NetworkOnly()
);
&lt;/code>&lt;/pre>
&lt;h4 id="cachefirst-优先缓存">CacheFirst 优先缓存&lt;/h4>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-19_cache-falling-to-networ-f4c1aa5570621_1440.webp"
alt="CacheFirst" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>CacheFirst&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>这种缓存策略以速度为优先，会首先尝试从缓存中获取响应，以尽快向用户显示内容。如果缓存中没有所需数据，它才会向网络发起请求获取数据。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp(regex),
new workbox.strategies.CacheFirst()
);
&lt;/code>&lt;/pre>
&lt;h4 id="networkfirst-优先网络">NetworkFirst 优先网络&lt;/h4>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-19_network-falling-to-cache-39d267a044b35_1440.webp"
alt="NetworkFirst" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>NetworkFirst&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>这种缓存策略优先使用最新数据，因此会首先尝试从网络获取响应。如果网络请求失败，例如用户离线或网络连接不稳定，它会回退使用缓存中的数据，确保用户仍然可以访问内容。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp(regex),
new workbox.strategies.NetworkFirst()
);
&lt;/code>&lt;/pre>
&lt;h4 id="stalewhilerevalidate-读取缓存同时发起网络请求">StaleWhileRevalidate 读取缓存，同时发起网络请求&lt;/h4>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-19_cache-network-873b1ec5f25cc_1440.webp"
alt="StaleWhileRevalidate" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>StaleWhileRevalidate&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>这种缓存策略优先返回缓存内容（如果有）。即使缓存内容有效，它也会在后台发起网络请求以获取最新数据，保证用户最终能看到最新内容。虽然这种策略能确保用户定期更新缓存，但也意味着每次请求都会产生网络流量，即使数据没有变化，也比较浪费带宽。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp(regex),
new workbox.strategies.StaleWhileRevalidate()
);
&lt;/code>&lt;/pre>
&lt;h4 id="策略配置">策略配置&lt;/h4>
&lt;p>Workbox 不仅提供上述策略，还允许通过 cacheName、plugins 和 expiration 等配置项进行自定义。你可以通过定义要使用的插件来自定义路由行为。例如，你可以配置缓存名称、缓存有效期以及可缓存的响应状态码，如下所示：&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp(regex),
new workbox.strategies.CacheFirst({
cacheName: 'my-cache',
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: 60,
maxAgeSeconds: 30 * 24 * 60 * 60, // 30 Days
}),
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
&lt;/code>&lt;/pre>
&lt;h3 id="本站配置">本站配置&lt;/h3>
&lt;h4 id="全局配置">全局配置&lt;/h4>
&lt;p>以下是全局缓存配置：&lt;/p>
&lt;pre>&lt;code class="language-javascript">// 缓存版本号
let cacheVersion = '-240619';
// 最大条目数
const maxEntries = 100;
&lt;/code>&lt;/pre>
&lt;h4 id="twitto-配置">Twitto 配置&lt;/h4>
&lt;p>为了确保用户即使在离线状态下也能查看评论，Twitto 评论 API 采用了 &lt;code>NetworkFirst&lt;/code> 缓存策略。这意味着浏览器会优先尝试从网络获取最新数据，如果网络不可用，则使用缓存中的数据。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('^https://comment\.cuterwrite\.top'),
new workbox.strategies.NetworkFirst({
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
&lt;/code>&lt;/pre>
&lt;h4 id="rss-与-sitemap-配置">RSS 与 Sitemap 配置&lt;/h4>
&lt;p>为了确保用户始终获取最新的 RSS 和 Sitemap 数据，这些页面配置为仅使用网络策略 (&lt;code>NetworkOnly&lt;/code>)，不进行缓存。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('^https://cuterwrite\.top/(index|sitemap)\.xml'),
new workbox.strategies.NetworkOnly()
);
&lt;/code>&lt;/pre>
&lt;h4 id="html-配置">HTML 配置&lt;/h4>
&lt;p>为了在保证用户快速加载页面的同时，也能获取到最新内容，网站对 HTML 页面采用了 &lt;code>StaleWhileRevalidate&lt;/code> 缓存策略。这意味着浏览器会优先使用缓存中的页面进行展示，同时在后台向服务器发起请求，获取最新版本，并在下次请求时使用。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('.*\.html'),
new workbox.strategies.StaleWhileRevalidate({
cacheName: 'html-cache' + cacheVersion,
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
&lt;/code>&lt;/pre>
&lt;h4 id="google-fonts-配置">Google Fonts 配置&lt;/h4>
&lt;p>为了在保证字体文件更新的同时，也能利用缓存加速页面加载速度，网站对 Google Fonts 资源采用了 &lt;code>CacheFirst&lt;/code> 缓存策略，并设置了较长的缓存过期时间。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('.*\.(?:woff|woff2|ttf|otf|eot)'),
new workbox.strategies.StaleWhileRevalidate({
cacheName: 'google-fonts' + cacheVersion,
plugins: [
// 使用 expiration 插件实现缓存条目数目和时间控制
new workbox.expiration.ExpirationPlugin({
// 最大缓存条目数
maxEntries: maxEntries,
// 最长缓存时间 30 天
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
// 使用 cacheableResponse 插件缓存状态码为 0 的请求
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
&lt;/code>&lt;/pre>
&lt;h4 id="cdn-配置">CDN 配置&lt;/h4>
&lt;p>为了最大程度地利用缓存加速页面加载速度，网站对来自常用 CDN 的资源采用了 &lt;code>CacheFirst&lt;/code> 缓存策略，并设置了较长的缓存过期时间。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('^https://(?:cdn\.bootcdn\.net|unpkg\.com|cdn\.jsdelivr\.net)'),
new workbox.strategies.CacheFirst({
cacheName: 'cdn' + cacheVersion,
fetchOptions: {
mode: 'cors',
credentials: 'omit',
},
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
],
})
);
&lt;/code>&lt;/pre>
&lt;h4 id="umani-网站统计配置">Umani 网站统计配置&lt;/h4>
&lt;p>为了确保网站统计数据的准确性，网站对 Umani 网站统计请求采用了 &lt;code>NetworkOnly&lt;/code> 策略，并使用 &lt;code>BackgroundSyncPlugin&lt;/code> 插件来实现即使在网络离线的情况下也能保证数据最终发送成功。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('^https://analytics\.cuterwrite\.top/uma'),
new workbox.strategies.NetworkOnly({
plugins: [
// 使用 background sync 插件实现后台同步
new workbox.backgroundSync.BackgroundSyncPlugin('Optical_Collect', {
maxRetentionTime: 12 * 60,
}),
],
})
);
&lt;/code>&lt;/pre>
&lt;h4 id="图片配置">图片配置&lt;/h4>
&lt;p>为了加速图片加载速度，并减少网络请求次数，网站对图片资源采用了 &lt;code>CacheFirst&lt;/code> 缓存策略，并设置了较长的缓存过期时间。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('^(https://cuterwrite-1302252842\.file\.myqcloud\.com|https://s2\.loli\.net)'),
new workbox.strategies.CacheFirst({
cacheName: 'image-cache' + cacheVersion,
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
&lt;/code>&lt;/pre>
&lt;h4 id="后缀匹配配置">后缀匹配配置&lt;/h4>
&lt;p>为了兼顾加载速度和内容更新，网站对未被域名匹配到的静态文件（例如图片、CSS 和 JavaScript 文件）采用了 &lt;code>StaleWhileRevalidate&lt;/code> 缓存策略。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('.*\.(?:png|jpg|jpeg|svg|gif|webp|ico)'),
new workbox.strategies.StaleWhileRevalidate()
);
workbox.routing.registerRoute(
new RegExp('.*\.(css|js)'),
new workbox.strategies.StaleWhileRevalidate()
);
&lt;/code>&lt;/pre>
&lt;h4 id="默认行为配置">默认行为配置&lt;/h4>
&lt;p>为了处理未被任何自定义路由规则匹配到的请求，网站配置了默认缓存行为，使用 &lt;code>NetworkFirst&lt;/code> 策略并设置了网络超时时间，以兼顾资源获取速度和离线可用性。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.setDefaultHandler(
// 优先使用缓存，缓存没有则使用网络请求
new workbox.strategies.NetworkFirst({
networkTimeoutSeconds: 3,
})
);
&lt;/code>&lt;/pre>
&lt;h3 id="完整配置">完整配置&lt;/h3>
&lt;details>
&lt;summary>sw.js&lt;/summary>
&lt;pre>&lt;code class="language-javascript">importScripts('https://cdn.bootcdn.net/ajax/libs/workbox-sw/7.1.0/workbox-sw.js');
// 缓存版本号
let cacheVersion = '-240619';
// 最大条目数
const maxEntries = 100;
if (workbox) {
console.log(`Yay! Workbox is loaded 🎉`);
// 评论缓存
workbox.routing.registerRoute(
new RegExp('^https://comment\.cuterwrite\.top'),
new workbox.strategies.NetworkFirst({
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
// rss 、sitemap 不缓存
workbox.routing.registerRoute(
new RegExp('^https://cuterwrite\.top/(index|sitemap)\.xml'),
new workbox.strategies.NetworkOnly()
);
// 缓存 HTML
workbox.routing.registerRoute(
new RegExp('.*\.html'),
new workbox.strategies.StaleWhileRevalidate({
cacheName: 'html-cache' + cacheVersion,
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
// 缓存 Google Fonts
workbox.routing.registerRoute(
new RegExp('.*\.(?:woff|woff2|ttf|otf|eot)'),
new workbox.strategies.StaleWhileRevalidate({
cacheName: 'google-fonts' + cacheVersion,
plugins: [
// 使用 expiration 插件实现缓存条目数目和时间控制
new workbox.expiration.ExpirationPlugin({
// 最大缓存条目数
maxEntries: maxEntries,
// 最长缓存时间 30 天
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
// 使用 cacheableResponse 插件缓存状态码为 0 的请求
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
// 缓存 bootcdn、unpkg、jsdelivr 等公共库，用正则匹配
workbox.routing.registerRoute(
new RegExp('^https://(?:cdn\.bootcdn\.net|unpkg\.com|cdn\.jsdelivr\.net)'),
new workbox.strategies.CacheFirst({
cacheName: 'cdn' + cacheVersion,
fetchOptions: {
mode: 'cors',
credentials: 'omit',
},
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
],
})
);
// 自建 UMA 统计脚本: https://analytics.cuterwrite.top/uma
workbox.routing.registerRoute(
new RegExp('^https://analytics\.cuterwrite\.top/uma'),
new workbox.strategies.NetworkOnly({
plugins: [
// 使用 background sync 插件实现后台同步
new workbox.backgroundSync.BackgroundSyncPlugin('Optical_Collect', {
maxRetentionTime: 12 * 60,
}),
],
})
);
// 缓存存储桶图片 https://cuterwrite-1302252842.file.myqcloud.com/
workbox.routing.registerRoute(
new RegExp('^(https://cuterwrite-1302252842\.file\.myqcloud\.com|https://s2\.loli\.net)'),
new workbox.strategies.CacheFirst({
cacheName: 'image-cache' + cacheVersion,
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
// 后缀匹配，针对其余没有被域名匹配到的静态文件
workbox.routing.registerRoute(
new RegExp('.*\.(?:png|jpg|jpeg|svg|gif|webp|ico)'),
new workbox.strategies.StaleWhileRevalidate()
);
workbox.routing.registerRoute(
new RegExp('.*\.(css|js)'),
new workbox.strategies.StaleWhileRevalidate()
);
// 默认匹配剩下的请求
workbox.routing.setDefaultHandler(
// 优先使用缓存，缓存没有则使用网络请求
new workbox.strategies.NetworkFirst({
networkTimeoutSeconds: 3,
})
);
} else {
console.log(`Boo! Workbox didn't load 😬`);
}
&lt;/code>&lt;/pre>
&lt;/details>
&lt;h3 id="manifestjson">manifest.json&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>创建 manifest.json 文件&lt;/strong>&lt;/li>
&lt;/ol>
&lt;p>在你的 Hugo 博客的根目录 &lt;code>static&lt;/code> 文件夹下创建 &lt;code>manifest.json&lt;/code> 文件，该文件包含了关于你的博客的元数据，例如名称、图标和显示选项。&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;name&amp;quot;: &amp;quot;你的博客名称&amp;quot;,
&amp;quot;short_name&amp;quot;: &amp;quot;博客简称&amp;quot;,
&amp;quot;start_url&amp;quot;: &amp;quot;/&amp;quot;,
&amp;quot;display&amp;quot;: &amp;quot;standalone&amp;quot;,
&amp;quot;background_color&amp;quot;: &amp;quot;#ffffff&amp;quot;,
&amp;quot;theme_color&amp;quot;: &amp;quot;#000000&amp;quot;,
&amp;quot;icons&amp;quot;: [{
&amp;quot;src&amp;quot;: &amp;quot;/icon-192x192.png&amp;quot;,
&amp;quot;sizes&amp;quot;: &amp;quot;192x192&amp;quot;,
&amp;quot;type&amp;quot;: &amp;quot;image/png&amp;quot;
},
{
&amp;quot;src&amp;quot;: &amp;quot;/icon-512x512.png&amp;quot;,
&amp;quot;sizes&amp;quot;: &amp;quot;512x512&amp;quot;,
&amp;quot;type&amp;quot;: &amp;quot;image/png&amp;quot;
}
]
}
&lt;/code>&lt;/pre>
&lt;div class="notice notice-tip" >
&lt;div class="notice-title">&lt;svg t="1705945832245" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="19409" width="200" height="200">&lt;path d="M512 74.666667C270.933333 74.666667 74.666667 270.933333 74.666667 512S270.933333 949.333333 512 949.333333 949.333333 753.066667 949.333333 512 753.066667 74.666667 512 74.666667z m238.933333 349.866666l-2.133333 2.133334-277.333333 277.333333c-10.666667 10.666667-29.866667 12.8-42.666667 2.133333L426.666667 704l-149.333334-149.333333c-12.8-12.8-12.8-32 0-44.8 10.666667-10.666667 29.866667-12.8 42.666667-2.133334l2.133333 2.133334 125.866667 125.866666 253.866667-253.866666c10.666667-10.666667 29.866667-12.8 42.666666-2.133334l2.133334 2.133334c12.8 12.8 12.8 32 4.266666 42.666666z" fill="#ffffff" p-id="19410">&lt;/path>&lt;/svg>&lt;/div>&lt;p>注意：将 icon-192x192.png 和 icon-512x512.png 替换为你自己的图标文件名。并确保将这两个图标文件放置在 Hugo 博客的 &lt;code>static&lt;/code> 文件夹中。如果你想修改主题颜色和背景颜色，可以修改 theme_color 和 background_color 字段。&lt;/p>&lt;/div>
&lt;ol start="2">
&lt;li>&lt;strong>链接 manifest.json 文件&lt;/strong>&lt;/li>
&lt;/ol>
&lt;p>在你的 Hugo 博客的 &lt;code>layouts/partials/head/custom.html&lt;/code> 文件中添加以下代码，将 &lt;code>manifest.json&lt;/code> 文件链接到你的网站：&lt;/p>
&lt;pre>&lt;code class="language-html">&amp;lt;link rel=&amp;quot;manifest&amp;quot; href=&amp;quot;/manifest.json&amp;quot;&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>完成以上步骤后，你的 Hugo 博客就具备了 PWA 功能，用户可以像使用原生应用程序一样访问你的网站。&lt;/p>
&lt;h2 id="参考资料">参考资料&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;a class="link" href="https://web.dev/articles/offline-cookbook?hl=zh-cn" target="_blank" rel="noopener" >离线实战宝典
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a class="link" href="https://developers.google.com/web/tools/workbox" target="_blank" rel="noopener" >Workbox
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a class="link" href="https://github.com/GoogleChrome/workbox" target="_blank" rel="noopener" >Workbox Github
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>Ollama：从入门到进阶</title><link>https://cuterwrite.top/p/ollama/</link><pubDate>Sat, 15 Jun 2024 23:10:00 +0000</pubDate><guid>https://cuterwrite.top/p/ollama/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116903387_p0_master1200.webp" alt="Featured image of post Ollama：从入门到进阶" />&lt;p>近年来，大型语言模型（LLM）以其强大的文本生成和理解能力，成为了人工智能领域的中坚力量。商业 LLM 的价格通常高昂且代码封闭，限制了研究者和开发者的探索空间。幸运的是，开源社区提供了像 Ollama 这样优秀的替代方案，让每个人都能够轻松体验 LLM 的魅力，并能结合 HPC 和 IDE 插件，打造更强大的个人助手。&lt;/p>
&lt;h2 id="什么是-ollama">什么是 Ollama？&lt;/h2>
&lt;p>Ollama 是一个用于构建大型语言模型应用的工具，它提供了一个简洁易用的命令行界面和服务器，让你能够轻松下载、运行和管理各种开源 LLM。与需要复杂配置和强大硬件的传统 LLM 不同，Ollama 让你能够方便地像使用手机 App 一样体验 LLM 的强大功能。&lt;/p>
&lt;h2 id="ollama-的优势">Ollama 的优势&lt;/h2>
&lt;p>Ollama 拥有以下显著优势：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>开源免费&lt;/strong>： Ollama 及其支持的模型完全开源免费，任何人都可以自由使用、修改和分发。&lt;/li>
&lt;li>&lt;strong>简单易用&lt;/strong>： 无需复杂的配置和安装过程，只需几条命令即可启动和运行 Ollama。&lt;/li>
&lt;li>&lt;strong>模型丰富&lt;/strong>： Ollama 支持 Llama 3、Mistral、Qwen2 等众多热门开源 LLM，并提供一键下载和切换功能。&lt;/li>
&lt;li>&lt;strong>资源占用低&lt;/strong>： 相比于商业 LLM，Ollama 对硬件要求更低，即使在普通笔记本电脑上也能流畅运行。&lt;/li>
&lt;li>&lt;strong>社区活跃&lt;/strong>： Ollama 拥有庞大且活跃的社区，用户可以轻松获取帮助、分享经验和参与模型开发。&lt;/li>
&lt;/ul>
&lt;h2 id="如何使用-ollama">如何使用 Ollama？&lt;/h2>
&lt;p>使用 Ollama 非常简单，只需要按照以下步骤：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>安装 Ollama&lt;/strong>： 根据你的操作系统，从 &lt;a class="link" href="https://ollama.com/" target="_blank" rel="noopener" >Ollama 官网
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
下载并安装最新版本。&lt;/li>
&lt;li>&lt;strong>启动 Ollama&lt;/strong>： 打开终端或命令行，输入 &lt;code>ollama serve&lt;/code> 命令启动 Ollama 服务器。&lt;/li>
&lt;li>&lt;strong>下载模型&lt;/strong>： 在&lt;a class="link" href="https://ollama.com/library" target="_blank" rel="noopener" >模型仓库
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
找到想要的模型，然后使用 &lt;code>ollama pull&lt;/code> 命令下载，例如 &lt;code>ollama pull llama3:70b&lt;/code> 。&lt;/li>
&lt;li>&lt;strong>运行模型&lt;/strong>： 使用 &lt;code>ollama run&lt;/code> 命令启动模型，例如 &lt;code>ollama run llama3:70b&lt;/code> 。&lt;/li>
&lt;li>&lt;strong>开始聊天&lt;/strong>： 在终端中输入你的问题或指令，Ollama 会根据模型生成相应的回复。&lt;/li>
&lt;/ol>
&lt;h3 id="安装-ollama">安装 Ollama&lt;/h3>
&lt;h4 id="macos">macOS&lt;/h4>
&lt;p>&lt;a class="link" href="https://ollama.com/download/Ollama-darwin.zip" target="_blank" rel="noopener" >下载 Ollama for macOS
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h4 id="windows">Windows&lt;/h4>
&lt;p>&lt;a class="link" href="https://ollama.com/download/OllamaSetup.exe" target="_blank" rel="noopener" >下载 Ollama for Windows
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h4 id="linux">Linux&lt;/h4>
&lt;pre>&lt;code class="language-bash">curl -fsSL https://ollama.com/install.sh | sh
&lt;/code>&lt;/pre>
&lt;h4 id="docker">Docker&lt;/h4>
&lt;h5 id="cpu-版本">CPU 版本&lt;/h5>
&lt;pre>&lt;code class="language-bash">docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
&lt;/code>&lt;/pre>
&lt;h5 id="gpu-版本">GPU 版本&lt;/h5>
&lt;ol>
&lt;li>安装 &lt;a class="link" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation" target="_blank" rel="noopener" >Nvidia container toolkit
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;li>在 Docker 容器中运行 Ollama&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
&lt;/code>&lt;/pre>
&lt;h3 id="启动-ollama">启动 Ollama&lt;/h3>
&lt;pre>&lt;code class="language-bash">ollama serve
&lt;/code>&lt;/pre>
&lt;p>输出以下信息表示 Ollama 服务器已成功启动（V100 机器）：&lt;/p>
&lt;pre>&lt;code class="language-bash">$ ollama serve
### 省略的日志输出 ###
Listening on [::]:11434 (version 0.1.42)
&lt;/code>&lt;/pre>
&lt;h3 id="下载模型">下载模型&lt;/h3>
&lt;pre>&lt;code class="language-bash">ollama pull qwen2:72b
&lt;/code>&lt;/pre>
&lt;h3 id="运行模型">运行模型&lt;/h3>
&lt;pre>&lt;code class="language-bash">ollama run qwen2:72b
&lt;/code>&lt;/pre>
&lt;p>例如，运行如下命令后：&lt;/p>
&lt;pre>&lt;code class="language-bash">$ ollama run qwen2:72b
&amp;gt;&amp;gt;&amp;gt; Who are you?
I am Qwen, a pre-trained language model developed by Alibaba Cloud. My purpose is to assist users in generating various types of text, such as articles, stories, poems, and answering
questions by using the natural language processing techniques. How may I assist you today?
&amp;gt;&amp;gt;&amp;gt; Send a message(/? for help)
&lt;/code>&lt;/pre>
&lt;h4 id="docker-容器中运行模型">Docker 容器中运行模型&lt;/h4>
&lt;pre>&lt;code class="language-bash">docker exec -it ollama ollama run qwen2:72b
&lt;/code>&lt;/pre>
&lt;h3 id="配置-ollama">配置 Ollama&lt;/h3>
&lt;p>Ollama 提供了多种环境变量以供配置：&lt;/p>
&lt;ul>
&lt;li>&lt;code>OLLAMA_DEBUG&lt;/code>：是否开启调试模式，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_FLASH_ATTENTION&lt;/code>：是否闪烁注意力，默认为 &lt;code>true&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_HOST&lt;/code>：Ollama 服务器的主机地址，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_KEEP_ALIVE&lt;/code>：保持连接的时间，默认为 &lt;code>5m&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_LLM_LIBRARY&lt;/code>：LLM 库，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_LOADED_MODELS&lt;/code>：最大加载模型数，默认为 &lt;code>1&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_QUEUE&lt;/code>：最大队列数，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_VRAM&lt;/code>：最大虚拟内存，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MODELS&lt;/code>：模型目录，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_NOHISTORY&lt;/code>：是否保存历史记录，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_NOPRUNE&lt;/code>：是否启用剪枝，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_NUM_PARALLEL&lt;/code>：并行数，默认为 &lt;code>1&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_ORIGINS&lt;/code>：允许的来源，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_RUNNERS_DIR&lt;/code>：运行器目录，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_SCHED_SPREAD&lt;/code>：调度分布，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_TMPDIR&lt;/code>：临时文件目录，默认为空。Here is the optimized list in the desired format:&lt;/li>
&lt;li>&lt;code>OLLAMA_DEBUG&lt;/code>：是否开启调试模式，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_FLASH_ATTENTION&lt;/code>：是否闪烁注意力，默认为 &lt;code>true&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_HOST&lt;/code>：Ollama 服务器的主机地址，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_KEEP_ALIVE&lt;/code>：保持连接的时间，默认为 &lt;code>5m&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_LLM_LIBRARY&lt;/code>：LLM 库，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_LOADED_MODELS&lt;/code>：最大加载模型数，默认为 &lt;code>1&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_QUEUE&lt;/code>：最大队列数，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_VRAM&lt;/code>：最大虚拟内存，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MODELS&lt;/code>：模型目录，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_NOHISTORY&lt;/code>：是否保存历史记录，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_NOPRUNE&lt;/code>：是否启用剪枝，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_NUM_PARALLEL&lt;/code>：并行数，默认为 &lt;code>1&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_ORIGINS&lt;/code>：允许的来源，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_RUNNERS_DIR&lt;/code>：运行器目录，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_SCHED_SPREAD&lt;/code>：调度分布，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_TMPDIR&lt;/code>：临时文件目录，默认为空。&lt;/li>
&lt;/ul>
&lt;h2 id="进阶用法hpc-集群上部署-ollama">进阶用法：HPC 集群上部署 Ollama&lt;/h2>
&lt;p>对于大型模型或需要更高性能的情况，可以利用 HPC 集群的强大算力来运行 Ollama。结合 Slurm 进行任务管理，并使用端口映射将服务暴露到本地，即可方便地进行远程访问和使用：&lt;/p>
&lt;ol>
&lt;li>在登录节点配置 Ollama 环境： 安装 Ollama，并下载需要的模型。&lt;/li>
&lt;li>&lt;strong>编写 slurm 脚本&lt;/strong>： 指定资源需求（CPU、内存、GPU 等），并使用 &lt;code>ollama serve&lt;/code> 命令启动模型服务，并绑定到特定端口。&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">#!/bin/bash
#SBATCH --job-name=ollama
#SBATCH -N 1
#SBATCH -p GPU
#SBATCH --gres=gpu:1
module load CUDA
ollama serve
&lt;/code>&lt;/pre>
&lt;ol start="3">
&lt;li>&lt;strong>提交 slurm 任务&lt;/strong>: 使用 &lt;code>sbatch&lt;/code> 命令提交脚本，Slurm 会将任务分配到计算节点运行。&lt;/li>
&lt;li>&lt;strong>本地端口映射&lt;/strong>： 使用 ssh -L 命令将计算节点的端口映射到本地，例如:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">ssh -t -t 用户名@登录节点 ip -L 11434:localhost:11434 -i 登录节点私钥 ssh 计算节点 IP -L 11434:127.0.0.1:11434
&lt;/code>&lt;/pre>
&lt;ol start="5">
&lt;li>&lt;strong>本地访问&lt;/strong>： 在浏览器或应用程序中访问 http://localhost:11434 即可使用 Ollama 服务。&lt;/li>
&lt;/ol>
&lt;div class="notice notice-tip" >
&lt;div class="notice-title">&lt;svg t="1705945832245" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="19409" width="200" height="200">&lt;path d="M512 74.666667C270.933333 74.666667 74.666667 270.933333 74.666667 512S270.933333 949.333333 512 949.333333 949.333333 753.066667 949.333333 512 753.066667 74.666667 512 74.666667z m238.933333 349.866666l-2.133333 2.133334-277.333333 277.333333c-10.666667 10.666667-29.866667 12.8-42.666667 2.133333L426.666667 704l-149.333334-149.333333c-12.8-12.8-12.8-32 0-44.8 10.666667-10.666667 29.866667-12.8 42.666667-2.133334l2.133333 2.133334 125.866667 125.866666 253.866667-253.866666c10.666667-10.666667 29.866667-12.8 42.666666-2.133334l2.133334 2.133334c12.8 12.8 12.8 32 4.266666 42.666666z" fill="#ffffff" p-id="19410">&lt;/path>&lt;/svg>&lt;/div>&lt;p>注意：由于计算节点不联网，需要提前在登录节点使用 &lt;code>ollama pull&lt;/code> 下载所需模型。此外，需要设置 &lt;code>OLLAMA_ORIGINS&lt;/code> 为 &lt;code>*&lt;/code>，设置 &lt;code>OLLAMA_HOST&lt;/code> 为 &lt;code>0.0.0.0&lt;/code>，以允许所有来源访问服务。&lt;/p>&lt;/div>
&lt;h2 id="进阶用法本地代码补全助手">进阶用法：本地代码补全助手&lt;/h2>
&lt;p>Ollama 不仅可以用于聊天和文本创作，还可以结合代码生成模型和 IDE 插件，打造强大的代码补全助手。例如，使用 Codeqwen 7B 模型和 VS Code 插件 &lt;a class="link" href="https://continue.dev/" target="_blank" rel="noopener" >Continue
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
，可以实现高效便捷的代码补全功能。&lt;/p>
&lt;p>首先介绍一下 Continue :
&lt;blockquote>
&lt;p>&lt;p>&lt;a class="link" href="https://continue.dev/" target="_blank" rel="noopener" >Continue
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
enables you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs. All this can run entirely on your own laptop or have Ollama deployed on a server to remotely power code completion and chat experiences based on your needs.&lt;/p>
&lt;p>Continue 使您能够轻松地在 Visual Studio Code 和 JetBrains 中创建自己的代码助手，利用开源 LLM。这一切都可以完全在您的笔记本电脑上运行，或者在服务器上部署 Ollama，远程根据您的需求提供代码补全和聊天体验。&lt;/p>
&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Continue&lt;/span>&lt;cite>&lt;/cite>&lt;/span>&lt;/blockquote>&lt;/p>
&lt;p>在开始之前，你需要安装如下工具：&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://docs.continue.dev/quickstart" target="_blank" rel="noopener" >Continue
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
：&lt;a class="link" href="https://marketplace.visualstudio.com/items?itemName=Continue.continue" target="_blank" rel="noopener" >VS Code 版本
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
或 &lt;a class="link" href="https://plugins.jetbrains.com/plugin/22707-continue" target="_blank" rel="noopener" >JetBrains 版本
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;li>&lt;a class="link" href="https://ollama.com/" target="_blank" rel="noopener" >Ollama
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;/ul>
&lt;p>接下来，我们以 VS Code 为例，介绍如何使用 Ollama + Continue 实现代码补全功能：&lt;/p>
&lt;h3 id="codestral-22b-模型">Codestral 22B 模型&lt;/h3>
&lt;p>Codestral 既能完成代码自动补全，也支持聊天功能。但鉴于其拥有 220 亿参数且不具备生产许可，它对显存要求颇高，仅限于研究和测试使用，因此可能并不适合日常本地应用。&lt;/p>
&lt;h4 id="下载并运行-codestral-模型">下载并运行 Codestral 模型&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run codestral
&lt;/code>&lt;/pre>
&lt;h4 id="配置-configjson">配置 config.json&lt;/h4>
&lt;ul>
&lt;li>在 VS Code 侧边栏点击 Continue 插件图标，然后在面板右下角点击 “齿轮” 图标，打开 &lt;code>config.json&lt;/code> 文件。然后复制以下配置到 &lt;code>config.json&lt;/code> 文件中：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-json">{
&amp;quot;models&amp;quot;: [
{
&amp;quot;title&amp;quot;: &amp;quot;Codestral&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;codestral&amp;quot;
}
],
&amp;quot;tabAutocompleteModel&amp;quot;: {
&amp;quot;title&amp;quot;: &amp;quot;Codestral&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;codestral&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="deepseek-coder-67b-模型--llama-3-8b-模型">DeepSeek Coder 6.7B 模型 + Llama 3 8B 模型&lt;/h3>
&lt;p>根据机器的显存大小，可以利用 Ollama 同时运行多个模型并处理多个并发请求的能力，使用 &lt;code>DeepSeek Coder 6.7B&lt;/code> 进行自动补全，&lt;code>Llama 3 8B&lt;/code> 进行聊天。如果你的机器无法同时运行两者，那么可以分别尝试，决定你更偏好本地自动补全还是本地聊天体验。&lt;/p>
&lt;h4 id="下载并运行-deepseek-coder-模型">下载并运行 DeepSeek Coder 模型&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run deepseek-coder:6.7b-base
&lt;/code>&lt;/pre>
&lt;h4 id="下载并运行-llama-3-模型">下载并运行 Llama 3 模型&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run llama3:8b
&lt;/code>&lt;/pre>
&lt;h4 id="配置-configjson-1">配置 config.json&lt;/h4>
&lt;pre>&lt;code class="language-json">{
&amp;quot;models&amp;quot;: [
{
&amp;quot;title&amp;quot;: &amp;quot;Llama 3 8B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;llama3:8b&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
],
&amp;quot;tabAutocompleteModel&amp;quot;: {
&amp;quot;title&amp;quot;: &amp;quot;DeepSeek Coder 6.7B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;deepseek-coder:6.7b-base&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="codeqwen-7b-模型--qwen2-7b-模型">Codeqwen 7B 模型 + Qwen2 7B 模型&lt;/h3>
&lt;p>Codeqwen 7B 模型是一个专门用于代码补全的模型，而 Qwen2 7B 模型则是一个通用的聊天模型。这两个模型可以很好地结合在一起，实现代码补全和聊天功能。&lt;/p>
&lt;h4 id="下载并运行-codeqwen-模型">下载并运行 Codeqwen 模型&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run codeqwen
&lt;/code>&lt;/pre>
&lt;h4 id="下载并运行-qwen2-模型">下载并运行 Qwen2 模型&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run qwen2:7b
&lt;/code>&lt;/pre>
&lt;h4 id="配置-configjson-2">配置 config.json&lt;/h4>
&lt;pre>&lt;code class="language-json">{
&amp;quot;models&amp;quot;: [
{
&amp;quot;title&amp;quot;: &amp;quot;Codeqwen 7B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;codeqwen&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
],
&amp;quot;tabAutocompleteModel&amp;quot;: {
&amp;quot;title&amp;quot;: &amp;quot;Qwen2 7B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;qwen2:7b&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="利用-rag-向量检索优化聊天">利用 RAG 向量检索优化聊天&lt;/h3>
&lt;p>Continue 内置了 &lt;a class="link" href="https://docs.continue.dev/customization/context-providers#codebase-retrieval" target="_blank" rel="noopener" >@codebase
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
上下文提供器，能自动从代码库中检索到最相关的代码片段。假如你已经设置好了聊天模型（例如 Codestral、Llama 3），那么借助 Ollama 和 &lt;a class="link" href="https://blog.lancedb.com/lancedb-x-continue/" target="_blank" rel="noopener" >LanceDB
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的向量化技术，可以实现更高效的代码检索和聊天体验。&lt;/p>
&lt;p>这里，我们使用 &lt;code>nomic-embed-text&lt;/code> 模型作为向量检索模型：&lt;/p>
&lt;h4 id="下载并运行-nomic-embed-text-模型">下载并运行 Nomic Embed Text 模型&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run nomic-embed-text
&lt;/code>&lt;/pre>
&lt;h4 id="配置-configjson-3">配置 config.json&lt;/h4>
&lt;ul>
&lt;li>在文件中添加以下内容：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-json">{
&amp;quot;embeddingsProvider&amp;quot;: {
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;nomic-embed-text&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="代码补全效果">代码补全效果&lt;/h3>
&lt;ul>
&lt;li>&lt;code>Ctrl + I&lt;/code>: 根据指令生成代码片段。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_1-2024-06-17.webp"
alt="codeqwen_1-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_2-2024-06-17.webp"
alt="codeqwen_2-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_3-2024-06-17.webp"
alt="codeqwen_3-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>光标悬停自动补全代码&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_4-2024-06-17.webp"
alt="codeqwen_4-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;h3 id="与-ollama-聊天">与 Ollama 聊天&lt;/h3>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_5-2024-06-17.webp"
alt="codeqwen_5-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;h3 id="代码自动注释">代码自动注释&lt;/h3>
&lt;ul>
&lt;li>选中代码打开右键菜单&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_6-2024-06-17.webp"
alt="codeqwen_6-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_7-2024-06-17.webp"
alt="codeqwen_7-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>Ollama 为我们打开了通往开源 LLM 世界的大门，让每个人都能轻松体验 LLM 的强大功能，并可以根据自身需求进行定制化应用。无论是进行研究、开发，还是日常使用，Ollama 都能为你提供探索 LLM 无限可能的平台。相信随着 Ollama 的不断发展，它将为我们带来更多惊喜，推动 LLM 技术在各个领域的应用和发展。&lt;/p></description></item><item><title>RDMA 之 Address Handle</title><link>https://cuterwrite.top/p/rdma-address-handle/</link><pubDate>Sat, 15 Jun 2024 01:00:00 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-address-handle/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116373922_p9_master1200.webp" alt="Featured image of post RDMA 之 Address Handle" />&lt;h1 id="rdma-之-address-handle">RDMA 之 Address Handle&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/163552044">&lt;cite>知乎专栏：8. RDMA 之 Address Handle&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>前面已经介绍过，RDMA 通信的基本单元是 QP。我们来思考一个问题，假设 A 节点的某个 QP 要跟 B 节点的某个 QP 交换信息，除了要知道 B 节点的 QP 序号——QPN 之外，还需要什么信息？要知道，QPN 是每个节点独立维护的序号，不是整个网络中唯一的。比如 A 的 QP 3 要跟 B 的 QP 5 通信，网络中可不止一个 QP5，可能有很多个节点都有自己的 QP 5。所以我们自然可以想到，还需要找到让每个节点都有一个独立的标识。&lt;/p>
&lt;p>在传统 TCP-IP 协议栈中，使用了家喻户晓的 IP 地址来标识网络层的每个节点。而 IB 协议中的这个标识被称为&lt;strong>GID（Global Identifier，全局 ID）&lt;/strong>，是一个 128 bits 的序列。关于 GID 本篇不展开讨论，将在后面介绍。&lt;/p>
&lt;h2 id="ah-是什么">AH 是什么&lt;/h2>
&lt;p>AH 全称为 Address Handle，没有想到特别合适的中文翻译，就先直译为“地址句柄”吧。这里的地址，指的是一组用于找到某个远端节点的信息的集合，在 IB 协议中，地址指的是 GID、端口号等等信息；而所谓句柄，我们可以理解为一个指向某个对象的指针。&lt;/p>
&lt;p>大家是否还记得 IB 协议中有四种基本服务类型——RC、UD、RD 和 UC，其中最常用的是 RC 和 UD。RC 的特点是两个节点的 QP 之间会建立可靠的连接，一旦建立连接关系便不容易改变，对端的信息是创建 QP 的时候储存在 QP Context 中的；&lt;/p>
&lt;p>而对于 UD 来说，QP 间没有连接关系，用户想发给谁，就在 WQE 中填好对端的地址信息就可以了。&lt;strong>用户不是直接把对端的地址信息填到 WQE 中的，而是提前准备了一个“地址薄”，每次通过一个索引来指定对端节点的地址信息，而这个索引就是 AH。&lt;/strong>&lt;/p>
&lt;p>AH 的概念大致可以用下图表示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_8_1.webp"
alt="2024-06-16_8_1" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Address Handle 功能示意图&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>对于每一个目的节点，本端都会创建一个对应的 AH，而同一个 AH 可以被多个 QP 共同使用。&lt;/p>
&lt;h2 id="ah-的作用">AH 的作用&lt;/h2>
&lt;p>每次进行 UD 服务类型的通信之前，用户都需要先通过 IB 框架提供的接口，来&lt;strong>为每一个可能的对端节点创建一个 AH&lt;/strong>，然后这些 AH 会被驱动放到一个“安全”的区域，并返回一个索引（指针/句柄）给用户。用户真正下发 WR（Work Request）时，就把这个索引传递进来就可以了。&lt;/p>
&lt;p>上述过程如下图所示，A 节点收到用户的这样一个任务——使用本端的 QP4 与 B 节点（通过 AH 指定）的 QP3 进行数据交换：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_8_2.webp"
alt="2024-06-16_8_2" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>UD 服务类型使用 AH 指定对端节点&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>IB 协议中并没有对为什么使用 AH 做出解释，我认为定义 AH 的概念的原因有以下三种：&lt;/p>
&lt;ol>
&lt;li>保证目的地址可用，提高效率&lt;/li>
&lt;/ol>
&lt;p>因为 UD 无连接的特点，用户可以在用户态直接通过 WR 来指定目的地。而如果让用户随意填写地址信息，然后硬件就根据这些信息进行组包的话，是会带来问题的。比如有这样一种场景：用户通过 WR 告诉硬件请给 GID 为 X，MAC 地址为 Y 的节点的端口 Z 发送数据。然而 X，Y，Z 可能不是一个合法的组合，或者 GID 为 X 的节点压根都不存在于网络中，而硬件是无力校验这些内容的，只能乖乖的组包、发送数据，这个目的地无效的数据包就白白发送出去了。&lt;/p>
&lt;p>而提前准备好地址信息，则可以避免上述情况。用户在创建 AH 时会陷入内核态，如果用户传递的参数有效，内核会把这些目的节点信息储存起来，生成一个指针返回给用户；如果用户传递的参数无效，AH 将创建失败。这一过程可以保证地址信息是有效的。用户通过指针就可以快速指定目的节点，加快数据交互流程。&lt;/p>
&lt;p>可能有人会问，既然内核是可信的，为什么不能在发送数据时陷入内核态去校验用户传递的地址信息呢？请别忘了 RDMA 技术的一大优势在哪里——数据流程可以直接从用户空间到硬件，完全绕过内核，这样可以避免系统调用和拷贝的开销。如果每次发送都要检验地址合法性的话，必然会降低通信速率。&lt;/p>
&lt;ol start="2">
&lt;li>向用户隐藏底层地址细节&lt;/li>
&lt;/ol>
&lt;p>用户创建 AH 时，只需要传递 gid、端口号、静态速率等信息，而其他通信所需的地址信息（主要是 MAC 地址）是内核驱动通过查询系统邻居表等方式解析到的，底层没有必要暴露这些额外的信息给用户层。&lt;/p>
&lt;ol start="3">
&lt;li>可以使用 PD 对目的地址进行管理&lt;/li>
&lt;/ol>
&lt;p>前文我们介绍保护域时曾经提过，除了 QP、MR 之外，AH 也由 PD 来进行资源划分。当定义了 AH 这个软件实体之后，我们就可以对所有的 QP 可达的目的地进行相互隔离和管理。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_8_3.webp"
alt="2024-06-16_8_3" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>使用 PD 隔离 AH&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>​比如上图中，AH1~3 只能被同一个 PD 下的 QP3 和 QP9 使用，而 AH4 只能被 QP5 使用。&lt;/p>
&lt;h2 id="协议相关章节">协议相关章节&lt;/h2>
&lt;p>协议中关于 AH 的篇幅并不多，甚至没有独立介绍其概念的章节：&lt;/p>
&lt;p>[1] 9.8.3 UD 服务类型中的目的地址由哪些部分组成：包括 AH、 QPN 和 Q_key&lt;/p>
&lt;p>[2] 10.2.2.2 目的地址的相关注意事项&lt;/p>
&lt;p>[3] 11.2.2.1 AH 相关的 Verbs 接口&lt;/p>
&lt;p>AH 就介绍到这里，感谢阅读。下一篇打算向大家描述更多关于 QP 的细节。&lt;/p></description></item></channel></rss>