<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>cuterwrite</title><link>https://cuterwrite.top/</link><description>Recent content on cuterwrite</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>cuterwrite</copyright><lastBuildDate>Tue, 20 Feb 2024 01:51:00 +0000</lastBuildDate><atom:link href="https://cuterwrite.top/index.xml" rel="self" type="application/rss+xml"/><item><title>搭建玄铁 900 系列工具链与 xuantie-qemu 环境</title><link>https://cuterwrite.top/p/thead-tools/</link><pubDate>Tue, 20 Feb 2024 01:51:00 +0000</pubDate><guid>https://cuterwrite.top/p/thead-tools/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/59aa9fecb7e1a3a2b2c88811e6360647195413.jpg@1256w_774h_!web-article-pic-2024-02-20.webp" alt="Featured image of post 搭建玄铁 900 系列工具链与 xuantie-qemu 环境" />&lt;h1 id="搭建玄铁-900-系列工具链与-xuantie-qemu-环境">搭建玄铁 900 系列工具链与 xuantie-qemu 环境&lt;/h1>
&lt;h2 id="一搭建平台">一、搭建平台&lt;/h2>
&lt;ul>
&lt;li>Linux 发行版：CentOS Linux release 7.6.1810 (Core)&lt;/li>
&lt;li>内核版本：3.10.0-957.el7.x86_64&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">$ cat /etc/centos-release
CentOS Linux release 7.6.1810 (Core)
$ uname -r
3.10.0-957.el7.x86_64
&lt;/code>&lt;/pre>
&lt;h2 id="二搭建玄铁-900-系列工具链环境">二、搭建玄铁 900 系列工具链环境&lt;/h2>
&lt;h3 id="1-下载玄铁-900-系列工具链">1. 下载玄铁 900 系列工具链&lt;/h3>
&lt;p>首先，我们需要下载适用于 RISC-V 架构的 Xuantie GNU 工具链。前往&lt;a class="link" href="https://www.xrvm.cn/" target="_blank" rel="noopener" >玄铁官网
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
获取最新版本的预编译包，并根据你的操作系统进行安装。在 Linux 系统中，通常解压后通过添加 &lt;code>bin&lt;/code> 路径到 &lt;code>$PATH&lt;/code> 环境变量即可。&lt;/p>
&lt;p>工具链安装包由于执行平台和目标程序平台的不同分为不同的版本，如 Xuantie-&lt;em>-elf-&lt;/em>-x86_64-V*-.tar.gz 是 64 位 linux 平台的 riscv 裸程序工具链套件。具体分类如下：&lt;/p>
&lt;ul>
&lt;li>根据执行平台
&lt;ul>
&lt;li>x86_64：64 位 linux 平台&lt;/li>
&lt;li>i386：32 位 linux 平台&lt;/li>
&lt;li>mingw：Windows Mingw 平台&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>根据目标程序平台
&lt;ul>
&lt;li>elf：裸程序编译套件&lt;/li>
&lt;li>linux：linux 应用程序编译套件&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>这里我们下载最新的版本为 2.8.1 的适用于 64 位 linux 平台的 linux 应用程序编译套件，即 Xuantie-900-gcc-linux-5.10.4-glibc-x86_64 。&lt;/p>
&lt;pre>&lt;code class="language-bash">wget https://occ-oss-prod.oss-cn-hangzhou.aliyuncs.com/resource//1705395627867/Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.1-20240115.tar.gz
tar -xzvf Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.1-20240115.tar.gz
sudo mv Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.1-20240115 /opt
export PATH=/opt/Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.1-20240115/bin:$PATH
&lt;/code>&lt;/pre>
&lt;h3 id="2-验证工具链安装">2. 验证工具链安装&lt;/h3>
&lt;pre>&lt;code class="language-bash">$ riscv64-unknown-linux-gnu-gcc -v
Using built-in specs.
COLLECT_GCC=riscv64-unknown-linux-gnu-gcc
COLLECT_LTO_WRAPPER=/opt/Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0/bin/../libexec/gcc/riscv64-unknown-linux-gnu/10.4.0/lto-wrapper
Target: riscv64-unknown-linux-gnu
Configured with: /mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/./source/riscv/riscv-gcc/configure --target=riscv64-unknown-linux-gnu --with-gmp=/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/build-gcc-riscv64-unknown-linux-gnu/build-Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0/lib-for-gcc-x86_64-linux --with-mpfr=/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/build-gcc-riscv64-unknown-linux-gnu/build-Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0/lib-for-gcc-x86_64-linux --with-mpc=/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/build-gcc-riscv64-unknown-linux-gnu/build-Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0/lib-for-gcc-x86_64-linux --with-libexpat-prefix=/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/build-gcc-riscv64-unknown-linux-gnu/build-Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0/lib-for-gcc-x86_64-linux --with-libmpfr-prefix=/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/build-gcc-riscv64-unknown-linux-gnu/build-Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0/lib-for-gcc-x86_64-linux --with-pkgversion='Xuantie-900 linux-5.10.4 glibc gcc Toolchain V2.8.0 B-20231018' CXXFLAGS='-g -O2 -DTHEAD_VERSION_NUMBER=2.8.0 ' --prefix=/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/build-gcc-riscv64-unknown-linux-gnu/Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0 --with-sysroot=/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/build-gcc-riscv64-unknown-linux-gnu/Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0/sysroot --with-system-zlib --enable-shared --enable-tls --enable-languages=c,c++,fortran --disable-libmudflap --disable-libssp --disable-libquadmath --enable-libsanitizer --disable-nls --disable-bootstrap --src=https://cuterwrite.top/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/./source/riscv/riscv-gcc --enable-multilib --with-abi=lp64d --with-arch=rv64gc_zfh_xtheadc 'CFLAGS_FOR_TARGET=-O2 -mcmodel=medany' 'CXXFLAGS_FOR_TARGET=-O2 -mcmodel=medany'
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 10.4.0 (Xuantie-900 linux-5.10.4 glibc gcc Toolchain V2.8.0 B-20231018)
&lt;/code>&lt;/pre>
&lt;p>可以看到输出了 gcc 的版本信息，说明工具链安装成功。&lt;/p>
&lt;h2 id="三搭建-xuantie-qemu-环境">三、搭建 xuantie-qemu 环境&lt;/h2>
&lt;h3 id="1-前提条件">1. 前提条件&lt;/h3>
&lt;p>在安装 xuantie-qemu 之前，需要确保系统含有以下工具或库。&lt;/p>
&lt;ul>
&lt;li>gcc 编译器&lt;/li>
&lt;li>automake&lt;/li>
&lt;li>autoconf&lt;/li>
&lt;li>libtool&lt;/li>
&lt;li>glib2 库&lt;/li>
&lt;li>其它&amp;hellip;..&lt;/li>
&lt;/ul>
&lt;p>通过以下命令安装上述工具或库。&lt;/p>
&lt;pre>&lt;code class="language-bash">sudo yum update -y
sudo yum install -y autoconf automake libtool make gcc gcc-c++ gawk bison flex texinfo gperf patchutils bc \
zlib-devel mpfr-devel gmp-devel curl-devel expat-devel git \
glib2-devel libfdt-devel pixman-devel ncurses-devel ncurses-compat-libs
&lt;/code>&lt;/pre>
&lt;p>如果是 Ubuntu/Dedian 系统，可以使用以下命令安装。&lt;/p>
&lt;pre>&lt;code class="language-bash">sudo apt-get update
sudo apt-get install -y autoconf automake autotools-dev curl libmpc-dev libmpfr-dev libgmp-dev \
gawk build-essential bison flex texinfo gperf libtool patchutils bc \
zlib1g-dev libexpat-dev git \
libglib2.0-dev libfdt-dev libpixman-1-dev \
libncurses5-dev libncursesw5-dev
&lt;/code>&lt;/pre>
&lt;h3 id="2-下载并安装-xuantie-qemu">2. 下载并安装 xuantie-qemu&lt;/h3>
&lt;p>访问 &lt;a class="link" href="https://github.com/T-head-Semi/qemu.git" target="_blank" rel="noopener" >Xuantie QEMU 官方仓库
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
，获取适用于玄铁 900 系列芯片的 xuantie-qemu 源代码，然后按照常规步骤编译安装：&lt;/p>
&lt;pre>&lt;code class="language-bash">git clone https://github.com/T-head-Semi/qemu.git
git checkout xuantie-qemu-6.1.0
&lt;/code>&lt;/pre>
&lt;h3 id="3-编译安装-xuantie-qemu">3. 编译安装 xuantie-qemu&lt;/h3>
&lt;pre>&lt;code class="language-bash">cd qemu
mkdir build
cd build
../configure --target-list=riscv64-softmmu,riscv64-linux-user --prefix=/opt/qemu/6.1.0-xuantie
make -j $(nproc)
sudo make install
export PATH=/opt/qemu/6.1.0-xuantie/bin:$PATH
&lt;/code>&lt;/pre>
&lt;h3 id="4-验证-xuantie-qemu-安装">4. 验证 xuantie-qemu 安装&lt;/h3>
&lt;p>安装完毕后如果执行如下命令后能够查看到 qemu 的具体版本，则说明安装成功&lt;/p>
&lt;pre>&lt;code class="language-bash">$ qemu-riscv64 --version
qemu-riscv64 version 6.0.94 (v6.1.0-12-g03813c9)
Copyright (c) 2003-2021 Fabrice Bellard and the QEMU Project developers
&lt;/code>&lt;/pre>
&lt;p>编写一段 C 语言程序，如下所示：&lt;/p>
&lt;pre>&lt;code class="language-c">#include &amp;lt;stdio.h&amp;gt;
int main() {
printf(&amp;quot;Hello RISC-V \n&amp;quot;);
return 0;
}
&lt;/code>&lt;/pre>
&lt;p>使用 Xuantie 900 系列工具链编译该程序，并使用用户模式的 xuantie-qemu 运行程序。&lt;/p>
&lt;pre>&lt;code class="language-bash">$ riscv64-unknown-linux-gnu-gcc -static -o hello hello.c
$ qemu-riscv64 ./hello
Hello RISC-V
&lt;/code>&lt;/pre>
&lt;p>再写一段 RVV 向量化的 C 语言程序，如下所示：&lt;/p>
&lt;details>
&lt;summary>&lt;strong>RVV 向量化 C 语言程序&lt;/strong>&lt;/summary>
&lt;pre>&lt;code class="language-c">#include &amp;lt;riscv_vector.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#define N 15
float vsum(float* v, int n) {
vfloat32m1_t vs, vv, vtmp;
float s = 0.0;
int i;
int vlmax;
vlmax = vsetvlmax_e32m1();
printf(&amp;quot;vlmax:%d\n&amp;quot;, vlmax);
vs = vfmv_v_f_f32m1(0.0, vlmax);
vtmp = vfmv_v_f_f32m1(0.0, vlmax);
for (i = 0; i &amp;lt; n - vlmax; i += vlmax) {
vv = vle32_v_f32m1(&amp;amp;v[i], vlmax);
vtmp = vfadd_vv_f32m1(vtmp, vv, vlmax);
}
vs = vfredusum_vs_f32m1_f32m1(vs, vtmp, vs, vlmax);
s = vfmv_f_s_f32m1_f32(vs);
for (; i &amp;lt; n; i++) {
s += v[i];
}
return s;
}
float vsum1(float* v, int n) {
vfloat32m1_t vs, vv;
float s;
int i;
int vl, vlmax;
vlmax = vsetvlmax_e32m1();
vs = vfmv_v_f_f32m1(0.0, vlmax);
for (i = 0; n &amp;gt; 0; i += vl, n -= vl) {
vl = vsetvl_e32m1(n);
printf(&amp;quot;vl:%d\n&amp;quot;, vl);
vv = vle32_v_f32m1(&amp;amp;v[i], vl);
vs = vfredusum_vs_f32m1_f32m1(vs, vv, vs, vl);
}
s = vfmv_f_s_f32m1_f32(vs);
return s;
}
float vsum2(float* v, int n) {
vfloat32m2_t vv;
vfloat32m1_t vs;
float s;
int i;
int vl, vlmax;
vlmax = vsetvlmax_e32m1();
vs = vfmv_v_f_f32m1(0.0, vlmax);
for (i = 0; n &amp;gt; 0; i += vl, n -= vl) {
vl = vsetvl_e32m2(n);
printf(&amp;quot;vl:%d\n&amp;quot;, vl);
vv = vle32_v_f32m2(&amp;amp;v[i], vl);
vs = vfredusum_vs_f32m2_f32m1(vs, vv, vs, vl);
}
s = vfmv_f_s_f32m1_f32(vs);
return s;
}
int main() {
int i;
float v[N], sum = 0.0;
printf(&amp;quot;Hello RISC-V!\n&amp;quot;);
for (i = 0; i &amp;lt; N; i++) {
v[i] = i;
}
sum = vsum(v, N);
printf(&amp;quot;%f\n&amp;quot;, sum);
return 0;
}
&lt;/code>&lt;/pre>
&lt;/details>
&lt;p>编译并运行该程序（这时需要指定 &lt;code>-cpu&lt;/code> ，否则会报非法指定的异常，即 Illegal instruction (core dumped)）：&lt;/p>
&lt;pre>&lt;code class="language-bash">$ riscv64-unknown-linux-gnu-gcc -static -O3 -march=rv64imafdcv0p7_zfh_xtheadc -o test_vec test_vec.c
$ qemu-riscv64 -cpu c920 ./test_vec
Hello RISC-V!
vlmax:4
105.000000
&lt;/code>&lt;/pre>
&lt;h2 id="四在-qemu-上运行-risc-v-64-位-linux-系统">四、在 QEMU 上运行 RISC-V 64 位 Linux 系统&lt;/h2>
&lt;h3 id="1-制作内核">1. 制作内核&lt;/h3>
&lt;h4 id="11-下载内核源码">1.1 下载内核源码&lt;/h4>
&lt;pre>&lt;code class="language-bash">$ wget https://mirrors.edge.kernel.org/pub/linux/kernel/v5.x/linux-5.10.42.tar.gz
$ tar -xzvf linux-5.10.42.tar.gz
&lt;/code>&lt;/pre>
&lt;p>下载后进入内核源码目录&lt;/p>
&lt;pre>&lt;code class="language-bash">$ cd linux-5.10.42
&lt;/code>&lt;/pre>
&lt;h4 id="12-配置和编译内核">1.2 配置和编译内核&lt;/h4>
&lt;pre>&lt;code class="language-bash">$ make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- defconfig
$ make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- -j $(nproc)
...
AR drivers/built-in.a
GEN .version
CHK include/generated/compile.h
LD vmlinux.o
MODPOST vmlinux.symvers
MODINFO modules.builtin.modinfo
GEN modules.builtin
LD .tmp_vmlinux.kallsyms1
KSYMS .tmp_vmlinux.kallsyms1.S
AS .tmp_vmlinux.kallsyms1.S
LD .tmp_vmlinux.kallsyms2
KSYMS .tmp_vmlinux.kallsyms2.S
AS .tmp_vmlinux.kallsyms2.S
LD vmlinux
SYSMAP System.map
MODPOST modules-only.symvers
GEN Module.symvers
CC [M] fs/efivarfs/efivarfs.mod.o
OBJCOPY arch/riscv/boot/Image
GZIP arch/riscv/boot/Image.gz
LD [M] fs/efivarfs/efivarfs.ko
Kernel: arch/riscv/boot/Image.gz is ready
&lt;/code>&lt;/pre>
&lt;h3 id="2-制作-rootfs">2. 制作 rootfs&lt;/h3>
&lt;h4 id="21-下载-busybox-源码">2.1 下载 busybox 源码&lt;/h4>
&lt;pre>&lt;code class="language-bash">$ wget https://busybox.net/downloads/busybox-1.33.1.tar.bz2
&lt;/code>&lt;/pre>
&lt;p>下载完后进入 busybox 源码目录&lt;/p>
&lt;pre>&lt;code class="language-bash">cd busybox-1.33.1
&lt;/code>&lt;/pre>
&lt;h4 id="22-配置-busybox">2.2 配置 busybox&lt;/h4>
&lt;pre>&lt;code class="language-bash">$ make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- defconfig
$ make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- menuconfig
&lt;/code>&lt;/pre>
&lt;p>打开配置菜单后进入第一行的 &amp;ldquo;Settings&amp;rdquo;，在 &amp;ldquo;Build Options&amp;rdquo; 节中，选中 “Build static binary (no shared libs)”，设置好后退出保存配置。&lt;/p>
&lt;p>检查 &lt;code>.config&lt;/code> 文件中是否有 &lt;code>CONFIG_STATIC=y&lt;/code> ，如果没有则手动添加。&lt;/p>
&lt;h4 id="23-编译和安装-busybox">2.3 编译和安装 busybox&lt;/h4>
&lt;pre>&lt;code class="language-bash">$ make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- -j $(nproc)
$ make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- install
&lt;/code>&lt;/pre>
&lt;p>此时源码目录 busyboxsource 下会新出现一个 &lt;code>_install&lt;/code> 目录 ，可以看到生成的东西。&lt;/p>
&lt;pre>&lt;code class="language-bash">$ ls _install
bin linuxrc sbin usr
&lt;/code>&lt;/pre>
&lt;p>进入 &lt;code>_install&lt;/code> 目录，创建以下目录&lt;/p>
&lt;pre>&lt;code class="language-bash">$ cd _install
$ mkdir proc sys dev etc etc/init.d
$ ls
bin dev etc linuxrc proc sbin sys usr
&lt;/code>&lt;/pre>
&lt;p>然后另外再新建一个最简单的 init 的 RC 文件：&lt;/p>
&lt;pre>&lt;code class="language-bash">$ cd etc/init.d/
$ touch rcS
$ vim rcS
&lt;/code>&lt;/pre>
&lt;p>编辑该文件内容为：&lt;/p>
&lt;pre>&lt;code class="language-bash">#!/bin/sh
mount -t proc none /proc
mount -t sysfs none /sys
/sbin/mdev -s
&lt;/code>&lt;/pre>
&lt;p>然后修改 rcS 文件权限，加上可执行权限&lt;/p>
&lt;pre>&lt;code class="language-bash">$ chmod +x rcS
&lt;/code>&lt;/pre>
&lt;h4 id="24-制作文件系统">2.4 制作文件系统&lt;/h4>
&lt;p>继续在 &lt;code>_install&lt;/code> 目录下执行如下命令：&lt;/p>
&lt;pre>&lt;code class="language-bash">$ find -print0 | cpio -0oH newc | gzip -9 &amp;gt; ../rootfs.img
3276 blocks
&lt;/code>&lt;/pre>
&lt;h3 id="3-启动运行">3. 启动运行&lt;/h3>
&lt;p>创建一个新的目录，将编译好的内核 &lt;code>Image&lt;/code> 和制作好的 &lt;code>rootfs.img&lt;/code> 移动到该目录下。&lt;/p>
&lt;pre>&lt;code class="language-bash">$ mkdir riscv64-linux
$ cd riscv64-linux
$ cp ../linux-5.10.42/arch/riscv/boot/Image .
$ cp ../busybox-1.33.1/rootfs.img .
&lt;/code>&lt;/pre>
&lt;p>执行如下命令：&lt;/p>
&lt;pre>&lt;code class="language-bash">$ qemu-system-riscv64 \
-nographic -machine virt \
-kernel Image \
-initrd rootfs.img \
-append &amp;quot;root=/dev/ram rdinit=/sbin/init&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>将显示 Linux Kernel 启动流程：&lt;/p>
&lt;details>
&lt;summary>&lt;strong>点击展开&lt;/strong>&lt;/summary>
&lt;pre>&lt;code class="language-bash">OpenSBI v0.9
____ _____ ____ _____
/ __ \ / ____| _ \_ _|
| | | |_ __ ___ _ __ | (___ | |_) || |
| | | | '_ \ / _ \ '_ \ \___ \| _ &amp;lt; | |
| |__| | |_) | __/ | | |____) | |_) || |_
\____/| .__/ \___|_| |_|_____/|____/_____|
| |
|_|
Platform Name : riscv-virtio,qemu
Platform Features : timer,mfdeleg
Platform HART Count : 1
Firmware Base : 0x80000000
Firmware Size : 100 KB
Runtime SBI Version : 0.2
Domain0 Name : root
Domain0 Boot HART : 0
Domain0 HARTs : 0*
Domain0 Region00 : 0x0000000080000000-0x000000008001ffff ()
Domain0 Region01 : 0x0000000000000000-0xffffffffffffffff (R,W,X)
Domain0 Next Address : 0x0000000080200000
Domain0 Next Arg1 : 0x0000000087000000
Domain0 Next Mode : S-mode
Domain0 SysReset : yes
Boot HART ID : 0
Boot HART Domain : root
Boot HART ISA : rv64imafdcvsu
Boot HART Features : scounteren,mcounteren,time
Boot HART PMP Count : 16
Boot HART PMP Granularity : 4
Boot HART PMP Address Bits: 54
Boot HART MHPM Count : 0
Boot HART MHPM Count : 0
Boot HART MIDELEG : 0x0000000000000222
Boot HART MEDELEG : 0x000000000000b109
[ 0.000000] Linux version 5.10.42 (root@centos) (riscv64-unknown-linux-gnu-gcc (Xuantie-900 linux-5.10.4 glibc gcc Toolchain V2.8.0 B-20231018) 10.4.0, GNU ld (GNU Binutils) 2.35) #1 SMP Wed Feb 21 02:07:46 CST 2024
[ 0.000000] OF: fdt: Ignoring memory range 0x80000000 - 0x80200000
[ 0.000000] efi: UEFI not found.
[ 0.000000] Initial ramdisk at: 0x(____ptrval____) (1085440 bytes)
[ 0.000000] Zone ranges:
[ 0.000000] DMA32 [mem 0x0000000080200000-0x0000000087ffffff]
[ 0.000000] Normal empty
[ 0.000000] Movable zone start for each node
[ 0.000000] Early memory node ranges
[ 0.000000] node 0: [mem 0x0000000080200000-0x0000000087ffffff]
[ 0.000000] Initmem setup node 0 [mem 0x0000000080200000-0x0000000087ffffff]
[ 0.000000] software IO TLB: Cannot allocate buffer
[ 0.000000] SBI specification v0.2 detected
[ 0.000000] SBI implementation ID=0x1 Version=0x9
[ 0.000000] SBI v0.2 TIME extension detected
[ 0.000000] SBI v0.2 IPI extension detected
[ 0.000000] SBI v0.2 RFENCE extension detected
[ 0.000000] SBI v0.2 HSM extension detected
[ 0.000000] riscv: ISA extensions acdfimsuv
[ 0.000000] riscv: ELF capabilities acdfim
[ 0.000000] percpu: Embedded 17 pages/cpu s32360 r8192 d29080 u69632
[ 0.000000] Built 1 zonelists, mobility grouping on. Total pages: 31815
[ 0.000000] Kernel command line: root=/dev/ram rdinit=/sbin/init
[ 0.000000] Dentry cache hash table entries: 16384 (order: 5, 131072 bytes, linear)
[ 0.000000] Inode-cache hash table entries: 8192 (order: 4, 65536 bytes, linear)
[ 0.000000] Sorting __ex_table...
[ 0.000000] mem auto-init: stack:off, heap alloc:off, heap free:off
[ 0.000000] Memory: 108240K/129024K available (7084K kernel code, 3993K rwdata, 4096K rodata, 223K init, 342K bss, 20784K reserved, 0K cma-reserved)
[ 0.000000] Virtual kernel memory layout:
[ 0.000000] fixmap : 0xffffffcefee00000 - 0xffffffceff000000 (2048 kB)
[ 0.000000] pci io : 0xffffffceff000000 - 0xffffffcf00000000 ( 16 MB)
[ 0.000000] vmemmap : 0xffffffcf00000000 - 0xffffffcfffffffff (4095 MB)
[ 0.000000] vmalloc : 0xffffffd000000000 - 0xffffffdfffffffff (65535 MB)
[ 0.000000] lowmem : 0xffffffe000000000 - 0xffffffe007e00000 ( 126 MB)
[ 0.000000] SLUB: HWalign=64, Order=0-3, MinObjects=0, CPUs=1, Nodes=1
[ 0.000000] rcu: Hierarchical RCU implementation.
[ 0.000000] rcu: RCU restricting CPUs from NR_CPUS=8 to nr_cpu_ids=1.
[ 0.000000] rcu: RCU debug extended QS entry/exit.
[ 0.000000] Tracing variant of Tasks RCU enabled.
[ 0.000000] rcu: RCU calculated value of scheduler-enlistment delay is 25 jiffies.
[ 0.000000] rcu: Adjusting geometry for rcu_fanout_leaf=16, nr_cpu_ids=1
[ 0.000000] NR_IRQS: 64, nr_irqs: 64, preallocated irqs: 0
[ 0.000000] riscv-intc: 64 local interrupts mapped
[ 0.000000] plic: plic@c000000: mapped 53 interrupts with 1 handlers for 2 contexts.
[ 0.000000] random: get_random_bytes called from start_kernel+0x31a/0x48c with crng_init=0
[ 0.000000] riscv_timer_init_dt: Registering clocksource cpuid [0] hartid [0]
[ 0.000000] clocksource: riscv_clocksource: mask: 0xffffffffffffffff max_cycles: 0x24e6a1710, max_idle_ns: 440795202120 ns
[ 0.000150] sched_clock: 64 bits at 10MHz, resolution 100ns, wraps every 4398046511100ns
[ 0.003557] Console: colour dummy device 80x25
[ 0.008887] printk: console [tty0] enabled
[ 0.012368] Calibrating delay loop (skipped), value calculated using timer frequency.. 20.00 BogoMIPS (lpj=40000)
[ 0.012666] pid_max: default: 32768 minimum: 301
[ 0.014227] Mount-cache hash table entries: 512 (order: 0, 4096 bytes, linear)
[ 0.014306] Mountpoint-cache hash table entries: 512 (order: 0, 4096 bytes, linear)
[ 0.040922] rcu: Hierarchical SRCU implementation.
[ 0.042741] EFI services will not be available.
[ 0.044926] smp: Bringing up secondary CPUs ...
[ 0.045062] smp: Brought up 1 node, 1 CPU
[ 0.054128] devtmpfs: initialized
[ 0.061463] clocksource: jiffies: mask: 0xffffffff max_cycles: 0xffffffff, max_idle_ns: 7645041785100000 ns
[ 0.061753] futex hash table entries: 256 (order: 2, 16384 bytes, linear)
[ 0.067460] NET: Registered protocol family 16
[ 0.131233] vgaarb: loaded
[ 0.132530] SCSI subsystem initialized
[ 0.134485] usbcore: registered new interface driver usbfs
[ 0.134834] usbcore: registered new interface driver hub
[ 0.135035] usbcore: registered new device driver usb
[ 0.150024] clocksource: Switched to clocksource riscv_clocksource
[ 0.167109] NET: Registered protocol family 2
[ 0.168330] IP idents hash table entries: 2048 (order: 2, 16384 bytes, linear)
[ 0.172076] tcp_listen_portaddr_hash hash table entries: 128 (order: 0, 5120 bytes, linear)
[ 0.172242] TCP established hash table entries: 1024 (order: 1, 8192 bytes, linear)
[ 0.172480] TCP bind hash table entries: 1024 (order: 3, 32768 bytes, linear)
[ 0.172690] TCP: Hash tables configured (established 1024 bind 1024)
[ 0.173861] UDP hash table entries: 256 (order: 2, 24576 bytes, linear)
[ 0.174481] UDP-Lite hash table entries: 256 (order: 2, 24576 bytes, linear)
[ 0.175963] NET: Registered protocol family 1
[ 0.179024] RPC: Registered named UNIX socket transport module.
[ 0.179111] RPC: Registered udp transport module.
[ 0.179150] RPC: Registered tcp transport module.
[ 0.179186] RPC: Registered tcp NFSv4.1 backchannel transport module.
[ 0.179332] PCI: CLS 0 bytes, default 64
[ 0.182716] Unpacking initramfs...
[ 0.263706] Freeing initrd memory: 1056K
[ 0.265678] workingset: timestamp_bits=62 max_order=15 bucket_order=0
[ 0.281052] NFS: Registering the id_resolver key type
[ 0.282003] Key type id_resolver registered
[ 0.282074] Key type id_legacy registered
[ 0.282505] nfs4filelayout_init: NFSv4 File Layout Driver Registering...
[ 0.282631] nfs4flexfilelayout_init: NFSv4 Flexfile Layout Driver Registering...
[ 0.283481] 9p: Installing v9fs 9p2000 file system support
[ 0.284918] NET: Registered protocol family 38
[ 0.285416] Block layer SCSI generic (bsg) driver version 0.4 loaded (major 251)
[ 0.285593] io scheduler mq-deadline registered
[ 0.285692] io scheduler kyber registered
[ 0.295484] pci-host-generic 30000000.pci: host bridge /soc/pci@30000000 ranges:
[ 0.296336] pci-host-generic 30000000.pci: IO 0x0003000000..0x000300ffff -&amp;gt; 0x0000000000
[ 0.296861] pci-host-generic 30000000.pci: MEM 0x0040000000..0x007fffffff -&amp;gt; 0x0040000000
[ 0.296961] pci-host-generic 30000000.pci: MEM 0x0400000000..0x07ffffffff -&amp;gt; 0x0400000000
[ 0.299940] pci-host-generic 30000000.pci: ECAM at [mem 0x30000000-0x3fffffff] for [bus 00-ff]
[ 0.301083] pci-host-generic 30000000.pci: PCI host bridge to bus 0000:00
[ 0.301328] pci_bus 0000:00: root bus resource [bus 00-ff]
[ 0.301486] pci_bus 0000:00: root bus resource [io 0x0000-0xffff]
[ 0.301528] pci_bus 0000:00: root bus resource [mem 0x40000000-0x7fffffff]
[ 0.301568] pci_bus 0000:00: root bus resource [mem 0x400000000-0x7ffffffff]
[ 0.302864] pci 0000:00:00.0: [1b36:0008] type 00 class 0x060000
[ 0.377412] Serial: 8250/16550 driver, 4 ports, IRQ sharing disabled
[ 0.389894] 10000000.uart: ttyS0 at MMIO 0x10000000 (irq = 2, base_baud = 230400) is a 16550A
[ 0.428017] printk: console [ttyS0] enabled
[ 0.430410] [drm] radeon kernel modesetting enabled.
[ 0.457312] loop: module loaded
[ 0.460726] libphy: Fixed MDIO Bus: probed
[ 0.464996] e1000e: Intel(R) PRO/1000 Network Driver
[ 0.465383] e1000e: Copyright(c) 1999 - 2015 Intel Corporation.
[ 0.466272] ehci_hcd: USB 2.0 'Enhanced' Host Controller (EHCI) Driver
[ 0.466724] ehci-pci: EHCI PCI platform driver
[ 0.467203] ehci-platform: EHCI generic platform driver
[ 0.467683] ohci_hcd: USB 1.1 'Open' Host Controller (OHCI) Driver
[ 0.468129] ohci-pci: OHCI PCI platform driver
[ 0.468593] ohci-platform: OHCI generic platform driver
[ 0.469968] usbcore: registered new interface driver uas
[ 0.470477] usbcore: registered new interface driver usb-storage
[ 0.471603] mousedev: PS/2 mouse device common for all mice
[ 0.475055] goldfish_rtc 101000.rtc: registered as rtc0
[ 0.476070] goldfish_rtc 101000.rtc: setting system clock to 2024-02-20T19:37:51 UTC (1708457871)
[ 0.478889] syscon-poweroff soc:poweroff: pm_power_off already claimed (____ptrval____) sbi_shutdown
[ 0.479494] syscon-poweroff: probe of soc:poweroff failed with error -16
[ 0.480977] usbcore: registered new interface driver usbhid
[ 0.481324] usbhid: USB HID core driver
[ 0.483516] NET: Registered protocol family 10
[ 0.491589] Segment Routing with IPv6
[ 0.492256] sit: IPv6, IPv4 and MPLS over IPv4 tunneling driver
[ 0.495528] NET: Registered protocol family 17
[ 0.497086] 9pnet: Installing 9P2000 support
[ 0.497667] Key type dns_resolver registered
[ 0.498706] debug_vm_pgtable: [debug_vm_pgtable ]: Validating architecture page table helpers
[ 0.533266] Freeing unused kernel memory: 220K
[ 0.539682] Run /sbin/init as init process
Please press Enter to activate this console.
&lt;/code>&lt;/pre>
&lt;/details>
&lt;p>见到 &lt;code>&amp;quot;Please press Enter to activate this console.&amp;quot;&lt;/code> 提示后直接回车，无需密码就进入系统了。&lt;/p>
&lt;p>执行几个常用命令测试一下，都能正常工作：&lt;/p>
&lt;pre>&lt;code class="language-bash">/ # ls
bin etc proc sbin usr
dev linuxrc root sys
/ # pwd
/
/ # cd bin
/bin #
/ # ls
arch dumpkmap kill netstat setarch
ash echo link nice setpriv
base32 ed linux32 nuke setserial
base64 egrep linux64 pidof sh
busybox false ln ping sleep
cat fatattr login ping6 stat
chattr fdflush ls pipe_progress stty
chgrp fgrep lsattr printenv su
chmod fsync lzop ps sync
chown getopt makemime pwd tar
conspy grep mkdir reformime touch
cp gunzip mknod resume true
cpio gzip mktemp rev umount
cttyhack hostname more rm uname
date hush mount rmdir usleep
dd ionice mountpoint rpm vi
df iostat mpstat run-parts watch
dmesg ipcalc mt scriptreplay zcat
dnsdomainname kbd_mode mv sed
/bin #
&lt;/code>&lt;/pre>
&lt;p>退出 QEMU 的方法是按下 &lt;code>Ctrl + A&lt;/code> ，松开后再按下 &lt;code>x&lt;/code> 键即可退出 QEMU 。&lt;/p>
&lt;p>如果想要往 QEMU 里面传输文件，可以使用挂载的方式，如下所示：&lt;/p>
&lt;pre>&lt;code class="language-bash">$ mkdir rootfs
$ sudo mount -o loop rootfs.img rootfs
$ cp [-r] [file] ./rootfs/
$ sudo umount rootfs
&lt;/code>&lt;/pre>
&lt;h2 id="五总结">五、总结&lt;/h2>
&lt;p>至此，我们已经成功搭建了玄铁 900 系列的工具链环境以及 xuantie-qemu 仿真环境，这为后续的开发、编译、链接以及运行和调试基于玄铁 900 系列芯片的 RISC-V 应用程序奠定了基础。&lt;/p></description></item><item><title>OpenMP 简介</title><link>https://cuterwrite.top/p/openmp-intro/</link><pubDate>Mon, 19 Feb 2024 01:36:00 +0000</pubDate><guid>https://cuterwrite.top/p/openmp-intro/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/c17b7451a44c1d4370d5ba2b966298ea195413_crop-2024-02-19.webp" alt="Featured image of post OpenMP 简介" />&lt;h1 id="openmp-简介">OpenMP 简介&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;h3 id="什么是-openmp">什么是 OpenMP？&lt;/h3>
&lt;p>OpenMP（Open Multi-Processing）是一种广泛应用的多线程并行编程模型，它为共享内存系统上的并行计算提供了丰富的指令集和 API。起源于 1997 年，OpenMP 由多个领先硬件和软件供应商共同制定标准，旨在简化并行程序的设计与实现过程，以充分利用现代多核处理器的计算能力。&lt;/p>
&lt;p>OpenMP 支持多种编程语言，包括 C、C++ 以及 Fortran 等，并通过在源代码中插入特定的编译指示（pragma），使得开发者能够轻松地将串行代码转化为高效的并行代码。其主要优势在于其简洁性和易用性，允许程序员使用熟悉的编程语言和开发环境，同时提供良好的可移植性和扩展性。&lt;/p>
&lt;p>OpenMP 由非营利性组织管理，由多家软硬件厂家参与，包括 Arm，IBM，Intel，AMD，NVIDIA，Cray，Oracle 等。&lt;/p>
&lt;h3 id="历史版本">历史版本&lt;/h3>
&lt;ul>
&lt;li>在 &lt;a class="link" href="https://www.openmp.org/" target="_blank" rel="noopener" >官网页面
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
可以查询到 OpenMP 的历史版本和发布日期。&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>版本&lt;/th>
&lt;th>发布日期&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Fortran 1.0&lt;/td>
&lt;td>October 1997&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>C/C++ 1.0&lt;/td>
&lt;td>October 1998&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>C/C++ 2.0&lt;/td>
&lt;td>March 2002&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 2.5&lt;/td>
&lt;td>May 2005&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 3.0&lt;/td>
&lt;td>May 2008&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 3.1&lt;/td>
&lt;td>July 2011&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 4.0&lt;/td>
&lt;td>July 2013&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 4.5&lt;/td>
&lt;td>November 2015&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 5.0&lt;/td>
&lt;td>November 2018&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 5.1&lt;/td>
&lt;td>November 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 5.2&lt;/td>
&lt;td>November 2021&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="基础知识">基础知识&lt;/h2>
&lt;h3 id="技术框架">技术框架&lt;/h3>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/openmp-arch-2024-02-20.webp"
alt="openmp-arch-2024-02-20" width="auto" loading="lazy"/>&lt;figcaption>
&lt;h4>OpenMP 技术框架&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>&lt;strong>OpenMP Runtime Library&lt;/strong> 是 OpenMP 规范中定义的一组函数和运行时支持结构，它是 OpenMP 并行编程框架的关键组成部分。这个库在编译器的支持下与用户程序链接，在程序执行时负责管理线程的创建、同步、调度以及数据共享等任务。它实现了 OpenMP 编译指导所指示的所有并行化机制。&lt;/p>
&lt;p>&lt;strong>OpenMP Runtime Library&lt;/strong> 包括了如下功能：&lt;/p>
&lt;ul>
&lt;li>线程管理（创建、销毁、同步）
= 工作共享（动态工作分配给各个线程）
= 任务调度
= 同步原语（如 barriers, locks, atomic operations）
= 动态调整线程数量&lt;/li>
&lt;li>内存模型支持（数据环境变量、private, shared, reduction 变量等）&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Compiler Directives&lt;/strong> 编译指导是以 &lt;code>#pragma omp&lt;/code> 开头的预处理器指令，程序员在源代码中插入这些指令来指导编译器如何将串行程序转换为并行程序。例如，使用 &lt;code>#pragma omp parallel&lt;/code> 指令定义一个并行区域，编译器会在此区域内生成多线程执行逻辑。&lt;/p>
&lt;p>&lt;strong>Environment Variables&lt;/strong> 环境变量是 OpenMP 运行时库的一部分，它们用于控制运行时行为，例如线程数量、调度策略等。&lt;/p>
&lt;p>&lt;strong>OpenMP Library&lt;/strong> 是一组函数库，包括了一些用于线程同步、原子操作、锁、并行循环等的函数。这些函数可以在用户程序中直接调用，以实现更细粒度的并行化。&lt;/p>
&lt;p>总的来说，OpenMP 技术框架包括了编译器指导、运行时库、环境变量和函数库等多个组成部分，它们共同构成了一个完整的并行编程环境，共同协作以支持在共享内存系统上的并行编程。&lt;/p>
&lt;h3 id="执行模型fork-join-model">执行模型：Fork-Join Model&lt;/h3>
&lt;p>OpenMP 的执行模型采用的是 Fork-Join 机制，这是一种用于并行编程中的同步原语模型。在该模型下，程序执行遵循以下步骤：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Fork（派生）阶段&lt;/strong>： 程序开始时以单个主线程执行，当遇到 OpenMP 编译指导（pragma）指示的并行区域时，主线程会通过 Runtime Library 创建一个或多个工作线程（worker threads）。这些工作线程是对主线程的派生，每个线程负责执行并行区域内的部分任务。并行区域可以是循环、段（sections）、单一任务或其他可并行化的代码块。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Parallel Execution（并行执行）阶段&lt;/strong>： 创建出的工作线程独立并发地执行分配给它们的任务，并且能够访问共享的数据结构。OpenMP 提供了一套丰富的指令来管理数据的同步和通信，确保在多线程环境下的正确性和一致性。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Join（合并）阶段&lt;/strong>： 当所有工作线程完成其在并行区域内的任务后，它们会自动或者通过显式同步指令（如 &lt;code>omp barrier&lt;/code> ）汇聚到 join 点。在此阶段，所有线程等待直至所有其他线程都到达了同步点，随后 join 操作发生。这意味着主线程和其他工作线程重新同步，恢复为串行执行模式或继续执行后续的非并行代码。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Synchronization and Data Consistency（同步与数据一致性）&lt;/strong>： Fork-Join 模型确保了在并行执行过程中，通过适当的锁机制、原子操作和同步原语保证了对共享资源的互斥访问以及数据的一致性。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>总结来说，OpenMP 的 Fork-Join 执行模型是一种基于动态线程创建和同步的并行处理框架，它允许开发者方便地将串行代码转化为并行执行的代码片段，同时简化了并行编程中常见的复杂性，如线程管理和数据同步问题。&lt;/p>
&lt;h3 id="线程与进程">线程与进程&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>进程&lt;/p>
&lt;ul>
&lt;li>每个进程都有自己独立的地址空间&lt;/li>
&lt;li>CPU 在进程间切换时需要进行上下文切换&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>线程&lt;/p>
&lt;ul>
&lt;li>一个进程下的线程共享相同的地址空间&lt;/li>
&lt;li>CPU 在线程之间切换开销较小&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>操作系统的线程设计&lt;/p>
&lt;ul>
&lt;li>现代操作系统如 Linux、Windows 等都支持一个进程下有多个线程。&lt;/li>
&lt;li>线程是操作系统调度的基本单位，进程是资源分配的基本单位。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/slide_10-2024-02-20.webp"
alt="slide_10-2024-02-20" width="80%" loading="lazy"/>&lt;figcaption>
&lt;h4>操作系统的线程设计&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="线程的硬件调度">线程的硬件调度&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>硬件调度机制与操作系统协同，负责将线程智能地映射至可用的 CPU 物理核心上执行。&lt;/strong>&lt;/li>
&lt;li>因此，在多线程应用中，当活跃线程数超过了实际 CPU 物理核心的数量时，操作系统将不得不进行密集的上下文切换，以确保多个线程在有限的核心资源上交替运行，这种线程竞争过载的现象会导致整体性能瓶颈和效率下降。&lt;/li>
&lt;li>&lt;strong>超线程技术（Hyper-Threading）&lt;/strong> 通过在单个物理 CPU 核心上虚拟化出额外的逻辑处理单元，当前通常配置为每个物理核心承载两个逻辑核心。这些逻辑核心能够并行执行独立的任务流，尽管它们共享同一物理核心的基础计算资源，如执行引擎、缓存和其他底层硬件结构。通过这种方式，超线程旨在提高资源利用率和并发处理能力，尤其是在存在大量并行任务且其对计算资源需求相对较小的情况下，可以有效提升系统的总体吞吐量。然而，在某些高度依赖单一核心性能或内存带宽的应用场景下，如部分对 CPU 敏感的游戏或特定类型的数据密集型运算，增加逻辑核心可能并不一定能带来显著的性能提升。&lt;/li>
&lt;/ul>
&lt;h3 id="硬件的内存模型">硬件的内存模型&lt;/h3>
&lt;ul>
&lt;li>在现代多核处理器体系结构中，每个 CPU 核心为了进一步提升数据访问速度，在与主存之间设计有多级缓存层次结构。最靠近 CPU 核心的是 L1 缓存，通常其后是 L2 缓存，部分高端架构还包含 L3 缓存，这些高速缓存层级存储容量逐层增大，但访问延迟逐层增加。&lt;/li>
&lt;li>L1 和 L2 缓存通常是与特定 CPU 核心紧密耦合且私有的，这意味着每个核心拥有自己的独立缓存空间，以降低数据访问冲突并提高缓存命中率。L1 缓存由于距离计算单元最近，其访问速度最快，但容量最小；而 L2 缓存作为 L1 缓存的有效补充，具有相对较大的容量。&lt;/li>
&lt;li>为确保在多核环境中不同 CPU 核心的缓存中对共享数据的一致性，硬件和操作系统共同实现了缓存一致性协议（如 MESI 协议）。这种机制允许系统自动维护一个全局一致的数据视图，即使数据在多个核心的缓存中存在副本也能保证它们同步更新，这一特性在某些架构中被称作 &lt;strong>ccNUMA（cache-coherent non-uniform memory access）&lt;/strong> ，即缓存一致性非统一内存访问。&lt;/li>
&lt;li>然而，这种缓存一致性管理也带来了一些挑战，其中之一就是“伪共享”(False Sharing)问题。当不同的线程修改位于同一缓存行内的各自独立变量时，尽管这些变量本身并无关联，但由于它们物理上相邻而被存储在同一缓存行内，因此任何针对其中一个变量的写操作都会导致整个缓存行失效并在所有核心间重新同步。这会引发不必要的缓存无效化与重新填充操作，从而显著降低性能。解决伪共享问题通常需要精心设计数据布局或利用缓存行对齐等技术手段来避免无关数据之间的争用。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20170115165700476-2024-02-20.webp"
alt="20170115165700476-2024-02-20" width="auto" loading="lazy"/>&lt;figcaption>
&lt;h4>典型的现代 CPU 内存结构&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="线程亲和性和线程绑定">线程亲和性和线程绑定&lt;/h3>
&lt;ul>
&lt;li>线程亲和性（Thread Affinity）是指操作系统或应用程序控制特定线程与处理器核心之间关联的能力。在多核或多处理器系统中，线程亲和性允许程序员或调度器决定将某个线程固定在特定的 CPU 核心上运行，而不是让操作系统自由地在所有可用的核心间进行动态调度。这种机制有助于减少上下文切换开销，提高缓存命中率，并且对于需要保持数据局部性的并行计算任务特别有益。&lt;/li>
&lt;li>线程绑定（Thread Pinning）是实现线程亲和性的具体技术手段，它指明了将特定线程与特定硬件资源（如 CPU 核心或 NUMA 节点）之间的强制关联。通过线程绑定，可以确保指定的线程始终在其分配的核心上执行，避免被操作系统迁移到其他核心，从而优化性能、减少延迟并解决伪共享等问题。在 OpenMP 等并行编程模型中，可以通过相关的环境变量或编译指导来设置线程绑定策略，以适应不同的并行计算需求和硬件特性。&lt;/li>
&lt;li>同一个插槽上的 CPU 对 L3 缓存的访问延迟是一致的，但不同插槽上的 CPU 对 L3 缓存的访问延迟是不一致的。因此，线程绑定的目的是为了减少线程在不同 CPU 之间的迁移，从而减少访存延迟。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/u=237293070,3563798054&amp;amp;fm=253&amp;amp;app=138&amp;amp;f=JPEG-2024-02-20.webp"
alt="u=237293070,3563798054&amp;amp;fm=253&amp;amp;app=138&amp;amp;f=JPEG-2024-02-20" width="auto" loading="lazy"/>&lt;figcaption>
&lt;h4>线程亲和性和线程绑定&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>OpenMP 支持控制线程的绑定
&lt;ul>
&lt;li>环境变量 &lt;code>OMP_PROC_BIND&lt;/code> 或从句 &lt;code>proc_bind(master|close|spread)&lt;/code> 控制线程绑定与否，以及线程对于绑定单元（称为 place）分布&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="openmp-编程">OpenMP 编程&lt;/h2>
&lt;h3 id="安装">安装&lt;/h3>
&lt;p>对于 Linux 系统，GCC 是常用的编译器，现代版本的 GCC 一般已默认支持 OpenMP。例如在 Ubuntu 20.04 LTS 上，可以通过以下命令安装含有 OpenMP 的 build-essential 包：&lt;/p>
&lt;pre>&lt;code class="language-bash">$ sudo apt-get update
$ sudo apt-get install -y build-essential
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>查看 OpenMP 版本&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">$ echo |cpp -fopenmp -dM |grep -i open
#define _OPENMP 201511
&lt;/code>&lt;/pre>
&lt;h3 id="编译使用">编译使用&lt;/h3>
&lt;ul>
&lt;li>直接在编译语句中添加 &lt;code>-fopenmp&lt;/code> 选项即可开启 OpenMP 支持。&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">g++ -O2 -std=c++17 -fopenmp hello.cpp -o hello
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>如果使用 CMake 构建项目, 加入 &lt;code>-Wunknown-pragmas&lt;/code> 选项可以在编译时报告未处理的 &lt;code>#pragma&lt;/code> 指令。&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-cmake">find_package(OpenMP REQUIRED)
add_compile_options(-Wunknown-pragmas)
add_executable(hello hello.cpp)
target_link_libraries(hello PRIVATE OpenMP::OpenMP_CXX)
&lt;/code>&lt;/pre>
&lt;h3 id="hello-world">Hello World!&lt;/h3>
&lt;ul>
&lt;li>第一个 OpenMP 程序&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-c">#include &amp;lt;omp.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
int main() {
#pragma omp parallel num_threads(8)
{
int id = omp_get_thread_num();
int num_threads = omp_get_num_threads();
printf(&amp;quot;Hello World from thread %d of %d \n&amp;quot;, id, num_threads);
}
return 0;
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>运行结果&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">$ gcc -fopenmp hello.c -o hello
$ ./hello
Hello World from thread 7 of 8
Hello World from thread 6 of 8
Hello World from thread 0 of 8
Hello World from thread 3 of 8
Hello World from thread 1 of 8
Hello World from thread 2 of 8
Hello World from thread 5 of 8
Hello World from thread 4 of 8
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>同一类 openmp 制导语句称为一种构造(construct)&lt;/li>
&lt;li>形式为 &lt;code>#pragma omp &amp;lt;directive name&amp;gt; &amp;lt;clause&amp;gt;&lt;/code>&lt;/li>
&lt;li>使用 &lt;code>{}&lt;/code> 包裹的代码块称为并行区域(parallel region)&lt;/li>
&lt;/ul>
&lt;h3 id="线程数设置">线程数设置&lt;/h3>
&lt;ul>
&lt;li>优先级由低到高
&lt;ul>
&lt;li>什么都不做，系统选择运行线程数&lt;/li>
&lt;li>设置环境变量 &lt;code>export OMP_NUM_THREADS=4&lt;/code>&lt;/li>
&lt;li>代码中使用库函数 &lt;code>void omp_set_num_threads(int)&lt;/code>&lt;/li>
&lt;li>通过制导语句 &lt;code>num_threads(4)&lt;/code>&lt;/li>
&lt;li>if 从句判断串行还是并行执行&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="常用库函数">常用库函数&lt;/h3>
&lt;ul>
&lt;li>设置并行区运行线程数：&lt;code>void omp_set_num_threads(int)&lt;/code>&lt;/li>
&lt;li>获取并行区运行线程数：&lt;code>int omp_get_num_threads()&lt;/code>&lt;/li>
&lt;li>获取当前线程编号：&lt;code>int omp_get_thread_num()&lt;/code>&lt;/li>
&lt;li>获得 OpenMP Wall Clock 时间（单位：秒）：&lt;code>double omp_get_wtime()&lt;/code>&lt;/li>
&lt;li>获得时间精度：&lt;code>double omp_get_wtick()&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="parallel-构造">Parallel 构造&lt;/h3>
&lt;p>&lt;strong>支持的从句&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;code>if(scalar_expression)&lt;/code>：如果 &lt;code>scalar_expression&lt;/code> 为真，则并行执行，否则串行执行。&lt;/li>
&lt;li>&lt;code>num_threads(integer_expression)&lt;/code>：指定并行区域中的线程数。&lt;/li>
&lt;li>&lt;code>default(shared|none)&lt;/code>：指定变量的默认共享性。
&lt;ul>
&lt;li>&lt;code>shared&lt;/code>：所有变量默认为共享。&lt;/li>
&lt;li>&lt;code>none&lt;/code>：无默认变量类型，每个变量都需要显式声明共享或私有。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>shared(list)&lt;/code>：指定共享变量列表。
&lt;ul>
&lt;li>共享变量在内存中只有一份，所有线程都可以访问。&lt;/li>
&lt;li>请保证共享变量访问不会冲突。&lt;/li>
&lt;li>不特别指定并行区变量默认为 &lt;strong>shared&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>private(list)&lt;/code>：指定私有变量列表。
&lt;ul>
&lt;li>私有变量在每个线程中都有一份独立的拷贝。&lt;/li>
&lt;li>变量需要 &lt;strong>重新初始化&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>firstprivate(list)&lt;/code>：指定首次私有变量列表。
&lt;ul>
&lt;li>同 &lt;code>private&lt;/code>&lt;/li>
&lt;li>对变量根据主线程中的数据进行初始化。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 1： no clause、private、firstprivate&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">int results[4];
int cnt;
cnt = 1;
#pragma omp parallel num_threads(4)
{
int tid = omp_get_thread_num();
for (int i = 0; i &amp;lt; 4; i++) {
cnt += 1;
}
results[tid] = cnt;
}
printf(&amp;quot;no clause: &amp;quot;);
for (int i = 0; i &amp;lt; 4; i++) {
printf(&amp;quot;%d &amp;quot;, results[i]);
}
printf(&amp;quot;\n&amp;quot;);
cnt = 1;
#pragma omp parallel num_threads(4) private(cnt)
{
int tid = omp_get_thread_num();
for (int i = 0; i &amp;lt; 4; i++) {
cnt += 1;
}
results[tid] = cnt;
}
printf(&amp;quot;private(not init): &amp;quot;);
for (int i = 0; i &amp;lt; 4; i++) {
printf(&amp;quot;%d &amp;quot;, results[i]);
}
printf(&amp;quot;\n&amp;quot;);
cnt = 1;
#pragma omp parallel num_threads(4) firstprivate(cnt)
{
int tid = omp_get_thread_num();
for (int i = 0; i &amp;lt; 4; i++) {
cnt += 1;
}
results[tid] = cnt;
}
printf(&amp;quot;firstprivate: &amp;quot;);
for (int i = 0; i &amp;lt; 4; i++) {
printf(&amp;quot;%d &amp;quot;, results[i]);
}
printf(&amp;quot;\n&amp;quot;);
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>打印结果&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">no clause: 5 9 13 17
private(not init): 4 1572916964 1572916964 1572916964
firstprivate: 5 5 5 5
&lt;/code>&lt;/pre>
&lt;h3 id="for-构造">For 构造&lt;/h3>
&lt;ul>
&lt;li>最常用的并行化构造之一&lt;/li>
&lt;/ul>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 2：并行化 for 循环&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">#pragma omp parallel num_threads(8)
{
int tid = omp_get_thread_num();
int num_threads = omp_get_num_threads();
#pragma omp for
for (int i = 0; i &amp;lt; num_threads; i++) {
#pragma omp ordered
printf(&amp;quot;Hello from thread %d of %d \n&amp;quot;, tid, num_threads);
}
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>打印结果&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">Hello from thread 0 of 8
Hello from thread 1 of 8
Hello from thread 2 of 8
Hello from thread 3 of 8
Hello from thread 4 of 8
Hello from thread 5 of 8
Hello from thread 6 of 8
Hello from thread 7 of 8
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>在并行区内对 for 循环进行线程划分，且 for 循环满足格式要求
&lt;ul>
&lt;li>init-expr:需要是 &lt;code>var=lb&lt;/code> 形式，类型也有限制&lt;/li>
&lt;li>test-expr:限制为 &lt;code>var relational-opb&lt;/code> 或者 &lt;code>b relational-op var&lt;/code>&lt;/li>
&lt;li>incr-expr:仅限加减法&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="parallel-for-构造">Parallel for 构造&lt;/h3>
&lt;ul>
&lt;li>常常将 &lt;code>parallel&lt;/code> 和 &lt;code>for&lt;/code> 结合使用，合并为 &lt;code>parallel for&lt;/code> 制导语句&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>parallel&lt;/th>
&lt;th>for   &lt;/th>
&lt;th>parallel for&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>if&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>num_threads&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>default&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>copyin&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>private&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>firstprivate&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>shared&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>reduction&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>lastprivate&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>schedule&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ordered&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>collapse&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>nowait&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>
&lt;p>&lt;code>lastprivate(list)&lt;/code>&lt;/p>
&lt;ul>
&lt;li>同 &lt;code>private&lt;/code>&lt;/li>
&lt;li>执行完 for 循环后，将最后一个线程的值赋给主线程的变量。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>nowait&lt;/code>：取消代码块结束时的栅栏同步(barrier)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>collapse(n)&lt;/code>：应用于 n 重循环，合并(展开)循环&lt;/p>
&lt;ul>
&lt;li>注意循环之间是否有数据依赖&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>ordered&lt;/code>：声明有潜在的顺序执行部分&lt;/p>
&lt;ul>
&lt;li>使用 &lt;code>#pragma omp ordered&lt;/code> 标记顺序执行代码(搭配使用)&lt;/li>
&lt;li>ordered 区内的语句任意时刻仅由最多一个线程执行&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>shedule(type[,chunk])&lt;/code>&lt;/p>
&lt;ul>
&lt;li>&lt;code>type&lt;/code>：指定循环迭代的调度策略
&lt;ul>
&lt;li>&lt;code>static&lt;/code>：静态调度，chunk 大小固定（默认 n/p ）&lt;/li>
&lt;li>&lt;code>dynamic&lt;/code>：动态调度，chunk 大小固定（默认为 1）&lt;/li>
&lt;li>&lt;code>guided&lt;/code>：引导调度，chunk 大小动态调整&lt;/li>
&lt;li>&lt;code>runtime&lt;/code>：由系统环境变量 &lt;code>OMP_SCHEDULE&lt;/code> 指定&lt;/li>
&lt;li>&lt;code>auto&lt;/code>：自动调度&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>chunk&lt;/code>：指定每个线程获取的迭代次数&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="特殊的数据从句reduction">特殊的数据从句：Reduction&lt;/h3>
&lt;p>在 OpenMP 中，reduction 是一种并行编程技术，用于解决多线程环境下的数据竞争问题，特别是在计算全局变量的累加或类似操作时。当多个线程需要同时修改同一个共享变量，并且这些修改可以通过某种二元运算符（如加法、乘法等）将所有线程的结果合并成一个最终结果时，可以使用 &lt;code>reduction&lt;/code> 子句。&lt;/p>
&lt;p>具体来说，reducton 的执行过程为：&lt;/p>
&lt;ul>
&lt;li>fork 线程并分配任务&lt;/li>
&lt;li>每一个线程定义一个私有变量 &lt;code>omp_priv&lt;/code>
&lt;ul>
&lt;li>同 &lt;code>private&lt;/code>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>各个线程执行计算&lt;/li>
&lt;li>所有 &lt;code>omp_priv&lt;/code> 和 &lt;code>omp_in&lt;/code> 一起顺序进行 reduction，写回原变量。&lt;/li>
&lt;/ul>
&lt;p>相比之下，&lt;strong>atomic&lt;/strong> 是 OpenMP 提供的另一种同步机制，它确保对单个内存位置的访问在多线程环境中是原子性的，即一次只允许一个线程对该内存位置进行读取或写入操作。通过 &lt;code>#pragma omp atomic&lt;/code> 指令，可以保证一条简单的赋值语句（或某些特定类型的读改写操作）在并发环境下不会发生数据竞争。&lt;/p>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 3：Reduction&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">int sum = 0;
double start = omp_get_wtime();
#pragma omp parallel for num_threads(8) reduction(+ : sum)
for (int i = 0; i &amp;lt; 100000; i++) {
sum += i;
}
printf(&amp;quot;sum = %d\n&amp;quot;, sum);
printf(&amp;quot;Reduction time: %.5lf s\n&amp;quot;, omp_get_wtime() - start);
// no reduction
sum = 0;
start = omp_get_wtime();
#pragma omp parallel for num_threads(8)
for (int i = 0; i &amp;lt; 100000; i++) {
#pragma omp atomic
sum += i;
}
printf(&amp;quot;sum = %d\n&amp;quot;, sum);
printf(&amp;quot;Atomic time: %.5lf s\n&amp;quot;, omp_get_wtime() - start);
return 0;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>打印结果&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">sum = 704982704
Reduction time: 0.00062 s
sum = 704982704
Atomic time: 0.01021 s
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>两者结果相同，但是 reduction 的执行时间更短，这是因为 reduction 通过为每个线程分配一个私有副本，线程可以在其私有空间内自由地执行归约操作，而不需要在更新全局结果时与其他线程争夺锁资源，加上高效的数据合并方法等。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/reduction-omp-2024-02-20.webp"
alt="reduction-omp-2024-02-20" width="auto" loading="lazy"/>&lt;figcaption>
&lt;h4>OpenMP reducton operation&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="同步构造">同步构造&lt;/h3>
&lt;h4 id="sections-构造">Sections 构造&lt;/h4>
&lt;ul>
&lt;li>将并行区的代码块划分为多个 section 分配执行。&lt;/li>
&lt;li>可以搭配 parallel 合成为 parallel sections 构造。&lt;/li>
&lt;li>每个 section 由一个线程执行
&lt;ul>
&lt;li>线程数大于 section 数目：部分线程空闲&lt;/li>
&lt;li>线程数小于 section 数目：部分线程分配多个 section&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>示例代码：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-c">#pragma omp sections
{
#pragma omp section
method1();
#pragma omp section
method2();
}
&lt;/code>&lt;/pre>
&lt;h4 id="barrier-构造">Barrier 构造&lt;/h4>
&lt;ul>
&lt;li>在特定位置进行栅栏同步&lt;/li>
&lt;li>在存在数据依赖的情况下，可以使用 barrier 保证数据的一致性&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/Barrier-2024-02-20.webp"
alt="Barrier-2024-02-20" width="auto" loading="lazy"/>&lt;figcaption>
&lt;h4>Barrier 同步示意图&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h4 id="single-构造">Single 构造&lt;/h4>
&lt;ul>
&lt;li>用于标记只有一个线程执行的代码块，带有隐式的 barrier 同步，可以使用 nowait 取消隐式的 barrier 同步。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/omp-single-2024-02-20.webp"
alt="omp-single-2024-02-20" width="70%" loading="lazy"/>&lt;figcaption>
&lt;h4>pragma single&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h4 id="atomic-构造">Atomic 构造&lt;/h4>
&lt;ul>
&lt;li>用于保证对共享变量的原子操作，避免数据竞争。&lt;/li>
&lt;/ul>
&lt;h3 id="false-sharing">False Sharing&lt;/h3>
&lt;ul>
&lt;li>伪共享简单来说就是指多个线程同时访问同一缓存行的不同部分，导致缓存行的无效化和重新填充，从而降低了程序的性能。&lt;/li>
&lt;li>不同核心对同一 Cache line 的同时读写会造成严重的冲突，导致改级缓存失效。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/false-sharing-2024-02-20.webp"
alt="false-sharing-2024-02-20" width="auto" loading="lazy"/>&lt;figcaption>
&lt;h4>False Sharing 问题&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>在 OpenMP 中，解决伪共享的方法主要有：
&lt;ul>
&lt;li>&lt;strong>数据结构对齐&lt;/strong> ：通过使用编译器提供的对齐指令或关键字确保相关变量分别处于不同的缓存行中。例如，在 C++中可以使用 &lt;code>alignas&lt;/code> 关键字来指定变量的内存对齐方式，确保每个线程的数据独立位于不同的缓存行。&lt;/li>
&lt;li>&lt;strong>增大缓存行之间的间距&lt;/strong> ：在相邻变量之间插入足够的填充空间，使得它们不会出现在同一个缓存行内。&lt;/li>
&lt;li>&lt;strong>避免无意义的竞争&lt;/strong> ：设计算法和数据结构以减少不必要的共享数据访问。如果可能，尽量让线程操作各自独立的数据段。&lt;/li>
&lt;li>&lt;strong>自定义内存分配&lt;/strong> ：使用特殊的内存分配函数，确保分配的连续内存区域对齐到缓存行边界，这样分配给不同线程的数据就不会落在同一缓存行上。&lt;/li>
&lt;li>在某些情况下，可以利用特定平台提供的硬件特性或者编译器支持的扩展，比如 Intel 的 &lt;code>__declspec(align(#))&lt;/code> 属性（对于 MSVC）或者 &lt;code>__attribute__((aligned(#)))&lt;/code>（对于 GCC/Clang）。&lt;/li>
&lt;li>也可以通过控制变量的作用域或者利用动态创建私有副本等技术来间接避免伪共享问题。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="任务构造">任务构造&lt;/h3>
&lt;ul>
&lt;li>除了 Fork-Join 模型外，OpenMP 还支持任务并行模型，通过 &lt;code>task&lt;/code> 制导语句来实现。&lt;/li>
&lt;li>即动态地管理线程池和任务池，线程池中的线程可以动态地获取任务池中的任务进行执行，从而实现任务的并行执行。&lt;/li>
&lt;/ul>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 4：任务并行&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">#include &amp;lt;iostream&amp;gt;
#include &amp;lt;omp.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
#include &amp;lt;iomanip&amp;gt;
void big_task(int i) {
sleep(10);
}
void small_task(int i) {
sleep(1);
}
int main() {
int ntasks = 8;
double start = omp_get_wtime();
#pragma omp parallel
{
#pragma omp single
{
std::cout &amp;lt;&amp;lt; &amp;quot;Task 0 Created&amp;quot; &amp;lt;&amp;lt; std::endl;
#pragma omp task
big_task(0);
std::cout &amp;lt;&amp;lt; &amp;quot;Task 1 Created&amp;quot; &amp;lt;&amp;lt; std::endl;
#pragma omp task
big_task(1);
for (int i = 2; i &amp;lt; ntasks; i++) {
std::cout &amp;lt;&amp;lt; &amp;quot;Task &amp;quot; &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &amp;quot; Created&amp;quot; &amp;lt;&amp;lt; std::endl;
#pragma omp task
small_task(i);
}
}
#pragma omp taskwait
}
std::cout &amp;lt;&amp;lt; &amp;quot;All tasks finished&amp;quot; &amp;lt;&amp;lt; std::endl;
std::cout &amp;lt;&amp;lt; &amp;quot;Time: &amp;quot; &amp;lt;&amp;lt; std::fixed &amp;lt;&amp;lt; std::setprecision(2) &amp;lt;&amp;lt; omp_get_wtime() - start &amp;lt;&amp;lt; &amp;quot;s&amp;quot; &amp;lt;&amp;lt; std::endl;
return 0;
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>运行结果&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">$ g++ -fopenmp task.cpp -o task
$ ./task
Task 0 Created
Task 1 Created
Task 2 Created
Task 3 Created
Task 4 Created
Task 5 Created
Task 6 Created
Task 7 Created
All tasks finished
Time: 10.00s
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>这段代码中，我们使用了 &lt;code>#pragma omp task&lt;/code> 制导语句来创建任务，任务的执行由线程池中的线程动态获取并执行。在任务创建后，我们使用 &lt;code>#pragma omp taskwait&lt;/code> 来等待所有任务执行完毕。达到了一个异步执行的效果。&lt;/li>
&lt;/ul>
&lt;h3 id="向量化simd-构造">向量化：SIMD 构造&lt;/h3>
&lt;ul>
&lt;li>SIMD（Single Instruction, Multiple Data）是一种并行计算模式，它通过一条指令同时对多个数据进行操作，从而实现高效的数据并行计算。&lt;/li>
&lt;li>在 OpenMP 中，可以使用 &lt;code>#pragma omp simd&lt;/code> 制导语句来实现向量化并行计算。
&lt;ul>
&lt;li>&lt;code>aligned&lt;/code> 用于列出内存对齐的指针&lt;/li>
&lt;li>&lt;code>safelen&lt;/code> 用于标记循环展开时的数据依赖&lt;/li>
&lt;li>&lt;code>linear&lt;/code> 用于标记循环变量的线性关系&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>编译器例如 gcc 也自带向量化功能，一般使用以下编译选项
&lt;ul>
&lt;li>-O3&lt;/li>
&lt;li>-ffast-math&lt;/li>
&lt;li>-fivopts&lt;/li>
&lt;li>-march=native&lt;/li>
&lt;li>-fopt-info-vec&lt;/li>
&lt;li>-fopt-info-vec-missed&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>RDMA 基本元素</title><link>https://cuterwrite.top/p/rdma-element/</link><pubDate>Fri, 02 Feb 2024 01:01:01 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-element/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/73c30b990886bf6988c97858a3e16011195413_crop-2024-02-04.webp" alt="Featured image of post RDMA 基本元素" />&lt;h1 id="rdma-基本元素">RDMA 基本元素&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/141267386">&lt;cite>知乎专栏：3. RDMA 基本元素&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>RDMA 技术中经常使用缩略语，很容易让刚接触的人一头雾水，本篇的目的是讲解 RDMA 中最基本的元素及其含义。&lt;/p>
&lt;p>我将常见的缩略语对照表写在前面，阅读的时候如果忘记了可以翻到前面查阅。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-b6723caa5b291ee161d94fd8fd8ce09c_720w-2024-02-03.webp"
alt="v2-b6723caa5b291ee161d94fd8fd8ce09c_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;h2 id="wq">WQ&lt;/h2>
&lt;p>Work Queue 简称 WQ，是 RDMA 技术中最重要的概念之一。WQ 是一个储存工作请求的队列，为了讲清楚 WQ 是什么，我们先介绍这个队列中的元素 WQE（Work Queue Element，工作队列元素）。&lt;/p>
&lt;h3 id="wqe">WQE&lt;/h3>
&lt;p>WQE 可以认为是一种“任务说明”，这个工作请求是软件下发给硬件的，这份说明中包含了软件所希望硬件去做的任务以及有关这个任务的详细信息。比如，某一份任务是这样的：“我想把位于地址 0x12345678 的长度为 10 字节的数据发送给对面的节点”，硬件接到任务之后，就会通过 DMA 去内存中取数据，组装数据包，然后发送。&lt;/p>
&lt;p>WQE 的含义应该比较明确了，那么我们最开始提到的 WQ 是什么呢？它就是用来存放“任务书”的“文件夹”，WQ 里面可以容纳很多 WQE。有数据结构基础的读者应该都了解，队列是一种先进先出的数据结构，在计算机系统中非常常见，我们可以用下图表示上文中描述的 WQ 和 WQE 的关系：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-40c7e57f2760323c6b6665306e8f8896_720w-2024-02-03.webp"
alt="v2-40c7e57f2760323c6b6665306e8f8896_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>WQ 这个队列总是由软件向其中增加 WQE（入队），硬件从中取出 WQE，这就是软件给硬件“下发任务”的过程。为什么用队列而不是栈？因为进行“存”和“取“操作的分别是软件和硬件，并且需要保证用户的请求按照顺序被处理在 RDMA 技术中，所有的通信请求都要按照上图这种方式告知硬件，这种方式常被称为“Post”。&lt;/p>
&lt;h3 id="qp">QP&lt;/h3>
&lt;p>Queue Pair 简称 QP，就是“一对”WQ 的意思。&lt;/p>
&lt;h3 id="sq-和-rq">SQ 和 RQ&lt;/h3>
&lt;p>任何通信过程都要有收发两端，QP 就是一个发送工作队列和一个接受工作队列的组合，这两个队列分别称为 SQ（Send Queue）和 RQ（Receive Queue）。我们再把上面的图丰富一下，左边是发送端，右边是接收端：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-b89b321b8d1ae5ab6dcbaf8d6085f107_720w-2024-02-03.webp"
alt="v2-b89b321b8d1ae5ab6dcbaf8d6085f107_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>WQ 怎么不见了？SQ 和 RQ 都是 WQ，WQ 只是表示一种可以存储 WQE 的单元，SQ 和 RQ 才是实例。&lt;/p>
&lt;p>SQ 专门用来存放发送任务，RQ 专门用来存放接收任务。在一次 SEND-RECV 流程中，发送端需要把表示一次发送任务的 WQE 放到 SQ 里面。同样的，接收端软件需要给硬件下发一个表示接收任务的 WQE，这样硬件才知道收到数据之后放到内存中的哪个位置。上文我们提到的 Post 操作，对于 SQ 来说称为 Post Send，对于 RQ 来说称为 Post Receive。&lt;/p>
&lt;p>需要注意的是，在 RDMA 技术中&lt;strong>通信的基本单元是 QP&lt;/strong>，而不是节点。如下图所示，对于每个节点来说，每个进程都可以使用若干个 QP，而每个本地 QP 可以“关联”一个远端的 QP。我们用“节点 A 给节点 B 发送数据”并不足以完整的描述一次 RDMA 通信，而应该是类似于“节点 A 上的 QP3 给节点 C 上的 QP4 发送数据”。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-71b3b17ef8aec45d74ef9e4a42a69201_720w-2024-02-03.webp"
alt="v2-71b3b17ef8aec45d74ef9e4a42a69201_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>每个节点的每个 QP 都有一个唯一的编号，称为 QPN（Queue Pair Number），通过 QPN 可以唯一确定一个节点上的 QP。&lt;/p>
&lt;h3 id="srq">SRQ&lt;/h3>
&lt;p>Shared Receive Queue 简称 SRQ，意为共享接收队列。概念很好理解，就是一种几个 QP 共享同一个 RQ 时，我们称其为 SRQ。以后我们会了解到，使用 RQ 的情况要远远小于使用 SQ，而每个队列都是要消耗内存资源的。当我们需要使用大量的 QP 时，可以通过 SRQ 来节省内存。如下图所示，QP2~QP4 一起使用同一个 RQ：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-4a21f2b1333877b4b0d97a1ca91d4096_720w-2024-02-03.webp"
alt="v2-4a21f2b1333877b4b0d97a1ca91d4096_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;h2 id="cq">CQ&lt;/h2>
&lt;p>Completion Queue 简称 CQ，意为完成队列。跟 WQ 一样，我们先介绍 CQ 这个队列当中的元素——CQE（Completion Queue Element）。可以认为 CQE 跟 WQE 是相反的概念，如果 WQE 是软件下发给硬件的“任务书”的话，那么 CQE 就是硬件完成任务之后返回给软件的“任务报告”。CQE 中描述了某个任务是被正确无误的执行，还是遇到了错误，如果遇到了错误，那么错误的原因是什么。&lt;/p>
&lt;p>而 CQ 就是承载 CQE 的容器——一个先进先出的队列。我们把表示 WQ 和 WQE 关系的图倒过来画，就得到了 CQ 和 CQE 的关系：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-31f9a407ab66381fbc557d8acc5573cb_720w-2024-02-03.webp"
alt="v2-31f9a407ab66381fbc557d8acc5573cb_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>每个 CQE 都包含某个 WQE 的完成信息，他们的关系如下图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-701fa8eacb10c90c45b0241c75254a01_720w-2024-02-03.webp"
alt="v2-701fa8eacb10c90c45b0241c75254a01_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>下面我们把 CQ 和 WQ（QP）放在一起，看一下一次 SEND-RECV 操作中，软硬件的互动（图中序号顺序不表示实际时序）：&lt;/p>
&lt;blockquote>
&lt;p>2022/5/23：下图及后面的列表顺序有修改，将原来第 2 条的“接收端硬件从 RQ 中拿到任务书，准备接收数据”移动到“接收端收到数据，进行校验后回复 ACK 报文给发送端”之后，并且修改了描述，现在为第 6 条。&lt;/p>
&lt;p>这里我犯了错误的点是 RQ 和 SQ 不同，是一个“被动接收”的过程，只有收到 Send 报文（或者带立即数的 Write 报文）时硬件才会消耗 RQ WQE。感谢 @连接改变世界 的指正。&lt;/p>
&lt;/blockquote>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-a8d38721903672037b27cc7e49ecee03_720w-2024-02-03.webp"
alt="v2-a8d38721903672037b27cc7e49ecee03_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;ol>
&lt;li>接收端 APP 以 WQE 的形式下发一次 RECV 任务到 RQ。&lt;/li>
&lt;li>发送端 APP 以 WQE 的形式下发一次 SEND 任务到 SQ。&lt;/li>
&lt;li>发送端硬件从 SQ 中拿到任务书，从内存中拿到待发送数据，组装数据包。&lt;/li>
&lt;li>发送端网卡将数据包通过物理链路发送给接收端网卡。&lt;/li>
&lt;li>接收端收到数据，进行校验后回复 ACK 报文给发送端。&lt;/li>
&lt;li>接收端硬件从 RQ 中取出一个任务书（WQE）。&lt;/li>
&lt;li>接收端硬件将数据放到 WQE 中指定的位置，然后生成“任务报告”CQE，放置到 CQ 中。&lt;/li>
&lt;li>接收端 APP 取得任务完成信息。&lt;/li>
&lt;li>发送端网卡收到 ACK 后，生成 CQE，放置到 CQ 中。&lt;/li>
&lt;li>发送端 APP 取得任务完成信息。&lt;/li>
&lt;/ol>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>NOTE: 需要注意的一点是，上图中的例子是可靠服务类型的交互流程，如果是不可靠服务，那么不会有步骤 5 的 ACK 回复，而且步骤 9 以及之后的步骤会在步骤 5 之后立即触发。关于服务类型以及可靠与不可靠，我们将在《RDMA 基本服务类型》一文中讲解。&lt;/p>&lt;/div>
&lt;p>至此，通过 WQ 和 CQ 这两种媒介，两端软硬件共同完成了一次收发过程。&lt;/p>
&lt;h2 id="wr-和-wc">WR 和 WC&lt;/h2>
&lt;p>说完了几个 Queue 之后，其实还有两个文章开头提到的概念没有解释，那就是 WR 和 WC（不是 Water Closet 的缩写）。&lt;/p>
&lt;p>WR 全称为 Work Request，意为工作请求；WC 全称 Work Completion，意为工作完成。这两者其实是 WQE 和 CQE 在用户层的“映射”。因为 APP 是通过调用协议栈接口来完成 RDMA 通信的，WQE 和 CQE 本身并不对用户可见，是驱动中的概念。用户真正通过 API 下发的是 WR，收到的是 WC。&lt;/p>
&lt;p>WR/WC 和 WQE/CQE 是相同的概念在不同层次的实体，他们都是“任务书”和“任务报告”。于是我们把前文的两个图又加了点内容：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-00b87c111a8e1701f96fbfb78e078b29_720w-2024-02-03.webp"
alt="v2-00b87c111a8e1701f96fbfb78e078b29_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>好了，我们用 IB 协议[1]3.2.1 中的 Figure 11 这张图总结一下本篇文章的内容：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-2107a9bf8230c45ad73aa5ff0b8626ff_720w-2024-02-03.webp"
alt="v2-2107a9bf8230c45ad73aa5ff0b8626ff_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>用户态的 WR，由驱动转化成了 WQE 填写到了 WQ 中，WQ 可以是负责发送的 SQ，也可以是负责接收的 RQ。硬件会从各个 WQ 中取出 WQE，并根据 WQE 中的要求完成发送或者接收任务。任务完成后，会给这个任务生成一个 CQE 填写到 CQ 中。驱动会从 CQ 中取出 CQE，并转换成 WC 返回给用户。&lt;/p>
&lt;p>基础概念就介绍到这里，下一篇将介绍 RDMA 的几种常见操作类型。&lt;/p>
&lt;h2 id="参考文献">参考文献&lt;/h2>
&lt;p>[1]《IB Specification Vol 1-Release-1.3-2015-03-03》&lt;/p></description></item><item><title>比较基于传统以太网与 RDMA 技术的通信</title><link>https://cuterwrite.top/p/ethernet-vs-rdma/</link><pubDate>Thu, 01 Feb 2024 02:01:01 +0000</pubDate><guid>https://cuterwrite.top/p/ethernet-vs-rdma/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/cropped_2024010202-2024-02-03.webp" alt="Featured image of post 比较基于传统以太网与 RDMA 技术的通信" />&lt;h1 id="比较基于传统以太网与-rdma-技术的通信">比较基于传统以太网与 RDMA 技术的通信&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/139548242">&lt;cite>知乎专栏：2. 比较基于传统以太网与 RDMA 技术的通信&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>本篇的目的是通过对比一次典型的基于 TCP/IP 协议栈的以太网和 RDMA 通信的过程，直观的展示 RDMA 技术相比传统以太网的优势，尽量不涉及协议和软件实现细节。&lt;/p>
&lt;p>假设本端的某个应用想把自己内存中的数据复制到对端某个应用可以访问的内存中（或者通俗的讲，本端要给对端发送数据），我们来看一下以太网和 RDMA 的 SEND-RECV 语义都做了哪些操作。&lt;/p>
&lt;h2 id="传统以太网">传统以太网&lt;/h2>
&lt;p>在描述通信过程时的软硬件关系时，我们通常将模型划分为用户层 Userspace，内核 Kernel 以及硬件 Hardware。Userspace 和 Kernel 实际上使用的是同一块物理内存，但是处于安全考虑，Linux 将内存划分为用户空间和内核空间。用户层没有权限访问和修改内核空间的内存内容，只能通过系统调用陷入内核态，Linux 的内存管理机制比较复杂，本文不展开讨论。&lt;/p>
&lt;p>一次典型的基于传统以太网的通信过程的可以如下图所示进行分层：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-0931cab010cbf296edeaa368c45a503b_720w-2024-02-03.webp"
alt="v2-0931cab010cbf296edeaa368c45a503b_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>一次收-发过程的步骤如下：&lt;/p>
&lt;ol>
&lt;li>发送端和接收端通过 Socket 库提供的接口建立链接（就是在两个节点间建立了一条逻辑上的道路，数据可以沿这条道路从一端发送到另一端）并分别在内存中申请好发送和接收 Buffer。&lt;/li>
&lt;li>发送端 APP 通过 Socket 接口陷入内核态，待发送数据经过 TCP/IP 协议栈的一层层封装，最后被 CPU 复制到 Socket Buffer 中。&lt;/li>
&lt;li>发送端通过网卡驱动，告知网卡可以发送数据了，网卡将通过 DMA 从 Buffer 中复制封装好的数据包到内部缓存中，然后将其发送到物理链路。&lt;/li>
&lt;li>接收端网卡收到数据包后，将数据包放到接收 Buffer 中，然后 CPU 将通过内核中的 TCP/IP 协议栈对报文进行层层解析，取出有效的数据。&lt;/li>
&lt;li>接收端 APP 通过 Socket 接口陷入内核态，CPU 将数据从内核空间复制到用户空间。&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-b89b321b8d1ae5ab6dcbaf8d6085f107_720w-2024-02-03.webp"
alt="v2-b89b321b8d1ae5ab6dcbaf8d6085f107_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>这个模型的数据流向大致是像上图这个样子，数据首先需要从用户空间复制一份到内核空间，这一次复制由 CPU 完成，将数据块从用户空间复制到内核空间的 Socket Buffer 中。内核中软件 TCP/IP 协议栈给数据添加各层头部和校验信息。最后网卡会通过 DMA 从内存中复制数据，并通过物理链路发送给对端的网卡。&lt;/p>
&lt;p>而对端是完全相反的过程：硬件将数据包 DMA 拷贝到内存中，然后 CPU 会对数据包进行逐层解析和校验，最后将数据复制到用户空间。&lt;/p>
&lt;p>上述过程中的关键点是需要 CPU 参与的把数据从用户空间拷贝到内核空间，以及同样需要 CPU 全程参与的数据包组装和解析，数据量大的情况下，这将对 CPU 将造成很大的负担。&lt;/p>
&lt;p>下面我们看一下 RDMA 是如何将 CPU“解放”出来的。&lt;/p>
&lt;h2 id="rdma">RDMA&lt;/h2>
&lt;p>同样是一端发送，一端接收的场景，我们将 RDMA 的分层模型分成两部分“控制通路”和“数据通路”，控制通路需要进入内核态准备通信所需的内存资源，而数据通路指的是实际数据交互过程中的流程。这一过程的分层关系如下图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-8ef2b015ba9d111fc2d42983cd5fe152_720w-2024-02-03.webp"
alt="v2-8ef2b015ba9d111fc2d42983cd5fe152_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>同 Socket 一样，我们简单描述下通信的过程：&lt;/p>
&lt;ol>
&lt;li>发送端和接收端分别通过控制通路陷入内核态创建好通信所需要的内存资源。&lt;/li>
&lt;li>在数据通路上，接收端 APP 通知硬件准备接收数据，告诉硬件将接收到的数据放在哪片内存中。&lt;/li>
&lt;li>在数据通路上，发送端 APP 通知硬件发送数据，告诉硬件待发送数据位于哪片内存中。&lt;/li>
&lt;li>发送端 RDMA 网卡从内存中搬移数据，组装报文发送给对端。&lt;/li>
&lt;li>对端收到报文，对其进行解析并通过 DMA 将有效载荷写入内存。然后以某种方式通知上层 APP，告知其数据已接收并妥善存放到指定位置。&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-cb1492c24773e725dd5f112bb67f9deb_720w-2024-02-03.webp"
alt="v2-cb1492c24773e725dd5f112bb67f9deb_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>这一过程中的数据流向大致如上图所示。通过和 Socket 的对比，我们可以明显看到，&lt;strong>数据收发绕过了内核并且数据交换过程并不需要 CPU 参与，报文的组装和解析是由硬件完成的&lt;/strong>。&lt;/p>
&lt;p>通过上面的对比，我们可以明显的体会到 RDMA 的优势，既将 CPU 从数据包封装和解析中解放出来，又减少了 CPU 拷贝数据的功率和时间损耗。需要注意的是，本文只描述了 SEND-RECV 流程，而 RDMA 技术所独有的，效率更高的 WRITE/READ 语义将在后续文章中介绍。&lt;/p>
&lt;p>下一篇我们将介绍一些 RDMA 技术中的重要且基本的概念。&lt;/p></description></item><item><title>编译安装 UCX 1.15.0 与 OpenMPI 5.0.0：详尽指南</title><link>https://cuterwrite.top/p/openmpi-with-ucx/</link><pubDate>Thu, 01 Feb 2024 01:01:01 +0000</pubDate><guid>https://cuterwrite.top/p/openmpi-with-ucx/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/cropped_2024010204-2024-02-03.webp" alt="Featured image of post 编译安装 UCX 1.15.0 与 OpenMPI 5.0.0：详尽指南" />&lt;h1 id="编译安装-ucx-1150-与-openmpi-500详尽指南">编译安装 UCX 1.15.0 与 OpenMPI 5.0.0：详尽指南&lt;/h1>
&lt;h2 id="一环境准备">一、环境准备&lt;/h2>
&lt;p>首先，请确保您的系统满足以下基本要求：&lt;/p>
&lt;ol>
&lt;li>操作系统：支持 Linux（如 Ubuntu 20.04 LTS）或其他类 Unix 操作系统。&lt;/li>
&lt;li>开发工具包：安装必要的构建工具和库，例如 &lt;code>build-essential&lt;/code> ，&lt;code>libnuma-dev&lt;/code> ，&lt;code>pkg-config&lt;/code> 等。&lt;/li>
&lt;li>内核版本：对于最佳性能，建议使用最新稳定版内核。&lt;/li>
&lt;li>需要支持 RDMA 的硬件环境或虚拟环境。&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">sudo apt-get update
sudo apt-get install -y build-essential libnuma-dev pkg-config
&lt;/code>&lt;/pre>
&lt;h2 id="二编译安装-ucx-1150">二、编译安装 UCX 1.15.0&lt;/h2>
&lt;ol>
&lt;li>下载 UCX 源码包：&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">wget https://github.com/openucx/ucx/releases/download/v1.15.0/ucx-1.15.0.tar.gz
tar -xzvf ucx-1.15.0.tar.gz
cd ucx-1.15.0
&lt;/code>&lt;/pre>
&lt;ol start="2">
&lt;li>配置 UCX 编译选项：&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">mkdir build &amp;amp;&amp;amp; cd build
../configure --prefix=/root/software/ucx/1.5.0
&lt;/code>&lt;/pre>
&lt;p>您可以根据实际需求添加更多配置选项，比如指定特定的网卡类型或者启用特定的功能。&lt;/p>
&lt;ol start="3">
&lt;li>编译并安装：&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">make -j 8
make install
&lt;/code>&lt;/pre>
&lt;ol start="4">
&lt;li>UCX 架构说明&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>UCX 1.15.0 的架构如下图所示：&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/Architecture-2024-02-03.webp"
alt="Architecture-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>组件&lt;/th>
&lt;th>角色&lt;/th>
&lt;th>说明&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>UCP&lt;/td>
&lt;td>Protocol&lt;/td>
&lt;td>实现高级抽象，如标记匹配、流、连接协商和建立、多轨以及处理不同的内存类型。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UCT&lt;/td>
&lt;td>Transport&lt;/td>
&lt;td>实现低级通信原语，如活动消息、远程内存访问和原子操作。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UCM&lt;/td>
&lt;td>Memory&lt;/td>
&lt;td>通用的数据结构、算法和系统实用程序的集合。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UCP&lt;/td>
&lt;td>Protocol&lt;/td>
&lt;td>截获内存注册缓存使用的内存分配和释放事件。&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="三编译安装-openmpi-500">三、编译安装 OpenMPI 5.0.0&lt;/h2>
&lt;ol>
&lt;li>下载 OpenMPI 源码包：&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">wget https://download.open-mpi.org/release/open-mpi/v5.0/openmpi-5.0.0.tar.gz
tar -xzvf openmpi-5.0.0.tar.gz
cd openmpi-5.0.0
&lt;/code>&lt;/pre>
&lt;ol start="2">
&lt;li>配置 OpenMPI 编译选项，指定使用 UCX 作为传输层：&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">mkdir build &amp;amp;&amp;amp; cd build
../configure --without-hcoll \
--enable-python-bindings \
--enable-mpirun-prefix-by-default \
--prefix=/root/software/openmpi/5.0.0-ucx-1.15.0 \
--with-ucx=/root/software/ucx/1.15.0 \
--enable-mca-no-build=btl-uct
&lt;/code>&lt;/pre>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;ul>
&lt;li>对于 OpenMPI 4.0 及更高版本，&lt;code>btl_uct&lt;/code> 组件可能存在编译错误。该组件对于使用 UCX 来说并不重要；因此可以通过 &lt;code>--enable-mca-no-build=btl-uct&lt;/code> 禁用：&lt;/li>
&lt;li>&lt;code>--enable-python-bindings&lt;/code> 选项用于启用 Python 绑定。&lt;/li>
&lt;li>&lt;code>--enable-mpirun-prefix-by-default&lt;/code> 选项用于在使用 &lt;code>mpirun&lt;/code> 启动 MPI 程序时自动添加 &lt;code>--prefix&lt;/code> 选项。&lt;/li>
&lt;li>&lt;code>--without-hcoll&lt;/code> 选项用于禁用 HCOLL 组件。不设置编译时会报错 &lt;code>cannot find -lnuma&lt;/code> 与 &lt;code>cannot find -ludev&lt;/code> 的错误。&lt;/li>
&lt;/ul>&lt;/div>
&lt;p>最后配置选项如下：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/ompi-config-2024-02-03.webp"
alt="ompi-config-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;ol start="3">
&lt;li>编译并安装：&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">make -j 8
make install
&lt;/code>&lt;/pre>
&lt;h2 id="四验证安装与设置环境变量">四、验证安装与设置环境变量&lt;/h2>
&lt;p>安装完成后，可以通过运行简单的 MPI 程序来验证 UCX 和 OpenMPI 是否成功集成：&lt;/p>
&lt;pre>&lt;code class="language-bash">mpirun -np 2 --mca pml ucx --mca btl ^vader,tcp,openib,uct [-x UCX_NET_DEVICES=mlx5_0:1] hostname
&lt;/code>&lt;/pre>
&lt;p>（如果在 root 上运行则需要加上 &lt;code>--allow-run-as-root&lt;/code> 选项，如果有 RDMA 设备可以设置 &lt;code>-x UCX_NET_DEVICES&lt;/code> ）&lt;/p>
&lt;p>如果一切正常，您会看到两台主机名的输出。为了方便使用，可以将 OpenMPI 的 bin 目录等添加到系统 PATH 环境变量中：&lt;/p>
&lt;pre>&lt;code class="language-bash">vim ~/.bashrc
export PATH=/root/software/openmpi/5.0.0-ucx-1.15.0/bin:$PATH
export LD_LIBRARY_PATH=/root/software/openmpi/5.0.0-ucx-1.15.0/lib:$LD_LIBRARY_PATH
export CPATH=/root/software/openmpi/5.0.0-ucx-1.15.0/include:$CPATH
export MANPATH=/root/software/openmpi/5.0.0-ucx-1.15.0/share/man:$MANPATH
source ~/.bashrc
&lt;/code>&lt;/pre>
&lt;h2 id="五ucx-性能测试">五、UCX 性能测试&lt;/h2>
&lt;p>发送方：&lt;/p>
&lt;pre>&lt;code class="language-bash">ucx_perftest -c 0 -d mlx5_0:1
&lt;/code>&lt;/pre>
&lt;p>接收方：&lt;/p>
&lt;pre>&lt;code class="language-bash">ucx_perftest -c 1 -d mlx5_0:1 &amp;lt;server_hostname&amp;gt; -t tag_lat
&lt;/code>&lt;/pre>
&lt;p>总之，通过以上步骤，我们已经成功地从源代码编译并安装了 UCX 1.15.0 和 OpenMPI 5.0.0，并将其整合为一个高效稳定的高性能计算环境。在实际应用中，可以根据具体需求进一步优化配置以获得更优性能。&lt;/p></description></item><item><title>GCC-13.2.0 编译安装</title><link>https://cuterwrite.top/p/gcc-13-source-install/</link><pubDate>Tue, 30 Jan 2024 11:00:00 +0000</pubDate><guid>https://cuterwrite.top/p/gcc-13-source-install/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/cropped-2024-01-30.webp" alt="Featured image of post GCC-13.2.0 编译安装" />&lt;h1 id="gcc-1320-编译安装">GCC-13.2.0 编译安装&lt;/h1>
&lt;p>GCC 13.1 已被发布为 GCC 13 的第一个稳定版本，作为 GNU 编译器集合的年度功能发布。&lt;/p>
&lt;p>GCC 13.1 是一个重大更新，为那些对一些老式编程感兴趣的人添加了 Modula-2 语言前端，虽然有新的 GCC Rust &lt;code>gccrs&lt;/code> 代码，但它在 v13.1 中被禁用。在这个版本，GCC 的静态分析器继续改进，有更多的 C23 和 C++23 功能，并支持许多新的 x86_64/RISC-V/AArch64 处理器。&lt;/p>
&lt;p>GCC 13.1 还提供了对 Ryzen 7000 系列和 EPYC 9004 系列处理器的初始 AMD Zen 4(Znver4)支持，OpenMP 卸载改进，支持以 JSON 为基础的 SARIF 格式发出诊断程序，Ada 2022 附加功能，各种新的 C/C++警告，支持 AMD 本能 MI200 系列用于 AMDGCN 后端，Ampere-1A 支持，Neoverse-V2/Cortex-X3/Cortex-X1C/Cortex-A715 支持，以及许多新的 Intel CPU 支持。GCC 13 增加了针对 Raptor Lake, Meteor Lake, Sierra Forest, Grand Ridge, Emerald Rapids 以及 Granite Rapids 的英特尔 CPU Target，以及相关的新英特尔 CPU 指令集扩展，如 AMX-FP16、AVX-IFMA、AVX-VNNI-INT8、AVX-NE-CONVERT、RAO-INT 和 AMX-Complex。&lt;/p>
&lt;p>为了体验 C++20 的新功能，GCC 13.1 也是一个很好的选择，因为它包括对 C++20 的许多新功能的支持。截止到本文撰写时，GCC-13.2 也已发布，所以我直接选择了最新的版本。&lt;/p>
&lt;h2 id="下载-gcc-1320-源码">下载 GCC-13.2.0 源码&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>下载地址：&lt;a class="link" href="https://mirror.koddos.net/gcc/releases/gcc-13.2.0/" target="_blank" rel="noopener" >Index of /gcc/releases/gcc-13.2.0
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>下载与解压 GCC-13.2.0 源码&lt;/p>
&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">wget https://mirror.koddos.net/gcc/releases/gcc-13.2.0/gcc-13.2.0.tar.gz
tar -xzvf gcc-13.2.0.tar.gz
cd gcc-13.2.0
&lt;/code>&lt;/pre>
&lt;h2 id="开始编译">开始编译&lt;/h2>
&lt;ul>
&lt;li>编译命令：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">./contrib/download_prerequisites
mkdir build &amp;amp;&amp;amp; cd build
../configure --prefix=/root/software/gcc-13.2.0 \
--with-pkgversion='glibc gcc V13.2.0' \
--enable-checking=release \
--enable-languages=c,c++ \
--disable-multilib \
--enable-bootstrap \
--enable-threads=posix \
--with-system-zlib \
--with-gmp=$GMP_HOME \
--with-mpfr=$MPFR_HOME \
--with-mpc=$MPC_HOME \
make -j$(nproc)
make install
&lt;/code>&lt;/pre>
&lt;h2 id="设置环境变量">设置环境变量&lt;/h2>
&lt;pre>&lt;code class="language-bash"># gcc-13.0.2.env
export GCC13_HOME=/root/software/gcc-13.2.0
export PATH=$GCC13_HOME/bin:$PATH
export LD_LIBRARY_PATH=$GCC13_HOME/lib64:$LD_LIBRARY_PATH
export LD_LIBRARY_PATH=$GCC13_HOME/lib:$LD_LIBRARY_PATH
export LD_LIBRARY_PATH=$GCC13_HOME/libexec:$LD_LIBRARY_PATH
export CPATH=$GCC13_HOME/include:$CPATH
export INCLUDE=$GCC13_HOME/include:$CPATH
export CC=$GCC13_HOME/bin/gcc
export CXX=$GCC13_HOME/bin/g++
export FC=$GCC13_HOME/bin/gfortran
export F77=$GCC13_HOME/bin/gfortran
export F90=$GCC13_HOME/bin/gfortran
export F95=$GCC13_HOME/bin/gfortran
&lt;/code>&lt;/pre>
&lt;h2 id="命令行测试">命令行测试&lt;/h2>
&lt;pre>&lt;code class="language-cmd">$ gcc -v
&lt;/code>&lt;/pre>
&lt;p>输出结果为：&lt;/p>
&lt;pre>&lt;code class="language-text">Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/root/software/gcc-13.2.0/libexec/gcc/x86_64-pc-linux-gnu/13.2.0/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../configure --prefix=/root/software/gcc-13.2.0 --with-pkgversion='glibc gcc V13.2.0' --enable-checking=release --enable-languages=c,c++,fortran --enable-threads=posix --enable-bootstrap --disable-multilib --with-system-zlib --with-gmp=/root/software/gmp/6.2.1 --with-mpfr=/root/software/mpfr/4.1.0 --with-mpc=/root/software/mpc/1.2.1
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 13.2.0 (glibc gcc V13.2.0)
&lt;/code>&lt;/pre>
&lt;p>即为编译安装成功。&lt;/p>
&lt;h2 id="c-20-主要新特性">C++ 20 主要新特性&lt;/h2>
&lt;ul>
&lt;li>C++ 20 的主要新特性如下：
&lt;ul>
&lt;li>Concepts（概念）：概念是对模板参数的类型约束，它们使得模板代码更加清晰和易于理解。概念允许开发者定义一个接口，模板参数必须满足这个接口才能被接受。&lt;/li>
&lt;li>Ranges（范围库）：这是对标准模板库（STL）的一个重大扩展，它引入了“范围”概念，以支持更加声明式的数据处理方式。&lt;/li>
&lt;li>Spaceship Operator(三路比较运算符)：&amp;lt;=&amp;gt;被称为三路比较运算符，它可以一次性比较两个值，返回它们的相对顺序（小于、等于、大于）。&lt;/li>
&lt;li>Modules (模块)：模块旨在替代传统的头文件和源文件分离方式，提供一种新的编译单元，可以显著改善编译时间和代码组织。&lt;/li>
&lt;li>&lt;strong>Coroutines&lt;/strong> (协程)：协程是一种轻量级的线程，它可以在不同的执行点之间切换，而不是在函数调用之间切换。协程是一种用于编写异步代码的新方法，它允许函数在不同的时间点暂停和恢复执行，而不需要回调函数或复杂的状态机。&lt;/li>
&lt;li>constexpr 改进：C++20 大大扩展了可以在编译时计算的代码的范围，包括允许 &lt;code>virtual&lt;/code> 函数、&lt;code>try&lt;/code> 和 &lt;code>catch&lt;/code> 块在 &lt;code>constexpr&lt;/code> 函数中使用。&lt;/li>
&lt;li>初始化器列表的 &lt;code>std::to_array&lt;/code> ：这允许将初始化器列表转换为 std::array ，从而提供了一种类型安全的方式来处理固定大小的数组。&lt;/li>
&lt;li>模板语法的简化：typename 和 class 在模板参数中可以互换使用，简化了模板的语法。&lt;/li>
&lt;li>新的标准属性：引入了多个新的属性，如 &lt;code>[[likely]]&lt;/code> 和 &lt;code>[[unlikely]]&lt;/code> ，用于向编译器提供分支预测的提示。&lt;/li>
&lt;li>新的标准库组件：例如 &lt;code>std::span&lt;/code> ，它提供了一个视图，可以表示数组或其他连续序列的一部分，而不需要复制数据。&lt;/li>
&lt;li>新的同步库：例如 &lt;code>std::latch&lt;/code> 和 &lt;code>std::barrier&lt;/code> ，为多线程编程提供了新的同步原语。&lt;/li>
&lt;li>std::format：这是一个新的格式化库，它提供了一种类型安全的方式来格式化字符串。&lt;/li>
&lt;li>其它等等&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="c-23-主要新特性">C++ 23 主要新特性&lt;/h2>
&lt;ul>
&lt;li>C++ 23 的主要新特性如下：
&lt;ul>
&lt;li>Lambada
&lt;ol>
&lt;li>修复省略参数括号 () 的问题。&lt;/li>
&lt;li>更改 lambda 尾部返回类型的作用域。&lt;/li>
&lt;li>让支持函数的 attributes 都支持 lambda。这个功能其实很多编译器早已支持。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>编译期计算：主要修复一些 bug 和继续完善编译期计算的能力。&lt;/li>
&lt;li>&lt;strong>Deducing this&lt;/strong> : Deducing this 是 C++23 中最重要的特性之一。它其实就是提供一种将非静态成员函数的“隐式对象参数”变为“显式对象参数”的方法。
&lt;ul>
&lt;li>Deducing this 的主要动机是消除成员函数修饰所带来的冗余。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>多维数组：
&lt;ol>
&lt;li>支持多维下标运算符，即 operator[a, b, c, …]。&lt;/li>
&lt;li>标准库引入 std::mdspan。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>标准库：
&lt;ul>
&lt;li>增强 std::string 和 std::string_view&lt;/li>
&lt;li>增强 std::optional&lt;/li>
&lt;li>std::flat_map 和 std::flat_set， 替代 std::map 和 std::set。&lt;/li>
&lt;li>std::stacktrace：用于 exception 捕获后，展开调用栈，方便调试。将 stacktrace 引入标准库，可以看作是对 C++ 异常处理能力的加强。&lt;/li>
&lt;li>std::excepted：对 C++ 通过返回值进行错误处理的能力加强。和 std::optional 差不多，但是 std::optional 只能表示正常值和空值（std::nullopt）。而 std::expected 则可以表示一个期望的值和一个错误的值，相当于两个成员的 std::variant，但是 std::excepted 的接口使用起来更方便。&lt;/li>
&lt;li>std::unreachable()：给编译器的优化提示，告诉编译器这个地方是不可到达的。如果 std::unreachable() 被调用，其结果是 undefined behavior。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>其它：
&lt;ul>
&lt;li>静态 operator() 和 静态 operator[]&lt;/li>
&lt;li>假定表达式 [[assume(expr)]]&lt;/li>
&lt;li>size_t 字面量&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="corutines-协程示例">Corutines 协程示例&lt;/h2>
&lt;pre>&lt;code class="language-cpp">#include &amp;lt;coroutine&amp;gt;
#include &amp;lt;iostream&amp;gt;
#include &amp;lt;optional&amp;gt;
template&amp;lt;typename T&amp;gt;
struct Generator {
struct promise_type;
using handle_type = std::coroutine_handle&amp;lt;promise_type&amp;gt;;
struct promise_type {
std::optional&amp;lt;T&amp;gt; current_value;
static auto get_return_object_on_allocation_failure() { return Generator{nullptr}; }
auto get_return_object() { return Generator{handle_type::from_promise(*this)}; }
auto initial_suspend() { return std::suspend_always{}; }
auto final_suspend() noexcept { return std::suspend_always{}; }
void unhandled_exception() { std::exit(1); }
template&amp;lt;typename U&amp;gt;
auto yield_value(U&amp;amp;&amp;amp; value) {
current_value = std::forward&amp;lt;U&amp;gt;(value);
return std::suspend_always{};
}
void return_void() {}
};
handle_type coro;
Generator(handle_type h): coro(h) {}
Generator(Generator const&amp;amp;) = delete;
Generator(Generator&amp;amp;&amp;amp; o) : coro(o.coro) { o.coro = nullptr; }
~Generator() { if (coro) coro.destroy(); }
T next() {
if (coro) {
coro.resume();
if (coro.done()) {
coro.promise().current_value.reset();
}
return *coro.promise().current_value;
}
return T{};
}
};
Generator&amp;lt;int&amp;gt; generateNumbers(int start, int end) {
for (int i = start; i &amp;lt;= end; ++i) {
co_yield i;
}
}
int main() {
auto numbers = generateNumbers(1, 5);
for (int i = 1; i &amp;lt;= 5; ++i) {
std::cout &amp;lt;&amp;lt; numbers.next() &amp;lt;&amp;lt; std::endl;
}
return 0;
}
&lt;/code>&lt;/pre>
&lt;p>编译命令：&lt;/p>
&lt;pre>&lt;code class="language-bash">g++ -o coroutines coroutines.cpp -std=c++20 -fcoroutines -O3
&lt;/code>&lt;/pre>
&lt;p>运行结果:&lt;/p>
&lt;pre>&lt;code class="language-bash">./coroutines
1
2
3
4
5
&lt;/code>&lt;/pre>
&lt;h2 id="deducing-this-示例">Deducing this 示例&lt;/h2>
&lt;pre>&lt;code class="language-cpp">#include &amp;lt;iostream&amp;gt;
struct Test {
template &amp;lt;typename Self&amp;gt;
void explicitCall(this Self&amp;amp;&amp;amp; self, const std::string&amp;amp; text) {
std::cout &amp;lt;&amp;lt; text &amp;lt;&amp;lt; &amp;quot;: &amp;quot;;
std::forward&amp;lt;Self&amp;gt;(self).implicitCall();
std::cout &amp;lt;&amp;lt; '\n';
}
void implicitCall() &amp;amp; {
std::cout &amp;lt;&amp;lt; &amp;quot;non const lvalue&amp;quot;;
}
void implicitCall() const&amp;amp; {
std::cout &amp;lt;&amp;lt; &amp;quot;const lvalue&amp;quot;;
}
void implicitCall() &amp;amp;&amp;amp; {
std::cout &amp;lt;&amp;lt; &amp;quot;non const rvalue&amp;quot;;
}
void implicitCall() const&amp;amp;&amp;amp; {
std::cout &amp;lt;&amp;lt; &amp;quot;const rvalue&amp;quot;;
}
};
int main() {
std::cout &amp;lt;&amp;lt; '\n';
Test test;
const Test constTest;
test.explicitCall(&amp;quot;test&amp;quot;);
constTest.explicitCall(&amp;quot;constTest&amp;quot;);
std::move(test).explicitCall(&amp;quot;std::move(test)&amp;quot;);
std::move(constTest).explicitCall(&amp;quot;std::move(consTest)&amp;quot;);
std::cout &amp;lt;&amp;lt; '\n';
}
&lt;/code>&lt;/pre></description></item><item><title>RDMA 概述</title><link>https://cuterwrite.top/p/rdma-overview/</link><pubDate>Mon, 01 Jan 2024 01:01:01 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-overview/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/crop_65b36f302c1d3715061e824224dcc9ca195413.jpg@1256w_1806h_!web-article-pic-2024-01-14.webp" alt="Featured image of post RDMA 概述" />&lt;h1 id="rdma-概述">RDMA 概述&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/138874738">&lt;cite>知乎专栏：1. RDMA 概述&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>本想完全靠自己的语言完成这篇概述，然而开篇并没有想象当中的好写，看样子从宏观上概括一个技术比从微观上探究细枝末节要困难不少。本文是以前人们对 RDMA 技术的介绍为主，加入了一些自己的理解。随着本专栏内容的增加，本篇概述也会更新和逐渐完善。&lt;/p>
&lt;h2 id="什么是-dma">什么是 DMA&lt;/h2>
&lt;p>DMA 全称为 Direct Memory Access，即直接内存访问。意思是外设对内存的读写过程可以不用 CPU 参与而直接进行。我们先来看一下没有 DMA 的时候：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20240114203040-2024-01-14.png"
alt="无 DMA 控制器时 I/O 设备和内存间的数据路径" width="auto" loading="lazy"/>&lt;figcaption>
&lt;h4>无 DMA 控制器时 I/O 设备和内存间的数据路径&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>假设 I/O 设备为一个普通网卡，为了从内存拿到需要发送的数据，然后组装数据包发送到物理链路上，网卡需要通过总线告知 CPU 自己的数据请求。然后 CPU 将会把内存缓冲区中的数据复制到自己内部的寄存器中，再复制到 I/O 设备的存储空间中。如果数据量比较大，那么很长一段时间内 CPU 都会忙于搬移数据，而无法投入到其他工作中去。&lt;/p>
&lt;p>CPU 的最主要工作是计算，而不是进行数据复制，这种工作属于白白浪费了它的计算能力。为了给 CPU“减负”，让它投入到更有意义的工作中去，后来人们设计了 DMA 机制：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20240114203114-2024-01-14.png"
alt="有 DMA 控制器时 I/O 设备和内存间的数据路径" width="auto" loading="lazy"/>&lt;figcaption>
&lt;h4>有 DMA 控制器时 I/O 设备和内存间的数据路径&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>可以看到总线上又挂了一个 DMA 控制器，它是专门用来读写内存的设备。有了它以后，当我们的网卡想要从内存中拷贝数据时，除了一些必要的控制命令外，整个数据复制过程都是由 DMA 控制器完成的。过程跟 CPU 复制是一样的，只不过这次是把内存中的数据通过总线复制到 DMA 控制器内部的寄存器中，再复制到 I/O 设备的存储空间中。CPU 除了关注一下这个过程的开始和结束以外，其他时间可以去做其他事情。&lt;/p>
&lt;p>DMA 控制器一般是和 I/O 设备在一起的，也就是说一块网卡中既有负责数据收发的模块，也有 DMA 模块。&lt;/p>
&lt;h2 id="什么是-rdma">什么是 RDMA&lt;/h2>
&lt;p>RDMA（ Remote Direct Memory Access ）意为远程直接地址访问，通过 RDMA，本端节点可以“直接”访问远端节点的内存。所谓直接，指的是可以像访问本地内存一样，绕过传统以太网复杂的 TCP/IP 网络协议栈读写远端内存，而这个过程对端是不感知的，而且这个读写过程的大部分工作是由硬件而不是软件完成的。&lt;/p>
&lt;p>为了能够直观的理解这一过程，请看下面两个图（图中箭头仅做示意，不表示实际逻辑或物理关系）：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20240115235555-2024-01-15.png"
alt="20240115235555-2024-01-15" width="90%" loading="lazy"/>
&lt;/figure>
&lt;p>传统网络中，“节点 A 给节点 B 发消息”实际上做的是“把节点 A 内存中的一段数据，通过网络链路搬移到节点 B 的内存中”，而这一过程无论是发端还是收段，都需要 CPU 的指挥和控制，包括网卡的控制，中断的处理，报文的封装和解析等等。&lt;/p>
&lt;p>上图中左边的节点在内存用户空间中的数据，需要经过 CPU 拷贝到内核空间的缓冲区中，然后才可以被网卡访问，这期间数据会经过软件实现的 TCP/IP 协议栈，加上各层头部和校验码，比如 TCP 头，IP 头等。网卡通过 DMA 拷贝内核中的数据到网卡内部的缓冲区中，进行处理后通过物理链路发送给对端。&lt;/p>
&lt;p>对端收到数据后，会进行相反的过程：从网卡内部存储空间，将数据通过 DMA 拷贝到内存内核空间的缓冲区中，然后 CPU 会通过 TCP/IP 协议栈对其进行解析，将数据取出来拷贝到用户空间中。&lt;/p>
&lt;p>可以看到，即使有了 DMA 技术，上述过程还是对 CPU 有较强的依赖。&lt;/p>
&lt;p>而使用了 RDMA 技术之后，这一过程可以简单的表示成下面的示意图：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20240115235612-2024-01-15.png"
alt="20240115235612-2024-01-15" width="90%" loading="lazy"/>
&lt;/figure>
&lt;p>同样是把本端内存中的一段数据，复制到对端内存中，在使用了 RDMA 技术时，两端的 CPU 几乎不用参与数据传输过程（只参与控制面）。本端的网卡直接从内存的用户空间 DMA 拷贝数据到内部存储空间，然后硬件进行各层报文的组装后，通过物理链路发送到对端网卡。对端的 RDMA 网卡收到数据后，剥离各层报文头和校验码，通过 DMA 将数据直接拷贝到用户空间内存中。&lt;/p>
&lt;h2 id="rdma-的优势">RDMA 的优势&lt;/h2>
&lt;p>RDMA 主要应用在高性能计算（HPC）领域和大型数据中心当中，并且设备相对普通以太网卡要昂贵不少（比如 Mellanox 公司的 Connext-X 5 100Gb PCIe 网卡市价在 4000 元以上）。由于使用场景和价格的原因，RDMA 与普通开发者和消费者的距离较远，目前主要是一些大型互联网企业在部署和使用。&lt;/p>
&lt;p>RDMA 技术为什么可以应用在上述场景中呢？这就涉及到它的以下几个特点：&lt;/p>
&lt;ul>
&lt;li>0 拷贝：指的是不需要在用户空间和内核空间中来回复制数据。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20240115235643-2024-01-15.png"
alt="20240115235643-2024-01-15" width="60%" loading="lazy"/>
&lt;/figure>
&lt;p>由于 Linux 等操作系统将内存划分为用户空间和内核空间，在传统的 Socket 通信流程中 CPU 需要多次把数据在内存中来回拷贝。而通过 RDMA 技术，我们可以直接访问远端已经注册的内存区域。&lt;/p>
&lt;p>关于 0 拷贝可以参考这篇文章：&lt;a class="link" href="https://www.jianshu.com/p/e76e3580e356" target="_blank" rel="noopener" >浅谈 Linux 下的零拷贝机制
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;ul>
&lt;li>内核 Bypass：指的是 IO（数据）流程可以绕过内核，即在用户层就可以把数据准备好并通知硬件准备发送和接收。避免了系统调用和上下文切换的开销。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20240115235721-2024-01-15.png"
alt="20240115235721-2024-01-15" width="90%" loading="lazy"/>
&lt;/figure>
&lt;p>上图（原图&lt;a class="link" href="https://pc.nanog.org/static/published/meetings/NANOG76/1999/20190612_Cardona_Towards_Hyperscale_High_v1.pdf" target="_blank" rel="noopener" >[1]
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
）可以很好的解释“0 拷贝”和“内核 Bypass”的含义。上下两部分分别是基于 Socket 的和基于 RDMA 的一次收-发流程，左右分别为两个节点。可以明显的看到 Socket 流程中在软件中多了一次拷贝动作。而 RDMA 绕过了内核同时也减少了内存拷贝，数据可以直接在用户层和硬件间传递。&lt;/p>
&lt;ul>
&lt;li>CPU 卸载：指的是可以在远端节点 CPU 不参与通信的情况下（当然要持有访问远端某段内存的“钥匙”才行）对内存进行读写，这实际上是 &lt;strong>把报文封装和解析放到硬件中做了&lt;/strong>。而传统的以太网通信，双方 CPU 都必须参与各层报文的解析，如果数据量大且交互频繁，对 CPU 来讲将是一笔不小的开销，而这些被占用的 CPU 计算资源本可以做一些更有价值的工作。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20240115235929-2024-01-15.png"
alt="20240115235929-2024-01-15" width="60%" loading="lazy"/>
&lt;/figure>
&lt;p>通信领域两大出场率最高的性能指标就是“带宽”和“时延”。简单的说，所谓带宽指的是指单位时间内能够传输的数据量，而时延指的是数据从本端发出到被对端接收所耗费的时间。因为上述几个特点，相比于传统以太网，RDMA 技术同时做到了更高带宽和更低时延，所以其在带宽敏感的场景——比如海量数据的交互，时延敏感——比如多个计算节点间的数据同步的场景下得以发挥其作用。&lt;/p>
&lt;h2 id="协议">协议&lt;/h2>
&lt;p>RDMA 本身指的是一种技术，具体协议层面，包含 Infiniband（IB），RDMA over Converged Ethernet（RoCE）和 internet Wide Area RDMA Protocol（iWARP）。三种协议都符合 RDMA 标准，使用相同的上层接口，在不同层次上有一些差别。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20240116000000-2024-01-16.png"
alt="20240116000000-2024-01-16" width="90%" loading="lazy"/>
&lt;/figure>
&lt;p>上图&lt;a class="link" href="https://www.snia.org/sites/default/files/ESF/RoCE-vs.-iWARP-Final.pdf" target="_blank" rel="noopener" >[2]g
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
对于几种常见的 RDMA 技术的协议层次做了非常清晰的对比&lt;/p>
&lt;h3 id="infiniband">Infiniband&lt;/h3>
&lt;p>2000 年由 IBTA（InfiniBand Trade Association）提出的 IB 协议是当之无愧的核心，其规定了一整套完整的链路层到传输层（非传统 OSI 七层模型的传输层，而是位于其之上）规范，但是其无法兼容现有以太网，除了需要支持 IB 的网卡之外，企业如果想部署的话还要重新购买配套的交换设备。&lt;/p>
&lt;h3 id="roce">RoCE&lt;/h3>
&lt;p>RoCE 从英文全称就可以看出它是基于以太网链路层的协议，v1 版本网络层仍然使用了 IB 规范，而 v2 使用了 UDP+IP 作为网络层，使得数据包也可以被路由。RoCE 可以被认为是 IB 的“低成本解决方案”，将 IB 的报文封装成以太网包进行收发。由于 RoCE v2 可以使用以太网的交换设备，所以现在在企业中应用也比较多，但是相同场景下相比 IB 性能要有一些损失。&lt;/p>
&lt;h3 id="iwarp">iWARP&lt;/h3>
&lt;p>iWARP 协议是 IETF 基于 TCP 提出的，因为 TCP 是面向连接的可靠协议，这使得 iWARP 在面对有损网络场景（可以理解为网络环境中可能经常出现丢包）时相比于 RoCE v2 和 IB 具有更好的可靠性，在大规模组网时也有明显的优势。但是大量的 TCP 连接会耗费很多的内存资源，另外 TCP 复杂的流控等机制会导致性能问题，所以从性能上看 iWARP 要比 UDP 的 RoCE v2 和 IB 差。&lt;/p>
&lt;p>需要注意的是，虽然有软件实现的 RoCE 和 iWARP 协议，但是真正商用时上述几种协议都需要专门的硬件（网卡）支持。&lt;/p>
&lt;p>iWARP 本身不是由 Infiniband 直接发展而来的，但是它继承了一些 Infiniband 技术的设计思想。这三种协议的关系如下图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20240116000125-2024-01-16.png"
alt="20240116000125-2024-01-16" width="60%" loading="lazy"/>
&lt;/figure>
&lt;h2 id="玩家">玩家&lt;/h2>
&lt;h3 id="标准生态组织">标准/生态组织&lt;/h3>
&lt;p>提到 IB 协议，就不得不提到两大组织——IBTA 和 OFA。&lt;/p>
&lt;h3 id="ibta3httpswwwinfinibandtaorg">IBTA&lt;a class="link" href="https://www.infinibandta.org/" target="_blank" rel="noopener" >[3]
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/h3>
&lt;p>成立于 1999 年，负责制定和维护 Infiniband 协议标准。IBTA 独立于各个厂商，通过赞助技术活动和推动资源共享来将整个行业整合在一起，并且通过线上交流、营销和线下活动等方式积极推广 IB 和 RoCE。&lt;/p>
&lt;p>IBTA 会对商用的 IB 和 RoCE 设备进行协议标准符合性和互操作性测试及认证，由很多大型的 IT 厂商组成的委员会领导，其主要成员包括博通，HPE，IBM，英特尔，Mellanox 和微软等，华为也是 IBTA 的会员。&lt;/p>
&lt;h3 id="ofa4httpswwwopenfabricsorg">OFA&lt;a class="link" href="https://www.openfabrics.org/" target="_blank" rel="noopener" >[4]
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/h3>
&lt;p>成立于 2004 年的非盈利组织，负责开发、测试、认证、支持和分发独立于厂商的开源跨平台 infiniband 协议栈，2010 年开始支持 RoCE。其对用于支撑 RDMA/Kernel bypass 应用的 OFED（OpenFabrics Enterprise Distribution）软件栈负责，保证其与主流软硬件的兼容性和易用性。OFED 软件栈包括驱动、内核、中间件和 API。&lt;/p>
&lt;p>上述两个组织是配合关系，IBTA 主要负责开发、维护和增强 Infiniband 协议标准；OFA 负责开发和维护 Infiniband 协议和上层应用 API。&lt;/p>
&lt;h2 id="开发社区">开发社区&lt;/h2>
&lt;h3 id="linux-社区">Linux 社区&lt;/h3>
&lt;p>Linux 内核的 RDMA 子系统还算比较活跃，经常会讨论一些协议细节，对框架的修改比较频繁，另外包括华为和 Mellanox 在内的一些厂商也会经常对驱动代码进行修改。&lt;/p>
&lt;p>邮件订阅：&lt;a class="link" href="http://vger.kernel.org/vger-lists.html#linux-rdma" target="_blank" rel="noopener" >http://vger.kernel.org/vger-lists.html#linux-rdma
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;p>代码位于内核 drivers/infiniband/目录下，包括框架核心代码和各厂商的驱动代码。&lt;/p>
&lt;p>代码仓：&lt;a class="link" href="https://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma.git/" target="_blank" rel="noopener" >https://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma.git/
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h3 id="rdma-社区">RDMA 社区&lt;/h3>
&lt;p>对于上层用户，IB 提供了一套与 Socket 套接字类似的接口——libibverbs，前文所述三种协议都可以使用。参考着协议、API 文档和示例程序很容易就可以写一个 Demo 出来。本专栏中的 RDMA 社区专指其用户态社区，在 github 上其仓库的名字为 linux-rdma。&lt;/p>
&lt;p>主要包含两个子仓库：&lt;/p>
&lt;ul>
&lt;li>rdma-core&lt;/li>
&lt;/ul>
&lt;p>用户态核心代码，API，文档以及各个厂商的用户态驱动。&lt;/p>
&lt;ul>
&lt;li>perftest
一个功能强大的用于测试 RDMA 性能的工具。&lt;/li>
&lt;/ul>
&lt;p>代码仓：&lt;a class="link" href="https://github.com/linux-rdma/" target="_blank" rel="noopener" >https://github.com/linux-rdma/
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h2 id="ucx5httpswwwopenucxorg">UCX&lt;a class="link" href="https://www.openucx.org/" target="_blank" rel="noopener" >[5]
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/h2>
&lt;p>UCX 是一个建立在 RDMA 等技术之上的用于数据处理和高性能计算的通信框架，RDMA 是其底层核心之一。我们可以将其理解为是位于应用和 RDMA API 之间的中间件，向上层用户又封装了一层更易开发的接口。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20240116000429-2024-01-16.png"
alt="20240116000429-2024-01-16" width="90%" loading="lazy"/>
&lt;/figure>
&lt;p>笔者对其并不了解太多，只知道业界有一些企业在基于 UCX 开发应用。&lt;/p>
&lt;p>代码仓：&lt;a class="link" href="https://github.com/openucx/ucx" target="_blank" rel="noopener" >https://github.com/openucx/ucx
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h2 id="硬件厂商">硬件厂商&lt;/h2>
&lt;p>设计和生产 IB 相关硬件的厂商有不少，包括 Mellanox、华为、收购了 Qlogic 的 IB 技术的 Intel，博通、Marvell，富士通等等，这里就不逐个展开了，仅简单提一下 Mellanox 和华为。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Mellanox
IB 领域的领头羊，协议标准制定、软硬件开发和生态建设都能看到 Mellanox 的身影，其在社区和标准制定上上拥有最大的话语权。目前最新一代的网卡是支持 200Gb/s 的 ConnextX-6 系列。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>华为
去年初推出的鲲鹏 920 芯片已经支持 100Gb/s 的 RoCE 协议，技术上在国内处于领先地位。但是软硬件和影响力方面距离 Mellanox 还有比较长的路要走，相信华为能够早日赶上老大哥的步伐。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="用户">用户&lt;/h2>
&lt;p>微软、IBM 和国内的阿里、京东都正在使用 RDMA，另外还有很多大型 IT 公司在做初步的开发和测试。在数据中心和高性能计算场景下，RDMA 代替传统网络是大势所趋。笔者对于市场接触不多，所以并不能提供更详细的应用情况。&lt;/p>
&lt;p>下一篇将用比较直观的方式比较一次典型的基于 Socket 的传统以太网和 RDMA 通信过程。&lt;/p></description></item></channel></rss>