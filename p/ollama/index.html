<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="本文着重介绍了 Ollama 的基本概念及它的显著优势，包括其开源免费、简单易用、模型丰富以及资源占用低的特点。接着，提供了详尽的安装和使用指南，涵盖不同操作系统和 Docker 环境的详细安装步骤，以及如何下载和运行模型。此外，文章还介绍了在 HPC 集群上部署 Ollama 以及如何结合 IDE 插件打造本地代码补全助手的方法。"><title>Ollama：从入门到进阶</title>
<link rel=canonical href=https://cuterwrite.top/p/ollama/><link rel=stylesheet href=/scss/style.min.2b724d31a9334eb879a7204b807f915cfebe5daa80340d704caa98b8da1011cf.css><meta property='og:title' content="Ollama：从入门到进阶"><meta property='og:description' content="本文着重介绍了 Ollama 的基本概念及它的显著优势，包括其开源免费、简单易用、模型丰富以及资源占用低的特点。接着，提供了详尽的安装和使用指南，涵盖不同操作系统和 Docker 环境的详细安装步骤，以及如何下载和运行模型。此外，文章还介绍了在 HPC 集群上部署 Ollama 以及如何结合 IDE 插件打造本地代码补全助手的方法。"><meta property='og:url' content='https://cuterwrite.top/p/ollama/'><meta property='og:site_name' content="Cuterwrite's Blog"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='llm'><meta property='article:published_time' content='2024-06-15T23:10:00+00:00'><meta property='article:modified_time' content='2024-06-15T23:10:00+00:00'><meta property='og:image' content='https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116903387_p0_master1200.webp'><meta name=twitter:title content="Ollama：从入门到进阶"><meta name=twitter:description content="本文着重介绍了 Ollama 的基本概念及它的显著优势，包括其开源免费、简单易用、模型丰富以及资源占用低的特点。接着，提供了详尽的安装和使用指南，涵盖不同操作系统和 Docker 环境的详细安装步骤，以及如何下载和运行模型。此外，文章还介绍了在 HPC 集群上部署 Ollama 以及如何结合 IDE 插件打造本地代码补全助手的方法。"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116903387_p0_master1200.webp'><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><script async src=https://analytics.cuterwrite.top/uma data-website-id=b13594a2-4d15-4a4e-a020-5e3cc1d88c12 data-domains=cuterwrite.top></script><link rel=manifest href=/manifest.json></head><body class="article-page
line-numbers"><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hue4d14694a57c01a222a16c47db12c89c_369633_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>😉</span></figure><div class=site-meta><h1 class=site-name><a href=/>Cuterwrite's Blog</a></h1><h2 class=site-description>Cuterwrite 的技术博客, 专注于高性能计算、操作系统、全栈开发、人工智能等领域的深度探讨和经验分享。</h2></div></header><ol class=menu-social><li><a href=https://analytics.cuterwrite.top/share/Ji0gm9OaLDk8gco7 target=_blank title=Analytics rel=me><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 5H7A2 2 0 005 7v12a2 2 0 002 2h10a2 2 0 002-2V7a2 2 0 00-2-2h-2"/><rect x="9" y="3" width="6" height="4" rx="2"/><path d="M9 17v-5"/><path d="M12 17v-1"/><path d="M15 17v-3"/></svg></a></li><li><a href=https://status.cuterwrite.top target=_blank title=Upptime rel=me><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-chart-line"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 19h16"/><path d="M4 15l4-6 4 2 4-5 4 4"/></svg></a></li><li><a href=/index.xml target=_blank title=RSS rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-rss" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="5" cy="19" r="1"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg></a></li><li><a href=https://github.com/PKUcoldkeyboard target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.zhihu.com/people/kong-tiao-cheng-tai-lang-30-57 target=_blank title=zhihu rel=me><svg t="1705591931290" class="icon" viewBox="0 0 1280 1024" xmlns="http://www.w3.org/2000/svg" p-id="21048" width="32" height="32"><path d="M341.08 296.26v435.08l46.86.02 15.42 52.74 84.02-52.74h99.06V296.26H341.08zm195.5 387.86H480.7l-55.8 35.02-10.16-34.94-23.8-.08V343.5h145.64v340.62zM299.66 495.34H195c3.48-54.2 4.4-103.18 4.4-146.92h102.32s3.94-45.12-17.16-44.62h-177c6.98-26.24 15.74-53.32 26.24-81.34.0.0-48.14.0-64.54 43.14-6.78 17.8-26.42 86.28-61.4 156.24 11.78-1.28 50.74-2.36 73.68-44.42 4.22-11.78 5.02-13.32 10.28-29.06h57.74c0 21-2.4 133.76-3.36 146.88H41.66c-23.48.0-31.12 47.24-31.12 47.24H141.7C132.9 642.2 85.66 726.24.0 792.68c40.98 11.7 81.82-1.86 102-19.8.0.0 45.96-41.8 71.18-138.5L281.1 764.26s15.82-53.78-2.48-79.98c-15.16-17.84-56.12-66.12-73.58-83.62L175.8 623.9c8.72-27.96 13.98-55.1 15.74-81.34h123.3s-.18-47.24-15.18-47.24v.02zm824.04-3.2c41.66-51.28 89.96-117.14 89.96-117.14s-37.3-29.6-54.76-8.12c-12 16.3-73.66 96.4-73.66 96.4l38.46 28.86zM823.52 373.96c-18.02-16.5-51.82 4.26-51.82 4.26s79.04 110.08 82.24 114.9l38.92-27.46s-51.34-75.22-69.32-91.72h-.02zM1280 516.7c-39.56.0-261.82 1.86-262.12 1.86v-202c9.62.0 24.84-.8 45.7-2.4 81.76-4.82 140.26-8 175.54-9.62.0.0 24.44-54.38-1.18-66.88-6.14-2.36-46.34 9.16-46.34 9.16s-330.44 32.98-464.72 36.1c3.2 17.64 15.24 34.16 31.56 39.1 26.62 6.96 45.38 3.4 98.3 1.78 49.66-3.2 87.36-4.86 113.02-4.86v199.62H702.82s5.64 44.62 51.02 45.7h215.88V706.1c0 27.94-22.38 43.98-48.96 42.24-28.16.22-52.16-2.3-83.38-3.62 3.98 7.94 12.66 28.78 38.62 43.68 19.76 9.62 32.34 13.14 52.04 13.14 59.12.0 91.34-34.56 89.78-90.62V564.28h244.72c19.36.0 17.4-47.56 17.4-47.56l.06-.02z" fill="#707070" p-id="21049"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页 | Home</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>关于 | About</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>归档 | Archives</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>搜索 | Search</span></a></li><li><a href=https://cuterwrite.top/image-hosting target=_blank><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-album"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 4m0 2a2 2 0 012-2h12a2 2 0 012 2v12a2 2 0 01-2 2H6a2 2 0 01-2-2z"/><path d="M12 4v7l2-2 2 2V4"/></svg>
<span>图册 | Gallery</span></a></li><li><a href=https://draw.cuterwrite.top target=_blank><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-artboard"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M8 8m0 1a1 1 0 011-1h6a1 1 0 011 1v6a1 1 0 01-1 1H9a1 1 0 01-1-1z"/><path d="M3 8h1"/><path d="M3 16h1"/><path d="M8 3v1"/><path d="M16 3v1"/><path d="M20 8h1"/><path d="M20 16h1"/><path d="M8 20v1"/><path d="M16 20v1"/></svg>
<span>画板 | Canvas</span></a></li><li><a href=https://it-tools.tech target=_blank><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-tools"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 21h4L20 8a1.5 1.5.0 00-4-4L3 17v4"/><path d="M14.5 5.5l4 4"/><path d="M12 8 7 3 3 7l5 5"/><path d="M7 8 5.5 9.5"/><path d="M16 12l5 5-4 4-5-5"/><path d="M16 17l-1.5 1.5"/></svg>
<span>工具 | Tools</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ul><li><a href=#什么是-ollama>什么是 Ollama？</a></li><li><a href=#ollama-的优势>Ollama 的优势</a></li><li><a href=#如何使用-ollama>如何使用 Ollama？</a><ul><li><a href=#安装-ollama>安装 Ollama</a><ul><li><a href=#macos>macOS</a></li><li><a href=#windows>Windows</a></li><li><a href=#linux>Linux</a></li><li><a href=#docker>Docker</a></li></ul></li><li><a href=#启动-ollama>启动 Ollama</a></li><li><a href=#下载模型>下载模型</a></li><li><a href=#运行模型>运行模型</a><ul><li><a href=#docker-容器中运行模型>Docker 容器中运行模型</a></li></ul></li><li><a href=#配置-ollama>配置 Ollama</a></li></ul></li><li><a href=#进阶用法hpc-集群上部署-ollama>进阶用法：HPC 集群上部署 Ollama</a></li><li><a href=#进阶用法本地代码补全助手>进阶用法：本地代码补全助手</a><ul><li><a href=#codestral-22b-模型>Codestral 22B 模型</a><ul><li><a href=#下载并运行-codestral-模型>下载并运行 Codestral 模型</a></li><li><a href=#配置-configjson>配置 config.json</a></li></ul></li><li><a href=#deepseek-coder-67b-模型--llama-3-8b-模型>DeepSeek Coder 6.7B 模型 + Llama 3 8B 模型</a><ul><li><a href=#下载并运行-deepseek-coder-模型>下载并运行 DeepSeek Coder 模型</a></li><li><a href=#下载并运行-llama-3-模型>下载并运行 Llama 3 模型</a></li><li><a href=#配置-configjson-1>配置 config.json</a></li></ul></li><li><a href=#codeqwen-7b-模型--qwen2-7b-模型>Codeqwen 7B 模型 + Qwen2 7B 模型</a><ul><li><a href=#下载并运行-codeqwen-模型>下载并运行 Codeqwen 模型</a></li><li><a href=#下载并运行-qwen2-模型>下载并运行 Qwen2 模型</a></li><li><a href=#配置-configjson-2>配置 config.json</a></li></ul></li><li><a href=#利用-rag-向量检索优化聊天>利用 RAG 向量检索优化聊天</a><ul><li><a href=#下载并运行-nomic-embed-text-模型>下载并运行 Nomic Embed Text 模型</a></li><li><a href=#配置-configjson-3>配置 config.json</a></li></ul></li><li><a href=#代码补全效果>代码补全效果</a></li><li><a href=#与-ollama-聊天>与 Ollama 聊天</a></li><li><a href=#代码自动注释>代码自动注释</a></li></ul></li><li><a href=#总结>总结</a></li></ul></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/ollama/><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116903387_p0_master1200.webp loading=lazy alt="Featured image of post Ollama：从入门到进阶"></a></div><div class=article-details><header class=article-category><a href=/categories/ai/ style=background-color:#eaaa60;color:#fff>人工智能与数据科学</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/ollama/>Ollama：从入门到进阶</a></h2><h3 class=article-subtitle>本文着重介绍了 Ollama 的基本概念及它的显著优势，包括其开源免费、简单易用、模型丰富以及资源占用低的特点。接着，提供了详尽的安装和使用指南，涵盖不同操作系统和 Docker 环境的详细安装步骤，以及如何下载和运行模型。此外，文章还介绍了在 HPC 集群上部署 Ollama 以及如何结合 IDE 插件打造本地代码补全助手的方法。</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>2024-06-15</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 7 分钟</time></div><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-keyboard"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M2 6m0 2a2 2 0 012-2h16a2 2 0 012 2v8a2 2 0 01-2 2H4a2 2 0 01-2-2z"/><path d="M6 10v.01"/><path d="M10 10v.01"/><path d="M14 10v.01"/><path d="M18 10v.01"/><path d="M6 14v.01"/><path d="M18 14v.01"/><path d="M10 14l4 .01"/></svg>
<time class=article-time--wordcount>字数统计: 3369 字</time></div></footer></div></header><section class=article-content><p>近年来，大型语言模型（LLM）以其强大的文本生成和理解能力，成为了人工智能领域的中坚力量。商业 LLM 的价格通常高昂且代码封闭，限制了研究者和开发者的探索空间。幸运的是，开源社区提供了像 Ollama 这样优秀的替代方案，让每个人都能够轻松体验 LLM 的魅力，并能结合 HPC 和 IDE 插件，打造更强大的个人助手。</p><h2 id=什么是-ollama>什么是 Ollama？</h2><p>Ollama 是一个用于构建大型语言模型应用的工具，它提供了一个简洁易用的命令行界面和服务器，让你能够轻松下载、运行和管理各种开源 LLM。与需要复杂配置和强大硬件的传统 LLM 不同，Ollama 让你能够方便地像使用手机 App 一样体验 LLM 的强大功能。</p><h2 id=ollama-的优势>Ollama 的优势</h2><p>Ollama 拥有以下显著优势：</p><ul><li><strong>开源免费</strong>： Ollama 及其支持的模型完全开源免费，任何人都可以自由使用、修改和分发。</li><li><strong>简单易用</strong>： 无需复杂的配置和安装过程，只需几条命令即可启动和运行 Ollama。</li><li><strong>模型丰富</strong>： Ollama 支持 Llama 3、Mistral、Qwen2 等众多热门开源 LLM，并提供一键下载和切换功能。</li><li><strong>资源占用低</strong>： 相比于商业 LLM，Ollama 对硬件要求更低，即使在普通笔记本电脑上也能流畅运行。</li><li><strong>社区活跃</strong>： Ollama 拥有庞大且活跃的社区，用户可以轻松获取帮助、分享经验和参与模型开发。</li></ul><h2 id=如何使用-ollama>如何使用 Ollama？</h2><p>使用 Ollama 非常简单，只需要按照以下步骤：</p><ol><li><strong>安装 Ollama</strong>： 根据你的操作系统，从 <a class=link href=https://ollama.com/ target=_blank rel=noopener>Ollama 官网
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>下载并安装最新版本。</li><li><strong>启动 Ollama</strong>： 打开终端或命令行，输入 <code>ollama serve</code> 命令启动 Ollama 服务器。</li><li><strong>下载模型</strong>： 在<a class=link href=https://ollama.com/library target=_blank rel=noopener>模型仓库
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>找到想要的模型，然后使用 <code>ollama pull</code> 命令下载，例如 <code>ollama pull llama3:70b</code> 。</li><li><strong>运行模型</strong>： 使用 <code>ollama run</code> 命令启动模型，例如 <code>ollama run llama3:70b</code> 。</li><li><strong>开始聊天</strong>： 在终端中输入你的问题或指令，Ollama 会根据模型生成相应的回复。</li></ol><h3 id=安装-ollama>安装 Ollama</h3><h4 id=macos>macOS</h4><p><a class=link href=https://ollama.com/download/Ollama-darwin.zip target=_blank rel=noopener>下载 Ollama for macOS
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a></p><h4 id=windows>Windows</h4><p><a class=link href=https://ollama.com/download/OllamaSetup.exe target=_blank rel=noopener>下载 Ollama for Windows
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a></p><h4 id=linux>Linux</h4><pre><code class=language-bash>curl -fsSL https://ollama.com/install.sh | sh
</code></pre><h4 id=docker>Docker</h4><h5 id=cpu-版本>CPU 版本</h5><pre><code class=language-bash>docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
</code></pre><h5 id=gpu-版本>GPU 版本</h5><ol><li>安装 <a class=link href=https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation target=_blank rel=noopener>Nvidia container toolkit
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a></li><li>在 Docker 容器中运行 Ollama</li></ol><pre><code class=language-bash>docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
</code></pre><h3 id=启动-ollama>启动 Ollama</h3><pre><code class=language-bash>ollama serve
</code></pre><p>输出以下信息表示 Ollama 服务器已成功启动（V100 机器）：</p><pre><code class=language-bash>$ ollama serve
### 省略的日志输出 ###
Listening on [::]:11434 (version 0.1.42)
</code></pre><h3 id=下载模型>下载模型</h3><pre><code class=language-bash>ollama pull qwen2:72b
</code></pre><h3 id=运行模型>运行模型</h3><pre><code class=language-bash>ollama run qwen2:72b
</code></pre><p>例如，运行如下命令后：</p><pre><code class=language-bash>$ ollama run qwen2:72b
&gt;&gt;&gt; Who are you?
I am Qwen, a pre-trained language model developed by Alibaba Cloud. My purpose is to assist users in generating various types of text, such as articles, stories, poems, and answering
questions by using the natural language processing techniques. How may I assist you today?
&gt;&gt;&gt; Send a message(/? for help)
</code></pre><h4 id=docker-容器中运行模型>Docker 容器中运行模型</h4><pre><code class=language-bash>docker exec -it ollama ollama run qwen2:72b
</code></pre><h3 id=配置-ollama>配置 Ollama</h3><p>Ollama 提供了多种环境变量以供配置：</p><ul><li><code>OLLAMA_DEBUG</code>：是否开启调试模式，默认为 <code>false</code>。</li><li><code>OLLAMA_FLASH_ATTENTION</code>：是否闪烁注意力，默认为 <code>true</code>。</li><li><code>OLLAMA_HOST</code>：Ollama 服务器的主机地址，默认为空。</li><li><code>OLLAMA_KEEP_ALIVE</code>：保持连接的时间，默认为 <code>5m</code>。</li><li><code>OLLAMA_LLM_LIBRARY</code>：LLM 库，默认为空。</li><li><code>OLLAMA_MAX_LOADED_MODELS</code>：最大加载模型数，默认为 <code>1</code>。</li><li><code>OLLAMA_MAX_QUEUE</code>：最大队列数，默认为空。</li><li><code>OLLAMA_MAX_VRAM</code>：最大虚拟内存，默认为空。</li><li><code>OLLAMA_MODELS</code>：模型目录，默认为空。</li><li><code>OLLAMA_NOHISTORY</code>：是否保存历史记录，默认为 <code>false</code>。</li><li><code>OLLAMA_NOPRUNE</code>：是否启用剪枝，默认为 <code>false</code>。</li><li><code>OLLAMA_NUM_PARALLEL</code>：并行数，默认为 <code>1</code>。</li><li><code>OLLAMA_ORIGINS</code>：允许的来源，默认为空。</li><li><code>OLLAMA_RUNNERS_DIR</code>：运行器目录，默认为空。</li><li><code>OLLAMA_SCHED_SPREAD</code>：调度分布，默认为空。</li><li><code>OLLAMA_TMPDIR</code>：临时文件目录，默认为空。Here is the optimized list in the desired format:</li><li><code>OLLAMA_DEBUG</code>：是否开启调试模式，默认为 <code>false</code>。</li><li><code>OLLAMA_FLASH_ATTENTION</code>：是否闪烁注意力，默认为 <code>true</code>。</li><li><code>OLLAMA_HOST</code>：Ollama 服务器的主机地址，默认为空。</li><li><code>OLLAMA_KEEP_ALIVE</code>：保持连接的时间，默认为 <code>5m</code>。</li><li><code>OLLAMA_LLM_LIBRARY</code>：LLM 库，默认为空。</li><li><code>OLLAMA_MAX_LOADED_MODELS</code>：最大加载模型数，默认为 <code>1</code>。</li><li><code>OLLAMA_MAX_QUEUE</code>：最大队列数，默认为空。</li><li><code>OLLAMA_MAX_VRAM</code>：最大虚拟内存，默认为空。</li><li><code>OLLAMA_MODELS</code>：模型目录，默认为空。</li><li><code>OLLAMA_NOHISTORY</code>：是否保存历史记录，默认为 <code>false</code>。</li><li><code>OLLAMA_NOPRUNE</code>：是否启用剪枝，默认为 <code>false</code>。</li><li><code>OLLAMA_NUM_PARALLEL</code>：并行数，默认为 <code>1</code>。</li><li><code>OLLAMA_ORIGINS</code>：允许的来源，默认为空。</li><li><code>OLLAMA_RUNNERS_DIR</code>：运行器目录，默认为空。</li><li><code>OLLAMA_SCHED_SPREAD</code>：调度分布，默认为空。</li><li><code>OLLAMA_TMPDIR</code>：临时文件目录，默认为空。</li></ul><h2 id=进阶用法hpc-集群上部署-ollama>进阶用法：HPC 集群上部署 Ollama</h2><p>对于大型模型或需要更高性能的情况，可以利用 HPC 集群的强大算力来运行 Ollama。结合 Slurm 进行任务管理，并使用端口映射将服务暴露到本地，即可方便地进行远程访问和使用：</p><ol><li>在登录节点配置 Ollama 环境： 安装 Ollama，并下载需要的模型。</li><li><strong>编写 slurm 脚本</strong>： 指定资源需求（CPU、内存、GPU 等），并使用 <code>ollama serve</code> 命令启动模型服务，并绑定到特定端口。</li></ol><pre><code class=language-bash>#!/bin/bash
#SBATCH --job-name=ollama
#SBATCH -N 1
#SBATCH -p GPU
#SBATCH --gres=gpu:1

module load CUDA
ollama serve
</code></pre><ol start=3><li><strong>提交 slurm 任务</strong>: 使用 <code>sbatch</code> 命令提交脚本，Slurm 会将任务分配到计算节点运行。</li><li><strong>本地端口映射</strong>： 使用 ssh -L 命令将计算节点的端口映射到本地，例如:</li></ol><pre><code class=language-bash>ssh -t -t  用户名@登录节点 ip -L 11434:localhost:11434 -i 登录节点私钥 ssh 计算节点 IP  -L 11434:127.0.0.1:11434
</code></pre><ol start=5><li><strong>本地访问</strong>： 在浏览器或应用程序中访问 http://localhost:11434 即可使用 Ollama 服务。</li></ol><div class="notice notice-tip"><div class=notice-title><svg t="1705945832245" class="icon notice-icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" p-id="19409" width="200" height="200"><path d="M512 74.666667C270.933333 74.666667 74.666667 270.933333 74.666667 512S270.933333 949.333333 512 949.333333 949.333333 753.066667 949.333333 512 753.066667 74.666667 512 74.666667zm238.933333 349.866666L748.8 426.666667 471.466667 704C460.8 714.666667 441.6 716.8 428.8 706.133333L426.666667 704 277.333333 554.666667c-12.8-12.8-12.8-32 0-44.8C288 499.2 307.2 497.066667 320 507.733333l2.133333 2.133334L448 635.733333l253.866667-253.866666c10.666667-10.666667 29.866667-12.8 42.666666-2.133334l2.133334 2.133334c12.8 12.8 12.8 32 4.266666 42.666666z" fill="#fff" p-id="19410"/></svg></div><p>注意：由于计算节点不联网，需要提前在登录节点使用 <code>ollama pull</code> 下载所需模型。此外，需要设置 <code>OLLAMA_ORIGINS</code> 为 <code>*</code>，设置 <code>OLLAMA_HOST</code> 为 <code>0.0.0.0</code>，以允许所有来源访问服务。</p></div><h2 id=进阶用法本地代码补全助手>进阶用法：本地代码补全助手</h2><p>Ollama 不仅可以用于聊天和文本创作，还可以结合代码生成模型和 IDE 插件，打造强大的代码补全助手。例如，使用 Codeqwen 7B 模型和 VS Code 插件 <a class=link href=https://continue.dev/ target=_blank rel=noopener>Continue
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>，可以实现高效便捷的代码补全功能。</p><p>首先介绍一下 Continue :<blockquote><p><p><a class=link href=https://continue.dev/ target=_blank rel=noopener>Continue
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>enables you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs. All this can run entirely on your own laptop or have Ollama deployed on a server to remotely power code completion and chat experiences based on your needs.</p><p>Continue 使您能够轻松地在 Visual Studio Code 和 JetBrains 中创建自己的代码助手，利用开源 LLM。这一切都可以完全在您的笔记本电脑上运行，或者在服务器上部署 Ollama，远程根据您的需求提供代码补全和聊天体验。</p></p><span class=cite><span>― </span><span>Continue</span><cite></cite></span></blockquote></p><p>在开始之前，你需要安装如下工具：</p><ul><li><a class=link href=https://docs.continue.dev/quickstart target=_blank rel=noopener>Continue
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>：<a class=link href="https://marketplace.visualstudio.com/items?itemName=Continue.continue" target=_blank rel=noopener>VS Code 版本
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>或 <a class=link href=https://plugins.jetbrains.com/plugin/22707-continue target=_blank rel=noopener>JetBrains 版本
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a></li><li><a class=link href=https://ollama.com/ target=_blank rel=noopener>Ollama
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a></li></ul><p>接下来，我们以 VS Code 为例，介绍如何使用 Ollama + Continue 实现代码补全功能：</p><h3 id=codestral-22b-模型>Codestral 22B 模型</h3><p>Codestral 既能完成代码自动补全，也支持聊天功能。但鉴于其拥有 220 亿参数且不具备生产许可，它对显存要求颇高，仅限于研究和测试使用，因此可能并不适合日常本地应用。</p><h4 id=下载并运行-codestral-模型>下载并运行 Codestral 模型</h4><pre><code class=language-bash>ollama run codestral
</code></pre><h4 id=配置-configjson>配置 config.json</h4><ul><li>在 VS Code 侧边栏点击 Continue 插件图标，然后在面板右下角点击 “齿轮” 图标，打开 <code>config.json</code> 文件。然后复制以下配置到 <code>config.json</code> 文件中：</li></ul><pre><code class=language-json>{
  &quot;models&quot;: [
    {
      &quot;title&quot;: &quot;Codestral&quot;,
      &quot;provider&quot;: &quot;ollama&quot;,
      &quot;model&quot;: &quot;codestral&quot;
    }
  ],
  &quot;tabAutocompleteModel&quot;: {
    &quot;title&quot;: &quot;Codestral&quot;,
    &quot;provider&quot;: &quot;ollama&quot;,
    &quot;model&quot;: &quot;codestral&quot;
  }
}
</code></pre><h3 id=deepseek-coder-67b-模型--llama-3-8b-模型>DeepSeek Coder 6.7B 模型 + Llama 3 8B 模型</h3><p>根据机器的显存大小，可以利用 Ollama 同时运行多个模型并处理多个并发请求的能力，使用 <code>DeepSeek Coder 6.7B</code> 进行自动补全，<code>Llama 3 8B</code> 进行聊天。如果你的机器无法同时运行两者，那么可以分别尝试，决定你更偏好本地自动补全还是本地聊天体验。</p><h4 id=下载并运行-deepseek-coder-模型>下载并运行 DeepSeek Coder 模型</h4><pre><code class=language-bash>ollama run deepseek-coder:6.7b-base
</code></pre><h4 id=下载并运行-llama-3-模型>下载并运行 Llama 3 模型</h4><pre><code class=language-bash>ollama run llama3:8b
</code></pre><h4 id=配置-configjson-1>配置 config.json</h4><pre><code class=language-json>{
  &quot;models&quot;: [
    {
      &quot;title&quot;: &quot;Llama 3 8B&quot;,
      &quot;provider&quot;: &quot;ollama&quot;,
      &quot;model&quot;: &quot;llama3:8b&quot;,
      &quot;apiBase&quot;: &quot;http://127.0.0.1:11434&quot;
    }
  ],
  &quot;tabAutocompleteModel&quot;: {
    &quot;title&quot;: &quot;DeepSeek Coder 6.7B&quot;,
    &quot;provider&quot;: &quot;ollama&quot;,
    &quot;model&quot;: &quot;deepseek-coder:6.7b-base&quot;,
    &quot;apiBase&quot;: &quot;http://127.0.0.1:11434&quot;
  }
}
</code></pre><h3 id=codeqwen-7b-模型--qwen2-7b-模型>Codeqwen 7B 模型 + Qwen2 7B 模型</h3><p>Codeqwen 7B 模型是一个专门用于代码补全的模型，而 Qwen2 7B 模型则是一个通用的聊天模型。这两个模型可以很好地结合在一起，实现代码补全和聊天功能。</p><h4 id=下载并运行-codeqwen-模型>下载并运行 Codeqwen 模型</h4><pre><code class=language-bash>ollama run codeqwen
</code></pre><h4 id=下载并运行-qwen2-模型>下载并运行 Qwen2 模型</h4><pre><code class=language-bash>ollama run qwen2:7b
</code></pre><h4 id=配置-configjson-2>配置 config.json</h4><pre><code class=language-json>{
  &quot;models&quot;: [
    {
      &quot;title&quot;: &quot;Codeqwen 7B&quot;,
      &quot;provider&quot;: &quot;ollama&quot;,
      &quot;model&quot;: &quot;codeqwen&quot;,
      &quot;apiBase&quot;: &quot;http://127.0.0.1:11434&quot;
    }
  ],
  &quot;tabAutocompleteModel&quot;: {
    &quot;title&quot;: &quot;Qwen2 7B&quot;,
    &quot;provider&quot;: &quot;ollama&quot;,
    &quot;model&quot;: &quot;qwen2:7b&quot;,
    &quot;apiBase&quot;: &quot;http://127.0.0.1:11434&quot;
  }
}
</code></pre><h3 id=利用-rag-向量检索优化聊天>利用 RAG 向量检索优化聊天</h3><p>Continue 内置了 <a class=link href=https://docs.continue.dev/customization/context-providers#codebase-retrieval target=_blank rel=noopener>@codebase
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>上下文提供器，能自动从代码库中检索到最相关的代码片段。假如你已经设置好了聊天模型（例如 Codestral、Llama 3），那么借助 Ollama 和 <a class=link href=https://blog.lancedb.com/lancedb-x-continue/ target=_blank rel=noopener>LanceDB
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>的向量化技术，可以实现更高效的代码检索和聊天体验。</p><p>这里，我们使用 <code>nomic-embed-text</code> 模型作为向量检索模型：</p><h4 id=下载并运行-nomic-embed-text-模型>下载并运行 Nomic Embed Text 模型</h4><pre><code class=language-bash>ollama run nomic-embed-text
</code></pre><h4 id=配置-configjson-3>配置 config.json</h4><ul><li>在文件中添加以下内容：</li></ul><pre><code class=language-json>{
    &quot;embeddingsProvider&quot;: {
        &quot;provider&quot;: &quot;ollama&quot;,
        &quot;model&quot;: &quot;nomic-embed-text&quot;,
        &quot;apiBase&quot;: &quot;http://127.0.0.1:11434&quot;
    }
}
</code></pre><h3 id=代码补全效果>代码补全效果</h3><ul><li><code>Ctrl + I</code>: 根据指令生成代码片段。</li></ul><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_1-2024-06-17.webp alt=codeqwen_1-2024-06-17 width=90% loading=lazy></figure><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_2-2024-06-17.webp alt=codeqwen_2-2024-06-17 width=90% loading=lazy></figure><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_3-2024-06-17.webp alt=codeqwen_3-2024-06-17 width=90% loading=lazy></figure><ul><li>光标悬停自动补全代码</li></ul><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_4-2024-06-17.webp alt=codeqwen_4-2024-06-17 width=90% loading=lazy></figure><h3 id=与-ollama-聊天>与 Ollama 聊天</h3><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_5-2024-06-17.webp alt=codeqwen_5-2024-06-17 width=90% loading=lazy></figure><h3 id=代码自动注释>代码自动注释</h3><ul><li>选中代码打开右键菜单</li></ul><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_6-2024-06-17.webp alt=codeqwen_6-2024-06-17 width=90% loading=lazy></figure><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_7-2024-06-17.webp alt=codeqwen_7-2024-06-17 width=90% loading=lazy></figure><h2 id=总结>总结</h2><p>Ollama 为我们打开了通往开源 LLM 世界的大门，让每个人都能轻松体验 LLM 的强大功能，并可以根据自身需求进行定制化应用。无论是进行研究、开发，还是日常使用，Ollama 都能为你提供探索 LLM 无限可能的平台。相信随着 Ollama 的不断发展，它将为我们带来更多惊喜，推动 LLM 技术在各个领域的应用和发展。</p></section><footer class=article-footer><section class=article-tags><a href=/tags/llm/>llm</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><script type=text/javascript src=/js/prism.js async></script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/llm-ecosystem/><div class=article-image><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_119269220_p0_master1200.webp loading=lazy data-key=llm-ecosystem data-hash=https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_119269220_p0_master1200.webp></div><div class=article-details><h2 class=article-title>LLM 生态介绍：从模型微调到应用落地</h2></div></a></article></div></div></aside><script src=https://cdn.bootcdn.net/ajax/libs/twikoo/1.6.20/twikoo.all.min.js></script><div id=tcomment></div><style>.twikoo{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}:root[data-scheme=dark]{--twikoo-body-text-color-main:rgba(255, 255, 255, 0.9);--twikoo-body-text-color:rgba(255, 255, 255, 0.7)}.twikoo .el-input-group__prepend,.twikoo .tk-action-icon,.twikoo .tk-time,.twikoo .tk-comments-count{color:var(--twikoo-body-text-color)}.twikoo .el-input__inner,.twikoo .el-textarea__inner,.twikoo .tk-preview-container,.twikoo .tk-content,.twikoo .tk-nick,.twikoo .tk-send{color:var(--twikoo-body-text-color-main)}.twikoo .el-button{color:var(--twikoo-body-text-color)!important}.OwO .OwO-body{background-color:var(--body-background)!important;color:var(--body-text-color)!important}</style><script>twikoo.init({envId:"https://comment.cuterwrite.top",el:"#tcomment",lang:"zh-CN"})</script><footer class=site-footer><section class=copyright>&copy;
2021 -
2024 cuterwrite</section><section class=running-time>本博客已稳定运行
<span id=runningdays class=running-days></span></section><section class=totalcount>发表了72篇文章 ·
总计317.46k字</section><section class=powerby>Welcome to cuterwrite's blog!<br>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.17.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计<br><span>基于 <a href=https://github.com/CaiJimmy/hugo-theme-stack/tree/v3.25.0 target=_blank rel=noopener><b style=color:#9e8f9f>v3.25.0</b></a> 分支版本修改</span><br></section></footer><script>let s1="2021-4-17";s1=new Date(s1.replace(/-/g,"/"));let s2=new Date,timeDifference=s2.getTime()-s1.getTime(),days=Math.floor(timeDifference/(1e3*60*60*24)),hours=Math.floor(timeDifference%(1e3*60*60*24)/(1e3*60*60)),minutes=Math.floor(timeDifference%(1e3*60*60)/(1e3*60)),result=days+"天"+hours+"小时"+minutes+"分钟";document.getElementById("runningdays").innerHTML=result</script><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.bootcdn.net/ajax/libs/photoswipe/4.1.3/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.bootcdn.net/ajax/libs/photoswipe/4.1.3/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.bootcdn.net/ajax/libs/photoswipe/4.1.3/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.bootcdn.net/ajax/libs/photoswipe/4.1.3/photoswipe.min.css crossorigin=anonymous></main></div><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.font.im/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script><meta name=apple-mobile-web-app-capable content="yes"><meta name=theme-color content="#ffffff"><script>"serviceWorker"in navigator&&window.addEventListener("load",()=>{navigator.serviceWorker.register("/sw.js").then(e=>{console.log("Service worker registered with scope: ",e.scope)},e=>{console.log("Service worker registration failed: ",e)})})</script></body></html>