<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="构建一个完整的 LLM 应用，仅仅拥有强大的模型是不够的。一个繁荣的 LLM 生态系统，需要涵盖从模型训练、优化到部署和应用的各个环节。本文将带您一览 LLM 生态的各个方面，探索如何将 LLM 真正应用到实际场景中。"><title>LLM 生态介绍：从模型微调到应用落地</title>
<link rel=canonical href=https://cuterwrite.top/p/llm-ecosystem/><link rel=stylesheet href=/scss/style.min.2b724d31a9334eb879a7204b807f915cfebe5daa80340d704caa98b8da1011cf.css><meta property='og:title' content="LLM 生态介绍：从模型微调到应用落地"><meta property='og:description' content="构建一个完整的 LLM 应用，仅仅拥有强大的模型是不够的。一个繁荣的 LLM 生态系统，需要涵盖从模型训练、优化到部署和应用的各个环节。本文将带您一览 LLM 生态的各个方面，探索如何将 LLM 真正应用到实际场景中。"><meta property='og:url' content='https://cuterwrite.top/p/llm-ecosystem/'><meta property='og:site_name' content="Cuterwrite's Blog"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='llm'><meta property='article:published_time' content='2024-07-05T22:46:00+00:00'><meta property='article:modified_time' content='2024-07-05T22:46:00+00:00'><meta property='og:image' content='https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_119269220_p0_master1200.webp'><meta name=twitter:title content="LLM 生态介绍：从模型微调到应用落地"><meta name=twitter:description content="构建一个完整的 LLM 应用，仅仅拥有强大的模型是不够的。一个繁荣的 LLM 生态系统，需要涵盖从模型训练、优化到部署和应用的各个环节。本文将带您一览 LLM 生态的各个方面，探索如何将 LLM 真正应用到实际场景中。"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_119269220_p0_master1200.webp'><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><script async src=https://analytics.cuterwrite.top/uma data-website-id=b13594a2-4d15-4a4e-a020-5e3cc1d88c12 data-domains=cuterwrite.top></script><link rel=manifest href=/manifest.json></head><body class="article-page
line-numbers"><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hue4d14694a57c01a222a16c47db12c89c_369633_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>😉</span></figure><div class=site-meta><h1 class=site-name><a href=/>Cuterwrite's Blog</a></h1><h2 class=site-description>Cuterwrite 的技术博客, 专注于高性能计算、操作系统、全栈开发、人工智能等领域的深度探讨和经验分享。</h2></div></header><ol class=menu-social><li><a href=https://analytics.cuterwrite.top/share/Ji0gm9OaLDk8gco7 target=_blank title=Analytics rel=me><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 5H7A2 2 0 005 7v12a2 2 0 002 2h10a2 2 0 002-2V7a2 2 0 00-2-2h-2"/><rect x="9" y="3" width="6" height="4" rx="2"/><path d="M9 17v-5"/><path d="M12 17v-1"/><path d="M15 17v-3"/></svg></a></li><li><a href=https://stats.uptimerobot.com/6NVhRHkSAQ target=_blank title=Upptime rel=me><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-chart-line"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 19h16"/><path d="M4 15l4-6 4 2 4-5 4 4"/></svg></a></li><li><a href=/index.xml target=_blank title=RSS rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-rss" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="5" cy="19" r="1"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg></a></li><li><a href=https://github.com/PKUcoldkeyboard target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.zhihu.com/people/kong-tiao-cheng-tai-lang-30-57 target=_blank title=zhihu rel=me><svg t="1705591931290" class="icon" viewBox="0 0 1280 1024" xmlns="http://www.w3.org/2000/svg" p-id="21048" width="32" height="32"><path d="M341.08 296.26v435.08l46.86.02 15.42 52.74 84.02-52.74h99.06V296.26H341.08zm195.5 387.86H480.7l-55.8 35.02-10.16-34.94-23.8-.08V343.5h145.64v340.62zM299.66 495.34H195c3.48-54.2 4.4-103.18 4.4-146.92h102.32s3.94-45.12-17.16-44.62h-177c6.98-26.24 15.74-53.32 26.24-81.34.0.0-48.14.0-64.54 43.14-6.78 17.8-26.42 86.28-61.4 156.24 11.78-1.28 50.74-2.36 73.68-44.42 4.22-11.78 5.02-13.32 10.28-29.06h57.74c0 21-2.4 133.76-3.36 146.88H41.66c-23.48.0-31.12 47.24-31.12 47.24H141.7C132.9 642.2 85.66 726.24.0 792.68c40.98 11.7 81.82-1.86 102-19.8.0.0 45.96-41.8 71.18-138.5L281.1 764.26s15.82-53.78-2.48-79.98c-15.16-17.84-56.12-66.12-73.58-83.62L175.8 623.9c8.72-27.96 13.98-55.1 15.74-81.34h123.3s-.18-47.24-15.18-47.24v.02zm824.04-3.2c41.66-51.28 89.96-117.14 89.96-117.14s-37.3-29.6-54.76-8.12c-12 16.3-73.66 96.4-73.66 96.4l38.46 28.86zM823.52 373.96c-18.02-16.5-51.82 4.26-51.82 4.26s79.04 110.08 82.24 114.9l38.92-27.46s-51.34-75.22-69.32-91.72h-.02zM1280 516.7c-39.56.0-261.82 1.86-262.12 1.86v-202c9.62.0 24.84-.8 45.7-2.4 81.76-4.82 140.26-8 175.54-9.62.0.0 24.44-54.38-1.18-66.88-6.14-2.36-46.34 9.16-46.34 9.16s-330.44 32.98-464.72 36.1c3.2 17.64 15.24 34.16 31.56 39.1 26.62 6.96 45.38 3.4 98.3 1.78 49.66-3.2 87.36-4.86 113.02-4.86v199.62H702.82s5.64 44.62 51.02 45.7h215.88V706.1c0 27.94-22.38 43.98-48.96 42.24-28.16.22-52.16-2.3-83.38-3.62 3.98 7.94 12.66 28.78 38.62 43.68 19.76 9.62 32.34 13.14 52.04 13.14 59.12.0 91.34-34.56 89.78-90.62V564.28h244.72c19.36.0 17.4-47.56 17.4-47.56l.06-.02z" fill="#707070" p-id="21049"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页 | Home</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>关于 | About</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>归档 | Archives</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>搜索 | Search</span></a></li><li><a href=https://cuterwrite.top/image-hosting target=_blank><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-album"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 4m0 2a2 2 0 012-2h12a2 2 0 012 2v12a2 2 0 01-2 2H6a2 2 0 01-2-2z"/><path d="M12 4v7l2-2 2 2V4"/></svg>
<span>图册 | Gallery</span></a></li><li><a href=https://draw.cuterwrite.top target=_blank><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-artboard"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M8 8m0 1a1 1 0 011-1h6a1 1 0 011 1v6a1 1 0 01-1 1H9a1 1 0 01-1-1z"/><path d="M3 8h1"/><path d="M3 16h1"/><path d="M8 3v1"/><path d="M16 3v1"/><path d="M20 8h1"/><path d="M20 16h1"/><path d="M8 20v1"/><path d="M16 20v1"/></svg>
<span>画板 | Canvas</span></a></li><li><a href=https://it-tools.tech target=_blank><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-tools"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 21h4L20 8a1.5 1.5.0 00-4-4L3 17v4"/><path d="M14.5 5.5l4 4"/><path d="M12 8 7 3 3 7l5 5"/><path d="M7 8 5.5 9.5"/><path d="M16 12l5 5-4 4-5-5"/><path d="M16 17l-1.5 1.5"/></svg>
<span>工具 | Tools</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ul><li><a href=#模型微调>模型微调</a><ul><li><a href=#axolotl>Axolotl</a></li><li><a href=#llama-factory>Llama-Factory</a></li><li><a href=#firefly>Firefly</a></li><li><a href=#xtuner>XTuner</a></li></ul></li><li><a href=#模型量化>模型量化</a><ul><li><a href=#autogptq>AutoGPTQ</a></li><li><a href=#autoawq>AutoAWQ</a></li><li><a href=#neural-compressor>Neural Compressor</a></li></ul></li><li><a href=#模型部署>模型部署</a><ul><li><a href=#vllm>vLLM</a></li><li><a href=#sgl>SGL</a></li><li><a href=#skypilot>SkyPilot</a></li><li><a href=#tensorrt-llm>TensorRT-LLM</a></li><li><a href=#openvino>OpenVino</a></li><li><a href=#tgi>TGI</a></li></ul></li><li><a href=#本地运行>本地运行</a><ul><li><a href=#mlx>MLX</a></li><li><a href=#llamacpp>Llama.cpp</a></li><li><a href=#ollama>Ollama</a></li></ul></li><li><a href=#agent-及-rag-框架>Agent 及 RAG 框架</a><ul><li><a href=#llamaindex>LlamaIndex</a></li><li><a href=#crewai>CrewAI</a></li><li><a href=#opendevin>OpenDevin</a></li></ul></li><li><a href=#模型评测>模型评测</a><ul><li><a href=#lmsys>LMSys</a></li><li><a href=#opencompass>OpenCompass</a></li><li><a href=#open-llm-leaderboard>Open LLM Leaderboard</a></li></ul></li><li><a href=#总结>总结</a></li></ul></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/llm-ecosystem/><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_119269220_p0_master1200.webp loading=lazy alt="Featured image of post LLM 生态介绍：从模型微调到应用落地"></a></div><div class=article-details><header class=article-category><a href=/categories/ai/ style=background-color:#eaaa60;color:#fff>人工智能与数据科学</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/llm-ecosystem/>LLM 生态介绍：从模型微调到应用落地</a></h2><h3 class=article-subtitle>构建一个完整的 LLM 应用，仅仅拥有强大的模型是不够的。一个繁荣的 LLM 生态系统，需要涵盖从模型训练、优化到部署和应用的各个环节。本文将带您一览 LLM 生态的各个方面，探索如何将 LLM 真正应用到实际场景中。</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>2024-07-05</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 16 分钟</time></div><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-keyboard"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M2 6m0 2a2 2 0 012-2h16a2 2 0 012 2v8a2 2 0 01-2 2H4a2 2 0 01-2-2z"/><path d="M6 10v.01"/><path d="M10 10v.01"/><path d="M14 10v.01"/><path d="M18 10v.01"/><path d="M6 14v.01"/><path d="M18 14v.01"/><path d="M10 14l4 .01"/></svg>
<time class=article-time--wordcount>字数统计: 7556 字</time></div></footer></div></header><section class=article-content><h1 id=llm-生态介绍从模型微调到应用落地>LLM 生态介绍：从模型微调到应用落地</h1><h2 id=模型微调>模型微调</h2><p>预训练的 LLM 通常具备广泛的知识，但要使其在特定任务上表现出色，微调是必不可少的。以下是一些常用的 LLM 微调工具：</p><h3 id=axolotl>Axolotl</h3><a href=https://github.com/OpenAccess-AI-Collective/axolotl target=_blank class="card-github fetch-waiting no-styling" repo=OpenAccess-AI-Collective/axolotl id=repo-ZEeX3AW715hbJvbE-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-ZEeX3AW715hbJvbE-avatar class=gc-avatar></div><div class=gc-user>OpenAccess-AI-Collective</div></div><div class=gc-divider>/</div><div class=gc-repo>axolotl</div></div><div class=github-logo></div></div><div id=repo-ZEeX3AW715hbJvbE-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-ZEeX3AW715hbJvbE-stars class=gc-stars>0</div><div id=repo-ZEeX3AW715hbJvbE-forks class=gc-forks>0</div><div id=repo-ZEeX3AW715hbJvbE-license class=gc-license>unkown</div><div id=repo-ZEeX3AW715hbJvbE-language class=gc-language>Waiting...</div></div></a><script id=repo-ZEeX3AW715hbJvbE-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/OpenAccess-AI-Collective/axolotl",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-ZEeX3AW715hbJvbE-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-ZEeX3AW715hbJvbE-language").innerText=e.language,document.getElementById("repo-ZEeX3AW715hbJvbE-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-ZEeX3AW715hbJvbE-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-ZEeX3AW715hbJvbE-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-ZEeX3AW715hbJvbE-license").innerText=e.license.spdx_id:document.getElementById("repo-ZEeX3AW715hbJvbE-license").classList.add="no-license",document.getElementById("repo-ZEeX3AW715hbJvbE-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for OpenAccess-AI-Collective/axolotl.")}).catch(e=>{const t=document.getElementById("repo-ZEeX3AW715hbJvbE-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for OpenAccess-AI-Collective/axolotl.")})</script><p>Axolotl 是一款旨在简化各种人工智能模型微调的工具，支持多种配置和架构。</p><p><strong>主要特点：</strong></p><ul><li>训练各种 Huggingface 模型，如 llama、pythia、falcon、mpt</li><li>支持 fullfinetune、lora、qlora、relora 和 gptq</li><li>使用简单的 yaml 文件或 CLI 重写功能自定义配置</li><li>加载不同的数据集格式，使用自定义格式，或自带标记化数据集</li><li>与 xformer、闪存关注、绳索缩放和多重包装集成</li><li>可通过 FSDP 或 Deepspeed 与单 GPU 或多 GPU 协同工作</li><li>使用 Docker 在本地或云端轻松运行</li><li>将结果和可选的检查点记录到 wandb 或 mlflow 中</li></ul><p><strong>快速入门：</strong>
要求： Python >=3.10 和 Pytorch >=2.1.1</p><pre><code class=language-bash>git clone https://github.com/OpenAccess-AI-Collective/axolotl
cd axolotl

pip3 install packaging ninja
pip3 install -e '.[flash-attn,deepspeed]'
</code></pre><p><strong>使用方法：</strong></p><pre><code class=language-bash># preprocess datasets - optional but recommended
CUDA_VISIBLE_DEVICES=&quot;&quot; python -m axolotl.cli.preprocess examples/openllama-3b/lora.yml

# finetune lora
accelerate launch -m axolotl.cli.train examples/openllama-3b/lora.yml

# inference
accelerate launch -m axolotl.cli.inference examples/openllama-3b/lora.yml \
    --lora_model_dir=&quot;./outputs/lora-out&quot;

# gradio
accelerate launch -m axolotl.cli.inference examples/openllama-3b/lora.yml \
    --lora_model_dir=&quot;./outputs/lora-out&quot; --gradio

# remote yaml files - the yaml config can be hosted on a public URL
# Note: the yaml config must directly link to the **raw** yaml
accelerate launch -m axolotl.cli.train https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/examples/openllama-3b/lora.yml
</code></pre><p>其它详细信息，请访问 <a class=link href=https://github.com/OpenAccess-AI-Collective/axolotl target=_blank rel=noopener>Axolotl
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>项目主页。</p><h3 id=llama-factory>Llama-Factory</h3><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/2024-07-07_llama-factory-logo.webp alt="Llama Factory Logo" width=80% loading=lazy></figure><a href=https://github.com/hiyouga/LLaMA-Factory target=_blank class="card-github fetch-waiting no-styling" repo=hiyouga/LLaMA-Factory id=repo-lVrAMBJr94VsckR9-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-lVrAMBJr94VsckR9-avatar class=gc-avatar></div><div class=gc-user>hiyouga</div></div><div class=gc-divider>/</div><div class=gc-repo>LLaMA-Factory</div></div><div class=github-logo></div></div><div id=repo-lVrAMBJr94VsckR9-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-lVrAMBJr94VsckR9-stars class=gc-stars>0</div><div id=repo-lVrAMBJr94VsckR9-forks class=gc-forks>0</div><div id=repo-lVrAMBJr94VsckR9-license class=gc-license>unkown</div><div id=repo-lVrAMBJr94VsckR9-language class=gc-language>Waiting...</div></div></a><script id=repo-lVrAMBJr94VsckR9-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/hiyouga/LLaMA-Factory",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-lVrAMBJr94VsckR9-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-lVrAMBJr94VsckR9-language").innerText=e.language,document.getElementById("repo-lVrAMBJr94VsckR9-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-lVrAMBJr94VsckR9-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-lVrAMBJr94VsckR9-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-lVrAMBJr94VsckR9-license").innerText=e.license.spdx_id:document.getElementById("repo-lVrAMBJr94VsckR9-license").classList.add="no-license",document.getElementById("repo-lVrAMBJr94VsckR9-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for hiyouga/LLaMA-Factory.")}).catch(e=>{const t=document.getElementById("repo-lVrAMBJr94VsckR9-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for hiyouga/LLaMA-Factory.")})</script><p>Llama-Factory 是 Meta 推出的，专注于 Llama 模型微调的框架。它构建于 PyTorch 生态之上，并提供高效的训练和评估工具。</p><p><strong>主要特点：</strong></p><ul><li><strong>多种模型</strong>：LLaMA、LLaVA、Mistral、Mixtral-MoE、Qwen、Yi、Gemma、Baichuan、ChatGLM、Phi 等等。</li><li><strong>集成方法</strong>：（增量）预训练、（多模态）指令监督微调、奖励模型训练、PPO 训练、DPO 训练、KTO 训练、ORPO 训练等等。</li><li><strong>多种精度</strong>：16 比特全参数微调、冻结微调、LoRA 微调和基于 AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ 的 2/3/4/5/6/8 比特 QLoRA 微调。</li><li><strong>先进算法</strong>：GaLore、BAdam、DoRA、LongLoRA、LLaMA Pro、Mixture-of-Depths、LoRA+、LoftQ、PiSSA 和 Agent 微调。</li><li><strong>实用技巧</strong>：FlashAttention-2、Unsloth、RoPE scaling、NEFTune 和 rsLoRA。</li><li><strong>实验监控</strong>：LlamaBoard、TensorBoard、Wandb、MLflow 等等。</li><li><strong>极速推理</strong>：基于 vLLM 的 OpenAI 风格 API、浏览器界面和命令行接口。</li></ul><link href=https://web.sdk.qcloud.com/player/tcplayer/release/v4.2.1/tcplayer.min.css rel=stylesheet><script src=https://web.sdk.qcloud.com/player/tcplayer/release/v4.2.1/libs/hls.min.0.13.2m.js></script><script src=https://web.sdk.qcloud.com/player/tcplayer/release/v4.2.1/tcplayer.v4.2.1.min.js></script><style>.tcplayer{position:absolute;width:100%;height:100%;left:0;top:0;border:0}</style><div class=video-wrapper><video id=player-container-id preload=auto width=100% height=100% playsinline webkit-playsinline></video></div><script>var tcplayer=TCPlayer("player-container-id",{reportable:!1,poster:""});tcplayer.src("https://cuterwrite-1302252842.file.myqcloud.com/img/2024-07-07_309488092-ec36a9dd-37f4-4f72-81bd-d76c6d0a6594_output.mp4")</script><p><strong>性能指标</strong></p><p>与 ChatGLM 官方的 <a class=link href=https://github.com/THUDM/ChatGLM2-6B/tree/main/ptuning target=_blank rel=noopener>P-Tuning
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>微调相比，LLaMA Factory 的 LoRA 微调提供了 <strong>3.7 倍</strong>的加速比，同时在广告文案生成任务上取得了更高的 Rouge 分数。结合 4 比特量化技术，LLaMA Factory 的 QLoRA 微调进一步降低了 GPU 显存消耗。</p><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/2024-07-07_benchmark.webp alt=Performance width=auto loading=lazy></figure><details><summary>变量定义</summary><ul><li><strong>Training Speed</strong>: 训练阶段每秒处理的样本数量。（批处理大小=4，截断长度=1024）</li><li><strong>Rouge Score</strong>: <a class=link href=https://aclanthology.org/D19-1321.pdf target=_blank rel=noopener>广告文案生成
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>任务验证集上的 Rouge-2 分数。（批处理大小=4，截断长度=1024）</li><li><strong>GPU Memory</strong>: 4 比特量化训练的 GPU 显存峰值。（批处理大小=1，截断长度=1024）</li><li>我们在 ChatGLM 的 P-Tuning 中采用 <code>pre_seq_len=128</code>，在 LLaMA Factory 的 LoRA 微调中采用 <code>lora_rank=32</code>。</li></ul></details><p><strong>快速入门</strong></p><pre><code class=language-bash>git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e &quot;.[torch,metrics]&quot;
</code></pre><p>可选的额外依赖项：torch、torch-npu、metrics、deepspeed、bitsandbytes、hqq、eetq、gptq、awq、aqlm、vllm、galore、badam、qwen、modelscope、quality</p><div class="notice notice-tip"><div class=notice-title><svg t="1705945832245" class="icon notice-icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" p-id="19409" width="200" height="200"><path d="M512 74.666667C270.933333 74.666667 74.666667 270.933333 74.666667 512S270.933333 949.333333 512 949.333333 949.333333 753.066667 949.333333 512 753.066667 74.666667 512 74.666667zm238.933333 349.866666L748.8 426.666667 471.466667 704C460.8 714.666667 441.6 716.8 428.8 706.133333L426.666667 704 277.333333 554.666667c-12.8-12.8-12.8-32 0-44.8C288 499.2 307.2 497.066667 320 507.733333l2.133333 2.133334L448 635.733333l253.866667-253.866666c10.666667-10.666667 29.866667-12.8 42.666666-2.133334l2.133334 2.133334c12.8 12.8 12.8 32 4.266666 42.666666z" fill="#fff" p-id="19410"/></svg></div><p>遇到包冲突时，可使用 pip install &ndash;no-deps -e . 解决。</p></div><details><summary>Windows 用户指南</summary><p>如果要在 Windows 平台上开启量化 LoRA（QLoRA），需要安装预编译的 <code>bitsandbytes</code> 库, 支持 CUDA 11.1 到 12.2, 请根据您的 CUDA 版本情况选择适合的<a class=link href=https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels target=_blank rel=noopener>发布版本
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>。</p><pre><code class=language-bash>pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl
</code></pre><p>如果要在 Windows 平台上开启 FlashAttention-2，需要安装预编译的 <code>flash-attn</code> 库，支持 CUDA 12.1 到 12.2，请根据需求到 <a class=link href=https://github.com/bdashore3/flash-attention/releases target=_blank rel=noopener>flash-attention
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>下载对应版本安装。</p></details><details><summary>昇腾 NPU 用户指南</summary><p>在昇腾 NPU 设备上安装 LLaMA Factory 时，需要指定额外依赖项，使用 <code>pip install -e ".[torch-npu,metrics]"</code> 命令安装。此外，还需要安装 <strong><a class=link href="https://www.hiascend.com/developer/download/community/result?module=cann" target=_blank rel=noopener>Ascend CANN Toolkit 与 Kernels
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a></strong>，安装方法请参考<a class=link href=https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/80RC2alpha002/quickstart/quickstart/quickstart_18_0004.html target=_blank rel=noopener>安装教程
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>或使用以下命令：</p><pre><code class=language-bash># 请替换 URL 为 CANN 版本和设备型号对应的 URL
# 安装 CANN Toolkit
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-toolkit_8.0.RC1.alpha001_linux-&quot;$(uname -i)&quot;.run
bash Ascend-cann-toolkit_8.0.RC1.alpha001_linux-&quot;$(uname -i)&quot;.run --install

# 安装 CANN Kernels
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run
bash Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run --install

# 设置环境变量
source /usr/local/Ascend/ascend-toolkit/set_env.sh
</code></pre><div class=table-wrapper><table><thead><tr><th>依赖项</th><th>至少</th><th>推荐</th></tr></thead><tbody><tr><td>CANN</td><td>8.0.RC1</td><td>8.0.RC1</td></tr><tr><td>torch</td><td>2.1.0</td><td>2.1.0</td></tr><tr><td>torch-npu</td><td>2.1.0</td><td>2.1.0.post3</td></tr><tr><td>deepspeed</td><td>0.13.2</td><td>0.13.2</td></tr></tbody></table></div><p>请使用 <code>ASCEND_RT_VISIBLE_DEVICES</code> 而非 <code>CUDA_VISIBLE_DEVICES</code> 来指定运算设备。</p><p>如果遇到无法正常推理的情况，请尝试设置 <code>do_sample: false</code>。</p><p>下载预构建 Docker 镜像：<a class=link href=http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html target=_blank rel=noopener>32GB
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>| <a class=link href=http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html target=_blank rel=noopener>64GB
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a></p></details><p>下面三行命令分别对 Llama3-8B-Instruct 模型进行 LoRA 微调、推理和合并。</p><pre><code class=language-bash>llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml
</code></pre><p>更多详细信息，请访问 <a class=link href=https://github.com/hiyouga/LLaMA-Factory target=_blank rel=noopener>Llama-Factory
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>项目主页。</p><h3 id=firefly>Firefly</h3><a href=https://github.com/yangjianxin1/Firefly target=_blank class="card-github fetch-waiting no-styling" repo=yangjianxin1/Firefly id=repo-mAcwuksGYxRBxbYr-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-mAcwuksGYxRBxbYr-avatar class=gc-avatar></div><div class=gc-user>yangjianxin1</div></div><div class=gc-divider>/</div><div class=gc-repo>Firefly</div></div><div class=github-logo></div></div><div id=repo-mAcwuksGYxRBxbYr-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-mAcwuksGYxRBxbYr-stars class=gc-stars>0</div><div id=repo-mAcwuksGYxRBxbYr-forks class=gc-forks>0</div><div id=repo-mAcwuksGYxRBxbYr-license class=gc-license>unkown</div><div id=repo-mAcwuksGYxRBxbYr-language class=gc-language>Waiting...</div></div></a><script id=repo-mAcwuksGYxRBxbYr-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/yangjianxin1/Firefly",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-mAcwuksGYxRBxbYr-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-mAcwuksGYxRBxbYr-language").innerText=e.language,document.getElementById("repo-mAcwuksGYxRBxbYr-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-mAcwuksGYxRBxbYr-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-mAcwuksGYxRBxbYr-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-mAcwuksGYxRBxbYr-license").innerText=e.license.spdx_id:document.getElementById("repo-mAcwuksGYxRBxbYr-license").classList.add="no-license",document.getElementById("repo-mAcwuksGYxRBxbYr-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for yangjianxin1/Firefly.")}).catch(e=>{const t=document.getElementById("repo-mAcwuksGYxRBxbYr-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for yangjianxin1/Firefly.")})</script><p><strong>Firefly</strong> 是一个开源的大模型训练项目，支持对主流的大模型进行预训练、指令微调和 DPO，包括但不限于 Qwen2、Yi-1.5、Llama3、Gemma、Qwen1.5、MiniCPM、Llama、InternLM、Baichuan、ChatGLM、Yi、Deepseek、Qwen、Orion、Ziya、Xverse、Mistral、Mixtral-8x7B、Zephyr、Vicuna、Bloom 等。
本项目支持<strong>全量参数训练、LoRA、QLoRA 高效训练</strong>，支持<strong>预训练、SFT、DPO</strong>。 如果你的训练资源有限，我们极力推荐使用 QLoRA 进行指令微调，因为我们在 Open LLM Leaderboard 上验证了该方法的有效性，并且取得了非常不错的成绩。</p><p><strong>主要特点：</strong></p><ul><li>📗 支持预训练、指令微调、DPO，支持全量参数训练、LoRA、QLoRA 高效训练。通过配置文件的方式训练不同的模型，小白亦可快速上手训练模型。</li><li>📗 支持使用<a class=link href=https://github.com/yangjianxin1/unsloth target=_blank rel=noopener>Unsloth
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>加速训练，并且节省显存。</li><li>📗 支持绝大部分主流的开源大模型，如 Llama3、Gemma、MiniCPM、Llama、InternLM、Baichuan、ChatGLM、Yi、Deepseek、Qwen、Orion、Ziya、Xverse、Mistral、Mixtral-8x7B、Zephyr、Vicuna、Bloom，训练时与各个官方的 chat 模型的 template 对齐。</li><li>📗 整理并开源指令微调数据集：firefly-train-1.1M 、moss-003-sft-data、ultrachat、 WizardLM_evol_instruct_V2_143k、school_math_0.25M。</li><li>📗 开源<a class=link href=https://huggingface.co/YeungNLP target=_blank rel=noopener>Firefly 系列指令微调模型权重
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>。</li><li>📗 在 Open LLM Leaderboard 上验证了 QLoRA 训练流程的有效性。</li></ul><p>该项目的 README 中包含了详细的使用说明，包括如何安装、如何训练、如何微调、如何评估等。请访问 <a class=link href=https://github.com/yangjianxin1/Firefly target=_blank rel=noopener>Firefly
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>项目主页。</p><h3 id=xtuner>XTuner</h3><a href=https://github.com/InternLM/xtuner target=_blank class="card-github fetch-waiting no-styling" repo=InternLM/xtuner id=repo-VLGtFSfHsZmPGfTw-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-VLGtFSfHsZmPGfTw-avatar class=gc-avatar></div><div class=gc-user>InternLM</div></div><div class=gc-divider>/</div><div class=gc-repo>xtuner</div></div><div class=github-logo></div></div><div id=repo-VLGtFSfHsZmPGfTw-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-VLGtFSfHsZmPGfTw-stars class=gc-stars>0</div><div id=repo-VLGtFSfHsZmPGfTw-forks class=gc-forks>0</div><div id=repo-VLGtFSfHsZmPGfTw-license class=gc-license>unkown</div><div id=repo-VLGtFSfHsZmPGfTw-language class=gc-language>Waiting...</div></div></a><script id=repo-VLGtFSfHsZmPGfTw-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/InternLM/xtuner",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-VLGtFSfHsZmPGfTw-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-VLGtFSfHsZmPGfTw-language").innerText=e.language,document.getElementById("repo-VLGtFSfHsZmPGfTw-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-VLGtFSfHsZmPGfTw-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-VLGtFSfHsZmPGfTw-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-VLGtFSfHsZmPGfTw-license").innerText=e.license.spdx_id:document.getElementById("repo-VLGtFSfHsZmPGfTw-license").classList.add="no-license",document.getElementById("repo-VLGtFSfHsZmPGfTw-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for InternLM/xtuner.")}).catch(e=>{const t=document.getElementById("repo-VLGtFSfHsZmPGfTw-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for InternLM/xtuner.")})</script><p>XTuner 是一个高效、灵活、全能的轻量化大模型微调工具库。</p><p><strong>主要特点：</strong></p><ul><li><strong>高效</strong><ul><li>支持大语言模型 LLM、多模态图文模型 VLM 的预训练及轻量级微调。XTuner 支持在 8GB 显存下微调 7B 模型，同时也支持多节点跨设备微调更大尺度模型（70B+）。</li><li>自动分发高性能算子（如 FlashAttention、Triton kernels 等）以加速训练吞吐。</li><li>兼容 <a class=link href=https://github.com/microsoft/DeepSpeed target=_blank rel=noopener>DeepSpeed
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>🚀，轻松应用各种 ZeRO 训练优化策略。</li></ul></li><li><strong>灵活</strong><ul><li>支持多种大语言模型，包括但不限于 <a class=link href=https://huggingface.co/internlm target=_blank rel=noopener>InternLM
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>、<a class=link href=https://huggingface.co/mistralai target=_blank rel=noopener>Mixtral-8x7B
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>、<a class=link href=https://huggingface.co/meta-llama target=_blank rel=noopener>Llama 2
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>、<a class=link href=https://huggingface.co/THUDM target=_blank rel=noopener>ChatGLM
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>、<a class=link href=https://huggingface.co/Qwen target=_blank rel=noopener>Qwen
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>、<a class=link href=https://huggingface.co/baichuan-inc target=_blank rel=noopener>Baichuan
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>。</li><li>支持多模态图文模型 LLaVA 的预训练与微调。利用 XTuner 训得模型 <a class=link href=https://huggingface.co/xtuner/llava-internlm2-20b target=_blank rel=noopener>LLaVA-InternLM2-20B
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>表现优异。</li><li>精心设计的数据管道，兼容任意数据格式，开源数据或自定义数据皆可快速上手。</li><li>支持 <a class=link href=http://arxiv.org/abs/2305.14314 target=_blank rel=noopener>QLoRA
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>、<a class=link href=http://arxiv.org/abs/2106.09685 target=_blank rel=noopener>LoRA
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>、全量参数微调等多种微调算法，支撑用户根据具体需求作出最优选择。</li></ul></li><li><strong>全能</strong><ul><li>支持增量预训练、指令微调与 Agent 微调。</li><li>预定义众多开源对话模版，支持与开源或训练所得模型进行对话。</li><li>训练所得模型可无缝接入部署工具库 <a class=link href=https://github.com/InternLM/lmdeploy target=_blank rel=noopener>LMDeploy
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>、大规模评测工具库 <a class=link href=https://github.com/open-compass/opencompass target=_blank rel=noopener>OpenCompass
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>及 <a class=link href=https://github.com/open-compass/VLMEvalKit target=_blank rel=noopener>VLMEvalKit
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>。</li></ul></li></ul><p><strong>快速上手：</strong><details><summary>安装</summary><ul><li><p>推荐使用 conda 先构建一个 Python-3.10 的虚拟环境</p><pre><code class=language-bash>conda create --name xtuner-env python=3.10 -y
conda activate xtuner-env
</code></pre></li><li><p>通过 pip 安装 XTuner：</p><pre><code class=language-shell>pip install -U xtuner
</code></pre><p>亦可集成 DeepSpeed 安装：</p><pre><code class=language-shell>pip install -U 'xtuner[deepspeed]'
</code></pre></li><li><p>从源码安装 XTuner：</p><pre><code class=language-shell>git clone https://github.com/InternLM/xtuner.git
cd xtuner
pip install -e '.[all]'

</code></pre></li></ul></details></p><details><summary>微调</summary><p>XTuner 支持微调大语言模型。数据集预处理指南请查阅<a class=link href=./docs/zh_cn/user_guides/dataset_prepare.md>文档
</a>。</p><ul><li><p><strong>步骤 0</strong>，准备配置文件。XTuner 提供多个开箱即用的配置文件，用户可以通过下列命令查看：</p><pre><code class=language-shell>xtuner list-cfg
</code></pre><p>或者，如果所提供的配置文件不能满足使用需求，请导出所提供的配置文件并进行相应更改：</p><pre><code class=language-shell>xtuner copy-cfg ${CONFIG_NAME} ${SAVE_PATH}
vi ${SAVE_PATH}/${CONFIG_NAME}_copy.py
</code></pre></li><li><p><strong>步骤 1</strong>，开始微调。</p><pre><code class=language-shell>xtuner train ${CONFIG_NAME_OR_PATH}
</code></pre><p>例如，我们可以利用 QLoRA 算法在 oasst1 数据集上微调 InternLM2.5-Chat-7B：</p><pre><code class=language-shell># 单卡
xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2
# 多卡
(DIST) NPROC_PER_NODE=${GPU_NUM} xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2
(SLURM) srun ${SRUN_ARGS} xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --launcher slurm --deepspeed deepspeed_zero2
</code></pre><ul><li><p><code>--deepspeed</code> 表示使用 <a class=link href=https://github.com/microsoft/DeepSpeed target=_blank rel=noopener>DeepSpeed
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>🚀 来优化训练过程。XTuner 内置了多种策略，包括 ZeRO-1、ZeRO-2、ZeRO-3 等。如果用户期望关闭此功能，请直接移除此参数。</p></li><li><p>更多示例，请查阅<a class=link href=./docs/zh_cn/user_guides/finetune.md>文档
</a>。</p></li></ul></li><li><p><strong>步骤 2</strong>，将保存的 PTH 模型（如果使用的 DeepSpeed，则将会是一个文件夹）转换为 HuggingFace 模型：</p><pre><code class=language-shell>xtuner convert pth_to_hf ${CONFIG_NAME_OR_PATH} ${PTH} ${SAVE_PATH}
</code></pre></li></ul></details><p>其它详细信息，请访问 <a class=link href=https://github.com/InternLM/xtuner target=_blank rel=noopener>XTuner
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>的项目主页。</p><h2 id=模型量化>模型量化</h2><p>LLM 通常体积庞大，对计算资源要求高。模型量化技术可以压缩模型大小，提高运行效率，使其更易于部署：</p><h3 id=autogptq>AutoGPTQ</h3><a href=https://github.com/PanQiWei/AutoGPTQ target=_blank class="card-github fetch-waiting no-styling" repo=PanQiWei/AutoGPTQ id=repo-1ZJfz1INWtnwf5jj-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-1ZJfz1INWtnwf5jj-avatar class=gc-avatar></div><div class=gc-user>PanQiWei</div></div><div class=gc-divider>/</div><div class=gc-repo>AutoGPTQ</div></div><div class=github-logo></div></div><div id=repo-1ZJfz1INWtnwf5jj-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-1ZJfz1INWtnwf5jj-stars class=gc-stars>0</div><div id=repo-1ZJfz1INWtnwf5jj-forks class=gc-forks>0</div><div id=repo-1ZJfz1INWtnwf5jj-license class=gc-license>unkown</div><div id=repo-1ZJfz1INWtnwf5jj-language class=gc-language>Waiting...</div></div></a><script id=repo-1ZJfz1INWtnwf5jj-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/PanQiWei/AutoGPTQ",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-1ZJfz1INWtnwf5jj-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-1ZJfz1INWtnwf5jj-language").innerText=e.language,document.getElementById("repo-1ZJfz1INWtnwf5jj-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-1ZJfz1INWtnwf5jj-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-1ZJfz1INWtnwf5jj-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-1ZJfz1INWtnwf5jj-license").innerText=e.license.spdx_id:document.getElementById("repo-1ZJfz1INWtnwf5jj-license").classList.add="no-license",document.getElementById("repo-1ZJfz1INWtnwf5jj-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for PanQiWei/AutoGPTQ.")}).catch(e=>{const t=document.getElementById("repo-1ZJfz1INWtnwf5jj-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for PanQiWei/AutoGPTQ.")})</script><p>AutoGPTQ 一个基于 GPTQ 算法，简单易用且拥有用户友好型接口的大语言模型量化工具包。</p><p><strong>快速安装</strong></p><ul><li>对于 CUDA 11.7：</li></ul><pre><code class=language-bash>pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/
</code></pre><ul><li>对于 CUDA 11.8：</li></ul><pre><code class=language-bash>pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/
</code></pre><ul><li>对于 RoCm 5.4.2：</li></ul><pre><code class=language-bash>pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/rocm542/
</code></pre><p>更多详细信息，请访问 <a class=link href=https://github.com/AutoGPTQ/AutoGPTQ/ target=_blank rel=noopener>AutoGPTQ
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>的项目主页。</p><h3 id=autoawq>AutoAWQ</h3><a href=https://github.com/casper-hansen/AutoAWQ target=_blank class="card-github fetch-waiting no-styling" repo=casper-hansen/AutoAWQ id=repo-u5nMiSkl1hkB1whE-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-u5nMiSkl1hkB1whE-avatar class=gc-avatar></div><div class=gc-user>casper-hansen</div></div><div class=gc-divider>/</div><div class=gc-repo>AutoAWQ</div></div><div class=github-logo></div></div><div id=repo-u5nMiSkl1hkB1whE-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-u5nMiSkl1hkB1whE-stars class=gc-stars>0</div><div id=repo-u5nMiSkl1hkB1whE-forks class=gc-forks>0</div><div id=repo-u5nMiSkl1hkB1whE-license class=gc-license>unkown</div><div id=repo-u5nMiSkl1hkB1whE-language class=gc-language>Waiting...</div></div></a><script id=repo-u5nMiSkl1hkB1whE-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/casper-hansen/AutoAWQ",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-u5nMiSkl1hkB1whE-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-u5nMiSkl1hkB1whE-language").innerText=e.language,document.getElementById("repo-u5nMiSkl1hkB1whE-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-u5nMiSkl1hkB1whE-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-u5nMiSkl1hkB1whE-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-u5nMiSkl1hkB1whE-license").innerText=e.license.spdx_id:document.getElementById("repo-u5nMiSkl1hkB1whE-license").classList.add="no-license",document.getElementById("repo-u5nMiSkl1hkB1whE-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for casper-hansen/AutoAWQ.")}).catch(e=>{const t=document.getElementById("repo-u5nMiSkl1hkB1whE-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for casper-hansen/AutoAWQ.")})</script><p>AutoAWQ 是另一款自动化模型量化工具，支持多种量化精度，并提供灵活的配置选项，可以根据不同的硬件平台和性能需求进行调整。</p><p>AutoAWQ 是一个易于使用的 4 位量化模型软件包。与 FP16 相比，AutoAWQ 可将模型速度提高 3 倍，内存需求减少 3 倍。AutoAWQ 实现了用于量化 LLMs 的激活感知权重量化（AWQ）算法。AutoAWQ 是在 MIT 的原始工作 <a class=link href=https://github.com/mit-han-lab/llm-awq target=_blank rel=noopener>AWQ
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>基础上创建和改进的。</p><p><strong>安装方法：</strong></p><p>安装前，确保安装了 CUDA >= 12.1（注意：以下只是最快捷的安装方法）</p><pre><code class=language-bash>pip install autoawq
</code></pre><p>更多详细信息和示例，请访问 <a class=link href=https://github.com/casper-hansen/AutoAWQ target=_blank rel=noopener>AutoAWQ
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>的项目主页。</p><h3 id=neural-compressor>Neural Compressor</h3><a href=https://github.com/intel/neural-compressor target=_blank class="card-github fetch-waiting no-styling" repo=intel/neural-compressor id=repo-NpiRlYB1WN5g28g0-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-NpiRlYB1WN5g28g0-avatar class=gc-avatar></div><div class=gc-user>intel</div></div><div class=gc-divider>/</div><div class=gc-repo>neural-compressor</div></div><div class=github-logo></div></div><div id=repo-NpiRlYB1WN5g28g0-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-NpiRlYB1WN5g28g0-stars class=gc-stars>0</div><div id=repo-NpiRlYB1WN5g28g0-forks class=gc-forks>0</div><div id=repo-NpiRlYB1WN5g28g0-license class=gc-license>unkown</div><div id=repo-NpiRlYB1WN5g28g0-language class=gc-language>Waiting...</div></div></a><script id=repo-NpiRlYB1WN5g28g0-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/intel/neural-compressor",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-NpiRlYB1WN5g28g0-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-NpiRlYB1WN5g28g0-language").innerText=e.language,document.getElementById("repo-NpiRlYB1WN5g28g0-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-NpiRlYB1WN5g28g0-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-NpiRlYB1WN5g28g0-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-NpiRlYB1WN5g28g0-license").innerText=e.license.spdx_id:document.getElementById("repo-NpiRlYB1WN5g28g0-license").classList.add="no-license",document.getElementById("repo-NpiRlYB1WN5g28g0-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for intel/neural-compressor.")}).catch(e=>{const t=document.getElementById("repo-NpiRlYB1WN5g28g0-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for intel/neural-compressor.")})</script><p>Neural Compressor 是英特尔开发的模型压缩工具包，支持所有主流深度学习框架（TensorFlow、PyTorch、ONNX Runtime 和 MXNet）上流行的模型压缩技术。</p><p><strong>安装方法：</strong></p><pre><code class=language-bash>pip install &quot;neural-compressor&gt;=2.3&quot; &quot;transformers&gt;=4.34.0&quot; torch torchvision
</code></pre><p>更多详细信息和示例，请访问 <a class=link href=https://github.com/intel/neural-compressor target=_blank rel=noopener>Neural Compressor
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>的项目主页。</p><h2 id=模型部署>模型部署</h2><p>将训练好的 LLM 部署到生产环境至关重要。以下是一些常用的 LLM 部署工具：</p><h3 id=vllm>vLLM</h3><a href=https://github.com/vllm-project/vllm target=_blank class="card-github fetch-waiting no-styling" repo=vllm-project/vllm id=repo-xWwucSNvkDGy3Jvh-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-xWwucSNvkDGy3Jvh-avatar class=gc-avatar></div><div class=gc-user>vllm-project</div></div><div class=gc-divider>/</div><div class=gc-repo>vllm</div></div><div class=github-logo></div></div><div id=repo-xWwucSNvkDGy3Jvh-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-xWwucSNvkDGy3Jvh-stars class=gc-stars>0</div><div id=repo-xWwucSNvkDGy3Jvh-forks class=gc-forks>0</div><div id=repo-xWwucSNvkDGy3Jvh-license class=gc-license>unkown</div><div id=repo-xWwucSNvkDGy3Jvh-language class=gc-language>Waiting...</div></div></a><script id=repo-xWwucSNvkDGy3Jvh-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/vllm-project/vllm",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-xWwucSNvkDGy3Jvh-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-xWwucSNvkDGy3Jvh-language").innerText=e.language,document.getElementById("repo-xWwucSNvkDGy3Jvh-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-xWwucSNvkDGy3Jvh-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-xWwucSNvkDGy3Jvh-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-xWwucSNvkDGy3Jvh-license").innerText=e.license.spdx_id:document.getElementById("repo-xWwucSNvkDGy3Jvh-license").classList.add="no-license",document.getElementById("repo-xWwucSNvkDGy3Jvh-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for vllm-project/vllm.")}).catch(e=>{const t=document.getElementById("repo-xWwucSNvkDGy3Jvh-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for vllm-project/vllm.")})</script><p>vLLM 是一个快速、易用的 LLM 推理服务库。</p><p><strong>主要特点：</strong></p><ul><li><strong>快速</strong><ul><li>SOTA 服务吞吐量</li><li>利用 PagedAttention 高效管理注意力键值内存</li><li>持续批量处理收到的请求</li><li>利用 CUDA/HIP 图进行加速</li><li>量化：支持 GPTQ、AWQ、SqueezeLLM、FP8 KV 高速缓存</li><li>优化的 CUDA 内核</li></ul></li><li><strong>灵活</strong><ul><li>与流行的 Hugging Face 模型无缝集成</li><li>利用各种解码算法（包括并行采样、波束搜索等）提供高吞吐量服务</li><li>为分布式推理提供张量并行支持</li><li>流输出</li><li>兼容 OpenAI 的应用程序接口服务器</li><li>支持 NVIDIA GPU、AMD GPU、Intel CPU 和 GPU
-（实验性）支持前缀缓存
-（试验性）支持多种语言</li></ul></li><li><strong>无缝支持</strong><ul><li>基于 Transformer 的模型，例如 Llama</li><li>基于 MoE 的模型，例如 Mixtral</li><li>多模态模型，例如 LLaVA</li></ul></li></ul><p><strong>快速安装：</strong></p><pre><code class=language-bash>pip install vllm
</code></pre><p>更多详细信息，请查看 <a class=link href=https://vllm.readthedocs.io/en/latest/ target=_blank rel=noopener>vLLM
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>官方文档。</p><h3 id=sgl>SGL</h3><a href=https://github.com/sgl-project/sglang target=_blank class="card-github fetch-waiting no-styling" repo=sgl-project/sglang id=repo-BP52kiELgLewVSc1-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-BP52kiELgLewVSc1-avatar class=gc-avatar></div><div class=gc-user>sgl-project</div></div><div class=gc-divider>/</div><div class=gc-repo>sglang</div></div><div class=github-logo></div></div><div id=repo-BP52kiELgLewVSc1-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-BP52kiELgLewVSc1-stars class=gc-stars>0</div><div id=repo-BP52kiELgLewVSc1-forks class=gc-forks>0</div><div id=repo-BP52kiELgLewVSc1-license class=gc-license>unkown</div><div id=repo-BP52kiELgLewVSc1-language class=gc-language>Waiting...</div></div></a><script id=repo-BP52kiELgLewVSc1-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/sgl-project/sglang",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-BP52kiELgLewVSc1-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-BP52kiELgLewVSc1-language").innerText=e.language,document.getElementById("repo-BP52kiELgLewVSc1-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-BP52kiELgLewVSc1-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-BP52kiELgLewVSc1-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-BP52kiELgLewVSc1-license").innerText=e.license.spdx_id:document.getElementById("repo-BP52kiELgLewVSc1-license").classList.add="no-license",document.getElementById("repo-BP52kiELgLewVSc1-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for sgl-project/sglang.")}).catch(e=>{const t=document.getElementById("repo-BP52kiELgLewVSc1-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for sgl-project/sglang.")})</script><p>SGLang 是一种结构化生成语言，专为大型语言模型（LLMs）而设计。它通过共同设计前端语言和运行系统，使你与 LLMs 的交互更快、更可控。</p><p><strong>主要特点：</strong></p><ul><li><strong>灵活的前端语言</strong>：通过链式生成调用、高级提示、控制流、多种模式、并行性和外部交互，可轻松编写 LLM 应用程序。</li><li><strong>高性能后端运行时</strong>：具有 RadixAttention 功能，可通过在多次调用中重复使用 KV 缓存来加速复杂的 LLM 程序。它还可以作为独立的推理引擎，实现所有常用技术（如连续批处理和张量并行）。</li></ul><p>更多详细信息，请访问 <a class=link href=https://github.com/sgl-project/ target=_blank rel=noopener>SGL
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>的项目主页。</p><h3 id=skypilot>SkyPilot</h3><a href=https://github.com/skypilot-org/skypilot target=_blank class="card-github fetch-waiting no-styling" repo=skypilot-org/skypilot id=repo-HCEjigV0XkY6jNq8-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-HCEjigV0XkY6jNq8-avatar class=gc-avatar></div><div class=gc-user>skypilot-org</div></div><div class=gc-divider>/</div><div class=gc-repo>skypilot</div></div><div class=github-logo></div></div><div id=repo-HCEjigV0XkY6jNq8-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-HCEjigV0XkY6jNq8-stars class=gc-stars>0</div><div id=repo-HCEjigV0XkY6jNq8-forks class=gc-forks>0</div><div id=repo-HCEjigV0XkY6jNq8-license class=gc-license>unkown</div><div id=repo-HCEjigV0XkY6jNq8-language class=gc-language>Waiting...</div></div></a><script id=repo-HCEjigV0XkY6jNq8-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/skypilot-org/skypilot",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-HCEjigV0XkY6jNq8-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-HCEjigV0XkY6jNq8-language").innerText=e.language,document.getElementById("repo-HCEjigV0XkY6jNq8-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-HCEjigV0XkY6jNq8-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-HCEjigV0XkY6jNq8-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-HCEjigV0XkY6jNq8-license").innerText=e.license.spdx_id:document.getElementById("repo-HCEjigV0XkY6jNq8-license").classList.add="no-license",document.getElementById("repo-HCEjigV0XkY6jNq8-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for skypilot-org/skypilot.")}).catch(e=>{const t=document.getElementById("repo-HCEjigV0XkY6jNq8-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for skypilot-org/skypilot.")})</script><p>SkyPilot 是 UC Berkeley RISELab 推出的灵活的云端 LLM 部署工具，支持多种云平台和硬件加速器，可以自动选择最优的部署方案，并提供成本优化功能。</p><p><strong>主要特点：</strong></p><ul><li><strong>多云支持:</strong> 支持 AWS, GCP, Azure 等多种云平台，方便用户选择合适的部署环境。</li><li><strong>轻松扩展</strong>：排队和运行多个作业，自动管理</li><li><strong>轻松接入对象存储</strong>：轻松访问对象存储（S3、GCS、R2）</li></ul><p>更多详细信息，请访问 <a class=link href=https://github.com/skypilot-org/skypilot target=_blank rel=noopener>SkyPilot
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>的项目主页。</p><h3 id=tensorrt-llm>TensorRT-LLM</h3><a href=https://github.com/NVIDIA/TensorRT-LLM target=_blank class="card-github fetch-waiting no-styling" repo=NVIDIA/TensorRT-LLM id=repo-0FwJysFJsVZj1fPU-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-0FwJysFJsVZj1fPU-avatar class=gc-avatar></div><div class=gc-user>NVIDIA</div></div><div class=gc-divider>/</div><div class=gc-repo>TensorRT-LLM</div></div><div class=github-logo></div></div><div id=repo-0FwJysFJsVZj1fPU-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-0FwJysFJsVZj1fPU-stars class=gc-stars>0</div><div id=repo-0FwJysFJsVZj1fPU-forks class=gc-forks>0</div><div id=repo-0FwJysFJsVZj1fPU-license class=gc-license>unkown</div><div id=repo-0FwJysFJsVZj1fPU-language class=gc-language>Waiting...</div></div></a><script id=repo-0FwJysFJsVZj1fPU-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/NVIDIA/TensorRT-LLM",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-0FwJysFJsVZj1fPU-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-0FwJysFJsVZj1fPU-language").innerText=e.language,document.getElementById("repo-0FwJysFJsVZj1fPU-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-0FwJysFJsVZj1fPU-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-0FwJysFJsVZj1fPU-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-0FwJysFJsVZj1fPU-license").innerText=e.license.spdx_id:document.getElementById("repo-0FwJysFJsVZj1fPU-license").classList.add="no-license",document.getElementById("repo-0FwJysFJsVZj1fPU-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for NVIDIA/TensorRT-LLM.")}).catch(e=>{const t=document.getElementById("repo-0FwJysFJsVZj1fPU-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for NVIDIA/TensorRT-LLM.")})</script><p>TensorRT-LLM 是 NVIDIA 推出的高性能 LLM 推理引擎，能够充分利用 GPU 加速计算，并针对 Transformer 模型结构进行了优化，大幅提升推理速度。</p><p>TensorRT-LLM 为用户提供了易于使用的 Python API，用于定义大型语言模型 (LLMs) 和构建 TensorRT 引擎，这些引擎包含最先进的优化技术，可在英伟达™（NVIDIA®）图形处理器上高效执行推理。TensorRT-LLM 还包含用于创建执行这些 TensorRT 引擎的 Python 和 C++ 运行时的组件。</p><p>详细信息，请访问 <a class=link href=https://github.com/NVIDIA/TensorRT-LLM target=_blank rel=noopener>TensorRT-LLM
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>的项目主页。</p><h3 id=openvino>OpenVino</h3><a href=https://github.com/openvinotoolkit/openvino target=_blank class="card-github fetch-waiting no-styling" repo=openvinotoolkit/openvino id=repo-ejxwYuMJ0lt0Cmyi-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-ejxwYuMJ0lt0Cmyi-avatar class=gc-avatar></div><div class=gc-user>openvinotoolkit</div></div><div class=gc-divider>/</div><div class=gc-repo>openvino</div></div><div class=github-logo></div></div><div id=repo-ejxwYuMJ0lt0Cmyi-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-ejxwYuMJ0lt0Cmyi-stars class=gc-stars>0</div><div id=repo-ejxwYuMJ0lt0Cmyi-forks class=gc-forks>0</div><div id=repo-ejxwYuMJ0lt0Cmyi-license class=gc-license>unkown</div><div id=repo-ejxwYuMJ0lt0Cmyi-language class=gc-language>Waiting...</div></div></a><script id=repo-ejxwYuMJ0lt0Cmyi-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/openvinotoolkit/openvino",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-ejxwYuMJ0lt0Cmyi-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-ejxwYuMJ0lt0Cmyi-language").innerText=e.language,document.getElementById("repo-ejxwYuMJ0lt0Cmyi-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-ejxwYuMJ0lt0Cmyi-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-ejxwYuMJ0lt0Cmyi-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-ejxwYuMJ0lt0Cmyi-license").innerText=e.license.spdx_id:document.getElementById("repo-ejxwYuMJ0lt0Cmyi-license").classList.add="no-license",document.getElementById("repo-ejxwYuMJ0lt0Cmyi-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for openvinotoolkit/openvino.")}).catch(e=>{const t=document.getElementById("repo-ejxwYuMJ0lt0Cmyi-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for openvinotoolkit/openvino.")})</script><p>OpenVINO™ 是用于优化和部署人工智能推理的开源工具包。</p><p><strong>主要特点：</strong></p><ul><li><strong>推理优化</strong>：提升深度学习在计算机视觉、自动语音识别、生成式人工智能、使用大型和小型语言模型的自然语言处理以及许多其他常见任务中的性能。</li><li><strong>灵活的模型支持</strong>：使用 TensorFlow、PyTorch、ONNX、Keras 和 PaddlePaddle 等流行框架训练的模型。无需原始框架即可转换和部署模型。</li><li><strong>广泛的平台兼容性</strong>：减少资源需求，在从边缘到云的一系列平台上高效部署。OpenVINO™ 支持在 CPU（x86、ARM）、GPU（支持 OpenCL 的集成和独立 GPU）和 AI 加速器（英特尔 NPU）上进行推理。</li><li><strong>社区和生态系统</strong>：加入一个活跃的社区，为提高各个领域的深度学习性能做出贡献。</li></ul><p>详细信息，请访问 <a class=link href=https://github.com/openvinotoolkit/openvino target=_blank rel=noopener>OpenVino
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>的项目主页。</p><h3 id=tgi>TGI</h3><a href=https://github.com/huggingface/text-generation-inference target=_blank class="card-github fetch-waiting no-styling" repo=huggingface/text-generation-inference id=repo-HCQToS1cK42JozvH-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-HCQToS1cK42JozvH-avatar class=gc-avatar></div><div class=gc-user>huggingface</div></div><div class=gc-divider>/</div><div class=gc-repo>text-generation-inference</div></div><div class=github-logo></div></div><div id=repo-HCQToS1cK42JozvH-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-HCQToS1cK42JozvH-stars class=gc-stars>0</div><div id=repo-HCQToS1cK42JozvH-forks class=gc-forks>0</div><div id=repo-HCQToS1cK42JozvH-license class=gc-license>unkown</div><div id=repo-HCQToS1cK42JozvH-language class=gc-language>Waiting...</div></div></a><script id=repo-HCQToS1cK42JozvH-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/huggingface/text-generation-inference",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-HCQToS1cK42JozvH-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-HCQToS1cK42JozvH-language").innerText=e.language,document.getElementById("repo-HCQToS1cK42JozvH-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-HCQToS1cK42JozvH-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-HCQToS1cK42JozvH-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-HCQToS1cK42JozvH-license").innerText=e.license.spdx_id:document.getElementById("repo-HCQToS1cK42JozvH-license").classList.add="no-license",document.getElementById("repo-HCQToS1cK42JozvH-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for huggingface/text-generation-inference.")}).catch(e=>{const t=document.getElementById("repo-HCQToS1cK42JozvH-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for huggingface/text-generation-inference.")})</script><p>文本生成推理（TGI）是一个用于部署和服务大型语言模型（LLMs）的工具包。TGI 可为最流行的开源 LLMs 实现高性能文本生成，包括 Llama、Falcon、StarCoder、BLOOM、GPT-NeoX 等。</p><p>TGI 实现了许多功能，可以在 <a class=link href=https://github.com/huggingface/text-generation-inference target=_blank rel=noopener>TGI
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>的项目主页上找到详细信息。</p><h2 id=本地运行>本地运行</h2><p>得益于模型压缩和优化技术，我们也可以在个人设备上运行 LLM：</p><h3 id=mlx>MLX</h3><a href=https://github.com/ml-explore/mlx target=_blank class="card-github fetch-waiting no-styling" repo=ml-explore/mlx id=repo-QXE1mpSqym81UKpm-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-QXE1mpSqym81UKpm-avatar class=gc-avatar></div><div class=gc-user>ml-explore</div></div><div class=gc-divider>/</div><div class=gc-repo>mlx</div></div><div class=github-logo></div></div><div id=repo-QXE1mpSqym81UKpm-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-QXE1mpSqym81UKpm-stars class=gc-stars>0</div><div id=repo-QXE1mpSqym81UKpm-forks class=gc-forks>0</div><div id=repo-QXE1mpSqym81UKpm-license class=gc-license>unkown</div><div id=repo-QXE1mpSqym81UKpm-language class=gc-language>Waiting...</div></div></a><script id=repo-QXE1mpSqym81UKpm-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/ml-explore/mlx",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-QXE1mpSqym81UKpm-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-QXE1mpSqym81UKpm-language").innerText=e.language,document.getElementById("repo-QXE1mpSqym81UKpm-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-QXE1mpSqym81UKpm-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-QXE1mpSqym81UKpm-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-QXE1mpSqym81UKpm-license").innerText=e.license.spdx_id:document.getElementById("repo-QXE1mpSqym81UKpm-license").classList.add="no-license",document.getElementById("repo-QXE1mpSqym81UKpm-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for ml-explore/mlx.")}).catch(e=>{const t=document.getElementById("repo-QXE1mpSqym81UKpm-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for ml-explore/mlx.")})</script><p>MLX 是一个专门支持在 Apple 设备上运行 LLM 的框架，充分利用 Metal 加速计算，并提供简单易用的 API，方便开发者将 LLM 集成到 iOS 应用中.</p><p><strong>主要特点：</strong></p><ul><li><strong>相似的应用程序接口</strong>：MLX 的 Python API 与 NumPy 非常相似。MLX 还拥有功能齐全的 C++、C 和 Swift API，这些 API 与 Python API 非常相似。MLX 拥有更高级别的软件包，如 <code>mlx.nn</code> 和 <code>mlx.optimizers</code> ，其 API 与 PyTorch 非常接近，可简化更复杂模型的构建。</li><li><strong>可组合函数变换</strong>：MLX 支持用于自动微分、自动矢量化和计算图优化的可组合函数变换。</li><li><strong>懒计算</strong>：MLX 中的计算只有在需要时才将数组实体化。</li><li><strong>动态图构建</strong>：MLX 中的计算图形是动态构建的。改变函数参数的形状不会导致编译速度变慢，而且调试简单直观。</li><li><strong>多设备</strong>：操作可在任何支持的设备（目前是 CPU 和 GPU）上运行。</li><li><strong>统一内存</strong>：统一内存模型是 MLX 与其他框架的一个显著区别。MLX 中的阵列位于共享内存中。对 MLX 数组的操作可在任何支持的设备类型上执行，而无需传输数据。</li></ul><p>MLX 是机器学习研究人员为机器学习研究人员设计的。该框架旨在方便用户使用，但仍能高效地训练和部署模型。框架本身的设计概念也很简单。我们的目标是让研究人员能够轻松扩展和改进 MLX，从而快速探索新思路。更多详细信息，请访问 <a class=link href=https://github.com/ml-explore/mlx target=_blank rel=noopener>MLX
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>的项目主页。</p><h3 id=llamacpp>Llama.cpp</h3><p>Llama.cpp 是使用 C++ 实现的 Llama 模型推理引擎，可以在 CPU 上高效运行，并支持多种操作系统和硬件平台，方便开发者在资源受限的设备上运行 LLM。</p><a href=https://github.com/ggerganov/llama.cpp target=_blank class="card-github fetch-waiting no-styling" repo=ggerganov/llama.cpp id=repo-dnxf59D4AIcIxy9g-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-dnxf59D4AIcIxy9g-avatar class=gc-avatar></div><div class=gc-user>ggerganov</div></div><div class=gc-divider>/</div><div class=gc-repo>llama.cpp</div></div><div class=github-logo></div></div><div id=repo-dnxf59D4AIcIxy9g-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-dnxf59D4AIcIxy9g-stars class=gc-stars>0</div><div id=repo-dnxf59D4AIcIxy9g-forks class=gc-forks>0</div><div id=repo-dnxf59D4AIcIxy9g-license class=gc-license>unkown</div><div id=repo-dnxf59D4AIcIxy9g-language class=gc-language>Waiting...</div></div></a><script id=repo-dnxf59D4AIcIxy9g-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/ggerganov/llama.cpp",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-dnxf59D4AIcIxy9g-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-dnxf59D4AIcIxy9g-language").innerText=e.language,document.getElementById("repo-dnxf59D4AIcIxy9g-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-dnxf59D4AIcIxy9g-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-dnxf59D4AIcIxy9g-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-dnxf59D4AIcIxy9g-license").innerText=e.license.spdx_id:document.getElementById("repo-dnxf59D4AIcIxy9g-license").classList.add="no-license",document.getElementById("repo-dnxf59D4AIcIxy9g-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for ggerganov/llama.cpp.")}).catch(e=>{const t=document.getElementById("repo-dnxf59D4AIcIxy9g-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for ggerganov/llama.cpp.")})</script><p><strong>主要特点：</strong></p><ul><li><strong>CPU 推理:</strong> 针对 CPU 平台进行优化，可以在没有 GPU 的设备上运行 LLM。</li><li><strong>跨平台支持:</strong> 支持 Linux, macOS, Windows 等多种操作系统，方便用户在不同平台上使用。</li><li><strong>轻量级部署:</strong> 编译后的二进制文件体积小，方便用户部署和使用.</li></ul><p>更多详细信息，请访问 <a class=link href=https://github.com/ggerganov/llama.cpp target=_blank rel=noopener>Llama.cpp
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>的项目主页。</p><h3 id=ollama>Ollama</h3><a href=https://github.com/ollama/ollama target=_blank class="card-github fetch-waiting no-styling" repo=ollama/ollama id=repo-Uj28rL0jrmUYW6CB-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-Uj28rL0jrmUYW6CB-avatar class=gc-avatar></div><div class=gc-user>ollama</div></div><div class=gc-divider>/</div><div class=gc-repo>ollama</div></div><div class=github-logo></div></div><div id=repo-Uj28rL0jrmUYW6CB-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-Uj28rL0jrmUYW6CB-stars class=gc-stars>0</div><div id=repo-Uj28rL0jrmUYW6CB-forks class=gc-forks>0</div><div id=repo-Uj28rL0jrmUYW6CB-license class=gc-license>unkown</div><div id=repo-Uj28rL0jrmUYW6CB-language class=gc-language>Waiting...</div></div></a><script id=repo-Uj28rL0jrmUYW6CB-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/ollama/ollama",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-Uj28rL0jrmUYW6CB-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-Uj28rL0jrmUYW6CB-language").innerText=e.language,document.getElementById("repo-Uj28rL0jrmUYW6CB-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-Uj28rL0jrmUYW6CB-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-Uj28rL0jrmUYW6CB-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-Uj28rL0jrmUYW6CB-license").innerText=e.license.spdx_id:document.getElementById("repo-Uj28rL0jrmUYW6CB-license").classList.add="no-license",document.getElementById("repo-Uj28rL0jrmUYW6CB-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for ollama/ollama.")}).catch(e=>{const t=document.getElementById("repo-Uj28rL0jrmUYW6CB-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for ollama/ollama.")})</script><p>在 <a class=link href=/p/ollama/>【Ollama：从入门到进阶】
</a>一文中介绍过，Ollama 是一个用于构建大型语言模型应用的工具，它提供了一个简洁易用的命令行界面和服务器，让你能够轻松下载、运行和管理各种开源 LLM。与需要复杂配置和强大硬件的传统 LLM 不同，Ollama 让你能够方便地像使用手机 App 一样体验 LLM 的强大功能。</p><p><strong>主要特点：</strong></p><ul><li><strong>简单易用</strong>：Ollama 提供了一个简洁易用的命令行工具，方便用户下载、运行和管理 LLM。</li><li><strong>多种模型</strong>：Ollama 支持多种开源 LLM，包括 Qwen2、Llama3、Mistral 等。</li><li><strong>兼容 OpenAI 接口</strong>：Ollama 支持 OpenAI API 接口，便于切换原有应用到 Ollama 上。</li></ul><p>更多详细信息，请访问 <a class=link href=https://github.com/ollama/ollama target=_blank rel=noopener>Ollama
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>的项目主页。</p><h2 id=agent-及-rag-框架>Agent 及 RAG 框架</h2><p>将 LLM 与外部数据和工具结合，可以构建更强大的应用。以下是一些常用的 Agent 及 RAG 框架：</p><h3 id=llamaindex>LlamaIndex</h3><a href=https://github.com/run-llama/llama_index target=_blank class="card-github fetch-waiting no-styling" repo=run-llama/llama_index id=repo-DWxilcS5a7Fs86qA-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-DWxilcS5a7Fs86qA-avatar class=gc-avatar></div><div class=gc-user>run-llama</div></div><div class=gc-divider>/</div><div class=gc-repo>llama_index</div></div><div class=github-logo></div></div><div id=repo-DWxilcS5a7Fs86qA-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-DWxilcS5a7Fs86qA-stars class=gc-stars>0</div><div id=repo-DWxilcS5a7Fs86qA-forks class=gc-forks>0</div><div id=repo-DWxilcS5a7Fs86qA-license class=gc-license>unkown</div><div id=repo-DWxilcS5a7Fs86qA-language class=gc-language>Waiting...</div></div></a><script id=repo-DWxilcS5a7Fs86qA-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/run-llama/llama_index",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-DWxilcS5a7Fs86qA-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-DWxilcS5a7Fs86qA-language").innerText=e.language,document.getElementById("repo-DWxilcS5a7Fs86qA-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-DWxilcS5a7Fs86qA-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-DWxilcS5a7Fs86qA-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-DWxilcS5a7Fs86qA-license").innerText=e.license.spdx_id:document.getElementById("repo-DWxilcS5a7Fs86qA-license").classList.add="no-license",document.getElementById("repo-DWxilcS5a7Fs86qA-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for run-llama/llama_index.")}).catch(e=>{const t=document.getElementById("repo-DWxilcS5a7Fs86qA-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for run-llama/llama_index.")})</script><p>LlamaIndex（GPT 索引）是用于 LLM 应用程序的数据框架。使用 LlamaIndex 构建应用程序通常需要使用 LlamaIndex 核心和一组选定的集成（或插件）。在 Python 中使用 LlamaIndex 构建应用程序有两种方法：</p><ul><li>启动器： <code>llama-index</code> ( <a class=link href=https://pypi.org/project/llama-index/ target=_blank rel=noopener>https://pypi.org/project/llama-index/
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>)。Python 入门包，包括核心 LlamaIndex 以及部分集成。</li><li>定制化：<code>llama-index-core</code> ( <a class=link href=https://pypi.org/project/llama-index-core/ target=_blank rel=noopener>https://pypi.org/project/llama-index-core/
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>)。安装核心 LlamaIndex，并在 LlamaHub 上添加应用程序所需的 LlamaIndex 集成包。目前有 300 多个 LlamaIndex 集成包可与核心无缝协作，让你可以使用自己喜欢的 LLM、嵌入和向量存储数据库进行构建</li></ul><p>LlamaIndex Python 库是以名字命名的，因此包含 <code>core</code> 的导入语句意味着使用的是核心包。相反，那些不含 <code>core</code> 的语句则意味着使用的是集成包。</p><pre><code class=language-python># typical pattern
from llama_index.core.xxx import ClassABC  # core submodule xxx
from llama_index.xxx.yyy import (
    SubclassABC,
)  # integration yyy for submodule xxx

# concrete example
from llama_index.core.llms import LLM
from llama_index.llms.openai import OpenAI
</code></pre><h3 id=crewai>CrewAI</h3><a href=https://github.com/joaomdmoura/crewAI target=_blank class="card-github fetch-waiting no-styling" repo=joaomdmoura/crewAI id=repo-6seMqzfwvclLL1uS-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-6seMqzfwvclLL1uS-avatar class=gc-avatar></div><div class=gc-user>joaomdmoura</div></div><div class=gc-divider>/</div><div class=gc-repo>crewAI</div></div><div class=github-logo></div></div><div id=repo-6seMqzfwvclLL1uS-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-6seMqzfwvclLL1uS-stars class=gc-stars>0</div><div id=repo-6seMqzfwvclLL1uS-forks class=gc-forks>0</div><div id=repo-6seMqzfwvclLL1uS-license class=gc-license>unkown</div><div id=repo-6seMqzfwvclLL1uS-language class=gc-language>Waiting...</div></div></a><script id=repo-6seMqzfwvclLL1uS-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/joaomdmoura/crewAI",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-6seMqzfwvclLL1uS-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-6seMqzfwvclLL1uS-language").innerText=e.language,document.getElementById("repo-6seMqzfwvclLL1uS-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-6seMqzfwvclLL1uS-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-6seMqzfwvclLL1uS-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-6seMqzfwvclLL1uS-license").innerText=e.license.spdx_id:document.getElementById("repo-6seMqzfwvclLL1uS-license").classList.add="no-license",document.getElementById("repo-6seMqzfwvclLL1uS-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for joaomdmoura/crewAI.")}).catch(e=>{const t=document.getElementById("repo-6seMqzfwvclLL1uS-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for joaomdmoura/crewAI.")})</script><p>CrewAI 是一个构建 AI Agent 的框架，可以将 LLM 与其他工具和 API 集成，实现更复杂的任务，例如自动执行网页操作、生成代码等。</p><p><strong>主要特点：</strong></p><ul><li><strong>基于角色的智能体设计</strong>：你可以使用特定的角色、目标和工具来自定义智能体。</li><li><strong>自主智能体间委托</strong>：智能体可以自主地将任务委托给其他智能体，并相互查询信息，从而提高解决问题的效率。</li><li><strong>灵活的任务管理</strong>：可以使用可定制的工具来定义任务，并动态地将任务分配给智能体。</li><li><strong>流程驱动</strong>：该系统以流程为中心，目前支持按顺序执行任务和分层流程。未来还会支持更复杂的流程，例如协商和自主流程。</li><li><strong>保存输出为文件</strong>：可以将单个任务的输出保存为文件，以便以后使用。</li><li><strong>将输出解析为 Pydantic 或 Json</strong>：可以将单个任务的输出解析为 Pydantic 模型或 Json 格式，以便于后续处理和分析。</li><li><strong>支持开源模型</strong>：可以使用 OpenAI 或其他开源模型来运行您的智能体团队。更多关于配置智能体与模型连接的信息，包括如何连接到本地运行的模型，请参阅<a class=link href=https://docs.crewai.com/how-to/LLM-Connections/ target=_blank rel=noopener>将 crewAI 连接到大型语言模型
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>。</li></ul><p>更多详细信息，请访问 <a class=link href=https://github.com/joaomdmoura/crewAI target=_blank rel=noopener>CrewAI
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>的项目主页。</p><h3 id=opendevin>OpenDevin</h3><a href=https://github.com/opendevin/opendevin target=_blank class="card-github fetch-waiting no-styling" repo=opendevin/opendevin id=repo-cR0duKkACFftmZqG-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-cR0duKkACFftmZqG-avatar class=gc-avatar></div><div class=gc-user>opendevin</div></div><div class=gc-divider>/</div><div class=gc-repo>opendevin</div></div><div class=github-logo></div></div><div id=repo-cR0duKkACFftmZqG-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-cR0duKkACFftmZqG-stars class=gc-stars>0</div><div id=repo-cR0duKkACFftmZqG-forks class=gc-forks>0</div><div id=repo-cR0duKkACFftmZqG-license class=gc-license>unkown</div><div id=repo-cR0duKkACFftmZqG-language class=gc-language>Waiting...</div></div></a><script id=repo-cR0duKkACFftmZqG-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/opendevin/opendevin",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-cR0duKkACFftmZqG-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-cR0duKkACFftmZqG-language").innerText=e.language,document.getElementById("repo-cR0duKkACFftmZqG-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-cR0duKkACFftmZqG-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-cR0duKkACFftmZqG-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-cR0duKkACFftmZqG-license").innerText=e.license.spdx_id:document.getElementById("repo-cR0duKkACFftmZqG-license").classList.add="no-license",document.getElementById("repo-cR0duKkACFftmZqG-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for opendevin/opendevin.")}).catch(e=>{const t=document.getElementById("repo-cR0duKkACFftmZqG-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for opendevin/opendevin.")})</script><p>OpenDevin 是一个由人工智能和 LLMs 驱动的自主软件工程师平台。</p><p>OpenDevin 智能体与人类开发人员合作编写代码、修复错误和发布功能。</p><p>详细信息，请访问 <a class=link href=https://github.com/opendevin/opendevin target=_blank rel=noopener>OpenDevin
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>的项目主页。</p><h2 id=模型评测>模型评测</h2><p>为了选择合适的 LLM 并评估其性能，我们需要进行模型评测：</p><h3 id=lmsys>LMSys</h3><p>LMSys Org 是由加州大学伯克利分校的学生和教师与加州大学圣地亚哥分校以及卡内基梅隆大学合作成立的开放式研究组织。</p><p>目标是通过共同开发开放模型、数据集、系统和评估工具，使大型模型对每个人都可访问。训练大型语言模型并广泛提供它们的应用，同时也在开发分布式系统以加速它们的训练和推理过程。</p><p>目前，LMSys Chatbot Area 是最被认可的大模型排行榜之一，受多家公司和研究机构的认可。</p><p>排行榜地址：<a class=link href=https://arena.lmsys.org/ target=_blank rel=noopener>https://arena.lmsys.org/
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a></p><h3 id=opencompass>OpenCompass</h3><p>OpenCompass 是一个 LLM 评估平台，支持 100 多个数据集上的各种模型（Llama3、Mistral、InternLM2、GPT-4、LLaMa2、Qwen、GLM、Claude 等）。</p><a href=https://github.com/open-compass/opencompass target=_blank class="card-github fetch-waiting no-styling" repo=open-compass/opencompass id=repo-qU0Gznh9y9NA6GnE-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-qU0Gznh9y9NA6GnE-avatar class=gc-avatar></div><div class=gc-user>open-compass</div></div><div class=gc-divider>/</div><div class=gc-repo>opencompass</div></div><div class=github-logo></div></div><div id=repo-qU0Gznh9y9NA6GnE-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-qU0Gznh9y9NA6GnE-stars class=gc-stars>0</div><div id=repo-qU0Gznh9y9NA6GnE-forks class=gc-forks>0</div><div id=repo-qU0Gznh9y9NA6GnE-license class=gc-license>unkown</div><div id=repo-qU0Gznh9y9NA6GnE-language class=gc-language>Waiting...</div></div></a><script id=repo-qU0Gznh9y9NA6GnE-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/open-compass/opencompass",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-qU0Gznh9y9NA6GnE-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-qU0Gznh9y9NA6GnE-language").innerText=e.language,document.getElementById("repo-qU0Gznh9y9NA6GnE-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-qU0Gznh9y9NA6GnE-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-qU0Gznh9y9NA6GnE-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-qU0Gznh9y9NA6GnE-license").innerText=e.license.spdx_id:document.getElementById("repo-qU0Gznh9y9NA6GnE-license").classList.add="no-license",document.getElementById("repo-qU0Gznh9y9NA6GnE-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for open-compass/opencompass.")}).catch(e=>{const t=document.getElementById("repo-qU0Gznh9y9NA6GnE-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for open-compass/opencompass.")})</script><h3 id=open-llm-leaderboard>Open LLM Leaderboard</h3><p>Open LLM Leaderboard 是一个持续更新的 LLM 排行榜，根据多个评测指标对不同模型进行排名，方便开发者了解最新的模型性能和发展趋势。</p><p>排行榜地址：<a class=link href=https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard target=_blank rel=noopener>https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a></p><h2 id=总结>总结</h2><p>LLM 生态正在蓬勃发展，涵盖了从模型训练到应用落地的各个环节。相信随着技术的不断进步，LLM 将会在更多领域发挥重要作用，为我们带来更加智能的应用体验。</p></section><footer class=article-footer><section class=article-tags><a href=/tags/llm/>llm</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><script type=text/javascript src=/js/prism.js async></script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/ollama/><div class=article-image><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116903387_p0_master1200.webp loading=lazy data-key=ollama data-hash=https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116903387_p0_master1200.webp></div><div class=article-details><h2 class=article-title>Ollama：从入门到进阶</h2></div></a></article></div></div></aside><script src=https://cdn.bootcdn.net/ajax/libs/twikoo/1.6.20/twikoo.all.min.js></script><div id=tcomment></div><style>.twikoo{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}:root[data-scheme=dark]{--twikoo-body-text-color-main:rgba(255, 255, 255, 0.9);--twikoo-body-text-color:rgba(255, 255, 255, 0.7)}.twikoo .el-input-group__prepend,.twikoo .tk-action-icon,.twikoo .tk-time,.twikoo .tk-comments-count{color:var(--twikoo-body-text-color)}.twikoo .el-input__inner,.twikoo .el-textarea__inner,.twikoo .tk-preview-container,.twikoo .tk-content,.twikoo .tk-nick,.twikoo .tk-send{color:var(--twikoo-body-text-color-main)}.twikoo .el-button{color:var(--twikoo-body-text-color)!important}.OwO .OwO-body{background-color:var(--body-background)!important;color:var(--body-text-color)!important}</style><script>twikoo.init({envId:"https://comment.cuterwrite.top",el:"#tcomment",lang:"zh-CN"})</script><footer class=site-footer><section class=copyright>&copy;
2021 -
2024 cuterwrite</section><section class=running-time>本博客已稳定运行
<span id=runningdays class=running-days></span></section><section class=totalcount>发表了72篇文章 ·
总计317.46k字</section><section class=powerby>Welcome to cuterwrite's blog!<br>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.17.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计<br><span>基于 <a href=https://github.com/CaiJimmy/hugo-theme-stack/tree/v3.25.0 target=_blank rel=noopener><b style=color:#9e8f9f>v3.25.0</b></a> 分支版本修改</span><br></section></footer><script>let s1="2021-4-17";s1=new Date(s1.replace(/-/g,"/"));let s2=new Date,timeDifference=s2.getTime()-s1.getTime(),days=Math.floor(timeDifference/(1e3*60*60*24)),hours=Math.floor(timeDifference%(1e3*60*60*24)/(1e3*60*60)),minutes=Math.floor(timeDifference%(1e3*60*60)/(1e3*60)),result=days+"天"+hours+"小时"+minutes+"分钟";document.getElementById("runningdays").innerHTML=result</script><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.bootcdn.net/ajax/libs/photoswipe/4.1.3/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.bootcdn.net/ajax/libs/photoswipe/4.1.3/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.bootcdn.net/ajax/libs/photoswipe/4.1.3/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.bootcdn.net/ajax/libs/photoswipe/4.1.3/photoswipe.min.css crossorigin=anonymous></main></div><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.font.im/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script><meta name=apple-mobile-web-app-capable content="yes"><meta name=theme-color content="#ffffff"><script>"serviceWorker"in navigator&&window.addEventListener("load",()=>{navigator.serviceWorker.register("/sw.js").then(e=>{console.log("Service worker registered with scope: ",e.scope)},e=>{console.log("Service worker registration failed: ",e)})})</script></body></html>