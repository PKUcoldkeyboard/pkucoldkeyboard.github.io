<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Apache Spark 是一个多语言引擎，用于在单节点机器或集群上执行数据工程、数据科学和机器学习任务。本文将为您提供在高性能计算（HPC）集群系统上运行多节点 Spark 集群的指南，并展示一个使用 PySpark 的作业示例。"><title>在 HPC 上运行 Apache Spark</title><link rel=canonical href=https://cuterwrite.top/p/run-spark-on-hpc/><link rel=stylesheet href=/scss/style.min.b97201a532522426f9261990263dade40123288553876f8521e4563585c58246.css><meta property="og:title" content="在 HPC 上运行 Apache Spark"><meta property="og:description" content="Apache Spark 是一个多语言引擎，用于在单节点机器或集群上执行数据工程、数据科学和机器学习任务。本文将为您提供在高性能计算（HPC）集群系统上运行多节点 Spark 集群的指南，并展示一个使用 PySpark 的作业示例。"><meta property="og:url" content="https://cuterwrite.top/p/run-spark-on-hpc/"><meta property="og:site_name" content="cuterwrite"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="Spark"><meta property="article:tag" content="分布式计算"><meta property="article:published_time" content="2023-12-30T01:00:00+00:00"><meta property="article:modified_time" content="2023-12-30T01:00:00+00:00"><meta property="og:image" content="https://cuterwrite-1302252842.file.myqcloud.com/img/crop_afb480a4096d16305dc5696f8072d0c0195413.jpg@1256w_2094h_!web-article-pic-2023-12-30.webp"><meta name=twitter:title content="在 HPC 上运行 Apache Spark"><meta name=twitter:description content="Apache Spark 是一个多语言引擎，用于在单节点机器或集群上执行数据工程、数据科学和机器学习任务。本文将为您提供在高性能计算（HPC）集群系统上运行多节点 Spark 集群的指南，并展示一个使用 PySpark 的作业示例。"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://cuterwrite-1302252842.file.myqcloud.com/img/crop_afb480a4096d16305dc5696f8072d0c0195413.jpg@1256w_2094h_!web-article-pic-2023-12-30.webp"><link rel="shortcut icon" href=/favicon.ico><script async src=https://umami-gamma-virid.vercel.app/uma data-website-id=635c2011-51a9-4ffc-b360-f5572bb94276></script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hue4d14694a57c01a222a16c47db12c89c_369633_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a>
<span class=emoji>😉</span></figure><div class=site-meta><h1 class=site-name><a href=/>cuterwrite</a></h1><h2 class=site-description>欢迎来到我的个人博客。我是cuterwrite，一个热爱生活、不断探索的人。在这里，我分享我的想法、经验和学习，希望可以帮助到你，也欢迎你与我分享你的看法。</h2></div></header><ol class=social-menu><li><a href=https://github.com/PKUcoldkeyboard target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.zhihu.com/people/kong-tiao-cheng-tai-lang-30-57 target=_blank title=zhihu rel=me><svg t="1705591931290" class="icon" viewBox="0 0 1280 1024" xmlns="http://www.w3.org/2000/svg" p-id="21048" width="32" height="32"><path d="M341.08 296.26v435.08l46.86.02 15.42 52.74 84.02-52.74h99.06V296.26H341.08zm195.5 387.86H480.7l-55.8 35.02-10.16-34.94-23.8-.08V343.5h145.64v340.62zM299.66 495.34H195c3.48-54.2 4.4-103.18 4.4-146.92h102.32s3.94-45.12-17.16-44.62h-177c6.98-26.24 15.74-53.32 26.24-81.34.0.0-48.14.0-64.54 43.14-6.78 17.8-26.42 86.28-61.4 156.24 11.78-1.28 50.74-2.36 73.68-44.42 4.22-11.78 5.02-13.32 10.28-29.06h57.74c0 21-2.4 133.76-3.36 146.88H41.66c-23.48.0-31.12 47.24-31.12 47.24H141.7C132.9 642.2 85.66 726.24.0 792.68c40.98 11.7 81.82-1.86 102-19.8.0.0 45.96-41.8 71.18-138.5L281.1 764.26s15.82-53.78-2.48-79.98c-15.16-17.84-56.12-66.12-73.58-83.62L175.8 623.9c8.72-27.96 13.98-55.1 15.74-81.34h123.3s-.18-47.24-15.18-47.24v.02zm824.04-3.2c41.66-51.28 89.96-117.14 89.96-117.14s-37.3-29.6-54.76-8.12c-12 16.3-73.66 96.4-73.66 96.4l38.46 28.86zM823.52 373.96c-18.02-16.5-51.82 4.26-51.82 4.26s79.04 110.08 82.24 114.9l38.92-27.46s-51.34-75.22-69.32-91.72h-.02zM1280 516.7c-39.56.0-261.82 1.86-262.12 1.86v-202c9.62.0 24.84-.8 45.7-2.4 81.76-4.82 140.26-8 175.54-9.62.0.0 24.44-54.38-1.18-66.88-6.14-2.36-46.34 9.16-46.34 9.16s-330.44 32.98-464.72 36.1c3.2 17.64 15.24 34.16 31.56 39.1 26.62 6.96 45.38 3.4 98.3 1.78 49.66-3.2 87.36-4.86 113.02-4.86v199.62H702.82s5.64 44.62 51.02 45.7h215.88V706.1c0 27.94-22.38 43.98-48.96 42.24-28.16.22-52.16-2.3-83.38-3.62 3.98 7.94 12.66 28.78 38.62 43.68 19.76 9.62 32.34 13.14 52.04 13.14 59.12.0 91.34-34.56 89.78-90.62V564.28h244.72c19.36.0 17.4-47.56 17.4-47.56l.06-.02z" fill="#707070" p-id="21049"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>首页</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg><span>个人</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>文章</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>搜索</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>暗色模式</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ul><li><a href=#一概述>一、概述</a></li><li><a href=#二开始>二、开始</a><ul><li><a href=#1-下载-openjdk-1102>1. 下载 OpenJDK-11.0.2</a></li><li><a href=#2-下载-spark-342>2. 下载 Spark-3.4.2</a></li><li><a href=#3-配置-modulefile>3. 配置 modulefile</a></li><li><a href=#4-使用-pip-安装-pyspark-库>4. 使用 pip 安装 pyspark 库</a></li><li><a href=#5-编写环境加载脚本-set-spark-envsh>5. 编写环境加载脚本 set-spark-env.sh</a></li><li><a href=#6-编写-sbatch-脚本>6. 编写 sbatch 脚本</a></li></ul></li><li><a href=#三总结>三、总结</a></li></ul></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/run-spark-on-hpc/><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/crop_afb480a4096d16305dc5696f8072d0c0195413.jpg@1256w_2094h_!web-article-pic-2023-12-30.webp loading=lazy alt="Featured image of post 在 HPC 上运行 Apache Spark"></a></div><div class=article-details><header class=article-category><a href=/categories/hpc/ style=background-color:#ffd06f;color:#fff>高性能计算</a>
<a href=/categories/bigdata/ style=background-color:#afb0b2;color:#fff>大数据技术</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/run-spark-on-hpc/>在 HPC 上运行 Apache Spark</a></h2><h3 class=article-subtitle>Apache Spark 是一个多语言引擎，用于在单节点机器或集群上执行数据工程、数据科学和机器学习任务。本文将为您提供在高性能计算（HPC）集群系统上运行多节点 Spark 集群的指南，并展示一个使用 PySpark 的作业示例。</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>2023-12-30</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>阅读时长: 4 分钟</time></div></footer></div></header><section class=article-content><h1 id=在-hpc-上运行-apache-spark>在 HPC 上运行 Apache Spark</h1><h2 id=一概述>一、概述</h2><p><a class=link href=https://spark.apache.org/ target=_blank rel=noopener>Apache Spark</a> 是一个多语言引擎，用于在单节点机器或集群上执行数据工程、数据科学和机器学习任务。本文将为您提供在高性能计算（HPC）集群系统上运行多节点 Spark 集群的指南，并展示一个使用 PySpark 的作业示例。</p><h2 id=二开始>二、开始</h2><h3 id=1-下载-openjdk-1102>1. 下载 OpenJDK-11.0.2</h3><p>从 <a class=link href=https://jdk.java.net/archive/ target=_blank rel=noopener>OpenJDK 官方网站</a> 下载 OpenJDK-11.0.2。选择 Linux 的对应版本并下载。解压下载的文件并将其放置在 <code>${HOME}/software/openjdk</code> 中并重命名为 <code>11.0.2</code> 。</p><h3 id=2-下载-spark-342>2. 下载 Spark-3.4.2</h3><p>从 <a class=link href=https://spark.apache.org/downloads.html target=_blank rel=noopener>Apache Spark 下载页面</a> 下载 Spark 。本文使用的是 Spark-3.4.2，但本指南应该也适用于更新的版本。解压下载的文件并将目录重命名为 3.4.2，放置在 ${HOME}/software/spark 文件夹中。</p><h3 id=3-配置-modulefile>3. 配置 modulefile</h3><p>在自定义目录中安装软件后，需要将软件的可执行文件路径等添加到相应的环境变量中才能使用。<code>module</code> 是一款环境变量管理工具，通过 <code>module</code> 实现软件环境变量的管理，快速加载和切换软件环境。集群中安装了一些常用的软件和库，可以通过 <code>module</code> 进行加载使用。</p><p>在这里，我们需要编写 <code>modulefile</code> 来管理自己的 JDK 和 Spark 软件环境，以便快速加载 Java 和 Spark 环境。</p><ul><li>在 <code>${HOME}/modulefiles/openjdk</code> 中创建名为 <code>11.0.2</code> 的文本文件，内容为：</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=c1>#%Module1.0</span>
</span></span><span class=line><span class=cl><span class=c1>##</span>
</span></span><span class=line><span class=cl><span class=c1>## openjdk modulefile</span>
</span></span><span class=line><span class=cl><span class=c1>##</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>proc ModulesHelp <span class=o>{</span> <span class=o>}</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    puts stderr <span class=s2>&#34;This module sets up the environment for OpenJdk 11.0.2 \n&#34;</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>module-whatis <span class=s2>&#34;For more information, \$ module help openjdk/11.0.2\n&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>conflict openjdk
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 注意！这里需要进行修改</span>
</span></span><span class=line><span class=cl><span class=nb>set</span> root &lt;PATH/WHERE/OPENJDK/DIRECTORY/IS&gt;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>prepend-path PATH <span class=si>${</span><span class=nv>root</span><span class=si>}</span>/bin
</span></span></code></pre></td></tr></table></div></div><ul><li>在 <code>${HOME}/modulefiles/spark</code> 中创建名为 <code>3.4.2</code> 的文本文件， 内容为：</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=c1>#%Module1.0</span>
</span></span><span class=line><span class=cl><span class=c1>##</span>
</span></span><span class=line><span class=cl><span class=c1>## spark modulefile</span>
</span></span><span class=line><span class=cl><span class=c1>##</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>proc ModulesHelp <span class=o>{</span> <span class=o>}</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    global version
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    puts stderr <span class=s2>&#34;This module loads Apache Spark environment variables and updates the PATH.&#34;</span>
</span></span><span class=line><span class=cl>    puts stderr <span class=s2>&#34; &#34;</span>
</span></span><span class=line><span class=cl>    puts stderr <span class=s2>&#34;Version: </span><span class=nv>$version</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>module-whatis <span class=s2>&#34;Loads Apache Spark environment variables and updates the PATH. \n For more information, \$ module help spark/3.4.2 .\n&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>conflict spark
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Set the version and installation path</span>
</span></span><span class=line><span class=cl><span class=nb>set</span> version 3.4.2
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 注意！这里需要进行修改</span>
</span></span><span class=line><span class=cl><span class=nb>set</span> root &lt;PATH/WHERE/SPARK/DIRECTORY/IS&gt;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Set the environment variables</span>
</span></span><span class=line><span class=cl>setenv SPARK_HOME <span class=si>${</span><span class=nv>root</span><span class=si>}</span>
</span></span><span class=line><span class=cl>setenv SPARK_CONF_DIR <span class=si>${</span><span class=nv>root</span><span class=si>}</span>/conf
</span></span><span class=line><span class=cl>setenv PYSPARK_PYTHON python3
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Update the PATH</span>
</span></span><span class=line><span class=cl>prepend-path PATH <span class=si>${</span><span class=nv>root</span><span class=si>}</span>/bin
</span></span><span class=line><span class=cl>prepend-path PATH <span class=si>${</span><span class=nv>root</span><span class=si>}</span>/sbin
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Update the CLASSPATH</span>
</span></span><span class=line><span class=cl>prepend-path CLASSPATH <span class=si>${</span><span class=nv>root</span><span class=si>}</span>/jars/*
</span></span></code></pre></td></tr></table></div></div><h3 id=4-使用-pip-安装-pyspark-库>4. 使用 pip 安装 pyspark 库</h3><ul><li>创建虚拟 Conda 环境 pyspark</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>conda create -n pyspark <span class=nv>python</span><span class=o>=</span>3.10
</span></span></code></pre></td></tr></table></div></div><ul><li>安装 pyspark</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>conda activate pyspark
</span></span><span class=line><span class=cl>pip install pyspark
</span></span></code></pre></td></tr></table></div></div><h3 id=5-编写环境加载脚本-set-spark-envsh>5. 编写环境加载脚本 set-spark-env.sh</h3><ul><li>在 <code>${HOME}/scripts</code> 目录下编写 <code>set-spark-env.sh</code> 脚本文件：</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=cp>#!/bin/bash
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=nb>source</span> /etc/profile
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 注意！这里需要修改为你的 Conda 的安装路径</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>CONDA_PATH</span><span class=o>=</span>&lt;PATH/WHERE/CONDA/DIRECTORY/IS&gt;
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>PATH</span><span class=o>=</span><span class=nv>$CONDA_PATH</span>/bin:<span class=nv>$PATH</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>MODULEPATH</span><span class=o>=</span><span class=si>${</span><span class=nv>HOME</span><span class=si>}</span>/modulefiles:<span class=nv>$MODULEPATH</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>source</span> activate
</span></span><span class=line><span class=cl>conda activate pyspark
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>module load openjdk
</span></span><span class=line><span class=cl>module load spark
</span></span></code></pre></td></tr></table></div></div><h3 id=6-编写-sbatch-脚本>6. 编写 sbatch 脚本</h3><ul><li>为了启动 Spark 集群，我们使用以下 Slurm 脚本来请求计算节点。Slurm 脚本请求四个节点，并生成一个 master 节点和三个 worker 节点的 Spark 集群。可以通过更改 Slurm 脚本中的 <code>-N</code> 选项的值来增加或减少工作节点的数量。</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=cp>#!/bin/bash
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=c1>#SBATCH --export=ALL</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH --mem=0</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH -p C28M250G</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH -t 1:00:00</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH -N 4</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH -J spark_test</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH -o o.spark_test</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH -e e.spark_test</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>source</span> ~/scripts/set-spark-env.sh
</span></span><span class=line><span class=cl><span class=nv>workdir</span><span class=o>=</span><span class=sb>`</span><span class=nb>pwd</span><span class=sb>`</span>
</span></span><span class=line><span class=cl><span class=nv>nodes</span><span class=o>=(</span><span class=k>$(</span>scontrol show hostnames <span class=si>${</span><span class=nv>SLURM_JOB_NODELIST</span><span class=si>}</span> <span class=p>|</span> sort <span class=p>|</span> uniq <span class=k>)</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=nv>numnodes</span><span class=o>=</span><span class=si>${#</span><span class=nv>nodes</span><span class=p>[@]</span><span class=si>}</span>
</span></span><span class=line><span class=cl><span class=nv>last</span><span class=o>=</span><span class=k>$((</span> <span class=nv>$numnodes</span> <span class=o>-</span> <span class=m>1</span> <span class=k>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>SCRATCH</span><span class=o>=</span><span class=si>${</span><span class=nv>workdir</span><span class=si>}</span>/scratch
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nv>master</span><span class=o>=</span><span class=si>${</span><span class=nv>nodes</span><span class=p>[0]</span><span class=si>}</span>
</span></span><span class=line><span class=cl><span class=nv>masterurl</span><span class=o>=</span><span class=s2>&#34;spark://</span><span class=si>${</span><span class=nv>master</span><span class=si>}</span><span class=s2>:7077&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>ssh <span class=si>${</span><span class=nv>nodes</span><span class=p>[0]</span><span class=si>}</span> <span class=s2>&#34;source ~/scripts/set-spark-env.sh; start-master.sh&#34;</span>
</span></span><span class=line><span class=cl><span class=k>for</span> i in <span class=k>$(</span> seq <span class=m>1</span> <span class=nv>$last</span> <span class=k>)</span>
</span></span><span class=line><span class=cl><span class=k>do</span>
</span></span><span class=line><span class=cl>    ssh <span class=si>${</span><span class=nv>nodes</span><span class=p>[</span><span class=nv>$i</span><span class=p>]</span><span class=si>}</span> <span class=s2>&#34;source ~/scripts/set-spark-env.sh; start-worker.sh </span><span class=si>${</span><span class=nv>masterurl</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl><span class=k>done</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>ssh <span class=si>${</span><span class=nv>nodes</span><span class=p>[0]</span><span class=si>}</span> <span class=s2>&#34;cd </span><span class=si>${</span><span class=nv>workdir</span><span class=si>}</span><span class=s2>; source ~/scripts/set-spark-env.sh; /usr/bin/time -v spark-submit --deploy-mode client --executor-cores 28 --executor-memory 240G --conf spark.standalone.submit.waitAppCompletion=true --master </span><span class=nv>$masterurl</span><span class=s2> spark_test.py&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>wait</span>
</span></span><span class=line><span class=cl><span class=nb>echo</span> <span class=s1>&#39;end&#39;</span>
</span></span><span class=line><span class=cl><span class=nb>exit</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>该 Slurm 脚本会提交一个用于测试的 python 脚本（ <code>spark_test.py</code> ），内容如下。此脚本运行 PySpark 代码来测试 Spark 集群。复制下面的内容，并将其保存在 sbatch 脚本所在目录中的 <code>spark_test.py</code> 文件。你也可以更改 <code>spark_test.py</code> 文件的路径，但必须适当地更新 Slurm 脚本。</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=c1>#spark_test.py</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>random</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql</span> <span class=kn>import</span> <span class=n>SparkSession</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pyspark.sql.functions</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span><span class=o>.</span><span class=n>appName</span><span class=p>(</span><span class=s1>&#39;Test-app&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#Generate sample dataset</span>
</span></span><span class=line><span class=cl><span class=n>cola_list</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;2022-01-01&#39;</span><span class=p>,</span> <span class=s1>&#39;2022-01-02&#39;</span><span class=p>,</span> <span class=s1>&#39;2022-01-03&#39;</span> <span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>colb_list</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;CSC&#39;</span><span class=p>,</span> <span class=s1>&#39;PHY&#39;</span><span class=p>,</span> <span class=s1>&#39;MAT&#39;</span><span class=p>,</span> <span class=s1>&#39;ENG&#39;</span><span class=p>,</span> <span class=s1>&#39;CHE&#39;</span><span class=p>,</span> <span class=s1>&#39;ENV&#39;</span><span class=p>,</span> <span class=s1>&#39;BIO&#39;</span><span class=p>,</span> <span class=s1>&#39;PHRM&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>colc_list</span> <span class=o>=</span> <span class=p>[</span><span class=mi>100</span><span class=p>,</span> <span class=mi>200</span><span class=p>,</span> <span class=mi>300</span><span class=p>,</span> <span class=mi>400</span><span class=p>,</span> <span class=mi>500</span><span class=p>,</span> <span class=mi>600</span><span class=p>,</span> <span class=mi>700</span><span class=p>,</span> <span class=mi>800</span><span class=p>,</span> <span class=mi>900</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># declaring a random.seed value to generate same data in every run</span>
</span></span><span class=line><span class=cl><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>sample_data</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>idx</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1000</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>sample_data</span><span class=o>.</span><span class=n>append</span><span class=p>([</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span><span class=n>cola_list</span><span class=p>),</span> <span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span><span class=n>colb_list</span><span class=p>),</span> <span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span><span class=n>colc_list</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>columns</span><span class=o>=</span> <span class=p>[</span><span class=s2>&#34;date&#34;</span><span class=p>,</span> <span class=s2>&#34;org&#34;</span><span class=p>,</span> <span class=s2>&#34;value&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=c1>#creating a Spark dataframe</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>createDataFrame</span><span class=p>(</span><span class=n>data</span> <span class=o>=</span> <span class=n>sample_data</span><span class=p>,</span> <span class=n>schema</span> <span class=o>=</span> <span class=n>columns</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>res</span> <span class=o>=</span> <span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s1>&#39;date&#39;</span><span class=p>,</span><span class=s1>&#39;org&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=o>.</span><span class=n>agg</span><span class=p>(</span><span class=n>F</span><span class=o>.</span><span class=n>count</span><span class=p>(</span><span class=s1>&#39;value&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s1>&#39;count_value&#39;</span><span class=p>)))</span>
</span></span><span class=line><span class=cl><span class=n>res</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>如果启动了 Spark 集群并且 <code>spark-test.py</code> 成功执行，那么日志文件 <code>o.spark_test</code> 中的输出应该如下：</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>starting org.apache.spark.deploy.master.Master, logging to ...
</span></span><span class=line><span class=cl>starting org.apache.spark.deploy.worker.Worker, logging to ...
</span></span><span class=line><span class=cl>starting org.apache.spark.deploy.worker.Worker, logging to ...
</span></span><span class=line><span class=cl>starting org.apache.spark.deploy.worker.Worker, logging to ...
</span></span><span class=line><span class=cl>+----------+----+-----------+
</span></span><span class=line><span class=cl>|      date| org|count_value|
</span></span><span class=line><span class=cl>+----------+----+-----------+
</span></span><span class=line><span class=cl>|2022-01-03| BIO|         37|
</span></span><span class=line><span class=cl>|2022-01-02| ENV|         53|
</span></span><span class=line><span class=cl>|2022-01-03| CHE|         39|
</span></span><span class=line><span class=cl>|2022-01-03| PHY|         46|
</span></span><span class=line><span class=cl>|2022-01-01| CSC|         45|
</span></span><span class=line><span class=cl>|2022-01-03| CSC|         48|
</span></span><span class=line><span class=cl>|2022-01-01| BIO|         39|
</span></span><span class=line><span class=cl>|2022-01-01| MAT|         42|
</span></span><span class=line><span class=cl>|2022-01-02| CHE|         44|
</span></span><span class=line><span class=cl>|2022-01-03| ENV|         33|
</span></span><span class=line><span class=cl>|2022-01-01| ENG|         33|
</span></span><span class=line><span class=cl>|2022-01-02| ENG|         28|
</span></span><span class=line><span class=cl>|2022-01-01| ENV|         33|
</span></span><span class=line><span class=cl>|2022-01-02| CSC|         45|
</span></span><span class=line><span class=cl>|2022-01-02| MAT|         51|
</span></span><span class=line><span class=cl>|2022-01-01| PHY|         38|
</span></span><span class=line><span class=cl>|2022-01-01|PHRM|         40|
</span></span><span class=line><span class=cl>|2022-01-03|PHRM|         42|
</span></span><span class=line><span class=cl>|2022-01-02|PHRM|         43|
</span></span><span class=line><span class=cl>|2022-01-03| ENG|         56|
</span></span><span class=line><span class=cl>+----------+----+-----------+
</span></span><span class=line><span class=cl>only showing top 20 rows
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>end
</span></span></code></pre></td></tr></table></div></div><ul><li>Spark 还提供了一个 web UI 来监控集群，您可以通过将 master 节点端口转发到本地机器来在本地机器上访问它。<ul><li>例如，如果 master 节点在 <code>cpu1</code> 上运行，则可以在本地计算机终端上运行以下代码。</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cmd data-lang=cmd><span class=line><span class=cl>  ssh -t -t  <span class=p>&lt;</span>USERNAME<span class=p>&gt;</span>@<span class=p>&lt;</span>LOGIN_NODE_IP<span class=p>&gt;</span> -L 8080:localhost:8080 \
</span></span><span class=line><span class=cl>  -i <span class=p>&lt;</span>PRIVATE_KEY_LOCATION<span class=p>&gt;</span> ssh cpu1  -L 8080:127.0.0.1:8080
</span></span></code></pre></td></tr></table></div></div><ul><li>然后就可以在本地机器上的 Web 浏览器上使用地址 <a class=link href=http://localhost:8080/ target=_blank rel=noopener>http://localhost:8080/</a> 访问 Spark Web UI。</li></ul></li></ul><h2 id=三总结>三、总结</h2><p>在本文中，我们介绍了如何在 HPC 集群上部署和运行 Apache Spark 集群。通过遵循本指南中的步骤，你应该能够成功地在 HPC 环境中运行 Spark 作业。请注意，根据你的具体 HPC 环境和配置，可能需要进行一些调整。</p><div class="notice notice-note"><div class=notice-title><svg xmlns="http://www.w3.org/2000/svg" class="icon notice-icon" viewBox="0 0 512 512"><path d="M504 256A248 248 0 118 256a248 248 0 01496 0zm-248 50a46 46 0 100 92 46 46 0 000-92zm-44-165 8 136c0 6 5 11 12 11h48c7 0 12-5 12-11l8-136c0-7-5-13-12-13h-64c-7 0-12 6-12 13z"/></svg></div><p><a class=link href=https://spark.apache.org/docs/latest/ target=_blank rel=noopener>Spark 官方文档</a> 是一个非常有用的工具，通过它可以帮助你找到 Spark 的具体说明并解决问题。所以实际遇到问题时要多使用它。</p></div></section><footer class=article-footer><section class=article-tags><a href=/tags/spark/>Spark</a>
<a href=/tags/distributed-computing/>分布式计算</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.bootcdn.net/ajax/libs/KaTeX/0.15.6/katex.min.css integrity="sha256-J+iAE0sgH8QSz9hpcDxXIftnj65JEZgNhGcgReTTK9s=" crossorigin=anonymous><script src=https://cdn.bootcdn.net/ajax/libs/KaTeX/0.15.6/katex.min.js integrity="sha256-InsNdER1b2xUewP+pKCUJpkhiqwHgqiPXDlIk7GzBu4=" crossorigin=anonymous defer></script><script src=https://cdn.bootcdn.net/ajax/libs/KaTeX/0.15.6/contrib/auto-render.min.js integrity="sha256-y39Mpg7V3D4lhBX4x6O0bUqTV4pSrfgwEfGKfxkOdgI=" crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.querySelector(`.article-content`),{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/spark-on-k8s/><div class=article-image><img src=https://cuterwrite-1302252842.file.myqcloud.com/blog/92.webp loading=lazy data-key=spark-on-k8s data-hash=https://cuterwrite-1302252842.file.myqcloud.com/blog/92.webp></div><div class=article-details><h2 class=article-title>基于 Spark on k8s 的词频统计实验</h2></div></a></article><article class=has-image><a href=/p/flink-native-k8s/><div class=article-image><img src=https://cuterwrite-1302252842.file.myqcloud.com/blog/YSFD_P2_50.webp loading=lazy data-key=flink-native-k8s data-hash=https://cuterwrite-1302252842.file.myqcloud.com/blog/YSFD_P2_50.webp></div><div class=article-details><h2 class=article-title>基于 Flink Native Kubernetes 的词频统计实验</h2></div></a></article><article class=has-image><a href=/p/rdma-overview/><div class=article-image><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/crop_65b36f302c1d3715061e824224dcc9ca195413.jpg@1256w_1806h_!web-article-pic-2024-01-14.webp loading=lazy data-key=rdma-overview data-hash=https://cuterwrite-1302252842.file.myqcloud.com/img/crop_65b36f302c1d3715061e824224dcc9ca195413.jpg@1256w_1806h_!web-article-pic-2024-01-14.webp></div><div class=article-details><h2 class=article-title>RDMA 概述</h2></div></a></article><article class=has-image><a href=/p/false-sharing/><div class=article-image><img src=https://cuterwrite-1302252842.file.myqcloud.com/blog/6c3f8961290e41f894f5a1cbb768aba9-2023-12-02.webp loading=lazy data-key=false-sharing data-hash=https://cuterwrite-1302252842.file.myqcloud.com/blog/6c3f8961290e41f894f5a1cbb768aba9-2023-12-02.webp></div><div class=article-details><h2 class=article-title>性能刺客之伪共享</h2></div></a></article><article class=has-image><a href=/p/nest-on-hpe-install/><div class=article-image><img src=https://cuterwrite-1302252842.file.myqcloud.com/blog/20231031002508-2023-10-31.webp loading=lazy data-key=nest-on-hpe-install data-hash=https://cuterwrite-1302252842.file.myqcloud.com/blog/20231031002508-2023-10-31.webp></div><div class=article-details><h2 class=article-title>NEST on HPC 安装教程</h2></div></a></article></div></div></aside><script>(function(){if(/^localhost|^127/.test(location.hostname))return;let e=document.createElement("script");e.src="https://giscus.app/client.js",e.dataset.repo="PKUcoldkeyboard/pkucoldkeyboard.github.io",e.dataset.repoId="MDEwOlJlcG9zaXRvcnkzMzU4NzI5OTI=",e.dataset.category="Comments",e.dataset.categoryId="DIC_kwDOFAUD4M4CZV4F",e.dataset.mapping="title",e.dataset.strict="0",e.dataset.reactionsEnabled="1",e.dataset.emitMetadata="0",e.dataset.inputPosition="top",e.dataset.theme="light",e.dataset.lang="zh-CN",e.dataset.loading="lazy",e.crossOrigin="anonymous",e.async=!0;let t=document.querySelector(".main"),n=t.childNodes[t.childNodes.length-1];t.insertBefore(e,n.nextSibling)})();function setGiscusTheme(e){let t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:{setConfig:{theme:e}}},"https://giscus.app")}(function(){if(/^localhost|^127/.test(location.hostname))return;addEventListener("message",t=>{if(event.origin!=="https://giscus.app")return;e()}),window.addEventListener("onColorSchemeChange",e);function e(){setGiscusTheme(document.documentElement.dataset.scheme==="light"?"light":"dark_dimmed")}})()</script><footer class=site-footer><section class=copyright>&copy;
2021 -
2024 cuterwrite</section><section class=powerby>欢迎来到Cuterwrite的博客网站<br>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.17.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.bootcdn.net/ajax/libs/photoswipe/4.1.3/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.bootcdn.net/ajax/libs/photoswipe/4.1.3/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.bootcdn.net/ajax/libs/photoswipe/4.1.3/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.bootcdn.net/ajax/libs/photoswipe/4.1.3/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://unpkg.com/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>