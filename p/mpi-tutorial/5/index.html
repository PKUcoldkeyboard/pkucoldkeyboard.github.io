<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="MPI 论坛在 1994 年推出的 MPI 基础上，根据 MPI 的发展和要求，于 1997 年推出了 MPI 的最新版本 MPI-2，同时原来的 MPI 更名为 MPI-1。相对于 MPI-1，MPI-2 加入了许多新特性：动态进程管理、I/O、远程存储访问等。本文将对 MPI-2 的新特性进行简要介绍。"><title>MPI 与并行计算（五）：MPI 扩展</title>
<link rel=canonical href=https://cuterwrite.top/p/mpi-tutorial/5/><link rel=stylesheet href=/scss/style.min.2b724d31a9334eb879a7204b807f915cfebe5daa80340d704caa98b8da1011cf.css><meta property='og:title' content="MPI 与并行计算（五）：MPI 扩展"><meta property='og:description' content="MPI 论坛在 1994 年推出的 MPI 基础上，根据 MPI 的发展和要求，于 1997 年推出了 MPI 的最新版本 MPI-2，同时原来的 MPI 更名为 MPI-1。相对于 MPI-1，MPI-2 加入了许多新特性：动态进程管理、I/O、远程存储访问等。本文将对 MPI-2 的新特性进行简要介绍。"><meta property='og:url' content='https://cuterwrite.top/p/mpi-tutorial/5/'><meta property='og:site_name' content="Cuterwrite's Blog"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='MPI'><meta property='article:tag' content='并行计算'><meta property='article:published_time' content='2023-07-20T03:00:00+00:00'><meta property='article:modified_time' content='2023-07-20T03:00:00+00:00'><meta property='og:image' content='https://cuterwrite-1302252842.file.myqcloud.com/img/9a5806864623b04c918b9d8bee35c49fc2790c52.jpg@1256w_828h_!web-article-pic.avif'><meta name=twitter:title content="MPI 与并行计算（五）：MPI 扩展"><meta name=twitter:description content="MPI 论坛在 1994 年推出的 MPI 基础上，根据 MPI 的发展和要求，于 1997 年推出了 MPI 的最新版本 MPI-2，同时原来的 MPI 更名为 MPI-1。相对于 MPI-1，MPI-2 加入了许多新特性：动态进程管理、I/O、远程存储访问等。本文将对 MPI-2 的新特性进行简要介绍。"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://cuterwrite-1302252842.file.myqcloud.com/img/9a5806864623b04c918b9d8bee35c49fc2790c52.jpg@1256w_828h_!web-article-pic.avif'><link rel="shortcut icon" href=/favicon.ico><script async src=https://analytics.cuterwrite.top/uma data-website-id=b13594a2-4d15-4a4e-a020-5e3cc1d88c12 data-domains=cuterwrite.top></script><link rel=manifest href=/manifest.json></head><body class="article-page
line-numbers"><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hue4d14694a57c01a222a16c47db12c89c_369633_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>😉</span></figure><div class=site-meta><h1 class=site-name><a href=/>Cuterwrite's Blog</a></h1><h2 class=site-description>Cuterwrite 的技术博客, 专注于高性能计算、操作系统、全栈开发、人工智能等领域的深度探讨和经验分享。</h2></div></header><ol class=menu-social><li><a href=https://analytics.cuterwrite.top/share/s26R5QvVpieSNLkk/cutewrite.top target=_blank title=Analytics rel=me><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 5H7A2 2 0 005 7v12a2 2 0 002 2h10a2 2 0 002-2V7a2 2 0 00-2-2h-2"/><rect x="9" y="3" width="6" height="4" rx="2"/><path d="M9 17v-5"/><path d="M12 17v-1"/><path d="M15 17v-3"/></svg></a></li><li><a href=https://status.cuterwrite.top target=_blank title=Upptime rel=me><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-chart-line"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 19h16"/><path d="M4 15l4-6 4 2 4-5 4 4"/></svg></a></li><li><a href=/index.xml target=_blank title=RSS rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-rss" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="5" cy="19" r="1"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg></a></li><li><a href=https://github.com/PKUcoldkeyboard target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.zhihu.com/people/kong-tiao-cheng-tai-lang-30-57 target=_blank title=zhihu rel=me><svg t="1705591931290" class="icon" viewBox="0 0 1280 1024" xmlns="http://www.w3.org/2000/svg" p-id="21048" width="32" height="32"><path d="M341.08 296.26v435.08l46.86.02 15.42 52.74 84.02-52.74h99.06V296.26H341.08zm195.5 387.86H480.7l-55.8 35.02-10.16-34.94-23.8-.08V343.5h145.64v340.62zM299.66 495.34H195c3.48-54.2 4.4-103.18 4.4-146.92h102.32s3.94-45.12-17.16-44.62h-177c6.98-26.24 15.74-53.32 26.24-81.34.0.0-48.14.0-64.54 43.14-6.78 17.8-26.42 86.28-61.4 156.24 11.78-1.28 50.74-2.36 73.68-44.42 4.22-11.78 5.02-13.32 10.28-29.06h57.74c0 21-2.4 133.76-3.36 146.88H41.66c-23.48.0-31.12 47.24-31.12 47.24H141.7C132.9 642.2 85.66 726.24.0 792.68c40.98 11.7 81.82-1.86 102-19.8.0.0 45.96-41.8 71.18-138.5L281.1 764.26s15.82-53.78-2.48-79.98c-15.16-17.84-56.12-66.12-73.58-83.62L175.8 623.9c8.72-27.96 13.98-55.1 15.74-81.34h123.3s-.18-47.24-15.18-47.24v.02zm824.04-3.2c41.66-51.28 89.96-117.14 89.96-117.14s-37.3-29.6-54.76-8.12c-12 16.3-73.66 96.4-73.66 96.4l38.46 28.86zM823.52 373.96c-18.02-16.5-51.82 4.26-51.82 4.26s79.04 110.08 82.24 114.9l38.92-27.46s-51.34-75.22-69.32-91.72h-.02zM1280 516.7c-39.56.0-261.82 1.86-262.12 1.86v-202c9.62.0 24.84-.8 45.7-2.4 81.76-4.82 140.26-8 175.54-9.62.0.0 24.44-54.38-1.18-66.88-6.14-2.36-46.34 9.16-46.34 9.16s-330.44 32.98-464.72 36.1c3.2 17.64 15.24 34.16 31.56 39.1 26.62 6.96 45.38 3.4 98.3 1.78 49.66-3.2 87.36-4.86 113.02-4.86v199.62H702.82s5.64 44.62 51.02 45.7h215.88V706.1c0 27.94-22.38 43.98-48.96 42.24-28.16.22-52.16-2.3-83.38-3.62 3.98 7.94 12.66 28.78 38.62 43.68 19.76 9.62 32.34 13.14 52.04 13.14 59.12.0 91.34-34.56 89.78-90.62V564.28h244.72c19.36.0 17.4-47.56 17.4-47.56l.06-.02z" fill="#707070" p-id="21049"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页 | Home</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>关于 | About</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>归档 | Archives</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>搜索 | Search</span></a></li><li><a href=https://cuterwrite.top/image-hosting target=_blank><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-album"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 4m0 2a2 2 0 012-2h12a2 2 0 012 2v12a2 2 0 01-2 2H6a2 2 0 01-2-2z"/><path d="M12 4v7l2-2 2 2V4"/></svg>
<span>图册 | Gallery</span></a></li><li><a href=https://draw.cuterwrite.top target=_blank><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-artboard"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M8 8m0 1a1 1 0 011-1h6a1 1 0 011 1v6a1 1 0 01-1 1H9a1 1 0 01-1-1z"/><path d="M3 8h1"/><path d="M3 16h1"/><path d="M8 3v1"/><path d="M16 3v1"/><path d="M20 8h1"/><path d="M20 16h1"/><path d="M8 20v1"/><path d="M16 20v1"/></svg>
<span>画板 | Canvas</span></a></li><li><a href=https://it-tools.tech target=_blank><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-tools"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 21h4L20 8a1.5 1.5.0 00-4-4L3 17v4"/><path d="M14.5 5.5l4 4"/><path d="M12 8 7 3 3 7l5 5"/><path d="M7 8 5.5 9.5"/><path d="M16 12l5 5-4 4-5-5"/><path d="M16 17l-1.5 1.5"/></svg>
<span>工具 | Tools</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ul><li><a href=#1-动态进程>1. 动态进程</a></li><li><a href=#2-远程存储访问remote-memory-accessrma>2. 远程存储访问（Remote Memory Access，RMA）</a></li><li><a href=#3-并行-iompi-io>3. 并行 I/O（MPI-IO）</a></li><li><a href=#4-正确地使用-mpi-io>4. 正确地使用 MPI-IO</a></li><li><a href=#5-总结>5. 总结</a></li></ul></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/mpi-tutorial/5/><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/9a5806864623b04c918b9d8bee35c49fc2790c52.jpg@1256w_828h_!web-article-pic.avif loading=lazy alt="Featured image of post MPI 与并行计算（五）：MPI 扩展"></a></div><div class=article-details><header class=article-category><a href=/categories/hpc/ style=background-color:#ffb900;color:#fff>高性能计算</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/mpi-tutorial/5/>MPI 与并行计算（五）：MPI 扩展</a></h2><h3 class=article-subtitle>MPI 论坛在 1994 年推出的 MPI 基础上，根据 MPI 的发展和要求，于 1997 年推出了 MPI 的最新版本 MPI-2，同时原来的 MPI 更名为 MPI-1。相对于 MPI-1，MPI-2 加入了许多新特性：动态进程管理、I/O、远程存储访问等。本文将对 MPI-2 的新特性进行简要介绍。</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>2023-07-20</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 9 分钟</time></div><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-keyboard"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M2 6m0 2a2 2 0 012-2h16a2 2 0 012 2v8a2 2 0 01-2 2H4a2 2 0 01-2-2z"/><path d="M6 10v.01"/><path d="M10 10v.01"/><path d="M14 10v.01"/><path d="M18 10v.01"/><path d="M6 14v.01"/><path d="M18 14v.01"/><path d="M10 14l4 .01"/></svg>
<time class=article-time--wordcount>字数统计: 4141 字</time></div></footer></div></header><section class=article-content><h1 id=mpi-与并行计算五mpi-扩展>MPI 与并行计算（五）：MPI 扩展</h1><h2 id=1-动态进程>1. 动态进程</h2><p>MPI-1 假定所有的进程都是静态的，运行时不能增加和删除进程。MPI-2 引入了动态进程的概念：</p><ul><li>MPI-1 不定义如何创建进程和如何建立通信。MPI-2 中的动态进程机制以可移植的方式(平台独立)提供了这种能力。</li><li>动态进程有利于将 PVM 程序移植到 MPI 上。并且还可能支持一些重要的应用类型， 如 Client/Server 和 Process farm。</li><li>动态进程允许更有效地使用资源和负载平衡。例如，所用节点数可以按需要减少和增加。</li><li>支持容错。当一个节点失效时，可以在另一个节点上创建一个新进程运行该节点上的进程的工作。</li></ul><p>在 MPI-1 中 一个 MPI 程序一旦启动，一直到该 MPI 程序结束，进程的个数是固定的，在程序运行过程中是不可能动态改变的。在 MPI-2 中，允许在程序运行过程中动态改变进程的数目，并提供了动态进程创建和管理的各种调用。</p><p><strong>组间通信域</strong>在动态进程管理中处于核心的地位，只有掌握了它的基本概念，才能准确把握和使用进程的动态特性和动态进程之间的通信。</p><p>在 MPI-2 中，对点到点通信和组通信都给出了使用组间通信域时的确切含义。在语法上，不管是使用组内还是组间通信域，二者没有任何区别，但其语义是不同的。</p><ol><li>对于构成组间通信域的两个进程组，调用进程把自己所在的组看作是本地组，而把另一个组称为远地组，使用组间通信域的一个特点是本地组进程发送的数据被远地组进程接收而本地组接收的数据必然来自远地组。</li><li>在使用组间通信域的点到点通信中，发送语句指定的目的进程是远地组中的进程编号，接收进程指出的源进程编号也是远地组的进程编号。</li></ol><ul><li>如图所示为组间通信域上的点到点通信<figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/blog/20230720171153.webp alt=20230720171153 width=90% loading=lazy></figure></li></ul><p>对于组通信，如果使用组间通信域，则其含义分不同的形式而有所不同：对于多对多通信，本地进程组的所有进程向远地进程组的所有进程发送数据，同时本地进程组的所有进程从远地进程组的所有进程接收数据，如图所示：</p><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/blog/20230720171308.webp alt=20230720171308 width=90% loading=lazy></figure><ul><li>此外，组间通信域上的一对多通信或多对一通信如图所示：<figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/blog/20230720171356.webp alt=20230720171356 width=90% loading=lazy></figure></li></ul><div class="notice notice-note"><div class=notice-title><svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200"><path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667L423.381333 264.576C281.088 341.546667 253.269333 441.429333 242.176 504.405333c22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864A149.333333 149.333333.0 01312.789333 789.333333a165.162667 165.162667.0 01-117.248-50.304zm426.666667.0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667L850.048 264.576c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864A149.333333 149.333333.0 01739.456 789.333333a165.162667 165.162667.0 01-117.248-50.304z" p-id="23142" fill="#fff"/></svg></div><p>示例 1：动态进程的创建和通信</p></div><pre><code class=language-c>// dynamic.c
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;mpi.h&gt;

int main(int argc, char *argv[])
{
    int rank, size, color, new_rank, new_size;
    MPI_Comm new_comm;
    MPI_Init(&amp;argc, &amp;argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);
    MPI_Comm_size(MPI_COMM_WORLD, &amp;size);
    color = rank / 2; // 0, 0, 1, 1, 2, 2, 3, 3
    MPI_Comm_split(MPI_COMM_WORLD, color, rank, &amp;new_comm);
    MPI_Comm_rank(new_comm, &amp;new_rank);
    MPI_Comm_size(new_comm, &amp;new_size);
    printf(&quot;rank = %d, size = %d, new_rank = %d, new_size = %d\n&quot;, rank, size, new_rank, new_size);
    MPI_Finalize();
    return 0;
}
</code></pre><ul><li>在 16 个进程中，每两个进程一组，共 8 组，每组的进程编号相同，运行结果如下：</li></ul><pre><code class=language-text>root@ubuntu:~# mpicc dynamic.c -o dynamic
root@ubuntu:~# mpirun -n 16 ./dynamic
rank = 0, size = 16, new_rank = 0, new_size = 2
rank = 1, size = 16, new_rank = 1, new_size = 2
rank = 2, size = 16, new_rank = 0, new_size = 2
rank = 3, size = 16, new_rank = 1, new_size = 2
rank = 4, size = 16, new_rank = 0, new_size = 2
rank = 5, size = 16, new_rank = 1, new_size = 2
rank = 6, size = 16, new_rank = 0, new_size = 2
rank = 7, size = 16, new_rank = 1, new_size = 2
rank = 8, size = 16, new_rank = 0, new_size = 2
rank = 9, size = 16, new_rank = 1, new_size = 2
rank = 10, size = 16, new_rank = 0, new_size = 2
rank = 11, size = 16, new_rank = 1, new_size = 2
rank = 12, size = 16, new_rank = 0, new_size = 2
rank = 13, size = 16, new_rank = 1, new_size = 2
rank = 14, size = 16, new_rank = 0, new_size = 2
rank = 15, size = 16, new_rank = 1, new_size = 2
</code></pre><div class="notice notice-note"><div class=notice-title><svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200"><path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667L423.381333 264.576C281.088 341.546667 253.269333 441.429333 242.176 504.405333c22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864A149.333333 149.333333.0 01312.789333 789.333333a165.162667 165.162667.0 01-117.248-50.304zm426.666667.0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667L850.048 264.576c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864A149.333333 149.333333.0 01739.456 789.333333a165.162667 165.162667.0 01-117.248-50.304z" p-id="23142" fill="#fff"/></svg></div><p>示例 2：更复杂的动态进程的创建和通信</p></div><pre><code class=language-cpp>#include &lt;mpi.h&gt;
#include &lt;cmath&gt;
#include &lt;fstream&gt;
#include &lt;iostream&gt;

int world_rank, world_size;
MPI_Comm custom_comm1, custom_comm2, custom_comm3, tmp;

void splitting()
{
    int color;
    MPI_Comm *new_comm;

    // 1- First splitting here.
    // With only one call to MPI_Comm_split you should be able to split processes 0-3 in
    // custom_comm1 and processes 4-6 in custom_comm2
    color    = MPI_UNDEFINED;
    new_comm = &amp;tmp;

    if (world_rank &gt;= 0 &amp;&amp; world_rank &lt;= 3)
    {
        color    = 0;
        new_comm = &amp;custom_comm1;
    }

    if (world_rank &gt;= 4 &amp;&amp; world_rank &lt;= 6)
    {
        color    = 1;
        new_comm = &amp;custom_comm2;
    }

    MPI_Comm_split(MPI_COMM_WORLD, color, world_rank, new_comm);

    // 2- Second splitting here
    // Now put processes 0 and 4 in custom_comm3
    color    = MPI_UNDEFINED;
    new_comm = &amp;tmp;

    if (world_rank == 0 || world_rank == 4)
    {
        color    = 2;
        new_comm = &amp;custom_comm3;
    }

    MPI_Comm_split(MPI_COMM_WORLD, color, world_rank, new_comm);
}

int main(int argc, char **argv)
{
    MPI_Init(&amp;argc, &amp;argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);
    MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);

    splitting();

    if (world_rank &gt;= 0 &amp;&amp; world_rank &lt;= 3)
    {
        int row_rank;
        int row_size;
        MPI_Comm_rank(custom_comm1, &amp;row_rank);
        MPI_Comm_size(custom_comm1, &amp;row_size);
        std::cout &lt;&lt; &quot;custom_comm1: &quot; &lt;&lt; row_rank &lt;&lt; &quot;/&quot; &lt;&lt; row_size &lt;&lt; std::endl;
    }

    if (world_rank &gt;= 4 &amp;&amp; world_rank &lt;= 6)
    {
        int row_rank;
        int row_size;
        MPI_Comm_rank(custom_comm2, &amp;row_rank);
        MPI_Comm_size(custom_comm2, &amp;row_size);
        std::cout &lt;&lt; &quot;custom_comm2: &quot; &lt;&lt; row_rank &lt;&lt; &quot;/&quot; &lt;&lt; row_size &lt;&lt; std::endl;
    }

    if (world_rank == 0 || world_rank == 4)
    {
        int row_rank;
        int row_size;
        MPI_Comm_rank(custom_comm3, &amp;row_rank);
        MPI_Comm_size(custom_comm3, &amp;row_size);
        std::cout &lt;&lt; &quot;custom_comm3: &quot; &lt;&lt; row_rank &lt;&lt; &quot;/&quot; &lt;&lt; row_size &lt;&lt; std::endl;
    }

    MPI_Finalize();
    return 0;
}
</code></pre><h2 id=2-远程存储访问remote-memory-accessrma>2. 远程存储访问（Remote Memory Access，RMA）</h2><ul><li>在 MPI-2 中增加远程存储访问的能力，主要是为了使 MPI 在编写特定算法和通信模型的并行程序时更加自然和简洁。因为在许多情况下，都需要一个进程对另外一个进程的存储区域进行直接访问。</li><li>MPI-2 对远程存储的访问主要是通过<strong>窗口</strong>来进行的，为了进行远程存储访问，首先需要定义一个窗口，该窗口开在各个进程的一段本地进程存储空间，其目的是为了让其它的进程可以通过这一窗口来访问本地的数据。</li><li>定义好窗口之后，就可以通过窗口来访问远程存储区域的数据了。MPI-2 提供了三种基本的访问形式，即读、写和累计，读操作只是从远端的窗口获取数据，并不对远端数据进行任何修改；写操作将本地的内容写入远端的窗口，它修改远端窗口的内容；累计操作就更复杂一些，它将远端窗口的数据和本地的数据进行某种指定方式的运算之后，再将运算的结果写入远端窗口。</li><li>MPI-2 就是通过读、写和累计三种操作来实现对远程存储的访问和更新的。除了基本的窗口操作之外 MPI-2 还提供了窗口管理功能 用来实现对窗口操作的同步管理。MPI-2 对窗口的同步管理有三种方式 ：<ul><li>栅栏方式 fence：在这种方式下，对窗口的操作必须放在一对栅栏语句之间，这样可以保证当栅栏语句结束之后，其内部的窗口操作可以正确完成。</li><li>握手方式：在这种方式下，调用窗口操作的进程需要将具体的窗口调用操作放在以 MPI_WIN_START 开始，以 MPI_WIN_COMPLETE 结束的调用之间。相应的,被访问的远端进程需要以一对调用 MPI_WIN_POST 和 MPI_WIN_WAIT 与之相适应。MPI_WIN_POST 允许其它的进程对自己的窗口进行访问，而 MPI_WIN_WAIT 调用结束之后可以保证对本窗口的调用操作全部完成。MPI_WIN_START 申请对远端进程窗口的访问，<strong>只有当远端窗口执行了 MPI_WIN_POST 操作之后才可以访问远端窗口</strong>，MPI_WIN_COMPLETE 完成对远端窗口访问操作。</li><li>锁方式：在这种方式下，不同的进程通过对特定的窗口加锁来实现互斥访问。当然用户根据需要可以使用共享的锁，这是就可以允许使用共享锁的进程对同一窗口同时访问。远端存储的访问窗口是具体的实现形式，通过窗口操作实现来实现单边通信，通过对窗口的管理操作来实现对窗口操作的同步控制。</li></ul></li></ul><div class=table-wrapper><table><thead><tr><th style=text-align:left>窗口操作</th><th style=text-align:left>说明</th></tr></thead><tbody><tr><td style=text-align:left>MPI_Win_create</td><td style=text-align:left>创建窗口</td></tr><tr><td style=text-align:left>MPI_Win_free</td><td style=text-align:left>释放窗口</td></tr><tr><td style=text-align:left>MPI_Win_fence</td><td style=text-align:left>栅栏同步</td></tr><tr><td style=text-align:left>MPI_Win_start</td><td style=text-align:left>握手同步</td></tr><tr><td style=text-align:left>MPI_Win_complete</td><td style=text-align:left>握手同步</td></tr><tr><td style=text-align:left>MPI_Win_post</td><td style=text-align:left>握手同步</td></tr><tr><td style=text-align:left>MPI_Win_wait</td><td style=text-align:left>握手同步</td></tr><tr><td style=text-align:left>MPI_Win_lock</td><td style=text-align:left>锁同步</td></tr><tr><td style=text-align:left>MPI_Win_unlock</td><td style=text-align:left>锁同步</td></tr><tr><td style=text-align:left>MPI_Win_test</td><td style=text-align:left>锁同步</td></tr><tr><td style=text-align:left>MPI_Win_lock_all</td><td style=text-align:left>锁同步</td></tr><tr><td style=text-align:left>MPI_Win_unlock_all</td><td style=text-align:left>锁同步</td></tr><tr><td style=text-align:left>MPI_Win_flush</td><td style=text-align:left>锁同步</td></tr><tr><td style=text-align:left>MPI_Win_flush_all</td><td style=text-align:left>锁同步</td></tr><tr><td style=text-align:left>MPI_Win_flush_local</td><td style=text-align:left>锁同步</td></tr><tr><td style=text-align:left>MPI_Win_flush_local_all</td><td style=text-align:left>锁同步</td></tr><tr><td style=text-align:left>MPI_Win_shared_query</td><td style=text-align:left>查询窗口</td></tr></tbody></table></div><ul><li>小结：窗口是远程存储访问中的重要概念，其实 MPI-2 的远程存储访问就是各进程将自己的一部分内存区域开辟成其它所有进程都可以访问的窗口，从而使其它的进程实现对自己数据的远程访问，窗口操作是相对简单的，<strong>对窗口访问的同步控制</strong>是需要注意的问题。</li></ul><div class="notice notice-note"><div class=notice-title><svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200"><path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667L423.381333 264.576C281.088 341.546667 253.269333 441.429333 242.176 504.405333c22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864A149.333333 149.333333.0 01312.789333 789.333333a165.162667 165.162667.0 01-117.248-50.304zm426.666667.0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667L850.048 264.576c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864A149.333333 149.333333.0 01739.456 789.333333a165.162667 165.162667.0 01-117.248-50.304z" p-id="23142" fill="#fff"/></svg></div><p>示例 3：远程存储访问</p></div><pre><code class=language-c>// rma.c
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;mpi.h&gt;

int main(int argc, char *argv[])
{
    int rank, size, i, j, *buf, *winbuf;
    MPI_Win win;
    MPI_Init(&amp;argc, &amp;argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);
    MPI_Comm_size(MPI_COMM_WORLD, &amp;size);
    buf = (int *)malloc(size * sizeof(int));
    MPI_Win_create(buf, size * sizeof(int), sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &amp;win);
    for (i = 0; i &lt; size; i++)
        buf[i] = 0;
    MPI_Win_fence(0, win);
    if (rank == 0)
    {
        for (i = 0; i &lt; size; i++)
            buf[i] = i;
    }
    MPI_Win_fence(0, win);
    if (rank == 1)
    {
        for (i = 0; i &lt; size; i++)
            printf(&quot;buf[%d] = %d\n&quot;, i, buf[i]);
    }
    MPI_Win_free(&amp;win);
    MPI_Finalize();
    return 0;
}
</code></pre><h2 id=3-并行-iompi-io>3. 并行 I/O（MPI-IO）</h2><p>MPI-1 没有对并行文件 I/O 给出任何定义，原因在于并行 I/O 过于复杂，很难找到一个统一的标准。但是，I/O 是很多应用不可缺少的部分，MPI-2 在大量实践的基础上，提出了一个并行 I/O 的标准接口。MPI-2 提供的关于并行文件 I/O 的调用十分丰富，根据读写定位方法的不同，可以分为三种：</p><ol><li>指定显示的偏移：这种调用没有文件指针的概念 每次读写操作都必须明确指定读写文件的位置。</li><li>各进程拥有独立的文件指针：这种方式的文件操作不需要指定读写的位置每一个进程都有一个相互独立的文件指针，读写的起始位置就是当前指针的位置。读写完成后文件指针自动移到下一个有效数据的位置。这种方式的文件操作需要每一个进程都定义各自在文件中的<strong>文件视图（view）</strong>，文件视图（view）数据是文件连续或不连续的一部分，各个进程对文件视图（view）的操作就如同是对一个打开的独立的连续文件的操作一样。</li><li>共享文件指针：在这种情况下，每一个进程对文件的操作都是从当前共享文件指针的位置开始，操作结束后共享文件指针自动转移到下一个位置。共享指针位置的变化对所有进程都是可见的，各进程使用的是同一个文件指针。任何一个进程对文件的读写操作都会引起其它所有进程文件指针的改变。</li></ol><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/blog/20230720201633.webp alt=20230720201633 width=90% loading=lazy></figure><ul><li>MPI-IO 文件访问过程<ul><li>在进行 I/O 之前，必须要通过调用 MPI_File_open 打开文件</li><li>每个进程都需要定义文件指针用来控制文件访问</li><li>I/O 操作完成后，必须通过调用 MPI_File_close 来关闭文件</li></ul></li><li>并行文件的基本操作<ul><li>打开：<code>MPI_File_open(comm, filename, amode, info, fh)</code><ul><li>comm：组内通信域</li><li>filename：文件名</li><li>amode：打开模式</li><li>info：传递给运行时的信息</li><li>fh：返回的文件句柄<div class=table-wrapper><table><thead><tr><th style=text-align:left>文件访问模式</th><th style=text-align:left>含义</th></tr></thead><tbody><tr><td style=text-align:left>MPI_MODE_RDONLY</td><td style=text-align:left>只读</td></tr><tr><td style=text-align:left>MPI_MODE_RDWR</td><td style=text-align:left>读写</td></tr><tr><td style=text-align:left>MPI_MODE_WRONLY</td><td style=text-align:left>只写</td></tr><tr><td style=text-align:left>MPI_MODE_CREATE</td><td style=text-align:left>若文件不存在则创建</td></tr><tr><td style=text-align:left>MPI_MODE_EXCL</td><td style=text-align:left>创建不存在的新文件，若文件已存在则报错</td></tr><tr><td style=text-align:left>MPI_MODE_DELETE_ON_CLOSE</td><td style=text-align:left>关闭文件时删除文件</td></tr><tr><td style=text-align:left>MPI_MODE_UNIQUE_OPEN</td><td style=text-align:left>文件只能被一个进程打开</td></tr><tr><td style=text-align:left>MPI_MODE_SEQUENTIAL</td><td style=text-align:left>文件只能被顺序访问</td></tr><tr><td style=text-align:left>MPI_MODE_APPEND</td><td style=text-align:left>追加方式打开，初始文件指针指向文件末尾</td></tr></tbody></table></div></li></ul></li><li>关闭：<code>MPI_File_close(fh)</code><ul><li>fh：文件句柄</li></ul></li><li>删除：<code>MPI_File_delete(filename, info)</code><ul><li>filename：文件名</li><li>info：传递给运行时的信息</li></ul></li><li>修改文件大小：<code>MPI_File_set_size(fh, size)</code><ul><li>fh：文件句柄</li><li>size：新的文件大小(字节)</li></ul></li><li>查看文件大小：<code>MPI_File_get_size(fh, size)</code><ul><li>fh：文件句柄</li><li>size：文件大小(字节)</li></ul></li><li>预申请空间：<code>MPI_File_preallocate(fh, size)</code><ul><li>fh：文件句柄</li><li>size：预申请的空间大小(字节)</li></ul></li></ul></li></ul><div class="notice notice-note"><div class=notice-title><svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200"><path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667L423.381333 264.576C281.088 341.546667 253.269333 441.429333 242.176 504.405333c22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864A149.333333 149.333333.0 01312.789333 789.333333a165.162667 165.162667.0 01-117.248-50.304zm426.666667.0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667L850.048 264.576c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864A149.333333 149.333333.0 01739.456 789.333333a165.162667 165.162667.0 01-117.248-50.304z" p-id="23142" fill="#fff"/></svg></div><p>示例 4：并行 I/O - 指定显示偏移并行读</p></div><pre><code class=language-c>#include &lt;mpi.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;sys/stat.h&gt;
#include &lt;unistd.h&gt;
#include &lt;string.h&gt;

int main(int argc, char **argv)
{
    int rank, size;
    MPI_File fh;
    MPI_Status status;
    MPI_Init(&amp;argc, &amp;argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);
    MPI_Comm_size(MPI_COMM_WORLD, &amp;size);

    char *filename = &quot;testfile&quot;;
    struct stat st;
    stat(filename, &amp;st);
    int filesize = st.st_size;
    int bufsize  = filesize / size;

    MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &amp;fh);

    MPI_Offset offset = rank * bufsize;

    if (rank == size - 1)
    {
        bufsize += filesize % size;
    }
    char* buf = (char*)malloc(bufsize * sizeof(char));
    printf(&quot;Buf size: %d\n&quot;, bufsize);

    MPI_File_read_at(fh, offset, buf, bufsize, MPI_CHAR, &amp;status);

    printf(&quot;Process %d read: %s\n&quot;, rank, buf);

    MPI_File_close(&amp;fh);

    MPI_Finalize();
    free(buf);
    return 0;
}
</code></pre><ul><li>tesfile 文件内容为：</li></ul><pre><code class=language-text>ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz01234567891
</code></pre><ul><li>运行结果为：</li></ul><pre><code class=language-text>root@ubuntu:~# mpicc read.c -o read
root@ubuntu:~# mpirun -n 2 ./read
Buf size: 125
Process 1 read: ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz01234567891
Buf size: 124
Process 0 read: ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789
</code></pre><h2 id=4-正确地使用-mpi-io>4. 正确地使用 MPI-IO</h2><ul><li>正确使用 MPI-IO<ul><li>根据 I/O 需求，每个应用都有其特定的 I/O 访问模式</li><li>对于不同的 I/O 系统，同样的 I/O 访问模式也可以使用不同的 I/O 函数和 I/O 方式实现</li><li>通常 MPI-IO 中 I/O 访问模式的实现方式可分为 4 级：<strong>level0-level3</strong></li></ul></li><li>以分布式数组访问为例</li><li>level0：每个进程对本地数组的一行发出一个独立的读请求（就像在 unix 中一样）<pre><code class=language-c>MPI_File_open(..., file, ..., &amp;fh);
for (i = 0; i &lt; n_local_rows; i++)
{
   MPI_File_seek(fh, ...);
   MPI_File_read(fh, &amp;(A[i][0]), ...);
}
MPI_File_close(&amp;fh);
</code></pre></li><li>level1：类似于 level 0，但每个过程都使用集合 I/O 函数<pre><code class=language-c>MPI_File_open(MPI_COMM_WORLD, file, ...,&amp;fh);
for (i = 0; i &lt; n_local_rows; i++)
{
   MPI_File_seek(fh, ...);
   MPI_File_read_all(fh, &amp;(A[i][0]), ...);
}
MPI_File_close(&amp;fh);
</code></pre></li><li>level2：每个进程创建一个派生数据类型来描述非连续访问模式，定义一个文件视图，并调用独立的 I/O 函数<pre><code class=language-c>MPI_Type_create_subarray(...,&amp;subarray, ...);
MPI_Type_commit(&amp;subarray);
MPI_File_open(..., file, ..., &amp;fh);
MPI_File_set_view(fh, ..., subarray, ...);
MPI_File_read(fh, A, ...);
MPI_File_close(&amp;fh);
</code></pre></li><li>level3：类似于 level 0，但每个过程都使用集合 I/O 函数<pre><code class=language-c>MPI_Type_create_subarray(...,&amp;subarray, ...);
MPI_Type_commit(&amp;subarray);
MPI_File_open(MPI_COMM_WORLD, file,...,&amp;fh);
MPI_File_set_view(fh, ..., subarray, ...);
MPI_File_read_all(fh, A, ...);
MPI_File_close(&amp;fh);
</code></pre></li></ul><h2 id=5-总结>5. 总结</h2><ul><li>MPI-IO 有许多功能，可以帮助用户获得高性能 I/O<ul><li>支持非连续性访问<ul><li>派生数据类型</li><li>文件视图</li></ul></li><li>集合 I/O</li></ul></li><li>用户应该根据应用程序 I/O 特性来选择适合的 I/O 访问模式实现</li><li>同时，MPI-IO 不是实现并行 I/O 的唯一选择。目前已有一些更高级的库可代替 MPI-IO<ul><li>HDF5、netCDF&mldr;&mldr;</li><li>这些库都是基于 MPI-IO 实现</li></ul></li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/mpi/>MPI</a>
<a href=/tags/parallel-computing/>并行计算</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><script type=text/javascript src=/js/prism.js async></script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/openmpi-with-ucx/><div class=article-image><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/cropped_2024010204-2024-02-03.webp loading=lazy data-key=openmpi-with-ucx data-hash=https://cuterwrite-1302252842.file.myqcloud.com/img/cropped_2024010204-2024-02-03.webp></div><div class=article-details><h2 class=article-title>编译安装 UCX 1.15.0 与 OpenMPI 5.0.0：详尽指南</h2></div></a></article><article class=has-image><a href=/p/mpi-tutorial/4/><div class=article-image><img src=https://cuterwrite-1302252842.file.myqcloud.com/blog/20230720120242.webp loading=lazy data-key=mpi-tutorial/4 data-hash=https://cuterwrite-1302252842.file.myqcloud.com/blog/20230720120242.webp></div><div class=article-details><h2 class=article-title>MPI 与并行计算（四）：数据类型</h2></div></a></article><article class=has-image><a href=/p/mpi-tutorial/3/><div class=article-image><img src=https://cuterwrite-1302252842.file.myqcloud.com/blog/20230720002052.webp loading=lazy data-key=mpi-tutorial/3 data-hash=https://cuterwrite-1302252842.file.myqcloud.com/blog/20230720002052.webp></div><div class=article-details><h2 class=article-title>MPI 与并行计算（三）：集合通信</h2></div></a></article><article class=has-image><a href=/p/mpi-tutorial/2/><div class=article-image><img src=https://cuterwrite-1302252842.file.myqcloud.com/blog/20230719212254.webp loading=lazy data-key=mpi-tutorial/2 data-hash=https://cuterwrite-1302252842.file.myqcloud.com/blog/20230719212254.webp></div><div class=article-details><h2 class=article-title>MPI 与并行计算（二）：点到点通信</h2></div></a></article><article class=has-image><a href=/p/mpi-tutorial/1/><div class=article-image><img src=https://cuterwrite-1302252842.file.myqcloud.com/blog/20230719012753.webp loading=lazy data-key=mpi-tutorial/1 data-hash=https://cuterwrite-1302252842.file.myqcloud.com/blog/20230719012753.webp></div><div class=article-details><h2 class=article-title>MPI 与并行计算（一）：并行环境及编程模型</h2></div></a></article></div></div></aside><script src=https://cdn.bootcdn.net/ajax/libs/twikoo/1.6.20/twikoo.all.min.js></script><div id=tcomment></div><style>.twikoo{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}:root[data-scheme=dark]{--twikoo-body-text-color-main:rgba(255, 255, 255, 0.9);--twikoo-body-text-color:rgba(255, 255, 255, 0.7)}.twikoo .el-input-group__prepend,.twikoo .tk-action-icon,.twikoo .tk-time,.twikoo .tk-comments-count{color:var(--twikoo-body-text-color)}.twikoo .el-input__inner,.twikoo .el-textarea__inner,.twikoo .tk-preview-container,.twikoo .tk-content,.twikoo .tk-nick,.twikoo .tk-send{color:var(--twikoo-body-text-color-main)}.twikoo .el-button{color:var(--twikoo-body-text-color)!important}.OwO .OwO-body{background-color:var(--body-background)!important;color:var(--body-text-color)!important}</style><script>twikoo.init({envId:"https://comment.cuterwrite.top",el:"#tcomment",lang:"zh-CN"})</script><footer class=site-footer><section class=copyright>&copy;
2021 -
2024 cuterwrite</section><section class=running-time>本博客已稳定运行
<span id=runningdays class=running-days></span></section><section class=totalcount>发表了70篇文章 ·
总计302.03k字</section><section class=powerby>Welcome to cuterwrite's blog!<br>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.17.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计<br><span>基于 <a href=https://github.com/CaiJimmy/hugo-theme-stack/tree/v3.25.0 target=_blank rel=noopener><b style=color:#9e8f9f>v3.25.0</b></a> 分支版本修改</span><br></section></footer><script>let s1="2021-4-17";s1=new Date(s1.replace(/-/g,"/"));let s2=new Date,timeDifference=s2.getTime()-s1.getTime(),days=Math.floor(timeDifference/(1e3*60*60*24)),hours=Math.floor(timeDifference%(1e3*60*60*24)/(1e3*60*60)),minutes=Math.floor(timeDifference%(1e3*60*60)/(1e3*60)),result=days+"天"+hours+"小时"+minutes+"分钟";document.getElementById("runningdays").innerHTML=result</script><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.bootcdn.net/ajax/libs/photoswipe/4.1.3/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.bootcdn.net/ajax/libs/photoswipe/4.1.3/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.bootcdn.net/ajax/libs/photoswipe/4.1.3/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.bootcdn.net/ajax/libs/photoswipe/4.1.3/photoswipe.min.css crossorigin=anonymous></main></div><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.font.im/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script><meta name=apple-mobile-web-app-capable content="yes"><meta name=theme-color content="#ffffff"><script>"serviceWorker"in navigator&&window.addEventListener("load",()=>{navigator.serviceWorker.register("/sw.js").then(e=>{console.log("Service worker registered with scope: ",e.scope)},e=>{console.log("Service worker registration failed: ",e)})})</script></body></html>