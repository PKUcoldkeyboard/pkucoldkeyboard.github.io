<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>转载 on Cuterwrite's Blog</title><link>https://cuterwrite.top/tags/repost/</link><description>Recent content in 转载 on Cuterwrite's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>cuterwrite</copyright><lastBuildDate>Sat, 24 Feb 2024 03:09:01 +0000</lastBuildDate><atom:link href="https://cuterwrite.top/tags/repost/index.xml" rel="self" type="application/rss+xml"/><item><title>RDMA 操作类型</title><link>https://cuterwrite.top/p/rdma-op/</link><pubDate>Sat, 24 Feb 2024 03:09:01 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-op/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/bcb5351691a864a6827138cf4c2e0642195413_crop-2024-02-25.webp" alt="Featured image of post RDMA 操作类型" />&lt;h1 id="rdma-操作类型">RDMA 操作类型&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/142175657">&lt;cite>知乎专栏：4. RDMA 操作类型&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>前面几篇涉及 RDMA 的通信流程时一直在讲 SEND-RECV，然而它其实称不上是“RDMA”，只是一种加入了 0 拷贝和协议栈卸载的传统收发模型的“升级版”，这种操作类型没有完全发挥 RDMA 技术全部实力，常用于两端交换控制信息等场景。当涉及大量数据的收发时，更多使用的是两种 RDMA 独有的操作：WRITE 和 READ。&lt;/p>
&lt;p>我们先来复习下双端操作——SEND 和 RECV，然后再对比介绍单端操作——WRITE 和 READ。&lt;/p>
&lt;h2 id="send--recv">SEND &amp;amp; RECV&lt;/h2>
&lt;p>SEND 和 RECV 是两种不同的操作类型，但是因为如果一端进行 SEND 操作，对端必须进行 RECV 操作，所以通常都把他们放到一起描述。&lt;/p>
&lt;p>为什么称之为“双端操作”？因为&lt;strong>完成一次通信过程需要两端 CPU 的参与&lt;/strong>，并且收端需要提前显式的下发 WQE。下图是一次 SEND-RECV 操作的过程示意图。原图来自于[1]，我做了一些修改。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/rdma-op-1-2024-02-25.webp"
alt="rdma-op-1-2024-02-25" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>上一篇我们讲过，上层应用通过 WQE（WR）来给硬件下任务。在 SEND-RECV 操作中，不止发送端需要下发 WQE，接收端也需要下发 WQE 来告诉硬件收到的数据需要放到哪个地址。发送端并不知道发送的数据会放到哪里，每次发送数据，接收端都要提前准备好接收 Buffer，而接收端 CPU 自然会感知这一过程。&lt;/p>
&lt;p>为了下文对比 SEND/RECV 与 WRITE/READ 的异同，我们将上一篇的 SEND-RECV 流程中补充内存读写这一环节，即下图中的步骤④——发送端硬件根据 WQE 从内存中取出数据封装成可在链路上传输数据包和步骤⑦——接收端硬件将数据包解析后根据 WQE 将数据放到指定内存区域，其他步骤不再赘述。另外再次强调一下，收发端的步骤未必是图中这个顺序，比如步骤⑧⑪⑫和步骤⑨⑩的先后顺序就是不一定的。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/rdma-op-2-2024-02-25.webp"
alt="rdma-op-2-2024-02-25" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>下面将介绍 WRITE 操作，对比之后相信大家可以理解的更好。&lt;/p>
&lt;h2 id="write">WRITE&lt;/h2>
&lt;p>WRITE 全称是 RDMA WRITE 操作，是本端主动写入远端内存的行为，除了准备阶段，远端 CPU 不需要参与，也不感知何时有数据写入、数据在何时接收完毕。所以这是一种单端操作。&lt;/p>
&lt;p>通过下图我们对比一下 WRITE 和 SEND-RECV 操作的差异，本端在准备阶段通过数据交互，获取了对端某一片可用的内存的&lt;strong>地址&lt;/strong>和“&lt;strong>钥匙&lt;/strong>” ，相当于获得了这片远端内存的读写权限。拿到权限之后，本端就可以像访问自己的内存一样&lt;strong>直接对这一远端内存区域进行读写&lt;/strong>，这也是 RDMA——远程直接地址访问的内涵所在。&lt;/p>
&lt;p>WRITE/READ 操作中的目的地址和钥匙是如何获取的呢？通常可以通过我们刚刚讲过的 SEND-RECV 操作来完成，因为拿到钥匙这个过程总归是要由远端内存的控制者——CPU 允许的。虽然准备工作还比较复杂， 但是一旦完成准备工作，RDMA 就可以发挥其优势，对大量数据进行读写。一旦远端的 CPU 把内存授权给本端使用，它便不再会参与数据收发的过程，这就解放了远端 CPU，也降低了通信的时延。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/rdma-op-3-2024-02-25.webp"
alt="rdma-op-3-2024-02-25" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>需要注意的是，本端是通过&lt;strong>虚拟地址&lt;/strong>来读写远端内存的，上层应用可以非常方便的对其进行操作。实际的虚拟地址—物理地址的转换是由 RDMA 网卡完成的。具体是如何转换的，将在后面的文章介绍。&lt;/p>
&lt;p>忽略准备阶段 key 和 addr 的获取过程，下面我们描述一次 WRITE 操作的流程，此后我们不再将本端和对端称为“发送”和“接收”端，而是改为“请求”和“响应”端，这样对于描述 WRITE 和 READ 操作都更恰当一些，也不容易产生歧义。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/rdma-op-4-2024-02-25.webp"
alt="rdma-op-4-2024-02-25" width="auto" loading="lazy"/>
&lt;/figure>
&lt;ol>
&lt;li>请求端 APP 以 WQE（WR）的形式下发一次 WRITE 任务。&lt;/li>
&lt;li>请求端硬件从 SQ 中取出 WQE，解析信息。&lt;/li>
&lt;li>请求端网卡根据 WQE 中的虚拟地址，转换得到物理地址，然后从内存中拿到待发送数据，组装数据包。&lt;/li>
&lt;li>请求端网卡将数据包通过物理链路发送给响应端网卡。&lt;/li>
&lt;li>响应端收到数据包，解析目的虚拟地址，转换成本地物理地址，解析数据，将数据放置到指定内存区域。&lt;/li>
&lt;li>响应端回复 ACK 报文给请求端。&lt;/li>
&lt;li>请求端网卡收到 ACK 后，生成 CQE，放置到 CQ 中。&lt;/li>
&lt;li>请求端 APP 取得任务完成信息。&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>注：严谨地说，第 6 步回复 ACK 之时，RDMA 网卡只能保证数据包中的 Payload 已经被”暂存“了下来，但不能保证一定已经把数据放到目的内存里面了。不过这一点不影响我们对整理流程的理解，感谢@nekomii 同学的提醒。&lt;/p>
&lt;p>IB Spec. 9.7.5.1.6 ACKNOWLEDGE MESSAGE SCHEDULING 原文：”For SEND or RDMA WRITE requests, an ACK may be scheduled before data is actually written into the responder’s memory. The ACK simply indicates that the data has successfully reached the fault domain of the responding node. That is, the data has been received by the channel adapter and the channel adapter will write that data to the memory system of the responding node, or the responding application will at least be informed of the failure.“&lt;/p>
&lt;/blockquote>
&lt;h2 id="read">READ&lt;/h2>
&lt;p>顾名思义，READ 跟 WRITE 是相反的过程，是本端主动读取远端内存的行为。同 WRITE 一样，远端 CPU 不需要参与，也不感知数据在内存中被读取的过程。&lt;/p>
&lt;p>获取 key 和虚拟地址的流程也跟 WRITE 没有区别，需要注意的是 &lt;strong>&amp;ldquo;读”这个动作所请求的数据&lt;/strong>，是在对端回复的报文中携带的。&lt;/p>
&lt;p>下面描述一次 READ 操作的流程，注意跟 WRITE 只是方向和步骤顺序的差别。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/rdma5-2024-02-25.webp"
alt="rdma5-2024-02-25" width="auto" loading="lazy"/>
&lt;/figure>
&lt;ol>
&lt;li>请求端 APP 以 WQE 的形式下发一次 READ 任务。&lt;/li>
&lt;li>请求端网卡从 SQ 中取出 WQE，解析信息。&lt;/li>
&lt;li>请求端网卡将 READ 请求包通过物理链路发送给响应端网卡。&lt;/li>
&lt;li>响应端收到数据包，解析目的虚拟地址，转换成本地物理地址，解析数据，从指定内存区域取出数据。&lt;/li>
&lt;li>响应端硬件将数据组装成回复数据包发送到物理链路。&lt;/li>
&lt;li>请求端硬件收到数据包，解析提取出数据后放到 READ WQE 指定的内存区域中。&lt;/li>
&lt;li>请求端网卡生成 CQE，放置到 CQ 中。&lt;/li>
&lt;li>请求端 APP 取得任务完成信息。&lt;/li>
&lt;/ol>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>我们忽略各种细节进行抽象，RDMA WRITE 和 READ 操作就是在利用网卡完成下面左图的内存拷贝操作而已，只不过复制的过程是由 RDMA 网卡通过网络链路完成的；而本地内存拷贝则如下面右图所示由 CPU 通过总线完成的：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/rdma-op-6-2024-02-25.webp"
alt="rdma-op-6-2024-02-25" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>RDMA 标准定义上述几种操作的时候使用的单词是非常贴切的，“收”和“发”是需要有对端主动参与的语义 ，而‘读“和”写“更像是本端对一个没有主动性的对端进行操作的语义。&lt;/p>
&lt;p>通过对比 SEND/RECV 和 WRITE/READ 操作，我们可以发现传输数据时不需要响应端 CPU 参与的 WRITE/READ 有更大的优势，缺点就是请求端需要在准备阶段获得响应端的一段内存的读写权限。但是实际数据传输时，这个准备阶段的功率和时间损耗都是可以忽略不计的，所以 RDMA WRITE/READ 才是大量传输数据时所应用的操作类型，SEND/RECV 通常只是用来传输一些控制信息。&lt;/p>
&lt;p>除了本文介绍的几种操作之外，还有 ATOMIC 等更复杂一些的操作类型，将在后面的协议解读部分详细分析。本篇就到这里，下一篇将介绍 RDMA 基本服务类型。&lt;/p>
&lt;h2 id="参考资料">参考资料&lt;/h2>
&lt;p>[1] part1-OFA_Training_Sept_2016.pdf&lt;/p></description></item><item><title>RDMA 基本元素</title><link>https://cuterwrite.top/p/rdma-element/</link><pubDate>Fri, 02 Feb 2024 01:01:01 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-element/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/73c30b990886bf6988c97858a3e16011195413_crop-2024-02-04.webp" alt="Featured image of post RDMA 基本元素" />&lt;h1 id="rdma-基本元素">RDMA 基本元素&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/141267386">&lt;cite>知乎专栏：3. RDMA 基本元素&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>RDMA 技术中经常使用缩略语，很容易让刚接触的人一头雾水，本篇的目的是讲解 RDMA 中最基本的元素及其含义。&lt;/p>
&lt;p>我将常见的缩略语对照表写在前面，阅读的时候如果忘记了可以翻到前面查阅。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-b6723caa5b291ee161d94fd8fd8ce09c_720w-2024-02-03.webp"
alt="v2-b6723caa5b291ee161d94fd8fd8ce09c_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;h2 id="wq">WQ&lt;/h2>
&lt;p>Work Queue 简称 WQ，是 RDMA 技术中最重要的概念之一。WQ 是一个储存工作请求的队列，为了讲清楚 WQ 是什么，我们先介绍这个队列中的元素 WQE（Work Queue Element，工作队列元素）。&lt;/p>
&lt;h3 id="wqe">WQE&lt;/h3>
&lt;p>WQE 可以认为是一种“任务说明”，这个工作请求是软件下发给硬件的，这份说明中包含了软件所希望硬件去做的任务以及有关这个任务的详细信息。比如，某一份任务是这样的：“我想把位于地址 0x12345678 的长度为 10 字节的数据发送给对面的节点”，硬件接到任务之后，就会通过 DMA 去内存中取数据，组装数据包，然后发送。&lt;/p>
&lt;p>WQE 的含义应该比较明确了，那么我们最开始提到的 WQ 是什么呢？它就是用来存放“任务书”的“文件夹”，WQ 里面可以容纳很多 WQE。有数据结构基础的读者应该都了解，队列是一种先进先出的数据结构，在计算机系统中非常常见，我们可以用下图表示上文中描述的 WQ 和 WQE 的关系：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-40c7e57f2760323c6b6665306e8f8896_720w-2024-02-03.webp"
alt="v2-40c7e57f2760323c6b6665306e8f8896_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>WQ 这个队列总是由软件向其中增加 WQE（入队），硬件从中取出 WQE，这就是软件给硬件“下发任务”的过程。为什么用队列而不是栈？因为进行“存”和“取“操作的分别是软件和硬件，并且需要保证用户的请求按照顺序被处理在 RDMA 技术中，所有的通信请求都要按照上图这种方式告知硬件，这种方式常被称为“Post”。&lt;/p>
&lt;h3 id="qp">QP&lt;/h3>
&lt;p>Queue Pair 简称 QP，就是“一对”WQ 的意思。&lt;/p>
&lt;h3 id="sq-和-rq">SQ 和 RQ&lt;/h3>
&lt;p>任何通信过程都要有收发两端，QP 就是一个发送工作队列和一个接受工作队列的组合，这两个队列分别称为 SQ（Send Queue）和 RQ（Receive Queue）。我们再把上面的图丰富一下，左边是发送端，右边是接收端：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-b89b321b8d1ae5ab6dcbaf8d6085f107_720w-2024-02-03.webp"
alt="v2-b89b321b8d1ae5ab6dcbaf8d6085f107_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>WQ 怎么不见了？SQ 和 RQ 都是 WQ，WQ 只是表示一种可以存储 WQE 的单元，SQ 和 RQ 才是实例。&lt;/p>
&lt;p>SQ 专门用来存放发送任务，RQ 专门用来存放接收任务。在一次 SEND-RECV 流程中，发送端需要把表示一次发送任务的 WQE 放到 SQ 里面。同样的，接收端软件需要给硬件下发一个表示接收任务的 WQE，这样硬件才知道收到数据之后放到内存中的哪个位置。上文我们提到的 Post 操作，对于 SQ 来说称为 Post Send，对于 RQ 来说称为 Post Receive。&lt;/p>
&lt;p>需要注意的是，在 RDMA 技术中&lt;strong>通信的基本单元是 QP&lt;/strong>，而不是节点。如下图所示，对于每个节点来说，每个进程都可以使用若干个 QP，而每个本地 QP 可以“关联”一个远端的 QP。我们用“节点 A 给节点 B 发送数据”并不足以完整的描述一次 RDMA 通信，而应该是类似于“节点 A 上的 QP3 给节点 C 上的 QP4 发送数据”。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-71b3b17ef8aec45d74ef9e4a42a69201_720w-2024-02-03.webp"
alt="v2-71b3b17ef8aec45d74ef9e4a42a69201_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>每个节点的每个 QP 都有一个唯一的编号，称为 QPN（Queue Pair Number），通过 QPN 可以唯一确定一个节点上的 QP。&lt;/p>
&lt;h3 id="srq">SRQ&lt;/h3>
&lt;p>Shared Receive Queue 简称 SRQ，意为共享接收队列。概念很好理解，就是一种几个 QP 共享同一个 RQ 时，我们称其为 SRQ。以后我们会了解到，使用 RQ 的情况要远远小于使用 SQ，而每个队列都是要消耗内存资源的。当我们需要使用大量的 QP 时，可以通过 SRQ 来节省内存。如下图所示，QP2~QP4 一起使用同一个 RQ：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-4a21f2b1333877b4b0d97a1ca91d4096_720w-2024-02-03.webp"
alt="v2-4a21f2b1333877b4b0d97a1ca91d4096_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;h2 id="cq">CQ&lt;/h2>
&lt;p>Completion Queue 简称 CQ，意为完成队列。跟 WQ 一样，我们先介绍 CQ 这个队列当中的元素——CQE（Completion Queue Element）。可以认为 CQE 跟 WQE 是相反的概念，如果 WQE 是软件下发给硬件的“任务书”的话，那么 CQE 就是硬件完成任务之后返回给软件的“任务报告”。CQE 中描述了某个任务是被正确无误的执行，还是遇到了错误，如果遇到了错误，那么错误的原因是什么。&lt;/p>
&lt;p>而 CQ 就是承载 CQE 的容器——一个先进先出的队列。我们把表示 WQ 和 WQE 关系的图倒过来画，就得到了 CQ 和 CQE 的关系：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-31f9a407ab66381fbc557d8acc5573cb_720w-2024-02-03.webp"
alt="v2-31f9a407ab66381fbc557d8acc5573cb_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>每个 CQE 都包含某个 WQE 的完成信息，他们的关系如下图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-701fa8eacb10c90c45b0241c75254a01_720w-2024-02-03.webp"
alt="v2-701fa8eacb10c90c45b0241c75254a01_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>下面我们把 CQ 和 WQ（QP）放在一起，看一下一次 SEND-RECV 操作中，软硬件的互动（图中序号顺序不表示实际时序）：&lt;/p>
&lt;blockquote>
&lt;p>2022/5/23：下图及后面的列表顺序有修改，将原来第 2 条的“接收端硬件从 RQ 中拿到任务书，准备接收数据”移动到“接收端收到数据，进行校验后回复 ACK 报文给发送端”之后，并且修改了描述，现在为第 6 条。&lt;/p>
&lt;p>这里我犯了错误的点是 RQ 和 SQ 不同，是一个“被动接收”的过程，只有收到 Send 报文（或者带立即数的 Write 报文）时硬件才会消耗 RQ WQE。感谢 @连接改变世界 的指正。&lt;/p>
&lt;/blockquote>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-a8d38721903672037b27cc7e49ecee03_720w-2024-02-03.webp"
alt="v2-a8d38721903672037b27cc7e49ecee03_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;ol>
&lt;li>接收端 APP 以 WQE 的形式下发一次 RECV 任务到 RQ。&lt;/li>
&lt;li>发送端 APP 以 WQE 的形式下发一次 SEND 任务到 SQ。&lt;/li>
&lt;li>发送端硬件从 SQ 中拿到任务书，从内存中拿到待发送数据，组装数据包。&lt;/li>
&lt;li>发送端网卡将数据包通过物理链路发送给接收端网卡。&lt;/li>
&lt;li>接收端收到数据，进行校验后回复 ACK 报文给发送端。&lt;/li>
&lt;li>接收端硬件从 RQ 中取出一个任务书（WQE）。&lt;/li>
&lt;li>接收端硬件将数据放到 WQE 中指定的位置，然后生成“任务报告”CQE，放置到 CQ 中。&lt;/li>
&lt;li>接收端 APP 取得任务完成信息。&lt;/li>
&lt;li>发送端网卡收到 ACK 后，生成 CQE，放置到 CQ 中。&lt;/li>
&lt;li>发送端 APP 取得任务完成信息。&lt;/li>
&lt;/ol>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>NOTE: 需要注意的一点是，上图中的例子是可靠服务类型的交互流程，如果是不可靠服务，那么不会有步骤 5 的 ACK 回复，而且步骤 9 以及之后的步骤会在步骤 5 之后立即触发。关于服务类型以及可靠与不可靠，我们将在《RDMA 基本服务类型》一文中讲解。&lt;/p>&lt;/div>
&lt;p>至此，通过 WQ 和 CQ 这两种媒介，两端软硬件共同完成了一次收发过程。&lt;/p>
&lt;h2 id="wr-和-wc">WR 和 WC&lt;/h2>
&lt;p>说完了几个 Queue 之后，其实还有两个文章开头提到的概念没有解释，那就是 WR 和 WC（不是 Water Closet 的缩写）。&lt;/p>
&lt;p>WR 全称为 Work Request，意为工作请求；WC 全称 Work Completion，意为工作完成。这两者其实是 WQE 和 CQE 在用户层的“映射”。因为 APP 是通过调用协议栈接口来完成 RDMA 通信的，WQE 和 CQE 本身并不对用户可见，是驱动中的概念。用户真正通过 API 下发的是 WR，收到的是 WC。&lt;/p>
&lt;p>WR/WC 和 WQE/CQE 是相同的概念在不同层次的实体，他们都是“任务书”和“任务报告”。于是我们把前文的两个图又加了点内容：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-00b87c111a8e1701f96fbfb78e078b29_720w-2024-02-03.webp"
alt="v2-00b87c111a8e1701f96fbfb78e078b29_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>好了，我们用 IB 协议[1]3.2.1 中的 Figure 11 这张图总结一下本篇文章的内容：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-2107a9bf8230c45ad73aa5ff0b8626ff_720w-2024-02-03.webp"
alt="v2-2107a9bf8230c45ad73aa5ff0b8626ff_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>用户态的 WR，由驱动转化成了 WQE 填写到了 WQ 中，WQ 可以是负责发送的 SQ，也可以是负责接收的 RQ。硬件会从各个 WQ 中取出 WQE，并根据 WQE 中的要求完成发送或者接收任务。任务完成后，会给这个任务生成一个 CQE 填写到 CQ 中。驱动会从 CQ 中取出 CQE，并转换成 WC 返回给用户。&lt;/p>
&lt;p>基础概念就介绍到这里，下一篇将介绍 RDMA 的几种常见操作类型。&lt;/p>
&lt;h2 id="参考文献">参考文献&lt;/h2>
&lt;p>[1]《IB Specification Vol 1-Release-1.3-2015-03-03》&lt;/p></description></item><item><title>比较基于传统以太网与 RDMA 技术的通信</title><link>https://cuterwrite.top/p/ethernet-vs-rdma/</link><pubDate>Thu, 01 Feb 2024 02:01:01 +0000</pubDate><guid>https://cuterwrite.top/p/ethernet-vs-rdma/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/cropped_2024010202-2024-02-03.webp" alt="Featured image of post 比较基于传统以太网与 RDMA 技术的通信" />&lt;h1 id="比较基于传统以太网与-rdma-技术的通信">比较基于传统以太网与 RDMA 技术的通信&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/139548242">&lt;cite>知乎专栏：2. 比较基于传统以太网与 RDMA 技术的通信&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>本篇的目的是通过对比一次典型的基于 TCP/IP 协议栈的以太网和 RDMA 通信的过程，直观的展示 RDMA 技术相比传统以太网的优势，尽量不涉及协议和软件实现细节。&lt;/p>
&lt;p>假设本端的某个应用想把自己内存中的数据复制到对端某个应用可以访问的内存中（或者通俗的讲，本端要给对端发送数据），我们来看一下以太网和 RDMA 的 SEND-RECV 语义都做了哪些操作。&lt;/p>
&lt;h2 id="传统以太网">传统以太网&lt;/h2>
&lt;p>在描述通信过程时的软硬件关系时，我们通常将模型划分为用户层 Userspace，内核 Kernel 以及硬件 Hardware。Userspace 和 Kernel 实际上使用的是同一块物理内存，但是处于安全考虑，Linux 将内存划分为用户空间和内核空间。用户层没有权限访问和修改内核空间的内存内容，只能通过系统调用陷入内核态，Linux 的内存管理机制比较复杂，本文不展开讨论。&lt;/p>
&lt;p>一次典型的基于传统以太网的通信过程的可以如下图所示进行分层：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-0931cab010cbf296edeaa368c45a503b_720w-2024-02-03.webp"
alt="v2-0931cab010cbf296edeaa368c45a503b_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>一次收-发过程的步骤如下：&lt;/p>
&lt;ol>
&lt;li>发送端和接收端通过 Socket 库提供的接口建立链接（就是在两个节点间建立了一条逻辑上的道路，数据可以沿这条道路从一端发送到另一端）并分别在内存中申请好发送和接收 Buffer。&lt;/li>
&lt;li>发送端 APP 通过 Socket 接口陷入内核态，待发送数据经过 TCP/IP 协议栈的一层层封装，最后被 CPU 复制到 Socket Buffer 中。&lt;/li>
&lt;li>发送端通过网卡驱动，告知网卡可以发送数据了，网卡将通过 DMA 从 Buffer 中复制封装好的数据包到内部缓存中，然后将其发送到物理链路。&lt;/li>
&lt;li>接收端网卡收到数据包后，将数据包放到接收 Buffer 中，然后 CPU 将通过内核中的 TCP/IP 协议栈对报文进行层层解析，取出有效的数据。&lt;/li>
&lt;li>接收端 APP 通过 Socket 接口陷入内核态，CPU 将数据从内核空间复制到用户空间。&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-b89b321b8d1ae5ab6dcbaf8d6085f107_720w-2024-02-03.webp"
alt="v2-b89b321b8d1ae5ab6dcbaf8d6085f107_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>这个模型的数据流向大致是像上图这个样子，数据首先需要从用户空间复制一份到内核空间，这一次复制由 CPU 完成，将数据块从用户空间复制到内核空间的 Socket Buffer 中。内核中软件 TCP/IP 协议栈给数据添加各层头部和校验信息。最后网卡会通过 DMA 从内存中复制数据，并通过物理链路发送给对端的网卡。&lt;/p>
&lt;p>而对端是完全相反的过程：硬件将数据包 DMA 拷贝到内存中，然后 CPU 会对数据包进行逐层解析和校验，最后将数据复制到用户空间。&lt;/p>
&lt;p>上述过程中的关键点是需要 CPU 参与的把数据从用户空间拷贝到内核空间，以及同样需要 CPU 全程参与的数据包组装和解析，数据量大的情况下，这将对 CPU 将造成很大的负担。&lt;/p>
&lt;p>下面我们看一下 RDMA 是如何将 CPU“解放”出来的。&lt;/p>
&lt;h2 id="rdma">RDMA&lt;/h2>
&lt;p>同样是一端发送，一端接收的场景，我们将 RDMA 的分层模型分成两部分“控制通路”和“数据通路”，控制通路需要进入内核态准备通信所需的内存资源，而数据通路指的是实际数据交互过程中的流程。这一过程的分层关系如下图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-8ef2b015ba9d111fc2d42983cd5fe152_720w-2024-02-03.webp"
alt="v2-8ef2b015ba9d111fc2d42983cd5fe152_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>同 Socket 一样，我们简单描述下通信的过程：&lt;/p>
&lt;ol>
&lt;li>发送端和接收端分别通过控制通路陷入内核态创建好通信所需要的内存资源。&lt;/li>
&lt;li>在数据通路上，接收端 APP 通知硬件准备接收数据，告诉硬件将接收到的数据放在哪片内存中。&lt;/li>
&lt;li>在数据通路上，发送端 APP 通知硬件发送数据，告诉硬件待发送数据位于哪片内存中。&lt;/li>
&lt;li>发送端 RDMA 网卡从内存中搬移数据，组装报文发送给对端。&lt;/li>
&lt;li>对端收到报文，对其进行解析并通过 DMA 将有效载荷写入内存。然后以某种方式通知上层 APP，告知其数据已接收并妥善存放到指定位置。&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-cb1492c24773e725dd5f112bb67f9deb_720w-2024-02-03.webp"
alt="v2-cb1492c24773e725dd5f112bb67f9deb_720w-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;p>这一过程中的数据流向大致如上图所示。通过和 Socket 的对比，我们可以明显看到，&lt;strong>数据收发绕过了内核并且数据交换过程并不需要 CPU 参与，报文的组装和解析是由硬件完成的&lt;/strong>。&lt;/p>
&lt;p>通过上面的对比，我们可以明显的体会到 RDMA 的优势，既将 CPU 从数据包封装和解析中解放出来，又减少了 CPU 拷贝数据的功率和时间损耗。需要注意的是，本文只描述了 SEND-RECV 流程，而 RDMA 技术所独有的，效率更高的 WRITE/READ 语义将在后续文章中介绍。&lt;/p>
&lt;p>下一篇我们将介绍一些 RDMA 技术中的重要且基本的概念。&lt;/p></description></item><item><title>RDMA 概述</title><link>https://cuterwrite.top/p/rdma-overview/</link><pubDate>Mon, 01 Jan 2024 01:01:01 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-overview/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/crop_65b36f302c1d3715061e824224dcc9ca195413.jpg@1256w_1806h_!web-article-pic-2024-01-14.webp" alt="Featured image of post RDMA 概述" />&lt;h1 id="rdma-概述">RDMA 概述&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/138874738">&lt;cite>知乎专栏：1. RDMA 概述&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>本想完全靠自己的语言完成这篇概述，然而开篇并没有想象当中的好写，看样子从宏观上概括一个技术比从微观上探究细枝末节要困难不少。本文是以前人们对 RDMA 技术的介绍为主，加入了一些自己的理解。随着本专栏内容的增加，本篇概述也会更新和逐渐完善。&lt;/p>
&lt;h2 id="什么是-dma">什么是 DMA&lt;/h2>
&lt;p>DMA 全称为 Direct Memory Access，即直接内存访问。意思是外设对内存的读写过程可以不用 CPU 参与而直接进行。我们先来看一下没有 DMA 的时候：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20240114203040-2024-01-14.png"
alt="无 DMA 控制器时 I/O 设备和内存间的数据路径" width="auto" loading="lazy"/>&lt;figcaption>
&lt;h4>无 DMA 控制器时 I/O 设备和内存间的数据路径&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>假设 I/O 设备为一个普通网卡，为了从内存拿到需要发送的数据，然后组装数据包发送到物理链路上，网卡需要通过总线告知 CPU 自己的数据请求。然后 CPU 将会把内存缓冲区中的数据复制到自己内部的寄存器中，再复制到 I/O 设备的存储空间中。如果数据量比较大，那么很长一段时间内 CPU 都会忙于搬移数据，而无法投入到其他工作中去。&lt;/p>
&lt;p>CPU 的最主要工作是计算，而不是进行数据复制，这种工作属于白白浪费了它的计算能力。为了给 CPU“减负”，让它投入到更有意义的工作中去，后来人们设计了 DMA 机制：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20240114203114-2024-01-14.png"
alt="有 DMA 控制器时 I/O 设备和内存间的数据路径" width="auto" loading="lazy"/>&lt;figcaption>
&lt;h4>有 DMA 控制器时 I/O 设备和内存间的数据路径&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>可以看到总线上又挂了一个 DMA 控制器，它是专门用来读写内存的设备。有了它以后，当我们的网卡想要从内存中拷贝数据时，除了一些必要的控制命令外，整个数据复制过程都是由 DMA 控制器完成的。过程跟 CPU 复制是一样的，只不过这次是把内存中的数据通过总线复制到 DMA 控制器内部的寄存器中，再复制到 I/O 设备的存储空间中。CPU 除了关注一下这个过程的开始和结束以外，其他时间可以去做其他事情。&lt;/p>
&lt;p>DMA 控制器一般是和 I/O 设备在一起的，也就是说一块网卡中既有负责数据收发的模块，也有 DMA 模块。&lt;/p>
&lt;h2 id="什么是-rdma">什么是 RDMA&lt;/h2>
&lt;p>RDMA（ Remote Direct Memory Access ）意为远程直接地址访问，通过 RDMA，本端节点可以“直接”访问远端节点的内存。所谓直接，指的是可以像访问本地内存一样，绕过传统以太网复杂的 TCP/IP 网络协议栈读写远端内存，而这个过程对端是不感知的，而且这个读写过程的大部分工作是由硬件而不是软件完成的。&lt;/p>
&lt;p>为了能够直观的理解这一过程，请看下面两个图（图中箭头仅做示意，不表示实际逻辑或物理关系）：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20240115235555-2024-01-15.png"
alt="20240115235555-2024-01-15" width="90%" loading="lazy"/>
&lt;/figure>
&lt;p>传统网络中，“节点 A 给节点 B 发消息”实际上做的是“把节点 A 内存中的一段数据，通过网络链路搬移到节点 B 的内存中”，而这一过程无论是发端还是收段，都需要 CPU 的指挥和控制，包括网卡的控制，中断的处理，报文的封装和解析等等。&lt;/p>
&lt;p>上图中左边的节点在内存用户空间中的数据，需要经过 CPU 拷贝到内核空间的缓冲区中，然后才可以被网卡访问，这期间数据会经过软件实现的 TCP/IP 协议栈，加上各层头部和校验码，比如 TCP 头，IP 头等。网卡通过 DMA 拷贝内核中的数据到网卡内部的缓冲区中，进行处理后通过物理链路发送给对端。&lt;/p>
&lt;p>对端收到数据后，会进行相反的过程：从网卡内部存储空间，将数据通过 DMA 拷贝到内存内核空间的缓冲区中，然后 CPU 会通过 TCP/IP 协议栈对其进行解析，将数据取出来拷贝到用户空间中。&lt;/p>
&lt;p>可以看到，即使有了 DMA 技术，上述过程还是对 CPU 有较强的依赖。&lt;/p>
&lt;p>而使用了 RDMA 技术之后，这一过程可以简单的表示成下面的示意图：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20240115235612-2024-01-15.png"
alt="20240115235612-2024-01-15" width="90%" loading="lazy"/>
&lt;/figure>
&lt;p>同样是把本端内存中的一段数据，复制到对端内存中，在使用了 RDMA 技术时，两端的 CPU 几乎不用参与数据传输过程（只参与控制面）。本端的网卡直接从内存的用户空间 DMA 拷贝数据到内部存储空间，然后硬件进行各层报文的组装后，通过物理链路发送到对端网卡。对端的 RDMA 网卡收到数据后，剥离各层报文头和校验码，通过 DMA 将数据直接拷贝到用户空间内存中。&lt;/p>
&lt;h2 id="rdma-的优势">RDMA 的优势&lt;/h2>
&lt;p>RDMA 主要应用在高性能计算（HPC）领域和大型数据中心当中，并且设备相对普通以太网卡要昂贵不少（比如 Mellanox 公司的 Connext-X 5 100Gb PCIe 网卡市价在 4000 元以上）。由于使用场景和价格的原因，RDMA 与普通开发者和消费者的距离较远，目前主要是一些大型互联网企业在部署和使用。&lt;/p>
&lt;p>RDMA 技术为什么可以应用在上述场景中呢？这就涉及到它的以下几个特点：&lt;/p>
&lt;ul>
&lt;li>0 拷贝：指的是不需要在用户空间和内核空间中来回复制数据。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20240115235643-2024-01-15.png"
alt="20240115235643-2024-01-15" width="60%" loading="lazy"/>
&lt;/figure>
&lt;p>由于 Linux 等操作系统将内存划分为用户空间和内核空间，在传统的 Socket 通信流程中 CPU 需要多次把数据在内存中来回拷贝。而通过 RDMA 技术，我们可以直接访问远端已经注册的内存区域。&lt;/p>
&lt;p>关于 0 拷贝可以参考这篇文章：&lt;a class="link" href="https://www.jianshu.com/p/e76e3580e356" target="_blank" rel="noopener" >浅谈 Linux 下的零拷贝机制
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;ul>
&lt;li>内核 Bypass：指的是 IO（数据）流程可以绕过内核，即在用户层就可以把数据准备好并通知硬件准备发送和接收。避免了系统调用和上下文切换的开销。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20240115235721-2024-01-15.png"
alt="20240115235721-2024-01-15" width="90%" loading="lazy"/>
&lt;/figure>
&lt;p>上图（原图&lt;a class="link" href="https://pc.nanog.org/static/published/meetings/NANOG76/1999/20190612_Cardona_Towards_Hyperscale_High_v1.pdf" target="_blank" rel="noopener" >[1]
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
）可以很好的解释“0 拷贝”和“内核 Bypass”的含义。上下两部分分别是基于 Socket 的和基于 RDMA 的一次收-发流程，左右分别为两个节点。可以明显的看到 Socket 流程中在软件中多了一次拷贝动作。而 RDMA 绕过了内核同时也减少了内存拷贝，数据可以直接在用户层和硬件间传递。&lt;/p>
&lt;ul>
&lt;li>CPU 卸载：指的是可以在远端节点 CPU 不参与通信的情况下（当然要持有访问远端某段内存的“钥匙”才行）对内存进行读写，这实际上是 &lt;strong>把报文封装和解析放到硬件中做了&lt;/strong>。而传统的以太网通信，双方 CPU 都必须参与各层报文的解析，如果数据量大且交互频繁，对 CPU 来讲将是一笔不小的开销，而这些被占用的 CPU 计算资源本可以做一些更有价值的工作。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20240115235929-2024-01-15.png"
alt="20240115235929-2024-01-15" width="60%" loading="lazy"/>
&lt;/figure>
&lt;p>通信领域两大出场率最高的性能指标就是“带宽”和“时延”。简单的说，所谓带宽指的是指单位时间内能够传输的数据量，而时延指的是数据从本端发出到被对端接收所耗费的时间。因为上述几个特点，相比于传统以太网，RDMA 技术同时做到了更高带宽和更低时延，所以其在带宽敏感的场景——比如海量数据的交互，时延敏感——比如多个计算节点间的数据同步的场景下得以发挥其作用。&lt;/p>
&lt;h2 id="协议">协议&lt;/h2>
&lt;p>RDMA 本身指的是一种技术，具体协议层面，包含 Infiniband（IB），RDMA over Converged Ethernet（RoCE）和 internet Wide Area RDMA Protocol（iWARP）。三种协议都符合 RDMA 标准，使用相同的上层接口，在不同层次上有一些差别。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20240116000000-2024-01-16.png"
alt="20240116000000-2024-01-16" width="90%" loading="lazy"/>
&lt;/figure>
&lt;p>上图&lt;a class="link" href="https://www.snia.org/sites/default/files/ESF/RoCE-vs.-iWARP-Final.pdf" target="_blank" rel="noopener" >[2]g
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
对于几种常见的 RDMA 技术的协议层次做了非常清晰的对比&lt;/p>
&lt;h3 id="infiniband">Infiniband&lt;/h3>
&lt;p>2000 年由 IBTA（InfiniBand Trade Association）提出的 IB 协议是当之无愧的核心，其规定了一整套完整的链路层到传输层（非传统 OSI 七层模型的传输层，而是位于其之上）规范，但是其无法兼容现有以太网，除了需要支持 IB 的网卡之外，企业如果想部署的话还要重新购买配套的交换设备。&lt;/p>
&lt;h3 id="roce">RoCE&lt;/h3>
&lt;p>RoCE 从英文全称就可以看出它是基于以太网链路层的协议，v1 版本网络层仍然使用了 IB 规范，而 v2 使用了 UDP+IP 作为网络层，使得数据包也可以被路由。RoCE 可以被认为是 IB 的“低成本解决方案”，将 IB 的报文封装成以太网包进行收发。由于 RoCE v2 可以使用以太网的交换设备，所以现在在企业中应用也比较多，但是相同场景下相比 IB 性能要有一些损失。&lt;/p>
&lt;h3 id="iwarp">iWARP&lt;/h3>
&lt;p>iWARP 协议是 IETF 基于 TCP 提出的，因为 TCP 是面向连接的可靠协议，这使得 iWARP 在面对有损网络场景（可以理解为网络环境中可能经常出现丢包）时相比于 RoCE v2 和 IB 具有更好的可靠性，在大规模组网时也有明显的优势。但是大量的 TCP 连接会耗费很多的内存资源，另外 TCP 复杂的流控等机制会导致性能问题，所以从性能上看 iWARP 要比 UDP 的 RoCE v2 和 IB 差。&lt;/p>
&lt;p>需要注意的是，虽然有软件实现的 RoCE 和 iWARP 协议，但是真正商用时上述几种协议都需要专门的硬件（网卡）支持。&lt;/p>
&lt;p>iWARP 本身不是由 Infiniband 直接发展而来的，但是它继承了一些 Infiniband 技术的设计思想。这三种协议的关系如下图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20240116000125-2024-01-16.png"
alt="20240116000125-2024-01-16" width="60%" loading="lazy"/>
&lt;/figure>
&lt;h2 id="玩家">玩家&lt;/h2>
&lt;h3 id="标准生态组织">标准/生态组织&lt;/h3>
&lt;p>提到 IB 协议，就不得不提到两大组织——IBTA 和 OFA。&lt;/p>
&lt;h3 id="ibta3httpswwwinfinibandtaorg">IBTA&lt;a class="link" href="https://www.infinibandta.org/" target="_blank" rel="noopener" >[3]
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/h3>
&lt;p>成立于 1999 年，负责制定和维护 Infiniband 协议标准。IBTA 独立于各个厂商，通过赞助技术活动和推动资源共享来将整个行业整合在一起，并且通过线上交流、营销和线下活动等方式积极推广 IB 和 RoCE。&lt;/p>
&lt;p>IBTA 会对商用的 IB 和 RoCE 设备进行协议标准符合性和互操作性测试及认证，由很多大型的 IT 厂商组成的委员会领导，其主要成员包括博通，HPE，IBM，英特尔，Mellanox 和微软等，华为也是 IBTA 的会员。&lt;/p>
&lt;h3 id="ofa4httpswwwopenfabricsorg">OFA&lt;a class="link" href="https://www.openfabrics.org/" target="_blank" rel="noopener" >[4]
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/h3>
&lt;p>成立于 2004 年的非盈利组织，负责开发、测试、认证、支持和分发独立于厂商的开源跨平台 infiniband 协议栈，2010 年开始支持 RoCE。其对用于支撑 RDMA/Kernel bypass 应用的 OFED（OpenFabrics Enterprise Distribution）软件栈负责，保证其与主流软硬件的兼容性和易用性。OFED 软件栈包括驱动、内核、中间件和 API。&lt;/p>
&lt;p>上述两个组织是配合关系，IBTA 主要负责开发、维护和增强 Infiniband 协议标准；OFA 负责开发和维护 Infiniband 协议和上层应用 API。&lt;/p>
&lt;h2 id="开发社区">开发社区&lt;/h2>
&lt;h3 id="linux-社区">Linux 社区&lt;/h3>
&lt;p>Linux 内核的 RDMA 子系统还算比较活跃，经常会讨论一些协议细节，对框架的修改比较频繁，另外包括华为和 Mellanox 在内的一些厂商也会经常对驱动代码进行修改。&lt;/p>
&lt;p>邮件订阅：&lt;a class="link" href="http://vger.kernel.org/vger-lists.html#linux-rdma" target="_blank" rel="noopener" >http://vger.kernel.org/vger-lists.html#linux-rdma
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;p>代码位于内核 drivers/infiniband/目录下，包括框架核心代码和各厂商的驱动代码。&lt;/p>
&lt;p>代码仓：&lt;a class="link" href="https://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma.git/" target="_blank" rel="noopener" >https://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma.git/
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h3 id="rdma-社区">RDMA 社区&lt;/h3>
&lt;p>对于上层用户，IB 提供了一套与 Socket 套接字类似的接口——libibverbs，前文所述三种协议都可以使用。参考着协议、API 文档和示例程序很容易就可以写一个 Demo 出来。本专栏中的 RDMA 社区专指其用户态社区，在 github 上其仓库的名字为 linux-rdma。&lt;/p>
&lt;p>主要包含两个子仓库：&lt;/p>
&lt;ul>
&lt;li>rdma-core&lt;/li>
&lt;/ul>
&lt;p>用户态核心代码，API，文档以及各个厂商的用户态驱动。&lt;/p>
&lt;ul>
&lt;li>perftest
一个功能强大的用于测试 RDMA 性能的工具。&lt;/li>
&lt;/ul>
&lt;p>代码仓：&lt;a class="link" href="https://github.com/linux-rdma/" target="_blank" rel="noopener" >https://github.com/linux-rdma/
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h2 id="ucx5httpswwwopenucxorg">UCX&lt;a class="link" href="https://www.openucx.org/" target="_blank" rel="noopener" >[5]
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/h2>
&lt;p>UCX 是一个建立在 RDMA 等技术之上的用于数据处理和高性能计算的通信框架，RDMA 是其底层核心之一。我们可以将其理解为是位于应用和 RDMA API 之间的中间件，向上层用户又封装了一层更易开发的接口。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20240116000429-2024-01-16.png"
alt="20240116000429-2024-01-16" width="90%" loading="lazy"/>
&lt;/figure>
&lt;p>笔者对其并不了解太多，只知道业界有一些企业在基于 UCX 开发应用。&lt;/p>
&lt;p>代码仓：&lt;a class="link" href="https://github.com/openucx/ucx" target="_blank" rel="noopener" >https://github.com/openucx/ucx
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h2 id="硬件厂商">硬件厂商&lt;/h2>
&lt;p>设计和生产 IB 相关硬件的厂商有不少，包括 Mellanox、华为、收购了 Qlogic 的 IB 技术的 Intel，博通、Marvell，富士通等等，这里就不逐个展开了，仅简单提一下 Mellanox 和华为。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Mellanox
IB 领域的领头羊，协议标准制定、软硬件开发和生态建设都能看到 Mellanox 的身影，其在社区和标准制定上上拥有最大的话语权。目前最新一代的网卡是支持 200Gb/s 的 ConnextX-6 系列。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>华为
去年初推出的鲲鹏 920 芯片已经支持 100Gb/s 的 RoCE 协议，技术上在国内处于领先地位。但是软硬件和影响力方面距离 Mellanox 还有比较长的路要走，相信华为能够早日赶上老大哥的步伐。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="用户">用户&lt;/h2>
&lt;p>微软、IBM 和国内的阿里、京东都正在使用 RDMA，另外还有很多大型 IT 公司在做初步的开发和测试。在数据中心和高性能计算场景下，RDMA 代替传统网络是大势所趋。笔者对于市场接触不多，所以并不能提供更详细的应用情况。&lt;/p>
&lt;p>下一篇将用比较直观的方式比较一次典型的基于 Socket 的传统以太网和 RDMA 通信过程。&lt;/p></description></item></channel></rss>