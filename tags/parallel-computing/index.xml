<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>并行计算 on Cuterwrite's Blog</title><link>https://cuterwrite.top/tags/parallel-computing/</link><description>Recent content in 并行计算 on Cuterwrite's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>cuterwrite</copyright><lastBuildDate>Mon, 19 Feb 2024 01:36:00 +0000</lastBuildDate><atom:link href="https://cuterwrite.top/tags/parallel-computing/index.xml" rel="self" type="application/rss+xml"/><item><title>OpenMP 简介</title><link>https://cuterwrite.top/p/openmp-intro/</link><pubDate>Mon, 19 Feb 2024 01:36:00 +0000</pubDate><guid>https://cuterwrite.top/p/openmp-intro/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/c17b7451a44c1d4370d5ba2b966298ea195413_crop-2024-02-19.webp" alt="Featured image of post OpenMP 简介" />&lt;h1 id="openmp-简介">OpenMP 简介&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;h3 id="什么是-openmp">什么是 OpenMP？&lt;/h3>
&lt;p>OpenMP（Open Multi-Processing）是一种广泛应用的多线程并行编程模型，它为共享内存系统上的并行计算提供了丰富的指令集和 API。起源于 1997 年，OpenMP 由多个领先硬件和软件供应商共同制定标准，旨在简化并行程序的设计与实现过程，以充分利用现代多核处理器的计算能力。&lt;/p>
&lt;p>OpenMP 支持多种编程语言，包括 C、C++ 以及 Fortran 等，并通过在源代码中插入特定的编译指示（pragma），使得开发者能够轻松地将串行代码转化为高效的并行代码。其主要优势在于其简洁性和易用性，允许程序员使用熟悉的编程语言和开发环境，同时提供良好的可移植性和扩展性。&lt;/p>
&lt;p>OpenMP 由非营利性组织管理，由多家软硬件厂家参与，包括 Arm，IBM，Intel，AMD，NVIDIA，Cray，Oracle 等。&lt;/p>
&lt;h3 id="历史版本">历史版本&lt;/h3>
&lt;ul>
&lt;li>在 &lt;a class="link" href="https://www.openmp.org/" target="_blank" rel="noopener" >官网页面
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
可以查询到 OpenMP 的历史版本和发布日期。&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>版本&lt;/th>
&lt;th>发布日期&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Fortran 1.0&lt;/td>
&lt;td>October 1997&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>C/C++ 1.0&lt;/td>
&lt;td>October 1998&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>C/C++ 2.0&lt;/td>
&lt;td>March 2002&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 2.5&lt;/td>
&lt;td>May 2005&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 3.0&lt;/td>
&lt;td>May 2008&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 3.1&lt;/td>
&lt;td>July 2011&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 4.0&lt;/td>
&lt;td>July 2013&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 4.5&lt;/td>
&lt;td>November 2015&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 5.0&lt;/td>
&lt;td>November 2018&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 5.1&lt;/td>
&lt;td>November 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 5.2&lt;/td>
&lt;td>November 2021&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="基础知识">基础知识&lt;/h2>
&lt;h3 id="技术框架">技术框架&lt;/h3>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/openmp-arch-2024-02-20.webp"
alt="openmp-arch-2024-02-20" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>OpenMP 技术框架&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>&lt;strong>OpenMP Runtime Library&lt;/strong> 是 OpenMP 规范中定义的一组函数和运行时支持结构，它是 OpenMP 并行编程框架的关键组成部分。这个库在编译器的支持下与用户程序链接，在程序执行时负责管理线程的创建、同步、调度以及数据共享等任务。它实现了 OpenMP 编译指导所指示的所有并行化机制。&lt;/p>
&lt;p>&lt;strong>OpenMP Runtime Library&lt;/strong> 包括了如下功能：&lt;/p>
&lt;ul>
&lt;li>线程管理（创建、销毁、同步）
= 工作共享（动态工作分配给各个线程）
= 任务调度
= 同步原语（如 barriers, locks, atomic operations）
= 动态调整线程数量&lt;/li>
&lt;li>内存模型支持（数据环境变量、private, shared, reduction 变量等）&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Compiler Directives&lt;/strong> 编译指导是以 &lt;code>#pragma omp&lt;/code> 开头的预处理器指令，程序员在源代码中插入这些指令来指导编译器如何将串行程序转换为并行程序。例如，使用 &lt;code>#pragma omp parallel&lt;/code> 指令定义一个并行区域，编译器会在此区域内生成多线程执行逻辑。&lt;/p>
&lt;p>&lt;strong>Environment Variables&lt;/strong> 环境变量是 OpenMP 运行时库的一部分，它们用于控制运行时行为，例如线程数量、调度策略等。&lt;/p>
&lt;p>&lt;strong>OpenMP Library&lt;/strong> 是一组函数库，包括了一些用于线程同步、原子操作、锁、并行循环等的函数。这些函数可以在用户程序中直接调用，以实现更细粒度的并行化。&lt;/p>
&lt;p>总的来说，OpenMP 技术框架包括了编译器指导、运行时库、环境变量和函数库等多个组成部分，它们共同构成了一个完整的并行编程环境，共同协作以支持在共享内存系统上的并行编程。&lt;/p>
&lt;h3 id="执行模型fork-join-model">执行模型：Fork-Join Model&lt;/h3>
&lt;p>OpenMP 的执行模型采用的是 Fork-Join 机制，这是一种用于并行编程中的同步原语模型。在该模型下，程序执行遵循以下步骤：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Fork（派生）阶段&lt;/strong>： 程序开始时以单个主线程执行，当遇到 OpenMP 编译指导（pragma）指示的并行区域时，主线程会通过 Runtime Library 创建一个或多个工作线程（worker threads）。这些工作线程是对主线程的派生，每个线程负责执行并行区域内的部分任务。并行区域可以是循环、段（sections）、单一任务或其他可并行化的代码块。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Parallel Execution（并行执行）阶段&lt;/strong>： 创建出的工作线程独立并发地执行分配给它们的任务，并且能够访问共享的数据结构。OpenMP 提供了一套丰富的指令来管理数据的同步和通信，确保在多线程环境下的正确性和一致性。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Join（合并）阶段&lt;/strong>： 当所有工作线程完成其在并行区域内的任务后，它们会自动或者通过显式同步指令（如 &lt;code>omp barrier&lt;/code> ）汇聚到 join 点。在此阶段，所有线程等待直至所有其他线程都到达了同步点，随后 join 操作发生。这意味着主线程和其他工作线程重新同步，恢复为串行执行模式或继续执行后续的非并行代码。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Synchronization and Data Consistency（同步与数据一致性）&lt;/strong>： Fork-Join 模型确保了在并行执行过程中，通过适当的锁机制、原子操作和同步原语保证了对共享资源的互斥访问以及数据的一致性。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>总结来说，OpenMP 的 Fork-Join 执行模型是一种基于动态线程创建和同步的并行处理框架，它允许开发者方便地将串行代码转化为并行执行的代码片段，同时简化了并行编程中常见的复杂性，如线程管理和数据同步问题。&lt;/p>
&lt;h3 id="线程与进程">线程与进程&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>进程&lt;/p>
&lt;ul>
&lt;li>每个进程都有自己独立的地址空间&lt;/li>
&lt;li>CPU 在进程间切换时需要进行上下文切换&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>线程&lt;/p>
&lt;ul>
&lt;li>一个进程下的线程共享相同的地址空间&lt;/li>
&lt;li>CPU 在线程之间切换开销较小&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>操作系统的线程设计&lt;/p>
&lt;ul>
&lt;li>现代操作系统如 Linux、Windows 等都支持一个进程下有多个线程。&lt;/li>
&lt;li>线程是操作系统调度的基本单位，进程是资源分配的基本单位。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/slide_10-2024-02-20.webp"
alt="slide_10-2024-02-20" width="80%" loading="lazy">&lt;figcaption>
&lt;h4>操作系统的线程设计&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="线程的硬件调度">线程的硬件调度&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>硬件调度机制与操作系统协同，负责将线程智能地映射至可用的 CPU 物理核心上执行。&lt;/strong>&lt;/li>
&lt;li>因此，在多线程应用中，当活跃线程数超过了实际 CPU 物理核心的数量时，操作系统将不得不进行密集的上下文切换，以确保多个线程在有限的核心资源上交替运行，这种线程竞争过载的现象会导致整体性能瓶颈和效率下降。&lt;/li>
&lt;li>&lt;strong>超线程技术（Hyper-Threading）&lt;/strong> 通过在单个物理 CPU 核心上虚拟化出额外的逻辑处理单元，当前通常配置为每个物理核心承载两个逻辑核心。这些逻辑核心能够并行执行独立的任务流，尽管它们共享同一物理核心的基础计算资源，如执行引擎、缓存和其他底层硬件结构。通过这种方式，超线程旨在提高资源利用率和并发处理能力，尤其是在存在大量并行任务且其对计算资源需求相对较小的情况下，可以有效提升系统的总体吞吐量。然而，在某些高度依赖单一核心性能或内存带宽的应用场景下，如部分对 CPU 敏感的游戏或特定类型的数据密集型运算，增加逻辑核心可能并不一定能带来显著的性能提升。&lt;/li>
&lt;/ul>
&lt;h3 id="硬件的内存模型">硬件的内存模型&lt;/h3>
&lt;ul>
&lt;li>在现代多核处理器体系结构中，每个 CPU 核心为了进一步提升数据访问速度，在与主存之间设计有多级缓存层次结构。最靠近 CPU 核心的是 L1 缓存，通常其后是 L2 缓存，部分高端架构还包含 L3 缓存，这些高速缓存层级存储容量逐层增大，但访问延迟逐层增加。&lt;/li>
&lt;li>L1 和 L2 缓存通常是与特定 CPU 核心紧密耦合且私有的，这意味着每个核心拥有自己的独立缓存空间，以降低数据访问冲突并提高缓存命中率。L1 缓存由于距离计算单元最近，其访问速度最快，但容量最小；而 L2 缓存作为 L1 缓存的有效补充，具有相对较大的容量。&lt;/li>
&lt;li>为确保在多核环境中不同 CPU 核心的缓存中对共享数据的一致性，硬件和操作系统共同实现了缓存一致性协议（如 MESI 协议）。这种机制允许系统自动维护一个全局一致的数据视图，即使数据在多个核心的缓存中存在副本也能保证它们同步更新，这一特性在某些架构中被称作 &lt;strong>ccNUMA（cache-coherent non-uniform memory access）&lt;/strong> ，即缓存一致性非统一内存访问。&lt;/li>
&lt;li>然而，这种缓存一致性管理也带来了一些挑战，其中之一就是“伪共享”(False Sharing)问题。当不同的线程修改位于同一缓存行内的各自独立变量时，尽管这些变量本身并无关联，但由于它们物理上相邻而被存储在同一缓存行内，因此任何针对其中一个变量的写操作都会导致整个缓存行失效并在所有核心间重新同步。这会引发不必要的缓存无效化与重新填充操作，从而显著降低性能。解决伪共享问题通常需要精心设计数据布局或利用缓存行对齐等技术手段来避免无关数据之间的争用。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20170115165700476-2024-02-20.webp"
alt="20170115165700476-2024-02-20" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>典型的现代 CPU 内存结构&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="线程亲和性和线程绑定">线程亲和性和线程绑定&lt;/h3>
&lt;ul>
&lt;li>线程亲和性（Thread Affinity）是指操作系统或应用程序控制特定线程与处理器核心之间关联的能力。在多核或多处理器系统中，线程亲和性允许程序员或调度器决定将某个线程固定在特定的 CPU 核心上运行，而不是让操作系统自由地在所有可用的核心间进行动态调度。这种机制有助于减少上下文切换开销，提高缓存命中率，并且对于需要保持数据局部性的并行计算任务特别有益。&lt;/li>
&lt;li>线程绑定（Thread Pinning）是实现线程亲和性的具体技术手段，它指明了将特定线程与特定硬件资源（如 CPU 核心或 NUMA 节点）之间的强制关联。通过线程绑定，可以确保指定的线程始终在其分配的核心上执行，避免被操作系统迁移到其他核心，从而优化性能、减少延迟并解决伪共享等问题。在 OpenMP 等并行编程模型中，可以通过相关的环境变量或编译指导来设置线程绑定策略，以适应不同的并行计算需求和硬件特性。&lt;/li>
&lt;li>同一个插槽上的 CPU 对 L3 缓存的访问延迟是一致的，但不同插槽上的 CPU 对 L3 缓存的访问延迟是不一致的。因此，线程绑定的目的是为了减少线程在不同 CPU 之间的迁移，从而减少访存延迟。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/u=237293070,3563798054&amp;amp;fm=253&amp;amp;app=138&amp;amp;f=JPEG-2024-02-20.webp"
alt="u=237293070,3563798054&amp;amp;fm=253&amp;amp;app=138&amp;amp;f=JPEG-2024-02-20" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>线程亲和性和线程绑定&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>OpenMP 支持控制线程的绑定
&lt;ul>
&lt;li>环境变量 &lt;code>OMP_PROC_BIND&lt;/code> 或从句 &lt;code>proc_bind(master|close|spread)&lt;/code> 控制线程绑定与否，以及线程对于绑定单元（称为 place）分布&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="openmp-编程">OpenMP 编程&lt;/h2>
&lt;h3 id="安装">安装&lt;/h3>
&lt;p>对于 Linux 系统，GCC 是常用的编译器，现代版本的 GCC 一般已默认支持 OpenMP。例如在 Ubuntu 20.04 LTS 上，可以通过以下命令安装含有 OpenMP 的 build-essential 包：&lt;/p>
&lt;pre>&lt;code class="language-bash">$ sudo apt-get update
$ sudo apt-get install -y build-essential
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>查看 OpenMP 版本&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">$ echo |cpp -fopenmp -dM |grep -i open
#define _OPENMP 201511
&lt;/code>&lt;/pre>
&lt;h3 id="编译使用">编译使用&lt;/h3>
&lt;ul>
&lt;li>直接在编译语句中添加 &lt;code>-fopenmp&lt;/code> 选项即可开启 OpenMP 支持。&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">g++ -O2 -std=c++17 -fopenmp hello.cpp -o hello
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>如果使用 CMake 构建项目, 加入 &lt;code>-Wunknown-pragmas&lt;/code> 选项可以在编译时报告未处理的 &lt;code>#pragma&lt;/code> 指令。&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-cmake">find_package(OpenMP REQUIRED)
add_compile_options(-Wunknown-pragmas)
add_executable(hello hello.cpp)
target_link_libraries(hello PRIVATE OpenMP::OpenMP_CXX)
&lt;/code>&lt;/pre>
&lt;h3 id="hello-world">Hello World!&lt;/h3>
&lt;ul>
&lt;li>第一个 OpenMP 程序&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-c">#include &amp;lt;omp.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
int main() {
#pragma omp parallel num_threads(8)
{
int id = omp_get_thread_num();
int num_threads = omp_get_num_threads();
printf(&amp;quot;Hello World from thread %d of %d \n&amp;quot;, id, num_threads);
}
return 0;
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>运行结果&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">$ gcc -fopenmp hello.c -o hello
$ ./hello
Hello World from thread 7 of 8
Hello World from thread 6 of 8
Hello World from thread 0 of 8
Hello World from thread 3 of 8
Hello World from thread 1 of 8
Hello World from thread 2 of 8
Hello World from thread 5 of 8
Hello World from thread 4 of 8
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>同一类 openmp 制导语句称为一种构造(construct)&lt;/li>
&lt;li>形式为 &lt;code>#pragma omp &amp;lt;directive name&amp;gt; &amp;lt;clause&amp;gt;&lt;/code>&lt;/li>
&lt;li>使用 &lt;code>{}&lt;/code> 包裹的代码块称为并行区域(parallel region)&lt;/li>
&lt;/ul>
&lt;h3 id="线程数设置">线程数设置&lt;/h3>
&lt;ul>
&lt;li>优先级由低到高
&lt;ul>
&lt;li>什么都不做，系统选择运行线程数&lt;/li>
&lt;li>设置环境变量 &lt;code>export OMP_NUM_THREADS=4&lt;/code>&lt;/li>
&lt;li>代码中使用库函数 &lt;code>void omp_set_num_threads(int)&lt;/code>&lt;/li>
&lt;li>通过制导语句 &lt;code>num_threads(4)&lt;/code>&lt;/li>
&lt;li>if 从句判断串行还是并行执行&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="常用库函数">常用库函数&lt;/h3>
&lt;ul>
&lt;li>设置并行区运行线程数：&lt;code>void omp_set_num_threads(int)&lt;/code>&lt;/li>
&lt;li>获取并行区运行线程数：&lt;code>int omp_get_num_threads()&lt;/code>&lt;/li>
&lt;li>获取当前线程编号：&lt;code>int omp_get_thread_num()&lt;/code>&lt;/li>
&lt;li>获得 OpenMP Wall Clock 时间（单位：秒）：&lt;code>double omp_get_wtime()&lt;/code>&lt;/li>
&lt;li>获得时间精度：&lt;code>double omp_get_wtick()&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="parallel-构造">Parallel 构造&lt;/h3>
&lt;p>&lt;strong>支持的从句&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;code>if(scalar_expression)&lt;/code>：如果 &lt;code>scalar_expression&lt;/code> 为真，则并行执行，否则串行执行。&lt;/li>
&lt;li>&lt;code>num_threads(integer_expression)&lt;/code>：指定并行区域中的线程数。&lt;/li>
&lt;li>&lt;code>default(shared|none)&lt;/code>：指定变量的默认共享性。
&lt;ul>
&lt;li>&lt;code>shared&lt;/code>：所有变量默认为共享。&lt;/li>
&lt;li>&lt;code>none&lt;/code>：无默认变量类型，每个变量都需要显式声明共享或私有。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>shared(list)&lt;/code>：指定共享变量列表。
&lt;ul>
&lt;li>共享变量在内存中只有一份，所有线程都可以访问。&lt;/li>
&lt;li>请保证共享变量访问不会冲突。&lt;/li>
&lt;li>不特别指定并行区变量默认为 &lt;strong>shared&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>private(list)&lt;/code>：指定私有变量列表。
&lt;ul>
&lt;li>私有变量在每个线程中都有一份独立的拷贝。&lt;/li>
&lt;li>变量需要 &lt;strong>重新初始化&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>firstprivate(list)&lt;/code>：指定首次私有变量列表。
&lt;ul>
&lt;li>同 &lt;code>private&lt;/code>&lt;/li>
&lt;li>对变量根据主线程中的数据进行初始化。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 1： no clause、private、firstprivate&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">int results[4];
int cnt;
cnt = 1;
#pragma omp parallel num_threads(4)
{
int tid = omp_get_thread_num();
for (int i = 0; i &amp;lt; 4; i++) {
cnt += 1;
}
results[tid] = cnt;
}
printf(&amp;quot;no clause: &amp;quot;);
for (int i = 0; i &amp;lt; 4; i++) {
printf(&amp;quot;%d &amp;quot;, results[i]);
}
printf(&amp;quot;\n&amp;quot;);
cnt = 1;
#pragma omp parallel num_threads(4) private(cnt)
{
int tid = omp_get_thread_num();
for (int i = 0; i &amp;lt; 4; i++) {
cnt += 1;
}
results[tid] = cnt;
}
printf(&amp;quot;private(not init): &amp;quot;);
for (int i = 0; i &amp;lt; 4; i++) {
printf(&amp;quot;%d &amp;quot;, results[i]);
}
printf(&amp;quot;\n&amp;quot;);
cnt = 1;
#pragma omp parallel num_threads(4) firstprivate(cnt)
{
int tid = omp_get_thread_num();
for (int i = 0; i &amp;lt; 4; i++) {
cnt += 1;
}
results[tid] = cnt;
}
printf(&amp;quot;firstprivate: &amp;quot;);
for (int i = 0; i &amp;lt; 4; i++) {
printf(&amp;quot;%d &amp;quot;, results[i]);
}
printf(&amp;quot;\n&amp;quot;);
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>打印结果&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">no clause: 5 9 13 17
private(not init): 4 1572916964 1572916964 1572916964
firstprivate: 5 5 5 5
&lt;/code>&lt;/pre>
&lt;h3 id="for-构造">For 构造&lt;/h3>
&lt;ul>
&lt;li>最常用的并行化构造之一&lt;/li>
&lt;/ul>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 2：并行化 for 循环&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">#pragma omp parallel num_threads(8)
{
int tid = omp_get_thread_num();
int num_threads = omp_get_num_threads();
#pragma omp for
for (int i = 0; i &amp;lt; num_threads; i++) {
#pragma omp ordered
printf(&amp;quot;Hello from thread %d of %d \n&amp;quot;, tid, num_threads);
}
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>打印结果&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">Hello from thread 0 of 8
Hello from thread 1 of 8
Hello from thread 2 of 8
Hello from thread 3 of 8
Hello from thread 4 of 8
Hello from thread 5 of 8
Hello from thread 6 of 8
Hello from thread 7 of 8
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>在并行区内对 for 循环进行线程划分，且 for 循环满足格式要求
&lt;ul>
&lt;li>init-expr:需要是 &lt;code>var=lb&lt;/code> 形式，类型也有限制&lt;/li>
&lt;li>test-expr:限制为 &lt;code>var relational-opb&lt;/code> 或者 &lt;code>b relational-op var&lt;/code>&lt;/li>
&lt;li>incr-expr:仅限加减法&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="parallel-for-构造">Parallel for 构造&lt;/h3>
&lt;ul>
&lt;li>常常将 &lt;code>parallel&lt;/code> 和 &lt;code>for&lt;/code> 结合使用，合并为 &lt;code>parallel for&lt;/code> 制导语句&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>parallel&lt;/th>
&lt;th>for   &lt;/th>
&lt;th>parallel for&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>if&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>num_threads&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>default&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>copyin&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>private&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>firstprivate&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>shared&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>reduction&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>lastprivate&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>schedule&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ordered&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>collapse&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>nowait&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>
&lt;p>&lt;code>lastprivate(list)&lt;/code>&lt;/p>
&lt;ul>
&lt;li>同 &lt;code>private&lt;/code>&lt;/li>
&lt;li>执行完 for 循环后，将最后一个线程的值赋给主线程的变量。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>nowait&lt;/code>：取消代码块结束时的栅栏同步(barrier)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>collapse(n)&lt;/code>：应用于 n 重循环，合并(展开)循环&lt;/p>
&lt;ul>
&lt;li>注意循环之间是否有数据依赖&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>ordered&lt;/code>：声明有潜在的顺序执行部分&lt;/p>
&lt;ul>
&lt;li>使用 &lt;code>#pragma omp ordered&lt;/code> 标记顺序执行代码(搭配使用)&lt;/li>
&lt;li>ordered 区内的语句任意时刻仅由最多一个线程执行&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>shedule(type[,chunk])&lt;/code>&lt;/p>
&lt;ul>
&lt;li>&lt;code>type&lt;/code>：指定循环迭代的调度策略
&lt;ul>
&lt;li>&lt;code>static&lt;/code>：静态调度，chunk 大小固定（默认 n/p ）&lt;/li>
&lt;li>&lt;code>dynamic&lt;/code>：动态调度，chunk 大小固定（默认为 1）&lt;/li>
&lt;li>&lt;code>guided&lt;/code>：引导调度，chunk 大小动态调整&lt;/li>
&lt;li>&lt;code>runtime&lt;/code>：由系统环境变量 &lt;code>OMP_SCHEDULE&lt;/code> 指定&lt;/li>
&lt;li>&lt;code>auto&lt;/code>：自动调度&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>chunk&lt;/code>：指定每个线程获取的迭代次数&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="特殊的数据从句reduction">特殊的数据从句：Reduction&lt;/h3>
&lt;p>在 OpenMP 中，reduction 是一种并行编程技术，用于解决多线程环境下的数据竞争问题，特别是在计算全局变量的累加或类似操作时。当多个线程需要同时修改同一个共享变量，并且这些修改可以通过某种二元运算符（如加法、乘法等）将所有线程的结果合并成一个最终结果时，可以使用 &lt;code>reduction&lt;/code> 子句。&lt;/p>
&lt;p>具体来说，reducton 的执行过程为：&lt;/p>
&lt;ul>
&lt;li>fork 线程并分配任务&lt;/li>
&lt;li>每一个线程定义一个私有变量 &lt;code>omp_priv&lt;/code>
&lt;ul>
&lt;li>同 &lt;code>private&lt;/code>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>各个线程执行计算&lt;/li>
&lt;li>所有 &lt;code>omp_priv&lt;/code> 和 &lt;code>omp_in&lt;/code> 一起顺序进行 reduction，写回原变量。&lt;/li>
&lt;/ul>
&lt;p>相比之下，&lt;strong>atomic&lt;/strong> 是 OpenMP 提供的另一种同步机制，它确保对单个内存位置的访问在多线程环境中是原子性的，即一次只允许一个线程对该内存位置进行读取或写入操作。通过 &lt;code>#pragma omp atomic&lt;/code> 指令，可以保证一条简单的赋值语句（或某些特定类型的读改写操作）在并发环境下不会发生数据竞争。&lt;/p>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 3：Reduction&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">int sum = 0;
double start = omp_get_wtime();
#pragma omp parallel for num_threads(8) reduction(+ : sum)
for (int i = 0; i &amp;lt; 100000; i++) {
sum += i;
}
printf(&amp;quot;sum = %d\n&amp;quot;, sum);
printf(&amp;quot;Reduction time: %.5lf s\n&amp;quot;, omp_get_wtime() - start);
// no reduction
sum = 0;
start = omp_get_wtime();
#pragma omp parallel for num_threads(8)
for (int i = 0; i &amp;lt; 100000; i++) {
#pragma omp atomic
sum += i;
}
printf(&amp;quot;sum = %d\n&amp;quot;, sum);
printf(&amp;quot;Atomic time: %.5lf s\n&amp;quot;, omp_get_wtime() - start);
return 0;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>打印结果&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">sum = 704982704
Reduction time: 0.00062 s
sum = 704982704
Atomic time: 0.01021 s
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>两者结果相同，但是 reduction 的执行时间更短，这是因为 reduction 通过为每个线程分配一个私有副本，线程可以在其私有空间内自由地执行归约操作，而不需要在更新全局结果时与其他线程争夺锁资源，加上高效的数据合并方法等。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/reduction-omp-2024-02-20.webp"
alt="reduction-omp-2024-02-20" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>OpenMP reducton operation&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="同步构造">同步构造&lt;/h3>
&lt;h4 id="sections-构造">Sections 构造&lt;/h4>
&lt;ul>
&lt;li>将并行区的代码块划分为多个 section 分配执行。&lt;/li>
&lt;li>可以搭配 parallel 合成为 parallel sections 构造。&lt;/li>
&lt;li>每个 section 由一个线程执行
&lt;ul>
&lt;li>线程数大于 section 数目：部分线程空闲&lt;/li>
&lt;li>线程数小于 section 数目：部分线程分配多个 section&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>示例代码：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-c">#pragma omp sections
{
#pragma omp section
method1();
#pragma omp section
method2();
}
&lt;/code>&lt;/pre>
&lt;h4 id="barrier-构造">Barrier 构造&lt;/h4>
&lt;ul>
&lt;li>在特定位置进行栅栏同步&lt;/li>
&lt;li>在存在数据依赖的情况下，可以使用 barrier 保证数据的一致性&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/Barrier-2024-02-20.webp"
alt="Barrier-2024-02-20" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Barrier 同步示意图&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h4 id="single-构造">Single 构造&lt;/h4>
&lt;ul>
&lt;li>用于标记只有一个线程执行的代码块，带有隐式的 barrier 同步，可以使用 nowait 取消隐式的 barrier 同步。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/omp-single-2024-02-20.webp"
alt="omp-single-2024-02-20" width="70%" loading="lazy">&lt;figcaption>
&lt;h4>pragma single&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h4 id="atomic-构造">Atomic 构造&lt;/h4>
&lt;ul>
&lt;li>用于保证对共享变量的原子操作，避免数据竞争。&lt;/li>
&lt;/ul>
&lt;h3 id="false-sharing">False Sharing&lt;/h3>
&lt;ul>
&lt;li>伪共享简单来说就是指多个线程同时访问同一缓存行的不同部分，导致缓存行的无效化和重新填充，从而降低了程序的性能。&lt;/li>
&lt;li>不同核心对同一 Cache line 的同时读写会造成严重的冲突，导致改级缓存失效。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/false-sharing-2024-02-20.webp"
alt="false-sharing-2024-02-20" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>False Sharing 问题&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>在 OpenMP 中，解决伪共享的方法主要有：
&lt;ul>
&lt;li>&lt;strong>数据结构对齐&lt;/strong> ：通过使用编译器提供的对齐指令或关键字确保相关变量分别处于不同的缓存行中。例如，在 C++中可以使用 &lt;code>alignas&lt;/code> 关键字来指定变量的内存对齐方式，确保每个线程的数据独立位于不同的缓存行。&lt;/li>
&lt;li>&lt;strong>增大缓存行之间的间距&lt;/strong> ：在相邻变量之间插入足够的填充空间，使得它们不会出现在同一个缓存行内。&lt;/li>
&lt;li>&lt;strong>避免无意义的竞争&lt;/strong> ：设计算法和数据结构以减少不必要的共享数据访问。如果可能，尽量让线程操作各自独立的数据段。&lt;/li>
&lt;li>&lt;strong>自定义内存分配&lt;/strong> ：使用特殊的内存分配函数，确保分配的连续内存区域对齐到缓存行边界，这样分配给不同线程的数据就不会落在同一缓存行上。&lt;/li>
&lt;li>在某些情况下，可以利用特定平台提供的硬件特性或者编译器支持的扩展，比如 Intel 的 &lt;code>__declspec(align(#))&lt;/code> 属性（对于 MSVC）或者 &lt;code>__attribute__((aligned(#)))&lt;/code>（对于 GCC/Clang）。&lt;/li>
&lt;li>也可以通过控制变量的作用域或者利用动态创建私有副本等技术来间接避免伪共享问题。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="任务构造">任务构造&lt;/h3>
&lt;ul>
&lt;li>除了 Fork-Join 模型外，OpenMP 还支持任务并行模型，通过 &lt;code>task&lt;/code> 制导语句来实现。&lt;/li>
&lt;li>即动态地管理线程池和任务池，线程池中的线程可以动态地获取任务池中的任务进行执行，从而实现任务的并行执行。&lt;/li>
&lt;/ul>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 4：任务并行&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">#include &amp;lt;iostream&amp;gt;
#include &amp;lt;omp.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
#include &amp;lt;iomanip&amp;gt;
void big_task(int i) {
sleep(10);
}
void small_task(int i) {
sleep(1);
}
int main() {
int ntasks = 8;
double start = omp_get_wtime();
#pragma omp parallel
{
#pragma omp single
{
std::cout &amp;lt;&amp;lt; &amp;quot;Task 0 Created&amp;quot; &amp;lt;&amp;lt; std::endl;
#pragma omp task
big_task(0);
std::cout &amp;lt;&amp;lt; &amp;quot;Task 1 Created&amp;quot; &amp;lt;&amp;lt; std::endl;
#pragma omp task
big_task(1);
for (int i = 2; i &amp;lt; ntasks; i++) {
std::cout &amp;lt;&amp;lt; &amp;quot;Task &amp;quot; &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &amp;quot; Created&amp;quot; &amp;lt;&amp;lt; std::endl;
#pragma omp task
small_task(i);
}
}
#pragma omp taskwait
}
std::cout &amp;lt;&amp;lt; &amp;quot;All tasks finished&amp;quot; &amp;lt;&amp;lt; std::endl;
std::cout &amp;lt;&amp;lt; &amp;quot;Time: &amp;quot; &amp;lt;&amp;lt; std::fixed &amp;lt;&amp;lt; std::setprecision(2) &amp;lt;&amp;lt; omp_get_wtime() - start &amp;lt;&amp;lt; &amp;quot;s&amp;quot; &amp;lt;&amp;lt; std::endl;
return 0;
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>运行结果&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">$ g++ -fopenmp task.cpp -o task
$ ./task
Task 0 Created
Task 1 Created
Task 2 Created
Task 3 Created
Task 4 Created
Task 5 Created
Task 6 Created
Task 7 Created
All tasks finished
Time: 10.00s
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>这段代码中，我们使用了 &lt;code>#pragma omp task&lt;/code> 制导语句来创建任务，任务的执行由线程池中的线程动态获取并执行。在任务创建后，我们使用 &lt;code>#pragma omp taskwait&lt;/code> 来等待所有任务执行完毕。达到了一个异步执行的效果。&lt;/li>
&lt;/ul>
&lt;h3 id="向量化simd-构造">向量化：SIMD 构造&lt;/h3>
&lt;ul>
&lt;li>SIMD（Single Instruction, Multiple Data）是一种并行计算模式，它通过一条指令同时对多个数据进行操作，从而实现高效的数据并行计算。&lt;/li>
&lt;li>在 OpenMP 中，可以使用 &lt;code>#pragma omp simd&lt;/code> 制导语句来实现向量化并行计算。
&lt;ul>
&lt;li>&lt;code>aligned&lt;/code> 用于列出内存对齐的指针&lt;/li>
&lt;li>&lt;code>safelen&lt;/code> 用于标记循环展开时的数据依赖&lt;/li>
&lt;li>&lt;code>linear&lt;/code> 用于标记循环变量的线性关系&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>编译器例如 gcc 也自带向量化功能，一般使用以下编译选项
&lt;ul>
&lt;li>-O3&lt;/li>
&lt;li>-ffast-math&lt;/li>
&lt;li>-fivopts&lt;/li>
&lt;li>-march=native&lt;/li>
&lt;li>-fopt-info-vec&lt;/li>
&lt;li>-fopt-info-vec-missed&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>编译安装 UCX 1.15.0 与 OpenMPI 5.0.0：详尽指南</title><link>https://cuterwrite.top/p/openmpi-with-ucx/</link><pubDate>Thu, 01 Feb 2024 01:01:01 +0000</pubDate><guid>https://cuterwrite.top/p/openmpi-with-ucx/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/cropped_2024010204-2024-02-03.webp" alt="Featured image of post 编译安装 UCX 1.15.0 与 OpenMPI 5.0.0：详尽指南" />&lt;h1 id="编译安装-ucx-1150-与-openmpi-500详尽指南">编译安装 UCX 1.15.0 与 OpenMPI 5.0.0：详尽指南&lt;/h1>
&lt;h2 id="一环境准备">一、环境准备&lt;/h2>
&lt;p>首先，请确保您的系统满足以下基本要求：&lt;/p>
&lt;ol>
&lt;li>操作系统：支持 Linux（如 Ubuntu 20.04 LTS）或其他类 Unix 操作系统。&lt;/li>
&lt;li>开发工具包：安装必要的构建工具和库，例如 &lt;code>build-essential&lt;/code> ，&lt;code>libnuma-dev&lt;/code> ，&lt;code>pkg-config&lt;/code> 等。&lt;/li>
&lt;li>内核版本：对于最佳性能，建议使用最新稳定版内核。&lt;/li>
&lt;li>需要支持 RDMA 的硬件环境或虚拟环境。&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">sudo apt-get update
sudo apt-get install -y build-essential libnuma-dev pkg-config
&lt;/code>&lt;/pre>
&lt;h2 id="二编译安装-ucx-1150">二、编译安装 UCX 1.15.0&lt;/h2>
&lt;ol>
&lt;li>下载 UCX 源码包：&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">wget https://github.com/openucx/ucx/releases/download/v1.15.0/ucx-1.15.0.tar.gz
tar -xzvf ucx-1.15.0.tar.gz
cd ucx-1.15.0
&lt;/code>&lt;/pre>
&lt;ol start="2">
&lt;li>配置 UCX 编译选项：&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">mkdir build &amp;amp;&amp;amp; cd build
../configure --prefix=/root/software/ucx/1.5.0
&lt;/code>&lt;/pre>
&lt;p>您可以根据实际需求添加更多配置选项，比如指定特定的网卡类型或者启用特定的功能。&lt;/p>
&lt;ol start="3">
&lt;li>编译并安装：&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">make -j 8
make install
&lt;/code>&lt;/pre>
&lt;ol start="4">
&lt;li>UCX 架构说明&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>UCX 1.15.0 的架构如下图所示：&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/Architecture-2024-02-03.webp"
alt="Architecture-2024-02-03" width="auto" loading="lazy">
&lt;/figure>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>组件&lt;/th>
&lt;th>角色&lt;/th>
&lt;th>说明&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>UCP&lt;/td>
&lt;td>Protocol&lt;/td>
&lt;td>实现高级抽象，如标记匹配、流、连接协商和建立、多轨以及处理不同的内存类型。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UCT&lt;/td>
&lt;td>Transport&lt;/td>
&lt;td>实现低级通信原语，如活动消息、远程内存访问和原子操作。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UCM&lt;/td>
&lt;td>Memory&lt;/td>
&lt;td>通用的数据结构、算法和系统实用程序的集合。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UCP&lt;/td>
&lt;td>Protocol&lt;/td>
&lt;td>截获内存注册缓存使用的内存分配和释放事件。&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="三编译安装-openmpi-500">三、编译安装 OpenMPI 5.0.0&lt;/h2>
&lt;ol>
&lt;li>下载 OpenMPI 源码包：&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">wget https://download.open-mpi.org/release/open-mpi/v5.0/openmpi-5.0.0.tar.gz
tar -xzvf openmpi-5.0.0.tar.gz
cd openmpi-5.0.0
&lt;/code>&lt;/pre>
&lt;ol start="2">
&lt;li>配置 OpenMPI 编译选项，指定使用 UCX 作为传输层：&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">mkdir build &amp;amp;&amp;amp; cd build
../configure --without-hcoll \
--enable-python-bindings \
--enable-mpirun-prefix-by-default \
--prefix=/root/software/openmpi/5.0.0-ucx-1.15.0 \
--with-ucx=/root/software/ucx/1.15.0 \
--enable-mca-no-build=btl-uct
&lt;/code>&lt;/pre>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;ul>
&lt;li>对于 OpenMPI 4.0 及更高版本，&lt;code>btl_uct&lt;/code> 组件可能存在编译错误。该组件对于使用 UCX 来说并不重要；因此可以通过 &lt;code>--enable-mca-no-build=btl-uct&lt;/code> 禁用：&lt;/li>
&lt;li>&lt;code>--enable-python-bindings&lt;/code> 选项用于启用 Python 绑定。&lt;/li>
&lt;li>&lt;code>--enable-mpirun-prefix-by-default&lt;/code> 选项用于在使用 &lt;code>mpirun&lt;/code> 启动 MPI 程序时自动添加 &lt;code>--prefix&lt;/code> 选项。&lt;/li>
&lt;li>&lt;code>--without-hcoll&lt;/code> 选项用于禁用 HCOLL 组件。不设置编译时会报错 &lt;code>cannot find -lnuma&lt;/code> 与 &lt;code>cannot find -ludev&lt;/code> 的错误。&lt;/li>
&lt;/ul>&lt;/div>
&lt;p>最后配置选项如下：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/ompi-config-2024-02-03.webp"
alt="ompi-config-2024-02-03" width="auto" loading="lazy">
&lt;/figure>
&lt;ol start="3">
&lt;li>编译并安装：&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">make -j 8
make install
&lt;/code>&lt;/pre>
&lt;h2 id="四验证安装与设置环境变量">四、验证安装与设置环境变量&lt;/h2>
&lt;p>安装完成后，可以通过运行简单的 MPI 程序来验证 UCX 和 OpenMPI 是否成功集成：&lt;/p>
&lt;pre>&lt;code class="language-bash">mpirun -np 2 --mca pml ucx --mca btl ^vader,tcp,openib,uct -x UCX_NET_DEVICES=mlx5_0:1 hostname
&lt;/code>&lt;/pre>
&lt;p>（如果在 root 上运行则需要加上 &lt;code>--allow-run-as-root&lt;/code> 选项，如果有 RDMA 设备可以设置 &lt;code>-x UCX_NET_DEVICES&lt;/code> ）&lt;/p>
&lt;div class="notice notice-tip" >
&lt;div class="notice-title">&lt;svg t="1705945832245" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="19409" width="200" height="200">&lt;path d="M512 74.666667C270.933333 74.666667 74.666667 270.933333 74.666667 512S270.933333 949.333333 512 949.333333 949.333333 753.066667 949.333333 512 753.066667 74.666667 512 74.666667z m238.933333 349.866666l-2.133333 2.133334-277.333333 277.333333c-10.666667 10.666667-29.866667 12.8-42.666667 2.133333L426.666667 704l-149.333334-149.333333c-12.8-12.8-12.8-32 0-44.8 10.666667-10.666667 29.866667-12.8 42.666667-2.133334l2.133333 2.133334 125.866667 125.866666 253.866667-253.866666c10.666667-10.666667 29.866667-12.8 42.666666-2.133334l2.133334 2.133334c12.8 12.8 12.8 32 4.266666 42.666666z" fill="#ffffff" p-id="19410">&lt;/path>&lt;/svg>&lt;/div>&lt;p>如果需要结合 &lt;code>Slurm&lt;/code> 使用，可以参考 &lt;a class="link" href="https://github.com/open-mpi/ompi/blob/v5.0.x/docs/launching-apps/slurm.rst" target="_blank" rel="noopener" >Launching with Slurm
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/p>
&lt;p>其中一种方式就是，先通过 &lt;code>salloc&lt;/code> 分配资源，然后在分配的资源上运行 &lt;code>mpirun&lt;/code> 命令。此时 &lt;code>--hostfile&lt;/code> 、 &lt;code>--host&lt;/code> 、 &lt;code>-n&lt;/code> 等是不需要设置的，例如：&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-bash">salloc -n 2
mpirun --mca pml ucx --mca btl ^vader,tcp,openib,uct -x UCX_NET_DEVICES=mlx5_0:1 hostname
&lt;/code>&lt;/pre>
&lt;p>如果一切正常，您会看到两台主机名的输出。为了方便使用，可以将 OpenMPI 的 bin 目录等添加到系统 PATH 环境变量中：&lt;/p>
&lt;pre>&lt;code class="language-bash">vim ~/.bashrc
export PATH=/root/software/openmpi/5.0.0-ucx-1.15.0/bin:$PATH
export LD_LIBRARY_PATH=/root/software/openmpi/5.0.0-ucx-1.15.0/lib:$LD_LIBRARY_PATH
export CPATH=/root/software/openmpi/5.0.0-ucx-1.15.0/include:$CPATH
export MANPATH=/root/software/openmpi/5.0.0-ucx-1.15.0/share/man:$MANPATH
source ~/.bashrc
&lt;/code>&lt;/pre>
&lt;h2 id="五ucx-性能测试">五、UCX 性能测试&lt;/h2>
&lt;p>发送方：&lt;/p>
&lt;pre>&lt;code class="language-bash">ucx_perftest -c 0 -d mlx5_0:1
&lt;/code>&lt;/pre>
&lt;p>接收方：&lt;/p>
&lt;pre>&lt;code class="language-bash">ucx_perftest -c 1 -d mlx5_0:1 &amp;lt;server_hostname&amp;gt; -t tag_lat
&lt;/code>&lt;/pre>
&lt;p>总之，通过以上步骤，我们已经成功地从源代码编译并安装了 UCX 1.15.0 和 OpenMPI 5.0.0，并将其整合为一个高效稳定的高性能计算环境。在实际应用中，可以根据具体需求进一步优化配置以获得更优性能。&lt;/p></description></item><item><title>性能刺客之伪共享</title><link>https://cuterwrite.top/p/false-sharing/</link><pubDate>Sat, 02 Dec 2023 00:00:00 +0000</pubDate><guid>https://cuterwrite.top/p/false-sharing/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/6c3f8961290e41f894f5a1cbb768aba9-2023-12-02.webp" alt="Featured image of post 性能刺客之伪共享" />&lt;h1 id="性能刺客之伪共享">性能刺客之伪共享&lt;/h1>
&lt;h2 id="一前言">一、前言&lt;/h2>
&lt;p>在多核并发编程中，如果将互斥锁的争用比作 &lt;strong>性能杀手&lt;/strong> 的话，那么伪共享则相当于 &lt;strong>性能刺客&lt;/strong>。&lt;strong>杀手&lt;/strong> 与 &lt;strong>刺客&lt;/strong> 的区别在于杀手是可见的，遇到杀手时我们可以选择战斗、逃跑、绕路、求饶等多种手段去应付，但 &lt;strong>刺客&lt;/strong> 却不同， &lt;strong>刺客&lt;/strong> 永远隐藏在暗处，伺机给你致命一击，防不胜防。具体到我们的并发编程中，遇到锁争用影响并发性能情况时，我们可以采取多种措施（如缩短临界区，原子操作等等）去提高程序性能，但是伪共享却是我们从所写代码中看不出任何蛛丝马迹的，发现不了问题也就无法解决问题，从而导致伪共享在暗处严重拖累程序的并发性能，但我们却束手无策。&lt;/p>
&lt;h2 id="二缓存行">二、缓存行&lt;/h2>
&lt;p>为了进行下面的讨论，我们需要首先熟悉缓存行的概念，学过操作系统课程存储结构这部分内容的同学应该对存储器层次结构的金字塔模型印象深刻，金字塔从上往下代表存储介质的成本降低、容量变大，从下往上则代表存取速度的提高。位于金字塔模型最上层的是 CPU 中的寄存器，其次是 CPU 缓存（L1，L2，L3），再往下是内存，最底层是磁盘，操作系统采用这种存储层次模型主要是为了解决 CPU 的高速与内存磁盘低速之间的矛盾，CPU 将最近使用的数据预先读取到 Cache 中，下次再访问同样数据的时候，可以直接从速度比较快的 CPU 缓存中读取，避免从内存或磁盘读取拖慢整体速度。&lt;/p>
&lt;p>CPU 缓存的最小单位就是缓存行，缓存行大小依据架构不同有不同大小，最常见的有 &lt;code>64Byte&lt;/code> 和 &lt;code>32Byte&lt;/code> ，CPU 缓存从内存取数据时以缓存行为单位进行，每一次都取需要读取数据所在的整个缓存行，即使相邻的数据没有被用到也会被缓存到 CPU 缓存中。&lt;/p>
&lt;h2 id="三缓存一致性">三、缓存一致性&lt;/h2>
&lt;p>在单核 CPU 情况下，上述方法可以正常工作，可以确保缓存到 CPU 缓存中的数据永远是 &lt;strong>干净&lt;/strong> 的，因为不会有其他 CPU 去更改内存中的数据，但是在多核 CPU 下，情况就变得更加复杂一些。多 CPU 中，每个 CPU 都有自己的私有缓存（可能共享 L3 缓存），当一个 CPU1 对 Cache 中缓存数据进行操作时，如果 CPU2 在此之前更改了该数据，则 CPU1 中的数据就不再是 &lt;strong>干净&lt;/strong> 的，即应该是失效数据，缓存一致性就是为了保证多 CPU 之间的缓存一致。&lt;/p>
&lt;p>Linux 系统中采用 &lt;code>MESI&lt;/code> 协议处理缓存一致性，所谓 &lt;code>MESI&lt;/code> 即是指 CPU 缓存的四种状态：&lt;/p>
&lt;ul>
&lt;li>M（修改，Modified）：本地处理器已经修改缓存行，即是脏行，它的内容与内存中的内容不一样，并且此 cache 只有本地一个拷贝(专有)；&lt;/li>
&lt;li>E（专有，Exclusive）：缓存行内容和内存中的一样，而且其它处理器都没有这行数据；&lt;/li>
&lt;li>S（共享，Shared）：缓存行内容和内存中的一样, 有可能其它处理器也存在此缓存行的拷贝；&lt;/li>
&lt;li>I（无效，Invalid）：缓存行失效, 不能使用。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20231202205445-2023-12-02.webp"
alt="20231202205445-2023-12-02" width="auto" loading="lazy">
&lt;/figure>
&lt;p>每个 CPU 缓存行都在四个状态之间互相转换，以此决定 CPU 缓存是否失效，比如 CPU1 对一个缓存行执行了写入操作，则此操作会导致其他 CPU 的该缓存行进入 Invalid 无效状态， CPU 需要使用该缓存行的时候需要从内存中重新读取。由此就解决了多 CPU 之间的缓存一致性问题。&lt;/p>
&lt;h2 id="四伪共享">四、伪共享&lt;/h2>
&lt;p>何谓伪共享？上面我们提过 CPU 的缓存是 &lt;strong>以缓存行为单位&lt;/strong> 进行的，即除了本身所需读写的数据之外还会缓存与该数据在同一缓存行的数据，假设缓存行大小是 32 字节，内存中有 &lt;code>abcdefgh&lt;/code> 八个 int 型数据，当 CPU 读取 &lt;code>d&lt;/code> 这个数据时， CPU 会将 &lt;code>abcdefgh&lt;/code> 八个 int 数据组成一个缓存行加入到 CPU 缓存中。假设计算机有两个 CPU：CPU1 和 CPU2 ， CPU1 只对 &lt;code>a&lt;/code> 这个数据进行频繁读写， CPU2 只对 &lt;code>b&lt;/code> 这个数据进行频繁读写，按理说这两个 CPU 读写数据没有任何关联，也就不会产生任何竞争，不会有性能问题，但是由于 CPU 缓存是以缓存行为单位进行存取的，也是以缓存行为单位失效的，即使 CPU1 只更改了缓存行中 &lt;code>a&lt;/code> 数据，也会导致 CPU2 中该缓存行完全失效，同理，CPU2 对&lt;code> b&lt;/code> 的改动也会导致 CPU1 中该缓存行失效，由此引发了该缓存行在两个 CPU 之间 &lt;strong>乒乓&lt;/strong> ，缓存行频繁失效，最终导致程序性能下降，这就是伪共享。&lt;/p>
&lt;p>下面是维基百科的定义：&lt;/p>
&lt;blockquote>
&lt;p>In computer science, &lt;strong>false sharing&lt;/strong> is a performance-degrading usage pattern that can arise in systems with distributed, coherent caches at the size of the smallest resource block managed by the caching mechanism. When a system participant attempts to periodically access data that is not being altered by another party, but that data shares a cache block with data that is being altered, the caching protocol may force the first participant to reload the whole cache block despite a lack of logical necessity. The caching system is unaware of activity within this block and forces the first participant to bear the caching system overhead required by true shared access of a resource.&lt;/p>
&lt;p>在计算机科学中，伪共享是一种性能降低的使用模式，可能出现在具有分布式、一致性缓存的系统中，缓存大小为缓存机制管理的最小资源块。当一个系统参与者试图定期访问未被其他方修改的数据，但该数据与正在被修改的数据共享一个缓存块时，缓存协议可能会强制第一个参与者重新加载整个缓存块，尽管在逻辑上没有必要。 缓存系统无法感知这个块内的活动，并强制第一个参与者承担由真正共享资源访问所需的缓存系统开销。&lt;/p>
&lt;/blockquote>
&lt;h2 id="五如何避免伪共享">五、如何避免伪共享&lt;/h2>
&lt;p>避免伪共享主要有以下两种方式：&lt;/p>
&lt;ul>
&lt;li>缓存行填充（Padding）：为了避免伪共享就需要将可能造成伪共享的多个变量处于不同的缓存行中，可以采用在变量后面填充字节的方式达到该目的。&lt;/li>
&lt;li>使用某些语言或编译器中强制变量对齐，将变量都对齐到缓存行大小，避免伪共享发生。&lt;/li>
&lt;/ul>
&lt;h2 id="六获取缓存行大小">六、获取缓存行大小&lt;/h2>
&lt;p>在 C++11 中，可以使用 &lt;code>std::hardware_destructive_interference_size&lt;/code> 和 &lt;code>std::hardware_constructive_interference_size&lt;/code> 获取缓存行大小，前者获取的是缓存行大小，后者获取的是缓存行大小的两倍，即 &lt;code>2 * std::hardware_destructive_interference_size&lt;/code>。&lt;/p>
&lt;p>在 C 语言中，可以读取 &lt;code>coherency_line_size&lt;/code> 文件获取缓存行大小，该文件位于 &lt;code>/sys/devices/system/cpu/cpu0/cache/index0/coherency_line_size&lt;/code> ，该文件中存储的是缓存行大小的字节数，可以使用 &lt;code>cat&lt;/code> 命令查看。也可以通过 &lt;code>long cache_line_size = sysconf(_SC_LEVEL1_DCACHE_LINESIZE)&lt;/code> 的方式获取。&lt;/p>
&lt;h2 id="七通过对齐解决伪共享">七、通过对齐解决伪共享&lt;/h2>
&lt;p>C 语言中可以使用 &lt;code>posix_memalign&lt;/code> 函数来实现对齐，该函数的声明如下：&lt;/p>
&lt;pre>&lt;code class="language-c">int posix_memalign(void **memptr, size_t alignment, size_t size);
&lt;/code>&lt;/pre>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>一般伪共享都很隐蔽，很难被发现，当伪共享真正构成性能瓶颈的时候，我们有必要去努力找到并解决它，但是在大部分对性能追求没有那么高的应用中，伪共享的存在对程序的危害很小，有时并不值得耗费精力和额外的内存空间（缓存行填充）去查找系统存在的伪共享。还是那句我们一直以来应该遵循的原则 &lt;strong>“不要过度优化，不要提前优化。”&lt;/strong> 。&lt;/p>
&lt;h2 id="参考资料">参考资料&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://zhuanlan.zhihu.com/p/37069591" target="_blank" rel="noopener" >C++性能榨汁机之伪共享
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;/ul></description></item><item><title>NEST on HPC 安装教程</title><link>https://cuterwrite.top/p/nest-on-hpe-install/</link><pubDate>Mon, 30 Oct 2023 00:00:00 +0000</pubDate><guid>https://cuterwrite.top/p/nest-on-hpe-install/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20231031002508-2023-10-31.webp" alt="Featured image of post NEST on HPC 安装教程" />&lt;h1 id="nest-on-hpc-安装教程">NEST on HPC 安装教程&lt;/h1>
&lt;h2 id="1-安装-miniconda3">1. 安装 MiniConda3&lt;/h2>
&lt;p>从 &lt;a class="link" href="https://repo.anaconda.com/miniconda/Miniconda3-py39_23.5.2-0-Linux-x86_64.sh" target="_blank" rel="noopener" >Miniconda3 官方网站
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
下载 Miniconda3_py39_23.5.2 。&lt;/p>
&lt;pre>&lt;code class="language-bast">wget https://repo.anaconda.com/miniconda/Miniconda3-py39_23.5.2-0-Linux-x86_64.sh
&lt;/code>&lt;/pre>
&lt;p>执行 Miniconda3-py39_23.5.2-0-Linux-x86_64.sh ，按照提示安装 Miniconda3。（安装在 &lt;code>$HOME/software/miniconda3/23.5.2&lt;/code> 目录下）&lt;/p>
&lt;p>然后，设置 Miniconda3 环境变量。&lt;/p>
&lt;pre>&lt;code class="language-bash">export PATH=$HOME/software/miniconda3/23.5.2/bin:$PATH
&lt;/code>&lt;/pre>
&lt;h2 id="2-安装-boost">2. 安装 Boost&lt;/h2>
&lt;p>从 &lt;a class="link" href="https://boostorg.jfrog.io/artifactory/main/release/1.77.0/source/boost_1_77_0.tar.gz" target="_blank" rel="noopener" >Boost 官方网站
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
下载 Boost。&lt;/p>
&lt;pre>&lt;code class="language-bash">wget https://boostorg.jfrog.io/artifactory/main/release/1.77.0/source/boost_1_77_0.tar.gz
tar -zxvf boost_1_77_0.tar.gz
cd boost_1_77_0
&lt;/code>&lt;/pre>
&lt;p>在 Boost 根目录下执行以下命令安装 Boost：&lt;/p>
&lt;pre>&lt;code class="language-bash">module load gcc/8.4.0
./bootstrap.sh --prefix=$HOME/software/boost/1.77.0-gcc-8.4.0 \
CC=gcc CXX=g++ FC=gfortran CFLAGS='-O3' CXXFLAGS='-O3' FCFLAGS='-O3'
&lt;/code>&lt;/pre>
&lt;p>配置环境变量：&lt;/p>
&lt;pre>&lt;code class="language-bash">export BOOST_ROOT=$HOME/software/boost/1.77.0-gcc-8.4.0
export LD_LIBRARY_PATH=$BOOST_ROOT/lib:$LD_LIBRARY_PATH
export LIBRARY_PATH=$BOOST_ROOT/lib:$LIBRARY_PATH
export CMAKE_PREFIX_PATH=$BOOST_ROOT/lib/cmake:$CMAKE_PREFIX_PATH
export CPATH=$BOOST_ROOT/include:$CPATH
export LD_RUN_PATH=$BOOST_ROOT/lib:$LD_RUN_PATH
&lt;/code>&lt;/pre>
&lt;h2 id="3-安装-gnu-scientific-library">3. 安装 GNU Scientific Library&lt;/h2>
&lt;p>从 &lt;a class="link" href="https://mirror.ibcp.fr/pub/gnu/gsl/gsl-latest.tar.gzz" target="_blank" rel="noopener" >GNU Scientific Library 镜像站
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
下载 GSL。&lt;/p>
&lt;pre>&lt;code class="language-bash">wget https://mirror.ibcp.fr/pub/gnu/gsl/gsl-latest.tar.gz
tar -zxvf gsl-latest.tar.gz
&lt;/code>&lt;/pre>
&lt;p>在 GSL 根目录执行以下命令安装 GSL：&lt;/p>
&lt;pre>&lt;code class="language-bash">module load gcc/8.4.0
./configure --prefix=$HOME/software/gsl/2.7.1-gcc-8.4.0 \
CC=gcc CXX=g++ FC=gfortran CFLAGS='-O3' CXXFLAGS='-O3' FCFLAGS='-O3'
make install
&lt;/code>&lt;/pre>
&lt;p>配置环境变量：&lt;/p>
&lt;pre>&lt;code class="language-bash">export GSL_ROOT=$HOME/software/gsl/2.7.1-gcc-8.4.0
export LD_LIBRARY_PATH=$GSL_ROOT/lib:$LD_LIBRARY_PATH
export PATH=$GSL_ROOT/bin:$PATH
export CPATH=$GSL_ROOT/include:$CPATH
export LIBRARY_PATH=$GSL_ROOT/lib:$LIBRARY_PATH
export LD_RUN_PATH=$GSL_ROOT/lib:$LD_RUN_PATH
&lt;/code>&lt;/pre>
&lt;h2 id="4-安装-nest">4. 安装 NEST&lt;/h2>
&lt;p>使用 Miniconda3 创建一个虚拟环境。&lt;/p>
&lt;pre>&lt;code class="language-bash">source activate
conda create -n nest python=3.9
conda activate nest
&lt;/code>&lt;/pre>
&lt;p>使用 pip 安装 numpy, scipy, cython==0.29.36&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install numpy scipy cython==0.29.36
&lt;/code>&lt;/pre>
&lt;p>从 &lt;a class="link" href="https://github.com/nest/nest-simulator/archive/refs/tags/v3.4.tar.gz" target="_blank" rel="noopener" >NEST github 仓库
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
下载 NEST 3.4。&lt;/p>
&lt;pre>&lt;code class="language-bash">wget https://github.com/nest/nest-simulator/archive/refs/tags/v3.4.tar.gz
tar -zxvf v3.4.tar.gz
&lt;/code>&lt;/pre>
&lt;p>在 nest-simulator-3.4 目录下执行:&lt;/p>
&lt;pre>&lt;code class="language-bash">module load gcc/8.4.0
module load mvaapich2/2.3.7-gcc-8.4.0
cmake -DCMAKE_C_COMPILER=mpicc \
-DCMAKE_CXX_COMPILER=mpicxx \
-Dwith-mpi=`which mpiexec` \
-DCMAKE_C_FLAGS='-O3 -fPIC' \
-DCMAKE_CXX_FLAGS='-O3' \
-Dwith-boost=$HOME/software/boost/1.77.0-gcc-8.4.0 \
-DGSL_INCLUDE_DIR=$HOME/software/gsl/2.7.1-gcc-8.4.0/include \
-DGSL_LIBRARY=$HOME/software/gsl/2.7.1-gcc-8.4.0/lib/libgsl.a \
-DGSL_CBLAS_LIBRARY=$HOME/software/gsl/2.7.1-gcc-8.4.0/lib/libgslcblas.a \
-DCMAKE_INSTALL_PREFIX:PATH=$HOME/software/nest-simulator/3.4-gcc-8.4.0 .
&lt;/code>&lt;/pre>
&lt;p>配置环境变量：&lt;/p>
&lt;pre>&lt;code class="language-bash">export NEST_ROOT=$HOME/software/nest-simulator/3.4-gcc-8.4.0
export LIBRARY_PATH=$NEST_ROOT/lib:$LIBRARY_PATH
export LD_LIBRARY_PATH=$NEST_ROOT/lib:$LD_LIBRARY_PATH
&lt;/code>&lt;/pre>
&lt;h2 id="5-运行-hpc_benchmark-测试">5. 运行 hpc_benchmark 测试&lt;/h2>
&lt;p>运行 NEST 前需要配置 nest 环境：&lt;/p>
&lt;pre>&lt;code class="language-bash">source $HOME/software/nest-simulator/3.4-gcc-8.4.0/bin/nest_vars.sh
&lt;/code>&lt;/pre>
&lt;p>接着找到 &lt;code>hpc_benchmark.py&lt;/code> 目录，该文件位于 &lt;code>$HOME/software/nest-simulator/3.4-gcc-8.4.0/share/doc/nest/examples/hpc_benchmark.py&lt;/code>。修改其中的 params 以并行运行更大的模型。&lt;/p>
&lt;ol>
&lt;li>修改 nvp 为所需 MPI 进程数 × 每进程线程数，如 2 MPI 进程 × 14 线程 = 28&lt;/li>
&lt;li>设置合适的 scale ，如 10 。更大的需要更多 nvp 。&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-python">params = {
'nvp': 28, # total number of virtual processes
'scale': 10., # scaling factor of the network size
# others...
}
&lt;/code>&lt;/pre>
&lt;p>在 &lt;code>hpc_benchmark.py&lt;/code> 目录下执行：&lt;/p>
&lt;pre>&lt;code class="language-bash">export OMP_NUM_THREADS=14
mpiexec -N 1 -n 2 -p &amp;lt;partition_name&amp;gt; --export=all python3 hpc_benchmark.py
&lt;/code>&lt;/pre>
&lt;p>其中 -N 指定节点数，-n 指定 MPI 进程数，-p 指定分区名，如 &lt;code>compute&lt;/code>，&amp;ndash;export=all 用于将环境变量导出到 MPI 进程中。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>本文介绍了在高性能计算机上安装 NEST-3.4 的方法。&lt;/p>
&lt;h2 id="参考资料">参考资料&lt;/h2>
&lt;ol>
&lt;li>&lt;a class="link" href="https://nest-simulator.readthedocs.io/en/latest/" target="_blank" rel="noopener" >NEST 官方文档
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;/ol></description></item><item><title>CUDA 基础：内存访问模式</title><link>https://cuterwrite.top/p/cuda-base-memory-access-mode/</link><pubDate>Mon, 04 Sep 2023 00:55:55 +0000</pubDate><guid>https://cuterwrite.top/p/cuda-base-memory-access-mode/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/14ce26d6f495200cea2cfa76fefadf88eaab94e5.jpg@1256w_754h_!web-article-pic-2023-09-04.webp" alt="Featured image of post CUDA 基础：内存访问模式" />&lt;h1 id="cuda-基础内存访问模式">CUDA 基础：内存访问模式&lt;/h1>
&lt;p>大多数设备端数据访问都是从全局内存开始的，并且多数 GPU 应用程序容易受内存带宽的限制。因此，最大限度地利用全局内存带宽是调控核函数性能的基本。如果不能正确地调控全局内存的使用，其他优化方案很可能也收效甚微。&lt;/p>
&lt;p>为了在读写数据时达到最佳的性能，内存访问操作必须满足一定的条件。CUDA 执行模型的显著特征之一就是指令必须以线程束为单位进行发布和执行。存储操作也是同样。在执行内存指令时，线程束中的每个线程都提供了一个正在加载或存储的内存地址。在线程束的 32 个线程中，每个线程都提出了一个包含请求地址的单一内存访问请求，它并由一个或多个设备内存传输提供服务。根据线程束中内存地址的分布，内存访问可以被分成不同的模式。&lt;/p>
&lt;h2 id="一对齐与合并访问">一、对齐与合并访问&lt;/h2>
&lt;p>全局内存通过缓存实现加载和存储的过程如下图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230904151729-2023-09-04.webp"
alt="20230904151729-2023-09-04" width="auto" loading="lazy">
&lt;/figure>
&lt;p>全局内存是一个逻辑内存空间，用户可以通过核函数访问它。所有应用程序数据最初存在于 DRAM 上，即物理设备内存中。核函数的内存请求通常是在 DRAM 设备和片上内存间以 128 字节或 32 字节内存事务来实现。&lt;/p>
&lt;p>所有对全局内存的访问都会通过二级缓存，也有许多访问会通过一级缓存，这取决于访问类型和 GPU 架构。如果这两级缓存都被用到，那么内存访问是由一个 128 字节的内存事务实现的。如果只使用二级缓存，那么这个内存访问是由一个 32 字节的内存事务来实现的。对全局内存缓存其架构，如果允许使用一级缓存，那么可以在编译时选择启用或禁用一级缓存。&lt;/p>
&lt;p>一行一级缓存是 128 字节，它映射到设备内存中一个 128 字节 的对齐段。如果线程束中的每个线程请求一个 4 字节的值，那么每次请求就会获取 128 字节的数据，这恰好与缓存行和设备内存段的大小相契合。&lt;/p>
&lt;p>因此在优化应用程序时，需要注意设备内存访问的两个特性：&lt;/p>
&lt;ul>
&lt;li>对齐内存访问&lt;/li>
&lt;li>合并内存访问&lt;/li>
&lt;/ul>
&lt;p>我们把一次内存请求：也就是从核函数发起请求，到硬件响应返回数据这个过程称为一个内存事务（加载和存储都行）。&lt;/p>
&lt;p>当一个内存事务的首个访问地址是缓存粒度（32 或 128 字节）的偶数倍的时候：比如二级缓存 32 字节的偶数倍 64，128 字节的偶数倍 256 的时候，这个时候被称为对齐内存访问，非对齐访问就是除上述的其他情况，&lt;strong>非对齐的内存访问会造成带宽浪费&lt;/strong>。&lt;/p>
&lt;p>当一个线程束内的线程访问的内存都在一个内存块里的时候，就会出现合并访问。&lt;/p>
&lt;p>&lt;strong>对齐合并访问的状态是理想化的，也是最高速的访问方式&lt;/strong>，当线程束内的所有线程访问的数据在一个内存块，并且数据是从内存块的首地址开始被需要的，那么对齐合并访问出现了。为了最大化全局内存访问的理想状态，尽量将线程束访问内存组织成对齐合并的方式，这样的效率是最高的。下面看一个例子。&lt;/p>
&lt;p>一个线程束加载数据，使用一级缓存，并且这个事务所请求的所有数据在一个 128 字节的对齐的地址段上，如下图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230904152703-2023-09-04.webp"
alt="20230904152703-2023-09-04" width="auto" loading="lazy">
&lt;/figure>
&lt;p>上面蓝色表示全局内存，下面橙色是线程束要的数据，绿色就是对齐的地址段。&lt;/p>
&lt;p>而如果一个事务加载的数据分布在不一个对齐的地址段上，就会有以下两种情况：&lt;/p>
&lt;ol>
&lt;li>连续的，但是不在一个对齐的段上，比如，请求访问的数据分布在内存地址 1~128 ，那么 0~127 和 128~255 这两段数据要传递两次到 SM 。&lt;/li>
&lt;li>不连续的，也不在一个对齐的段上，比如，请求访问的数据分布在内存地址 0~63 和 128~191 上，明显这也需要两次加载。&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230904152901-2023-09-04.webp"
alt="20230904152901-2023-09-04" width="auto" loading="lazy">
&lt;/figure>
&lt;p>上图就是典型的一个线程束，数据分散开了，thread 0 的请求在 128 之前，后面还有请求在 256 之后，所以需要三个内存事务，而利用率，也就是从主存取回来的数据被使用到的比例，只有 $\frac{128}{128 \times 3}$ 的比例。这个比例低会造成带宽的浪费，最极端的表现，就是如果每个线程的请求都在不同的段，也就是一个 128 字节的事务只有 1 个字节是有用的，那么利用率只有 $\frac{1}{128}$ 。&lt;/p>
&lt;p>这里总结一下内存事务的优化关键：&lt;strong>用最少的事务次数满足最多的内存请求&lt;/strong>。事务数量和吞吐量的需求随设备的计算能力变化。&lt;/p>
&lt;h2 id="二全局内存读取">二、全局内存读取&lt;/h2>
&lt;p>在 SM 中，数据通过以下 3 种缓存 / 缓冲路径进行传输，具体使用何种方式取决于引用了哪种类型的设备内存：&lt;/p>
&lt;ul>
&lt;li>一级和二级缓存&lt;/li>
&lt;li>常量缓存&lt;/li>
&lt;li>只读缓存&lt;/li>
&lt;/ul>
&lt;p>一 / 二级缓存是默认路径。想要通过其它两种路径传输数据需要&lt;strong>应用程序显式说明&lt;/strong>，但想要提升性能还要取决于使用地访问模式。全局内存加载操作是否会通过一级缓存取决于两个因素：&lt;/p>
&lt;ul>
&lt;li>设备的计算能力：比较老的设备可能没有一级缓存&lt;/li>
&lt;li>编译器选项&lt;/li>
&lt;/ul>
&lt;p>在 Fermi GPU 和 Kepler K40 及以后的 GPU （计算能力为 3.5 及以上）中，可以通过编译器标志启用或禁用全局内存负载的一级缓存。默认情况下，在 Fermi 设备上对于全局内存加载可以使用一级缓存，在 K40 及以上 GPU 中禁用。以下标志通知编译器禁用一级缓存：&lt;/p>
&lt;pre>&lt;code class="language-text">-Xptxas -dlcm=cg
&lt;/code>&lt;/pre>
&lt;p>如果一级缓存被禁用，所有对全局内存的加载请求将直接进入到二级缓存；如果二级缓存缺失，则由 DRAM 完成请求。每一次内存事务可由一个、两个或四个部分执行，每个部分有 32 个字节。一级缓存也可以使用下列标识符直接启用:&lt;/p>
&lt;pre>&lt;code class="language-text">-Xptxas -dlcm=ca
&lt;/code>&lt;/pre>
&lt;p>设置这个标志后，全局内存加载请求首先尝试通过一级缓存。如果一级缓存缺失，该请求转向二级缓存。如果二级缓存缺失，则请求由 DRAM 完成。在这种模式下，一个内存加载请求由一个 128 字节的设备内存事务实现。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230904164822-2023-09-04.webp"
alt="20230904164822-2023-09-04" width="auto" loading="lazy">
&lt;/figure>
&lt;p>在 Kepler K10、K20 和 K20X GPU 中一级缓存不用来缓存全局内存加载。一级缓存专门用于&lt;strong>缓存寄存器溢出到本地内存中的数据&lt;/strong>。&lt;/p>
&lt;p>内存加载可以分为两类：&lt;/p>
&lt;ul>
&lt;li>缓存加载&lt;/li>
&lt;li>没有缓存的加载&lt;/li>
&lt;/ul>
&lt;p>内存访问有以下特点：&lt;/p>
&lt;ul>
&lt;li>是否使用缓存：一级缓存是否介入加载过程&lt;/li>
&lt;li>对齐与非对齐的：如果访问的第一个地址是 32 的倍数&lt;/li>
&lt;li>合并与非合并，访问连续数据块则是合并的&lt;/li>
&lt;/ul>
&lt;h3 id="1-缓存加载">1. 缓存加载&lt;/h3>
&lt;p>下面是使用一级缓存的加载过程&lt;/p>
&lt;ol>
&lt;li>对齐合并的访问，总线利用率 $100\%$&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230904165226-2023-09-04.webp"
alt="20230904165226-2023-09-04" width="auto" loading="lazy">
&lt;/figure>
&lt;ol start="2">
&lt;li>对齐的，但是不是连续的，每个线程访问的数据都在一个块内，但是位置是交叉的，总线利用率 $100\%$&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230904165245-2023-09-04.webp"
alt="20230904165245-2023-09-04" width="auto" loading="lazy">
&lt;/figure>
&lt;ol start="3">
&lt;li>连续非对齐的，线程束请求一个连续的非对齐的，32 个 4 字节数据，那么会出现，数据横跨两个块，但是没有对齐，当启用一级缓存的时候，就要两个 128 字节的事务来完成，总线利用率为 $50\%$&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230904165306-2023-09-04.webp"
alt="20230904165306-2023-09-04" width="auto" loading="lazy">
&lt;/figure>
&lt;ol start="4">
&lt;li>线程束所有线程请求同一个地址，那么肯定落在一个缓存行范围内，那么如果按照请求的是 4 字节数据来说，总线利用率是 $\frac{4}{128}=3.125\% $&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230904165516-2023-09-04.webp"
alt="20230904165516-2023-09-04" width="auto" loading="lazy">
&lt;/figure>
&lt;ol start="5">
&lt;li>比较坏的情况，前面提到过最坏的，就是每个线程束内的线程请求的都是不同的缓存行内，这里比较坏的情况就是，所有数据分布在 $N$ 个缓存行，其中 $1\leq N \leq 32$ ，那么请求 32 个 4 字节的数据，就需要 $N$ 个事务来完成，总线利用率也是 $\frac{1}{N}$&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230904165524-2023-09-04.webp"
alt="20230904165524-2023-09-04" width="auto" loading="lazy">
&lt;/figure>
&lt;p>CPU 和 GPU 的一级缓存有显著的差异， GPU 的一级缓存可以通过编译选项等控制，CPU 不可以，而且 CPU 的一级缓存是的替换算法是有使用频率和时间局部性的， GPU 则没有。&lt;/p>
&lt;h3 id="2-没有缓存的加载">2. 没有缓存的加载&lt;/h3>
&lt;p>没有缓存的加载是指的没有通过一级缓存，二级缓存则是不得不经过的。&lt;/p>
&lt;p>当不使用一级缓存的时候，&lt;strong>内存事务的粒度变为 32 字节&lt;/strong>，更细粒度的加载可以为非对齐或非合并的内存访问带来更好的总线利用率。&lt;/p>
&lt;ol>
&lt;li>对齐合并访问 128 字节，不用说，还是最理想的情况，使用 4 个段，总线利用率 $100\%$&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230904170430-2023-09-04.webp"
alt="20230904170430-2023-09-04" width="auto" loading="lazy">
&lt;/figure>
&lt;ol start="2">
&lt;li>对齐不连续访问 128 字节，都在四个段内，且互不相同，这样的总线利用率也是 $100\%$&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230904170454-2023-09-04.webp"
alt="20230904170454-2023-09-04" width="auto" loading="lazy">
&lt;/figure>
&lt;ol start="3">
&lt;li>连续不对齐，一个段 32 字节，所以，一个连续的 128 字节的请求，即使不对齐，最多也不会超过五个段，总线利用率至少为 $\frac{4}{5}=80\%$&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230904170542-2023-09-04.webp"
alt="20230904170542-2023-09-04" width="auto" loading="lazy">
&lt;/figure>
&lt;ol start="4">
&lt;li>所有线程访问一个 4 字节的数据，那么此时的总线利用率是 $\frac{4}{32} = 12.5\%$ ，在这种情况下，非缓存加载性能也是优于缓存加载的性能。&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230904170609-2023-09-04.webp"
alt="20230904170609-2023-09-04" width="auto" loading="lazy">
&lt;/figure>
&lt;ol start="5">
&lt;li>最坏的情况：所有目标数据分散在内存的各个角落，那么需要 $N$ 个内存段，由于请求的 128 个字节最多落在 $N$ 个 32 字节的内存分段内而不是 $N$ 个 128 字节的缓存行内，所以相比于缓存加载，即便是最坏的情况也有所改善。需要注意这里比较的前提是$N$ 不变，然而在实际情况下，当使用大粒度的缓存行时，$N$ 有可能会减少。&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230904170847-2023-09-04.webp"
alt="20230904170847-2023-09-04" width="auto" loading="lazy">
&lt;/figure>
&lt;h3 id="3-只读缓存">3. 只读缓存&lt;/h3>
&lt;p>只读缓存最初是预留给纹理内存加载用的。对计算能力为 3.5 及以上的 GPU 来说，只读缓存也支持使用全局内存加载代替一级缓存。&lt;/p>
&lt;p>只读缓存的加载粒度是 32 个字节。通常，对分散读取来说，这些更细粒度的加载要优于一级缓存。&lt;/p>
&lt;p>有两种方式可以指导内存通过只读缓存进行读取:&lt;/p>
&lt;ul>
&lt;li>使用函数 __ldg&lt;/li>
&lt;li>在间接引用的指针上使用修饰符&lt;/li>
&lt;/ul>
&lt;p>例如：&lt;/p>
&lt;pre>&lt;code class="language-c">__global__ void copyKernel(float *in, float *out)
{
int idx = blockDim * blockIdx.x + threadIdx.x;
out[idx] = __ldg(&amp;amp;in[idx]);
}
&lt;/code>&lt;/pre>
&lt;p>然后就能强制使用只读缓存了。&lt;/p>
&lt;p>也可以将常量 restrict 修饰符应用到指针上。这些修饰符帮助 nvcc 编译器识别无别名指针(即专门用来访问特定数组的指针)。nvcc 将自动通过只读缓存指导无别名指针的加载。&lt;/p>
&lt;pre>&lt;code class="language-c">__global__ void copyKernel(int * __restrict__ out, const int* __restrict__ in)
{
int idx = blockDim * blockIdx.x + threadIdx.x;
out[idx] = in[idx];
}
&lt;/code>&lt;/pre>
&lt;h2 id="三全局内存写入">三、全局内存写入&lt;/h2>
&lt;p>内存的存储操作相对简单。一级缓存不能用在 Fermi 或 Kepler GPU 上进行存储操作，在发送到设备内存之间存储操作&lt;strong>只通过二级缓存&lt;/strong>。存储操作在 &lt;strong>32 个字节段&lt;/strong>的粒度上被执行。内存事务可以同时被分为一段、两段或四段。例如，如果两个地址同属于一个 128 字节区域，但是不属于一个对齐的 64 字节区域，则会执行一个四段事务（也就是说，执行一个四段事务比执行两个一段事务效果更好）。&lt;/p>
&lt;ol>
&lt;li>对齐的，访问一个连续的 128 字节范围。存储操作使用一个四段事务完成：&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230904172018-2023-09-04.webp"
alt="20230904172018-2023-09-04" width="auto" loading="lazy">
&lt;/figure>
&lt;ol start="2">
&lt;li>分散在一个 192 字节的范围内，不连续，使用 3 个一段事务完成：&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230904172036-2023-09-04.webp"
alt="20230904172036-2023-09-04" width="auto" loading="lazy">
&lt;/figure>
&lt;ol start="3">
&lt;li>对齐的，在一个 64 字节的范围内，使用一个两段事务完成：&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230904172052-2023-09-04.webp"
alt="20230904172052-2023-09-04" width="auto" loading="lazy">
&lt;/figure>
&lt;ol start="4">
&lt;li>非对齐写入示例与读取情况类似，且更简单，因为始终不经过一级缓存，这里就略过了。&lt;/li>
&lt;/ol>
&lt;h2 id="四结构体数组与数组结构体">四、结构体数组与数组结构体&lt;/h2>
&lt;p>数组结构体（AoS）和结构体数组（SoA）是 C 语言中常见的两种数组组织方式。当存储结构化数据集时，它们代表了可以采用的两种强大的数据组织方式（结构体和数组）。&lt;/p>
&lt;p>下面是存储成对的浮点数据数据集的例子。首先，考虑这些成对数据元素集如何使用 AoS 方法进行存储。如下定义一个结构体，命名为 innerStruct ：&lt;/p>
&lt;pre>&lt;code class="language-c">struct innerStruct
{
float x;
float y;
};
&lt;/code>&lt;/pre>
&lt;p>然后，按照下面的方法定义这些结构体数组。这是利用 AoS 方式来组织数据的。它存储的是空间上相邻的数据，这在 CPU 上会有良好的缓存局部性。&lt;/p>
&lt;pre>&lt;code class="language-c">struct innerStruct myAoS[N];
&lt;/code>&lt;/pre>
&lt;p>接下来，考虑使用 SoA 方法来存储数据：&lt;/p>
&lt;pre>&lt;code class="language-c">struct innerArray
{
float x[N];
float y[N];
};
&lt;/code>&lt;/pre>
&lt;p>这里，在原结构体中每个字段的所有值都被分到各自的数组中。这不仅能将相邻数据点紧密存储起来，也能将跨数组的独立数据点存储起来。可以使用如下结构体定义一个变量：&lt;/p>
&lt;pre>&lt;code class="language-c">struct innerArray mySoA;
&lt;/code>&lt;/pre>
&lt;p>下图说明了 AoS 和 SoA 方法的内存布局。用 AoS 模式在 GPU 上存储示例数据并执行一个只有 $x$ 字段的应用程序，将导致 $50\%$ 的带宽损失，因为 $y$ 值在每 32 个字节段或 128 个字节缓存行上隐式地被加载。 AoS 格式也在不需要的 $y$ 值上浪费了二级缓存空间。&lt;/p>
&lt;p>用 SoA 模式存储数据充分利用了 GPU 的内存带宽。由于没有相同字段元素的交叉存取， GPU 上的 SoA 布局提供了合并内存访问，并且可以对全局内存实现更高效的利用。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230904173436-2023-09-04.webp"
alt="20230904173436-2023-09-04" width="auto" loading="lazy">
&lt;/figure>
&lt;p>当 32 个线程同时访问的时候， SoA 的访问就是连续的，而 AoS 则是不连续的。&lt;/p>
&lt;p>对比 AoS 和 SoA 的内存布局，我们能得到下面结论：&lt;/p>
&lt;ul>
&lt;li>并行编程范式，尤其是 SIMD（单指令多数据）对 SoA 更友好。 CUDA 中普遍倾向于 SoA 因为这种内存访问可以有效地合并。&lt;/li>
&lt;/ul>
&lt;h2 id="五性能调整">五、性能调整&lt;/h2>
&lt;p>优化设备内存带宽利用率有两个目标：&lt;/p>
&lt;ol>
&lt;li>对齐及合并内存访问，以减少带宽的浪费&lt;/li>
&lt;li>足够的并发内存操作，以隐藏内存延迟&lt;/li>
&lt;/ol>
&lt;p>实现并发内存访问量最大化是通过以下方式得到的：&lt;/p>
&lt;ol>
&lt;li>增加每个线程中执行独立内存操作的数量&lt;/li>
&lt;li>对核函数启动的执行配置进行试验，已充分体现每个 SM 的并行性&lt;/li>
&lt;/ol>
&lt;p>按照这个思路对程序进行优化，则有两种方法：展开技术和增大并行性。&lt;/p>
&lt;h3 id="1-展开技术">1. 展开技术&lt;/h3>
&lt;p>包含了内存操作的展开循环增加了更独立的内存操作。考虑如下 readOffsetUnroll4 核函数，每个线程都执行 4 个独立的内存操作。因为每个加载过程都是独立的，所以可以调用更多的并发内存访问：&lt;/p>
&lt;pre>&lt;code class="language-c">__global__ void readOffsetUnroll4(float *A, float *B, float *C, const int n, int offset)
{
unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;
unsigned int k = i + offset;
if (k + 3 * blockDim.x &amp;lt; n)
{
C[i] = A[k];
C[i + blockDim.x] = A[k + blockDim.x] + B[k + blockDim.x];
C[i + 2 * blockDim.x] = A[k + 2 * blockDim.x] + B[k + 2 * blockDim.x];
C[i + 3 * blockDim.x] = A[k + 3 * blockDim.x] + B[k + 3 * blockDim.x];
}
}
&lt;/code>&lt;/pre>
&lt;p>启用一级缓存编译选项：&lt;/p>
&lt;pre>&lt;code class="language-shell">nvcc -O3 readSegmentUnroll.cu -o readSegmentUnroll -Xptxas -dlcm=ca
&lt;/code>&lt;/pre>
&lt;p>结果表明，展开技术对性能有非常好的影响，甚至比地址对齐还要好。对于 I/O 密集型的核函数，充分说明内存访问并行有很高的优先级。&lt;/p>
&lt;h3 id="2-增大并行性">2. 增大并行性&lt;/h3>
&lt;p>可以通过调整块的大小来实现并行性调整：&lt;/p>
&lt;ul>
&lt;li>线程块最内层维度的大小对性能起着关键的作用&lt;/li>
&lt;li>在所有其它情况下，线程块的数量越多，一般性能越高。因此，增大并行性仍然是性能优化的一个重要因素。&lt;/li>
&lt;/ul>
&lt;h2 id="参考资料">参考资料&lt;/h2>
&lt;p>[1] CUDA C 编程权威指南，机械工业出版社，（美）程润伟（John Cheng） 等著&lt;/p></description></item><item><title>CUDA 基础：内存管理</title><link>https://cuterwrite.top/p/cuda-base-memory-manage/</link><pubDate>Sat, 02 Sep 2023 05:55:55 +0000</pubDate><guid>https://cuterwrite.top/p/cuda-base-memory-manage/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/fb80f7f3a9a0e016420a324823ef950b9847fb8d.jpg@1256w_2128h_!web-article-pic-2023-09-02.webp" alt="Featured image of post CUDA 基础：内存管理" />&lt;h1 id="cuda-基础内存管理">CUDA 基础：内存管理&lt;/h1>
&lt;p>CUDA 编程的内存管理与 C 语言的类似，需要程序员显式地管理主机和设备之间的数据移动。随着 CUDA 版本的升级，NVIDIA 正系统地实现主机和设备内存空间的统一，但对于大多数应用程序来说，仍需要手动移动数据。本文重点在于如何使用 CUDA 函数来显式地管理内存和数据移动。&lt;/p>
&lt;ul>
&lt;li>分配和释放设备内存&lt;/li>
&lt;li>在主机和设备之间传输数据&lt;/li>
&lt;/ul>
&lt;p>为了达到最优性能，CUDA 提供了在主机端准备设备内存的函备内存的函数，并且显式地向设备传输数据和从设备中获取数据。&lt;/p>
&lt;h2 id="一内存分配和释放">一、内存分配和释放&lt;/h2>
&lt;p>CUDA 编程模型假设了一个包含一个主机和一个设备的异构系统，每一个异构系统都有自己独立的内存空间。核函数在设备内存空间中运行，CUDA 运行时提供函数以分配和释放设备内存。用户可以在主机上使下列函数分配全局内存：&lt;/p>
&lt;pre>&lt;code class="language-c">cudaError_t cudaMalloc(void **devPtr, size_t size);
&lt;/code>&lt;/pre>
&lt;p>这个函数在设备上分配了 count 字节的全局内存，并用 devptr 指针返回该内存的地址。所分配的内存支持任何变量类型，包括整型、浮点类型变量、布尔类型等。如果 cudaMalloc 函数执行失败则返回 cudaErrorMemoryAllocation 。在已分配的全局内存中的值不会被清除。用户需要用从主机上传输的数据来填充所分配的全局内存，或用下列函数将其初始化：&lt;/p>
&lt;pre>&lt;code class="language-c">cudaError_t cudaMemset(void *devPtr, int value, size_t count);
&lt;/code>&lt;/pre>
&lt;p>这个函数用存储在变量 value 中的值来填充从设备内存地址 devPtr 处开始的 count 字节。&lt;/p>
&lt;p>一旦一个应用程序不再使用已分配的全局内存，那么可以以下代码释放该内存空间：&lt;/p>
&lt;pre>&lt;code class="language-c">cudaError_t cudaFree(void *devPtr);
&lt;/code>&lt;/pre>
&lt;p>这个函数释放了 devPtr 指向的全局内存，该内存必须在此前使用了一个设备分配函数（如 cudaMalloc）来进行分配。否则，它将返回一个错误 cudaErrorInvalidDevicePointer 。如果地址空间已经被释放，那么 cudaFree 也返回一个错误。&lt;/p>
&lt;p>设备内存的分配和释放操作成本较高，所以应用程序应&lt;strong>重利用设备内存&lt;/strong>，以减少对整体性能的影响。&lt;/p>
&lt;h2 id="二内存传输">二、内存传输&lt;/h2>
&lt;p>一旦分配好了全局内存，就可以使用下列函数从主机向设备传输数据：&lt;/p>
&lt;pre>&lt;code class="language-c">cudaError_t cudaMemcpy(void *dst, const void *src, size_t count, cudaMemcpyKind kind);
&lt;/code>&lt;/pre>
&lt;p>这个函数从内存位置 src 复制了 count 字节到内存位置 dst 。变量 kind 指定了复制的方向，可以有下列取值：&lt;/p>
&lt;ul>
&lt;li>cudaMemcpyHostToHost：从主机内存复制到主机内存&lt;/li>
&lt;li>cudaMemcpyHostToDevice：从主机内存复制到设备内存&lt;/li>
&lt;li>cudaMemcpyDeviceToHost：从设备内存复制到主机内存&lt;/li>
&lt;li>cudaMemcpyDeviceToDevice：从设备内存复制到设备内存&lt;/li>
&lt;/ul>
&lt;p>如果指针 dst 和 src 与 kind 指定的复制方向不一致，那么 cudaMemcpy 的行为就是未定义行为。这个函数在大多数情况下都是同步的。&lt;/p>
&lt;p>下图为 CPU 内存和 GPU 内存间的连接性能。从图中可以看到 GPU 芯片和板载 GDDR5 GPU 内存之间的理论峰值带宽非常高，对于 Fermi C2050 GPU 来说为 144GB/s 。CPU 和 GPU 之间通过 PCIe Gen2 总线相连，这种连接的理论带宽要低得多，为 8GB/s（ PCIe Gen3 总线最大理论限制值是 16GB/s）。这种差距意味着如果管理不当的话，主机和设备间的数据传输会降低应用程序的整体性能。因此，CUDA 编程的一个基本原则应是尽可能地减少主机与设备之间的传输。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230902212815-2023-09-02.webp"
alt="20230902212815-2023-09-02" width="auto" loading="lazy">
&lt;/figure>
&lt;h2 id="三固定内存">三、固定内存&lt;/h2>
&lt;p>分配的主机内存默认是 pageable（可分页），它的意思也就是因页面错误导致的操作，该操作按照操作系统的要求将主机虚拟内存上的数据移动到不同的物理位置。虚拟内存给人一种比实际可用内存大得多的假象，就如同一级缓存好像比实际可用的片上内存大得多一样。&lt;/p>
&lt;p>GPU &lt;strong>不能在可分页主机内存上安全地访问数据&lt;/strong>，因为当主机操作系统在物理位置上移动该数据时，它无法控制。当从可分页主机内存传输数据到设备内存时，CUDA 驱动程序首先分配&lt;strong>临时页面锁定的或固定的&lt;/strong>主机内存，将主机源数据复制到固定内存中，然后从固定内存传输数据给设备内存，如下图左边部分所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230903003123-2023-09-03.webp"
alt="20230903003123-2023-09-03" width="auto" loading="lazy">
&lt;/figure>
&lt;p>左边是正常分配内存，传输过程是：锁页-复制到固定内存-复制到设备&lt;/p>
&lt;p>右边是分配时就是固定内存，直接传输到设备上。&lt;/p>
&lt;p>下面函数用来分配固定内存：&lt;/p>
&lt;pre>&lt;code class="language-c">cudaError_t cudaMallocHost(void ** devPtr,size_t count);
&lt;/code>&lt;/pre>
&lt;p>这个函数分配了 count 字节的主机内存，这些内存是页面锁定的并且对设备来说是可访问的。由于固定内存能被设备直接访问，所以它能用比可分页内存高得多的带宽进行读写。然而，分配过多的固定内存可能会降低主机系统的性能，因为它减少了用于存储虚拟内存数据的可分页内存的数量，其中分页内存对主机系统是可用的。&lt;/p>
&lt;p>固定的主机内存释放使用：&lt;/p>
&lt;pre>&lt;code class="language-c">cudaError_t cudaFreeHost(void * devPtr);
&lt;/code>&lt;/pre>
&lt;p>总的来说，固定内存的释放和分配成本比可分页内存要高很多，但是传输速度更快，所以对于大规模数据，固定内存效率更高。应该尽量使用流来使内存传输和计算之间同时进行。&lt;/p>
&lt;h2 id="四零拷贝内存">四、零拷贝内存&lt;/h2>
&lt;p>通常来说，主机不能直接访问设备变量，同时设备也不能直接访问主机变量。但有一个例外：零拷贝内存。&lt;strong>主机和设备都可以访问零拷贝内存&lt;/strong>。&lt;/p>
&lt;p>GPU 线程可以直接访问零拷贝内存。在 CUDA 核函数中使用零拷贝内存有以下几个优势。&lt;/p>
&lt;ul>
&lt;li>当设备内存不足时可利用主机内存&lt;/li>
&lt;li>避免主机和设备间的显式数据传输&lt;/li>
&lt;li>提高 PCIe 传输率&lt;/li>
&lt;/ul>
&lt;p>当使用零拷贝内存来共享主机和设备间的数据时，用户必须同步主机和设备间的内存访问，同时更改主机和设备的零拷贝内存中的数据将导致不可预知的后果。&lt;/p>
&lt;p>零拷贝内存是固定内存，不可分页，该内存映射到设备地址空间中。用户可以通过下列函数创建零拷贝内存：&lt;/p>
&lt;pre>&lt;code class="language-c">cudaError_t cudaHostAlloc(void ** pHost,size_t count,unsigned int flags)
&lt;/code>&lt;/pre>
&lt;p>最后一个标志参数，可以选择以下值：&lt;/p>
&lt;ul>
&lt;li>cudaHostAllocDefalt：和 cudaMallocHost 函数一致&lt;/li>
&lt;li>cudaHostAllocPortable：返回能被所有 CUDA 上下文使用的固定内存&lt;/li>
&lt;li>cudaHostAllocMapped：产生零拷贝内存，可以实现主机写入和设备读取被映射到设备地址空间中的主机内存&lt;/li>
&lt;li>cudaHostAllocWriteCombined：返回写结合内存，在某些设备上这种内存传输效率更高&lt;/li>
&lt;/ul>
&lt;p>注意，零拷贝内存虽然不需要显式的传递到设备上，但是设备还不能通过 pHost 直接访问对应的内存地址，设备需要访问主机上的零拷贝内存，需要先获得另一个地址，这个地址帮助设备访问到主机对应的内存，方法是：&lt;/p>
&lt;pre>&lt;code class="language-c">cudaError_t cudaHostGetDevicePointer(void ** pDevice,void * pHost,unsigned int flags)
&lt;/code>&lt;/pre>
&lt;p>pDevice 就是设备上访问主机零拷贝内存的指针了，此处 flags 必须设置为 0 。&lt;/p>
&lt;p>在进行频繁的读写操作时，使用零拷贝内存作为设备内存的补充将显著降低性能。因为每一次映射到内存的传输必须经过 PCIe 总线。与全局内存相比，延迟也显著增加。&lt;/p>
&lt;p>注意不要过度使用零拷贝内存。由于其延迟较高，从零拷贝内存中读取设备核函数可能很慢。&lt;/p>
&lt;h2 id="五统一虚拟寻址">五、统一虚拟寻址&lt;/h2>
&lt;p>计算能力为 2.0 及以上版本的设备支持一种特殊的寻址方式，称为&lt;strong>统一虚拟寻址（UVA）&lt;/strong>。UVA，在 CUDA 4.0 中被引入，支持 64 位 Linux 系统。有了 UVA，主机内存和设备内存可以共享同一个虚拟地址空间，如下图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230903004449-2023-09-03.webp"
alt="20230903004449-2023-09-03" width="auto" loading="lazy">
&lt;/figure>
&lt;p>UVA 之前，我们要管理所有的设备和主机内存，尤其是它们的指针，零拷贝内存尤其麻烦。有了 UVA，由指针指向的内存空间对应用程序代码来说是透明的。&lt;/p>
&lt;p>通过 UVA，由 cudaHostAlloc 分配的固定主机内存具有相同的主机和设备指针。因此，可以将返回的指针直接传递给核函数。&lt;/p>
&lt;p>前面的零拷贝内存，可以知道以下几个方面：&lt;/p>
&lt;ul>
&lt;li>分配映射的固定主机内存&lt;/li>
&lt;li>使用 CUDA 运行时函数获取映射到固定内存的设备指针&lt;/li>
&lt;li>将设备指针传递给核函数&lt;/li>
&lt;/ul>
&lt;p>有了 UVA ，可以不用上面的那个获得设备上访问零拷贝内存的函数了：&lt;/p>
&lt;pre>&lt;code class="language-c">cudaError_t cudaHostGetDevicePointer(void ** pDevice, void * pHost, unsigned flags);
&lt;/code>&lt;/pre>
&lt;p>因为 UVA 之后，主机和设备的指针都是一样的，所以可以直接传递给核函数了。&lt;/p>
&lt;h2 id="六统一内存寻址">六、统一内存寻址&lt;/h2>
&lt;p>在 CUDA 6.0 中，引入了&lt;strong>统一内存寻址&lt;/strong>这一新特性，它用于简化 CUDA 编程模型中的内存管理。统一内存中创建了一个托管内存池，内存池中已分配的空间可以用相同的内存地址（即指针）在 CPU 和 GPU 上进行访问。底层系统在统一内存空间中自动在主机和设备之间进行数据传输。这种数据传输对应用程序是透明的，这大大简化了程序代码。&lt;/p>
&lt;p>统一内存寻址依赖于 UVA 的支持，但它们是完全不同的技术。 UVA 为系统中的所有处理器提供了一个单一的虚拟内存地址空间。但是， UVA 不会自动将数据从一个物理位置转移到另一个位置，这是统一内存寻址的一个特有功能。&lt;/p>
&lt;p>统一内存寻址提供了一个&lt;strong>单指针到数据&lt;/strong>模型，在概念上它类似于零拷贝内存。但是零拷贝内存在主机内存中进行分配，因此，由于受到在 PCIe 总线上访问零拷贝内存的影响，核函数的性能将具有较高的延迟。另一方面，统一内存寻址将内存和执行空间分离，因此可以根据需要将数据透明地传输到主机或设备上，以提升局部性和性能。&lt;/p>
&lt;p>托管内存指的是由底层系统自动分配的统一内存，未托管内存就是用户自己分配的内存，这时候对于核函数，可以传递给它两种类型的内存，已托管和未托管内存，可以同时传递。&lt;/p>
&lt;p>托管内存可以是静态的，也可以是动态的，添加 managed 关键字修饰托管内存变量。静态声明的托管内存作用域是文件，这一点可以注意一下。&lt;/p>
&lt;p>托管内存分配方式：&lt;/p>
&lt;pre>&lt;code class="language-c">cudaError_t cudaMallocManaged(void ** devPtr, size_t size, unsigned int flags);
&lt;/code>&lt;/pre>
&lt;p>这个函数分配 size 字节的托管内存，并用 devPtr 返回一个指针。该指针在所有设备和主机上都是有效的。使用托管内存的程序行为与使用未托管内存的程序副本行为在功能上是一致的。但是，使用托管内存的程序可以利用自动数据传输和重复指针消除功能。&lt;/p>
&lt;p>在 CUDA 6.0 中，设备代码不能调用 cudaMallocManaged 函数。所有的托管内存必须在主机端动态声明或者在全局范围内静态声明。&lt;/p>
&lt;h2 id="参考资料">参考资料&lt;/h2>
&lt;p>[1] CUDA C 编程权威指南，机械工业出版社，（美）程润伟（John Cheng） 等著&lt;/p></description></item><item><title>CUDA 基础：内存模型概述</title><link>https://cuterwrite.top/p/cuda-base-memory-model/</link><pubDate>Fri, 01 Sep 2023 04:00:00 +0000</pubDate><guid>https://cuterwrite.top/p/cuda-base-memory-model/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/47a9b8a012cf3a3f552c9aba3aeaa93fe669cf70.jpg@1256w_970h_!web-article-pic-2023-09-01.webp" alt="Featured image of post CUDA 基础：内存模型概述" />&lt;h1 id="cuda-基础内存模型概述">CUDA 基础：内存模型概述&lt;/h1>
&lt;p>内存的访问和管理是所有编程语言的重要部分。在现代加速器中，内存管理对高性能计算有着很大的影响。&lt;/p>
&lt;p>因为多数工作负载被加载和存储数据的速度所限制，所以有大量低延迟、高带宽的内存对性能是十分有利的。然而，大容量、高性能的内存造价高且不容易生产。因此，在现有的硬件存储子系统下，必须依靠&lt;strong>内存模型&lt;/strong>获得最佳的延迟和带宽。CUDA 内存模型结合了主机和设备的内存系统，展现了完整的内存层次结构，能显式地控制数据布局以优化性能。&lt;/p>
&lt;h2 id="一内存层次结构的优点">一、内存层次结构的优点&lt;/h2>
&lt;p>程序具有局部性特点，包括：&lt;/p>
&lt;ol>
&lt;li>时间局部性：如果一个数据被访问，那么它在不久的将来也会被访问。&lt;/li>
&lt;li>空间局部性：如果一个数据被访问，那么它附近的数据也会被访问。&lt;/li>
&lt;/ol>
&lt;p>现代计算机的内存结构主要如下：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230901232704-2023-09-01.webp"
alt="20230901232704-2023-09-01" width="auto" loading="lazy">
&lt;/figure>
&lt;p>一个内存层次结构由具有不同延迟、带宽和容量的多级内存组成。通常，随着从处理器到内存延迟的增加，内存的容量也在增加。&lt;/p>
&lt;p>CPU 和 GPU 的主存都采用的是 DRAM（动态随机存取存储器），而低延迟内存（如 CPU 一级缓存）使用的则是 SRAM（静态随机存取存储器）。内存层次结构中最大且最慢的级别通常使用磁盘或闪存驱动来实现。在这种内存层次结构中，当数据被处理器频繁使用时，该数据保存在低延迟、低容量的存储器中；而当该数据被存储起来以备后用时，数据就存储在高延迟、大容量的存储器中。这种内存层次结构符合大内存低延迟的设想。&lt;/p>
&lt;p>GPU 和 CPU 的内存设计有相似的准则和模型。但它们的主要区别是，CUDA 编程模型能将内存层次结构更好地呈现给用户，能让我们显式地控制它的行为。&lt;/p>
&lt;h2 id="二cuda-内存模型">二、CUDA 内存模型&lt;/h2>
&lt;p>对于程序员来说，一般有两种类型的存储器：&lt;/p>
&lt;ol>
&lt;li>可编程的：你需要显式地控制哪些数据存放在可编程内存中&lt;/li>
&lt;li>不可编程的：你不能决定数据的存放位置，程序将自动生成存放位置以获得良好的性能&lt;/li>
&lt;/ol>
&lt;p>CPU 内存结构中，一级二级缓存都是不可编程（完全不可控制）的存储设备。另一方面，CUDA 内存模型相对于 CPU 来说更为丰富，提出了多种可编程内存的类型：&lt;/p>
&lt;ul>
&lt;li>寄存器&lt;/li>
&lt;li>共享内存&lt;/li>
&lt;li>本地内存&lt;/li>
&lt;li>常量内存&lt;/li>
&lt;li>纹理内存&lt;/li>
&lt;li>全局内存&lt;/li>
&lt;/ul>
&lt;p>下图所示为这些内存空间的层次结构，每种都有不同的作用域、生命周期和缓存行为。一个核函数中的线程都有自己私有的&lt;strong>本地内存&lt;/strong>。一个线程块有自己的&lt;strong>共享内存&lt;/strong>，对同一线程块中所有线程都可见，其内容持续线程块的整个生命周期。所有线程都可以访问&lt;strong>全局内存&lt;/strong>。所有线程都能访问的只读内存空间有：&lt;strong>常量内存空间和纹理内存空间&lt;/strong>。全局内存、常量内存和纹理内存空间有不同的用途。&lt;strong>纹理内存&lt;/strong>为各种数据布局提供了不同的寻址模式和滤波模式。对于一个应用程序来说， &lt;strong>全局内存、常量内存和纹理内存&lt;/strong>中的内容具有相同的生命周期。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230901233955-2023-09-01.webp"
alt="20230901233955-2023-09-01" width="auto" loading="lazy">
&lt;/figure>
&lt;h3 id="1-寄存器">1. 寄存器&lt;/h3>
&lt;p>寄存器无论是在 CPU 还是在 GPU 都是速度最快的内存空间，但是和 CPU 不同的是 GPU 的寄存器储量要多一些，而且当我们在核函数内不加修饰的声明一个变量，此变量就存储在寄存器中，但是 CPU 运行的程序有些不同，只有当前在计算的变量存储在寄存器中，其余在主存中，使用时传输至寄存器。在核函数声明的数组中，&lt;strong>如果用于引用该数组的索引是常量且能在编译时确定&lt;/strong>，那么该数组也存储在寄存器中。&lt;/p>
&lt;p>寄存器变量对于每个线程来说都是私有的，一个核函数通常使用寄存器来保存需要频繁访问的线程私有变量。寄存器变量与核函数的生命周期相同。一旦核函数执行完毕，就不能对寄存器变量进行访问了。&lt;/p>
&lt;p>寄存器是 SM 中的稀缺资源，Fermi 架构中每个线程最多 63 个寄存器。Kepler 结构扩展到 255 个 寄存器，一个线程如果使用更少的寄存器，那么就会有更多的常驻线程块，SM 上并发的线程块越多，效率越高，性能和使用率也就越高。&lt;/p>
&lt;p>那么问题就来了，如果一个线程里面的变量太多，以至于寄存器完全不够呢？这时候寄存器发生溢出，本地内存就会过来帮忙存储多出来的变量，这种情况会对效率产生非常负面的影响，所以，不到万不得已，一定要避免此种情况发生。&lt;/p>
&lt;p>为了避免寄存器溢出，可以在核函数的代码中配置额外的信息来辅助编译器优化，比如：&lt;/p>
&lt;pre>&lt;code class="language-cpp">__global__ void
__lauch_bounds__(maxThreadaPerBlock, minBlocksPerMultiprocessor)
kernel(...) {
/* kernel code */
}
&lt;/code>&lt;/pre>
&lt;p>这里面在核函数定义前加了一个 关键字 &lt;strong>lauch_bounds&lt;/strong> ，然后它后面对应了两个变量：&lt;/p>
&lt;ol>
&lt;li>maxThreadaPerBlock：线程块内包含的最大线程数，线程块由核函数来启动&lt;/li>
&lt;li>minBlocksPerMultiprocessor：可选参数，每个 SM 中预期的最小的常驻内存块参数。注意，对于一定的核函数，优化的启动边界会因为不同的结构而不同
也可以在编译选项中加入 &lt;strong>-maxrregcount=32&lt;/strong> 来指定每个线程使用的最大寄存器数。&lt;/li>
&lt;/ol>
&lt;h3 id="2-本地内存">2. 本地内存&lt;/h3>
&lt;p>核函数中符合存储在寄存器中但不能进入被该核函数分配的寄存器空间中的变量将溢出到本地内存中。编译器可能存放到本地内存中的变量有：&lt;/p>
&lt;ul>
&lt;li>在编译时使用未知索引引用的本地数组&lt;/li>
&lt;li>可能会占用大量寄存器空间的较大本地结构体或数组&lt;/li>
&lt;li>任何不满足核函数寄存器限定条件的变量&lt;/li>
&lt;/ul>
&lt;p>本地内存实质上是和全局内存一样在同一块存储区域当中的，其访问特点——高延迟，低带宽。对于计算能力 2.0 以上的设备，本地内存存储在每个 SM 的一级缓存，或者设备的二级缓存上。&lt;/p>
&lt;h3 id="3-共享内存">3. 共享内存&lt;/h3>
&lt;p>在核函数中使用 &lt;strong>__shared__&lt;/strong> 修饰符修饰的变量存放在共享内存中。&lt;/p>
&lt;p>因为共享内存是片上内存，所以与本地内存或全局内存相比，它具有更高的带宽和更低的延迟。它的使用类似于 CPU 一级缓存，但它是可编程的。&lt;/p>
&lt;p>每一个 SM 都有一定数量的由线程块分配的共享内存。因此，必须非常小心不要过度使用共享内存，否则将在不经意间限制活跃线程束的数量。&lt;/p>
&lt;p>共享内存在核函数的范围内声明，其生命周期伴随着整个线程块。当一个线程块执行结束后，其分配的共享内存将被释放并重新分配给其他线程块。&lt;/p>
&lt;p>共享内存是线程之间相互通信的基本方式。因为共享内存是块内线程可见的，所以就有竞争问题的存在，也可以通过共享内存进行通信，当然，为了避免内存竞争，可以使用同步语句：&lt;/p>
&lt;pre>&lt;code class="language-cpp">__syncthreads();
&lt;/code>&lt;/pre>
&lt;p>此语句相当于在线程块执行时各个线程的一个障碍点，当块内所有线程都执行到本障碍点的时候才能进行下一步的计算，这样可以设计出避免内存竞争的共享内存使用程序。但是，该语句频繁使用会影响内核执行效率。SM 中的一级缓存和共享内存都使用 64KB 的片上内存，它通过静态划分，但在运行时可以通过如下指令进行动态配置：&lt;/p>
&lt;pre>&lt;code class="language-cpp">cudaError_t cudaFuncSetCacheConfig ( const void* func, cudaFuncCache cacheConfig )
&lt;/code>&lt;/pre>
&lt;p>这个函数在每个核函数的基础上配置了片上内存划分，为 func 指定的核函数设置了配置。支持的缓存配置如下：&lt;/p>
&lt;pre>&lt;code class="language-text">cudaFuncCachePreferNone // 无参考值，默认设置
cudaFuncCachePreferShared // 48k 共享内存，16k 一级缓存
cudaFuncCachePreferL1 // 48k 一级缓存，16k 共享内存
cudaFuncCachePreferEqual // 32k 一级缓存，32k 共享内存
&lt;/code>&lt;/pre>
&lt;p>Fermi 架构支持前三种，后面的设备都支持。&lt;/p>
&lt;h3 id="4-常量内存">4. 常量内存&lt;/h3>
&lt;p>常量内存驻留在设备内存中，每个 SM 都有专用的常量内存缓存，常量内存使用 &lt;strong>__constant__&lt;/strong> 修饰符修饰。&lt;/p>
&lt;p>常量变量必须在全局空间内和所有核函数之外进行声明。对于所有计算能力的设备，都只可以声明 64kB 的常量内存，常量内存是静态声明的，并对同一编译单元中的所有核函数可见。&lt;/p>
&lt;p>核函数只能从常量内存中读取数据（只读）。因此，常量内存必须在主机端使用下面的函数来初始化：&lt;/p>
&lt;pre>&lt;code class="language-cpp">cudaError_t cudaMemcpyToSymbol ( const void* symbol, const void* src, size_t count, size_t offset = 0, cudaMemcpyKind kind = cudaMemcpyHostToDevice )
&lt;/code>&lt;/pre>
&lt;p>这个函数将 count 个字节从 src 指向的内存复制到 symbol 指向的内存中，这个变量存放在设备的全局内存或常量内存中。在大多数情况下这个函数是同步的。&lt;/p>
&lt;p>线程束中的所有线程从相同的内存地址中读取数据时，常量内存表现最好。举个例子，数学公式中的系数就是一个很好的使用常量内存的例子，因为一个线程束中所有的线程使用相同的系数来对不同数据进行相同的计算。如果线程束里每个线程都从不同的地址空间读取数据，并且只读一次，那么常量内存中就不是最佳选择，因为每从一个常量内存中读取一次数据，都会广播给线程束里的所有线程。&lt;/p>
&lt;h3 id="5-纹理内存">5. 纹理内存&lt;/h3>
&lt;p>纹理内存驻留在设备内存中，并在每个 SM 的只读缓存中缓存。&lt;strong>纹理内存&lt;/strong>是一种通过指定的只读缓存访问的全局内存。只读缓存包括硬件滤波的支持，它可以将浮点插入作为读过程的一部分来执行。纹理内存是对&lt;strong>二维空间局部性&lt;/strong>的优化所以线程束里使用纹理内存访问二维数据的线程可以达到最优性能。对于一些应用程序来说，这是理想的内存，并由于缓存和滤波硬件的支持所以有较好的性能优势。然而对于另一些应用程序来说，与全局内存相比，使用纹理内存更慢。&lt;/p>
&lt;p>总的来说纹理内存设计目的应该是为了 GPU 本职工作显示设计的，但是对于某些特定的程序可能效果更好，比如需要滤波的程序，可以直接通过硬件完成。&lt;/p>
&lt;h3 id="6-全局内存">6. 全局内存&lt;/h3>
&lt;p>全局内存是 GPU 中最大、&lt;strong>延迟最高&lt;/strong>并且最常使用的内存。 global 指的是其作用域和生命周期。它的声明可以在任何 SM 设备上被访问到，并且贯穿应用程序的整个生命周期。一个全局内存变量可以被静态声明或动态声明。可以使用 &lt;strong>__device__&lt;/strong> 修饰符在设备代码中静态地声明一个变量。&lt;/p>
&lt;p>默认通过 cudaMalloc 声明的所有在 GPU 上访问的内存都是全局内存，也就是没有对内存进行任何优化。因为全局内存的性质，当有多个核函数同时执行的时候，如果使用到了同一全局变量，应注意内存竞争。&lt;/p>
&lt;p>全局内存访问是对齐，也就是一次要读取指定大小 $(32，64，128)$ 整数倍字节的内存，所以当线程束执行内存加载/存储时，需要满足的传输数量通常取决与以下两个因素：&lt;/p>
&lt;ol>
&lt;li>跨线程的内存地址分布&lt;/li>
&lt;li>内存事务的对齐方式&lt;/li>
&lt;/ol>
&lt;p>在一般情况下，用来满足内存请求的事务越多，未使用的字节被传输回的可能性就越高，这就造成了数据吞吐率的降低。&lt;/p>
&lt;p>对于一个给定的线程束内存请求，事务数量和数据吞吐率是由设备的计算能力来确定的。对于计算能力为 1.0 和 1.1 的设备，全局内存访问的要求是非常严格的。对于计算能力高于 1.1 的设备，由于内存事务被缓存，所以要求较为宽松。缓存的内存事务利用数据局部性来提高数据吞吐率。&lt;/p>
&lt;h3 id="7-gpu-缓存">7. GPU 缓存&lt;/h3>
&lt;p>与 CPU 缓存类似， GPU 缓存是不可编程的内存。在 GPU 上有 4 种缓存：&lt;/p>
&lt;ul>
&lt;li>一级缓存&lt;/li>
&lt;li>二级缓存&lt;/li>
&lt;li>只读常量缓存&lt;/li>
&lt;li>只读纹理缓存&lt;/li>
&lt;/ul>
&lt;p>每个 SM 都有一个一级缓存，所有的 SM 共享一个二级缓存。一级和二级缓存都被用来在存储本地内存和全局内存中的数据，也包括寄存器溢出的部分。对 Fermi GPU 和 Kepler K40 或其后发布的 GPU 来说，CUDA 允许我们配置读操作的数据是使用一级和二级缓存，还是只使用二级缓存。&lt;/p>
&lt;p>在 CPU 上，内存的加载和存储都可以被缓存。但是，在 GPU 上只有内存加载操作可以被缓存，内存存储操作不能被缓存。&lt;/p>
&lt;p>每个 SM 也有一个只读常量缓存和只读纹理缓存，它们用于在设备内存中提高来自于各自内存空间内的读取性能。&lt;/p>
&lt;h3 id="8-cuda-变量声明总结">8. CUDA 变量声明总结&lt;/h3>
&lt;p>用表格进行总结：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">修饰符&lt;/th>
&lt;th style="text-align:center">变量名&lt;/th>
&lt;th style="text-align:center">存储器&lt;/th>
&lt;th style="text-align:center">作用域&lt;/th>
&lt;th style="text-align:center">生命周期&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">无&lt;/td>
&lt;td style="text-align:center">float var&lt;/td>
&lt;td style="text-align:center">寄存器&lt;/td>
&lt;td style="text-align:center">线程&lt;/td>
&lt;td style="text-align:center">线程&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">无&lt;/td>
&lt;td style="text-align:center">float var[100]&lt;/td>
&lt;td style="text-align:center">本地&lt;/td>
&lt;td style="text-align:center">线程&lt;/td>
&lt;td style="text-align:center">线程&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">__shared__&lt;/td>
&lt;td style="text-align:center">float var*&lt;/td>
&lt;td style="text-align:center">共享内存&lt;/td>
&lt;td style="text-align:center">块&lt;/td>
&lt;td style="text-align:center">块&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">__device__&lt;/td>
&lt;td style="text-align:center">float var*&lt;/td>
&lt;td style="text-align:center">全局内存&lt;/td>
&lt;td style="text-align:center">全局&lt;/td>
&lt;td style="text-align:center">应用程序&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">__constant__&lt;/td>
&lt;td style="text-align:center">float var*&lt;/td>
&lt;td style="text-align:center">常量内存&lt;/td>
&lt;td style="text-align:center">全局&lt;/td>
&lt;td style="text-align:center">应用程序&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>设备存储器的重要特征：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">存储器&lt;/th>
&lt;th style="text-align:center">片上/片外&lt;/th>
&lt;th style="text-align:center">缓存&lt;/th>
&lt;th style="text-align:center">存取&lt;/th>
&lt;th style="text-align:center">范围&lt;/th>
&lt;th style="text-align:center">生命周期&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">寄存器&lt;/td>
&lt;td style="text-align:center">片上&lt;/td>
&lt;td style="text-align:center">n/a&lt;/td>
&lt;td style="text-align:center">R/W&lt;/td>
&lt;td style="text-align:center">一个线程&lt;/td>
&lt;td style="text-align:center">线程&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">本地&lt;/td>
&lt;td style="text-align:center">片外&lt;/td>
&lt;td style="text-align:center">1.0 以上有&lt;/td>
&lt;td style="text-align:center">R/W&lt;/td>
&lt;td style="text-align:center">一个线程&lt;/td>
&lt;td style="text-align:center">线程&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">共享&lt;/td>
&lt;td style="text-align:center">片上&lt;/td>
&lt;td style="text-align:center">n/a&lt;/td>
&lt;td style="text-align:center">R/W&lt;/td>
&lt;td style="text-align:center">块内所有线程&lt;/td>
&lt;td style="text-align:center">块&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">全局&lt;/td>
&lt;td style="text-align:center">片外&lt;/td>
&lt;td style="text-align:center">1.0 以上有&lt;/td>
&lt;td style="text-align:center">R/W&lt;/td>
&lt;td style="text-align:center">所有线程+主机&lt;/td>
&lt;td style="text-align:center">主机配置&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">常量&lt;/td>
&lt;td style="text-align:center">片外&lt;/td>
&lt;td style="text-align:center">有&lt;/td>
&lt;td style="text-align:center">R&lt;/td>
&lt;td style="text-align:center">所有线程+主机&lt;/td>
&lt;td style="text-align:center">主机配置&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">纹理&lt;/td>
&lt;td style="text-align:center">片外&lt;/td>
&lt;td style="text-align:center">有&lt;/td>
&lt;td style="text-align:center">R&lt;/td>
&lt;td style="text-align:center">所有线程+主机&lt;/td>
&lt;td style="text-align:center">主机配置&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="9-静态全局内存">9. 静态全局内存&lt;/h3>
&lt;p>CPU 内存有动态分配和静态分配两种类型，从内存位置来说，动态分配在堆上进行，静态分配在站上进行，在代码上的表现是一个需要 new，malloc 等类似的函数动态分配空间，并用 delete 和 free 来释放。在 CUDA 中也有类似的动态静态之分，需要 cudaMalloc 的就是动态分配，静态分配与动态分配相同是，也需要显式的将内存 copy 到设备端。下面代码是一个静态分配的例子：&lt;/p>
&lt;pre>&lt;code class="language-cpp">#include &amp;lt;cuda_runtime.h&amp;gt;
#include &amp;quot;dbg.h&amp;quot;
__device__ float devData;
__global__ void checkGlobalVariable()
{
printf(&amp;quot;Device: the value of devData is %f\n&amp;quot;, devData);
devData += 2.0f;
}
int main(int argc, char **argv)
{
float value = 3.14f;
CHECK(cudaMemcpyToSymbol(devData, &amp;amp;value, sizeof(float)));
printf(&amp;quot;Host: copied %f to the global variable\n&amp;quot;, value);
checkGlobalVariable&amp;lt;&amp;lt;&amp;lt;1, 1&amp;gt;&amp;gt;&amp;gt;();
CHECK(cudaMemcpyFromSymbol(&amp;amp;value, devData, sizeof(float)));
printf(&amp;quot;Host: the value changed by the kernel to %f\n&amp;quot;, value);
CHECK(cudaDeviceReset());
return EXIT_SUCCESS;
}
&lt;/code>&lt;/pre>
&lt;p>运行结果为：&lt;/p>
&lt;pre>&lt;code class="language-text">Host: copied 3.140000 to the global variable
Device: the value of devData is 3.140000
Host: the value changed by the kernel to 5.140000
&lt;/code>&lt;/pre>
&lt;p>唯一要注意的就是这一句：&lt;/p>
&lt;pre>&lt;code class="language-cpp">cudaMemcpyToSymbol(devData,&amp;amp;value,sizeof(float));
&lt;/code>&lt;/pre>
&lt;p>设备上的变量定义和主机变量定义的不同，设备变量在代码中定义的时候其实就是一个指针，这个指针指向何处，主机端是不知道的，指向的内容也不知道，想知道指向的内容，唯一的办法还是通过显式的办法即 cudaMemcpyToSymbol 传输过来。&lt;/p>
&lt;p>此外还需要注意的是：&lt;/p>
&lt;ol>
&lt;li>在主机端，devData 只是一个标识符，不是设备全局内存的变量地址&lt;/li>
&lt;li>在核函数中，devData 就是一个全局内存中的变量。主机代码不能直接访问设备变量，设备也不能访问主机变量，这就是 CUDA 编程与 CPU 多核最大的不同之处&lt;/li>
&lt;/ol>
&lt;p>一方面，是无法使用 cudaMemcpy 来给静态变量赋值的，除非：&lt;/p>
&lt;pre>&lt;code class="language-cpp">float *dptr = NULL;
cudaGetSymbolAddress((void**)&amp;amp;dptr,devData);
cudaMemcpy(dptr, &amp;amp;value, sizeof(float), cudaMemcpyHostToDevice);
&lt;/code>&lt;/pre>
&lt;p>另一方面，主机端不可以对设备变量进行取地址操作，该操作是非法的。想要得到 devData 的地址可以用下面方法：&lt;/p>
&lt;pre>&lt;code class="language-cpp">float *dptr = NULL;
cudaGetSymbolAddress((void**)&amp;amp;dptr, devData);
&lt;/code>&lt;/pre>
&lt;p>当然也有一个例外，可以直接从主机引用 GPU 内存——CUDA 固定内存。&lt;/p>
&lt;p>CUDA 运行时 API 能访问主机和设备变量，但这取决于你给正确的函数是否提供了正确的参数，使用运行时 API ，如果参数填错，尤其是主机和设备上的指针，结果是无法预测的。&lt;/p>
&lt;h2 id="参考资料">参考资料&lt;/h2>
&lt;p>[1] CUDA C 编程权威指南，机械工业出版社，（美）程润伟（John Cheng） 等著&lt;/p></description></item><item><title>CUDA 基础：线程束执行的本质</title><link>https://cuterwrite.top/p/cuda-base-warp/</link><pubDate>Thu, 31 Aug 2023 00:00:00 +0000</pubDate><guid>https://cuterwrite.top/p/cuda-base-warp/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230831184001-2023-08-31.webp" alt="Featured image of post CUDA 基础：线程束执行的本质" />&lt;h1 id="cuda-基础线程束执行的本质">CUDA 基础：线程束执行的本质&lt;/h1>
&lt;h2 id="1-线程束和线程块">1. 线程束和线程块&lt;/h2>
&lt;p>线程束是 SM 中基本的执行单元，当一个线程块的网格被启动后，网格中的线程块分布在 SM 中。一旦线程块被调度在一个 SM 上，线程块中的线程会被进一步划分为线程束。一个线程束由 32 个连续的线程组成（目前的 GPU 都是 32 个线程，但不保证未来是 32 个），在一个线程束中，所有的线程按照单指令多线程（SIMT）方式执行；也就是说，所有线程都执行相同的指令，每个线程在私有数据上进行操作。下图展示了线程块的逻辑视图和硬件视图之间的关系：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230831184443-2023-08-31.webp"
alt="20230831184443-2023-08-31" width="90%" loading="lazy">
&lt;/figure>
&lt;p>然而，从硬件的角度来看，所有的线程都被组织成了一维的，线程块可以被配置为一维、二维、三维的。在一个块中，每个线程都有唯一的 ID 。对于一维的线程块，唯一的线程 ID 被存储在 CUDA 的内置变量 threadIdx.x 中，并且，threadIdx.x 中拥有连续值得线程被分组到线程束中。例如，一个有 128 个线程的一维线程块被组织到 4 个线程束里，如下所示：&lt;/p>
&lt;pre>&lt;code class="language-text">warp0: thread 0, .........., thread 31
warp1: thread 32, ........., thread 63
warp2: thread 64, ........., thread 95
warp3: thread 96, ........., thread 127
&lt;/code>&lt;/pre>
&lt;p>线程块是一个逻辑产物，因为在计算机里，内存总是一维线性存在的，所以执行起来也是一维的访问线程块中的线程，但是我们在写程序的时候却可以以二维三维的方式进行，原因是方便我们写程序，比如处理图像或者三维的数据，三维块就会变得很直接，很方便。&lt;/p>
&lt;ul>
&lt;li>在块中，每个线程有唯一的编号（可能是个三维的编号），threadIdx&lt;/li>
&lt;li>网格中，每个线程块也有唯一的编号(可能是个三维的编号)，blockIdx&lt;/li>
&lt;li>那么每个线程就有在网格中的唯一编号。&lt;/li>
&lt;/ul>
&lt;p>用 $x$ 维度作为最内层的维度， $y$ 维度作为第二个维度， $z$ 维度作为最外层的维度，则二维或三维线程块的逻辑布局可以转化为一维物理布局。例如，对于一个给定的二维线程块，在一个块中每个线程的独特标识符都可以用内置变量 threadIdx 和 blockDim 来计算：&lt;/p>
&lt;pre>&lt;code class="language-c">tid = threadIdx.x + threadIdx.y * blockDim.x;
&lt;/code>&lt;/pre>
&lt;p>对于一个三维线程块，可以用下面的方式计算：&lt;/p>
&lt;pre>&lt;code class="language-c">tid = threadIdx.x + threadIdx.y * blockDim.x + threadIdx.z * blockDim.x * blockDim.y;
&lt;/code>&lt;/pre>
&lt;p>在 C 语言中，假设三维数组 t 保存了所有的线程，那么 (threadIdx.x, threadIdx.y, threadIdx.z) 就相当于：&lt;/p>
&lt;pre>&lt;code class="language-c">t[z][y][x];
&lt;/code>&lt;/pre>
&lt;p>一个线程块的线程束的数量可以根据下式确定：&lt;/p>
&lt;p>$$
\mathrm{WarpsPerBlock} = \left \lceil \frac{\mathrm{threadsPerBlock}}{\mathrm{warpSize}} \right \rceil
$$&lt;/p>
&lt;p>因此，硬件总是给一个线程块分配一定数量的线程束。线程束不会在不同的线程块之间分离。如果线程块的大小不是线程束大小的偶数倍，那么在最后的线程束里有些线程就不会活跃。比如说一个在 $x$ 轴中有 40 个线程、在 $y$ 轴中有 2 个线程的二维线程块。从应用程序的角度来看，在一个二维网格中共有 80 个线程。&lt;/p>
&lt;p>硬件为这个线程块配置了 3 个线程束，使总共 96 个硬件线程去支持 80 个软件线程。注意，最后半个线程束是不活跃的。即使这些线程未被使用，它们仍然消耗 SM 的资源，如寄存器。&lt;/p>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>&lt;strong>从逻辑角度来看：&lt;/strong> 线程块是线程的集合，它们可以组织为一维、二维或三维布局。&lt;/p>
&lt;p>&lt;strong>从硬件角度来看：&lt;/strong> 线程块是一维线程束的集合。在线程块中线程被组织成一维布局，每 32 个连续线程组成一个线程束。&lt;/p>&lt;/div>
&lt;h2 id="2-线程束分化">2. 线程束分化&lt;/h2>
&lt;p>控制流是高级编程语言的基本构造中的一种。GPU 支持传统的、C 风格的、显式的控制流结构，例如，if···then···else、for 和 while。&lt;/p>
&lt;p>CPU 拥有复杂的硬件以执行分支预测，也就是在每个条件检查中预测应用程序的控制流会使用哪个分支。如果预测正确，CPU 中的分支只需付出很小的性能代价。如果预测不正确，CPU 可能会停止运行很多个周期，因为指令流水线被清空了。我们不必完全理解为什么 CPU 擅长处理复杂的控制流。这个解释只是作为对比的背景。当我们的程序包含大量的分支判断时，从程序角度来说，程序的逻辑是很复杂的，因为一个分支就会有两条路可以走，如果有 10 个分支，那么一共有 1024 条路走，CPU 采用流水线化作业，如果每次等到分支执行完再执行下面的指令会造成很大的延迟，所以现在处理器都采用分支预测技术，而 CPU 的这项技术相对于 GPU 来说高级了不止一点点，而这也是 GPU 与 CPU 的不同，设计初衷就是为了解决不同的问题。CPU 适合逻辑复杂计算量不大的程序，比如操作系统，控制系统，GPU 适合大量计算简单逻辑的任务，所以被用来算数。&lt;/p>
&lt;p>GPU 是相对简单的设备，它没有复杂的分支预测机制。一个线程束中的所有线程在同周期中必须执行相同的指令，如果一个线程执行一条指令，那么线程束中的所有线程都必须执行该指令。如果在同一线程束中的线程使用不同的路径通过同一个应用程序，这可能会产生问题。例如，思考下面的语句:&lt;/p>
&lt;pre>&lt;code class="language-c">if (cond) {
...
} else {
...
}
&lt;/code>&lt;/pre>
&lt;p>假设在一个线程束中有 16 个线程执行这段代码，cond 为 true，但对于其他 16 个来说 cond 为 false 。一半的线程束需要执行 if 语句块中的指令，而另一半需要执行 else 语句块中的指令。在同一线程束中的线程执行不同的指令，被称为&lt;strong>线程束分化&lt;/strong>。我们已经知道，在一个线程束中所有线程在每个周期中必须执行相同的指令，所以线程束分化似乎会产生一个悖论。&lt;/p>
&lt;p>如果一个线程束中的线程产生分化，线程束将连续执行每一个分支路径，而禁用不执行这一路径的线程。线程束分化会导致性能明显地下降。在前面的例子中可以看到，线程中并行线程的数量减少了一半: 只有 16 个线程同时活跃地执行，而其他 16 个被禁用了。条件分支越多，并行性削弱越严重。此外，线程束分化只发生在同一个线程束中。在不同的线程束中，不同的条件值不会引起线程束分化。&lt;/p>
&lt;p>执行过程如下：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230831205444-2023-08-31.webp"
alt="20230831205444-2023-08-31" width="90%" loading="lazy">
&lt;/figure>
&lt;p>因为线程束分化导致的性能下降就应该用线程束的方法解决，根本思路是&lt;strong>避免同一个线程束内的线程分化&lt;/strong>，而让我们能控制线程束内线程行为的原因是线程块中线程分配到线程束是有规律的而不是随机的。这就使得我们根据线程编号来设计分支是可以的，补充说明下，当一个线程束中所有的线程都执行 if 或者，都执行 else 时，不存在性能下降；只有当线程束内有分歧产生分支的时候，性能才会急剧下降。&lt;/p>
&lt;p>线程束内的线程是可以被我们控制的，那么我们就把都执行 if 的线程塞到一个线程束中，或者让一个线程束中的线程都执行 if ，另外线程都执行 else 的这种方式可以将效率提高很多。&lt;/p>
&lt;p>举以下一个低效的核函数为例：&lt;/p>
&lt;pre>&lt;code class="language-c">__global__ void mathKernel1(float *c) {
int tid = blockIdx.x * blockDim.x + threadIdx.x;
float a, b;
a = b = 0.0f;
if (tid % 2 == 0) {
a = 100.0f;
} else {
b = 200.0f;
}
c[tid] = a + b;
}
&lt;/code>&lt;/pre>
&lt;p>这种情况下，线程束内的线程会产生分化，因为线程束内的线程会有一半执行 if ，另一半执行 else ，这样就会导致性能下降。我们可以通过下面的方式来优化：&lt;/p>
&lt;pre>&lt;code class="language-c">__global__ void mathKernel2(float *c) {
int tid = blockIdx.x * blockDim.x + threadIdx.x;
float a, b;
a = b = 0.0f;
if ((tid / warpSize) % 2 == 0) {
a = 100.0f;
} else {
b = 200.0f;
}
c[tid] = a + b;
}
&lt;/code>&lt;/pre>
&lt;p>假设只配置一个大小为 64 的一维线程块，那么只有 2 个线程束，第一个线程束内的线程编号 tid 从 0 到 31， tid / warpSize 都等于 0，那么都执行 if 语句；第二个线程束内的线程编号 tid 从 32 到 63， tid / warpSize 都等于 1，那么都执行 else 语句。这样就避免了线程束内的线程分化，效率较高。&lt;/p>
&lt;p>在 CUDA 中，对线程束分化的评价指标为&lt;strong>分支效率 (branch efficiency)&lt;/strong>，它是一个 0 到 100 之间的百分比，表示线程束中的线程在同一周期中执行的分支指令的百分比。分支效率越高，性能越好。分支效率的计算公式如下：&lt;/p>
&lt;p>$$
\mathrm{branch\ efficiency} = \frac{\mathrm{branches - divergent\ branches}}{\mathrm{branches}}
$$&lt;/p>
&lt;p>以上线程束分化例子的完整代码如下：&lt;/p>
&lt;a href="https://github.com/PKUcoldkeyboard/cuda-demo/blob/main/Chap3/simpleDivergence.cu" target="_blank" class="card-github fetch-waiting no-styling"
repo="PKUcoldkeyboard/cuda-demo" id="repo-NfiG90XjpfLAnnu9-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-NfiG90XjpfLAnnu9-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">PKUcoldkeyboard&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">cuda-demo&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-NfiG90XjpfLAnnu9-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-NfiG90XjpfLAnnu9-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-NfiG90XjpfLAnnu9-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-NfiG90XjpfLAnnu9-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-NfiG90XjpfLAnnu9-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-NfiG90XjpfLAnnu9-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/PKUcoldkeyboard\/cuda-demo', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-NfiG90XjpfLAnnu9-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-NfiG90XjpfLAnnu9-language').innerText = data.language;
document.getElementById('repo-NfiG90XjpfLAnnu9-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-NfiG90XjpfLAnnu9-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-NfiG90XjpfLAnnu9-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-NfiG90XjpfLAnnu9-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-NfiG90XjpfLAnnu9-license').classList.add = "no-license"
};
document.getElementById('repo-NfiG90XjpfLAnnu9-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for PKUcoldkeyboard\/cuda-demo.")
}).catch(err => {
const c = document.getElementById('repo-NfiG90XjpfLAnnu9-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for PKUcoldkeyboard\/cuda-demo.")
})
&lt;/script>
&lt;p>编译命令为：（强制 CUDA 编译器不利用分支预测去优化内核，使用 Tesla T4 GPU）&lt;/p>
&lt;pre>&lt;code class="language-shell">nvcc -g -G -arch=sm_75 -o simpleDivergence simpleDivergence.cu
&lt;/code>&lt;/pre>
&lt;p>运行结果为：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230831223456-2023-08-31.webp"
alt="20230831223456-2023-08-31" width="90%" loading="lazy">
&lt;/figure>
&lt;p>代码中的 Warmup 部分是提前启动一次 GPU，因为第一次启动 GPU 时会比第二次速度慢一些，具体原因未知，可以去查一下 CUDA 的相关技术文档了解内容。我们可以通过 Nvidia Nsight Compute 来查看分支效率（&lt;strong>旧版的 nvprof 被弃用了&lt;/strong>，metrics 参数对应的修改可以参考 &lt;a class="link" href="https://blog.csdn.net/weixin_44334901/article/details/128596081" target="_blank" rel="noopener" >CUDA 编程性能分析工具 nvprof/ncu &amp;ndash;metrics 参数含义
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
，而且运行 ncu 时候必须使用 root 权限），结果如下所示：&lt;/p>
&lt;pre>&lt;code class="language-text">[58735] simpleDivergence@127.0.0.1
warmingup(float *) (1, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
Section: Command line profiler metrics
---------------------------------------------------- ----------- ------------
Metric Name Metric Unit Metric Value
---------------------------------------------------- ----------- ------------
smsp_sass_average_branch_targets_threads_uniform.pct 100.00%
---------------------------------------------------- ----------- ------------
mathKernel1(float *) (1, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
Section: Command line profiler metrics
---------------------------------------------------- ----------- ------------
Metric Name Metric Unit Metric Value
---------------------------------------------------- ----------- ------------
smsp_sass_average_branch_targets_threads_uniform.pct 83.33%
---------------------------------------------------- ----------- ------------
mathKernel2(float *) (1, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
Section: Command line profiler metrics
---------------------------------------------------- ----------- ------------
Metric Name Metric Unit Metric Value
---------------------------------------------------- ----------- ------------
smsp_sass_average_branch_targets_threads_uniform.pct 100.00%
---------------------------------------------------- ----------- ------------
mathKernel3(float *) (1, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
Section: Command line profiler metrics
---------------------------------------------------- ----------- ------------
Metric Name Metric Unit Metric Value
---------------------------------------------------- ----------- ------------
smsp_sass_average_branch_targets_threads_uniform.pct 71.43%
---------------------------------------------------- ----------- ------------
mathKernel4(float *) (1, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
Section: Command line profiler metrics
---------------------------------------------------- ----------- ------------
Metric Name Metric Unit Metric Value
---------------------------------------------------- ----------- ------------
smsp_sass_average_branch_targets_threads_uniform.pct 100.00%
---------------------------------------------------- ----------- ------------
&lt;/code>&lt;/pre>
&lt;p>CUDA 的 nvcc 编译器仍然是在 mathKernel1 和 mathKernel3 上执行有限的优化，以保证分支效率在 50% 以上。注意，mathKernel2 不报告分支分化的唯一原因是它的分支粒度是线程束大小的倍数。此外，把 mathKernel1 中的 if&amp;hellip;else 语句分离为 mathKernel3 的多个 if 语句，可以使分支分化的数量翻倍。&lt;/p>
&lt;h2 id="3-资源分配">3. 资源分配&lt;/h2>
&lt;p>前面提到，每个 SM 上执行的基本单位是线程束，也就是说，单指令通过指令调度器广播给某线程束的全部线程，这些线程同一时刻执行同一命令，当然也有分支情况，也有很多线程束没执行，那么这些没执行的线程束情况又如何呢？可以将这些没执行的线程束分为两类：一类是已经激活的，也就是说这类线程束其实已经在 SM 上准备就绪了，只是没轮到它执行，这时候它的状态为阻塞，另一类是可能分配到 SM 了，但是还没上片，这类就称之为未激活线程束。而每个 SM 上有多少个线程束处于激活状态，取决于以下资源：&lt;/p>
&lt;ul>
&lt;li>程序计数器&lt;/li>
&lt;li>寄存器&lt;/li>
&lt;li>共享内存&lt;/li>
&lt;/ul>
&lt;p>线程束一旦被激活来到片上，那么它就不会再离开 SM 直到执行结束。&lt;/p>
&lt;p>每个 SM 都有 32 位的寄存器组，每个架构寄存器的数量不一样，其存储于寄存器文件中，为每个线程进行分配，同时，固定数量的共享内存，在线程块之间分配。&lt;/p>
&lt;p>一个 SM 上被分配多少个线程块和线程束取决于 SM 中可用的寄存器和共享内存，以及内核需要的寄存器和共享内存大小。 当 kernel 占用的资源较少，那么更多的线程处于活跃状态，相反则线程越少。&lt;/p>
&lt;ol>
&lt;li>寄存器资源的分配&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230901150726-2023-09-01.webp"
alt="20230901150726-2023-09-01" width="90%" loading="lazy">
&lt;/figure>
&lt;ol start="2">
&lt;li>共享内存的分配&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230901150743-2023-09-01.webp"
alt="20230901150743-2023-09-01" width="90%" loading="lazy">
&lt;/figure>
&lt;p>上面讲的主要是线程束，如果从逻辑上来看线程块的话，可用资源的分配也会影响常驻线程块的数量。特别是当 SM 内的资源没办法处理一个完整块，那么程序将无法启动。&lt;/p>
&lt;p>以下是资源列表：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230901150843-2023-09-01.webp"
alt="20230901150843-2023-09-01" width="90%" loading="lazy">
&lt;/figure>
&lt;p>当寄存器和共享内存分配给了线程块，这个线程块处于活跃状态，所包含的线程束称为活跃线程束。活跃的线程束又分为三类：&lt;/p>
&lt;ul>
&lt;li>选定的线程束&lt;/li>
&lt;li>阻塞的线程束&lt;/li>
&lt;li>符合条件的线程束&lt;/li>
&lt;/ul>
&lt;p>当 SM 要执行某个线程束的时候，执行的这个线程束叫做选定的线程束，准备要执行的叫符合条件的线程束，如果线程束不符合条件还没准备好就是阻塞的线程束。
满足下面的要求，线程束才算是符合条件的：&lt;/p>
&lt;ul>
&lt;li>32 个 CUDA 核心可以用于执行&lt;/li>
&lt;li>执行所需要的资源全部就位&lt;/li>
&lt;/ul>
&lt;p>Kepler 活跃的线程束数量从开始到结束不得大于 64，可以等于。任何周期选定的线程束小于等于 4 。由于计算资源是在线程束之间分配的，且线程束的整个生命周期都在片上，所以线程束的上下文切换是非常快速的。下一节将说明如何通过大量的活跃的线程束切换来隐藏延迟。&lt;/p>
&lt;h2 id="4-延迟隐藏">4. 延迟隐藏&lt;/h2>
&lt;p>SM 依赖线程级并行，以最大化功能单元的利用率，因此，利用率与常驻线程束的数量直接相关。在指令发出和完成之间的时钟周期被定义为&lt;strong>指令延迟&lt;/strong>。当每个时钟周期中所有的线程调度器都有一个符合条件的线程束时，可以达到计算资源的完全利用。这就可以保证，通过在其他常驻线程束中发布其他指令，可以&lt;strong>隐藏每个指令的延迟&lt;/strong>。&lt;/p>
&lt;p>与在 CPU 上用 C 语言编程相比，&lt;strong>延迟隐藏&lt;/strong>在 CUDA 编程中尤为重要。CPU 核心是为同时最小化延迟一个或两个线程而设计的，而 GPU 则是为处理大量并发和轻量级线程以最大化吞吐量而设计的。GPU 的指令延迟被其他线程束的计算隐藏。&lt;/p>
&lt;p>考虑到指令延迟，指令可以被分为两种基本类型：&lt;/p>
&lt;ul>
&lt;li>算术指令&lt;/li>
&lt;li>内存指令&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>算术指令延迟&lt;/strong>是一个算术操作从开始到它产生输出之间的时间。&lt;strong>内存指令延迟&lt;/strong>是指发送出的加载或存储操作和数据到达目的地之间的时间。对于每种情况，相应的延迟大约为：&lt;/p>
&lt;ul>
&lt;li>算术操作为 10～20 个周期&lt;/li>
&lt;li>全局内存访问为 400～800 个周期&lt;/li>
&lt;/ul>
&lt;p>下图是阻塞线程束到可选线程束的过程逻辑图：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230901151502-2023-09-01.webp"
alt="20230901151502-2023-09-01" width="90%" loading="lazy">
&lt;/figure>
&lt;p>其中线程束 0 （Warp 0） 阻塞两段时间后恢复可选模式，但是在这段等待时间中，SM 没有闲置。那么至少需要多少线程，线程束来保证最小化延迟呢？可以根据利特尔法则（Little&amp;rsquo;s Law）提供一个合理的近似值。它起源于队列理论中的一个定理，也可以用于 GPU 中：&lt;/p>
&lt;p>$$
\mathrm{所需线程束}=\mathrm{延迟}\times \mathrm{吞吐量}
$$&lt;/p>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>注意带宽和吞吐量的区别，带宽一般指的是理论峰值，最大每个时钟周期能执行多少个指令，吞吐量是指实际操作过程中每分钟处理多少个指令。简单来说，带宽通常是指理论峰值，而吞吐量是指已达到的值。&lt;/p>&lt;/div>
&lt;p>这个可以想象成一个瀑布，像这样，绿箭头是线程束，只要线程束足够多，吞吐量是不会降低的：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230901151942-2023-09-01.webp"
alt="20230901151942-2023-09-01" width="90%" loading="lazy">
&lt;/figure>
&lt;p>假设在 kernel 里一条指令的平均延迟是 5 个周期。为了保持在每个周期内执行 6 个线程束的吞吐量，则至少需要 30 个未完成的线程束。&lt;/p>
&lt;p>对于算术运算来说，其所需的并行可以表示成隐藏算术延迟所需要的操作数量。下面的表格出了 Fermi 和 Kepler 设备所需的操作数量。示例中的算术运算是一个 32 位的浮点数乘加运算 (a + b $\times$ c)，表示在每个 SM 中每个时钟周期内的操作数量。吞吐量因不同的算术指令而不同。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230901152413-2023-09-01.webp"
alt="20230901152413-2023-09-01" width="90%" loading="lazy">
&lt;/figure>
&lt;p>吞吐量由 SM 中每个周期内的操作数量确定，而执行一条指令的一个线程束对应 32 个操作。因此，为保持计算资源的充分利用，对于 Fermi GPU 而言，每个 SM 中所需的线程束数量通过计算为 $640 \div 32 = 20 $ 个线程束。因此，算术运算所需的并行可以用操作的数量或线程束的数量来表示。这个简单的单位转换表明，有两种方法可以提高并行：&lt;/p>
&lt;ul>
&lt;li>指令级并行（ILP）：一个线程中有很多独立的指令&lt;/li>
&lt;li>线程级并行（TLP）：很多并发地符合条件的线程&lt;/li>
&lt;/ul>
&lt;p>同样，与指令周期隐藏延迟类似，&lt;strong>内存隐藏延迟&lt;/strong>是靠内存读取的并发操作来完成的，需要注意的是，指令隐藏的关键目的是使用全部的计算资源，而内存读取的延迟隐藏是为了使用全部的内存带宽，内存延迟的时候，计算资源正在被别的线程束使用，所以我们不考虑内存读取延迟的时候计算资源在做了什么，我们的根本目的是把计算资源，内存读取的带宽资源全部使用满，这样就能达到理论的最大效率。同样下表根据利特尔法则给出了需要多少线程束来最小化内存读取延迟，不过这里有个单位换算过程，机器的性能指标内存读取速度给出的是 GB/s 的单位，而我们需要的是每个时钟周期读取字节数，所以要用这个速度除以频率，例如 Tesla C2070 的内存带宽是 144 GB/s，转化成时钟周期： $\frac{144\mathrm{GB/s}}{1.566 \mathrm{GHz}}=92\mathrm{B/t}$，这样就能得到单位时间周期的内存带宽了。即下表的数据：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230901152915-2023-09-01.webp"
alt="20230901152915-2023-09-01" width="90%" loading="lazy">
&lt;/figure>
&lt;p>需要说明的是这个速度不是单个 SM 的而是整个 GPU 设备的。Fermi 需要并行的读取 74KB 的数据才能让 GPU 带宽满载，如果每个线程读取 4 个字节，我们大约需要 18500 个线程，大约 579 个线程束才能达到这个峰值。&lt;/p>
&lt;p>所以，延迟的隐藏取决于活动的线程束的数量，数量越多，隐藏得越好，但是线程束的数量又受到上面的说的资源影响。所以这里就需要寻找最优的执行配置来达到最优的延迟隐藏。&lt;/p>
&lt;p>那么我们怎么样确定一个线程束的下界呢，使得当高于这个数字时 SM 的延迟能充分的隐藏，其实这个公式很简单，也很好理解，就是 SM 的计算核心数乘以单条指令的延迟，比如 32 个单精度浮点计算器，每次计算延迟 20 个时钟周期，那么我需要最少 $32 \times 20 =640$ 个线程使设备处于忙碌状态。然而，这只是一个下边界。&lt;/p>
&lt;h2 id="5-占用率">5. 占用率&lt;/h2>
&lt;p>在每个 CUDA 核心里指令是顺序执行的。当一个线程束阻塞时，SM 切换执行其他符合条件的线程束。理想情况下，我们想要有足够的线程束占用设备的核心。占用率是每个 SM 中活跃的线程束占最大线程束数量的比值。即：&lt;/p>
&lt;p>$$
\mathrm{Occupancy} = \frac{\mathrm{Active\ Warps}}{\mathrm{Max\ Warps}}
$$&lt;/p>
&lt;p>通过以下代码可以查询设备的最大线程束数量：&lt;/p>
&lt;pre>&lt;code class="language-c">int dev = 0;
cudaDeviceProp deviceProp;
CHECK(cudaGetDeviceProperties(&amp;amp;deviceProp, dev));
log_info(&amp;quot;Device %d: %s&amp;quot;, dev, deviceProp.name);
log_info(&amp;quot;Number of SMs: %d&amp;quot;, deviceProp.multiProcessorCount);
log_info(&amp;quot;Total amount of constant memory: %4.2f KB&amp;quot;, deviceProp.totalConstMem / 1024.0);
log_info(&amp;quot;Total amount of shared memory per block: %4.2f KB&amp;quot;,
deviceProp.sharedMemPerBlock / 1024.0);
log_info(&amp;quot;Total number of registers available per block: %d&amp;quot;, deviceProp.regsPerBlock);
log_info(&amp;quot;Warp size: %d&amp;quot;, deviceProp.warpSize);
log_info(&amp;quot;Maximum number of threads per block: %d&amp;quot;, deviceProp.maxThreadsPerBlock);
log_info(&amp;quot;Maximum number of threads per multiprocessor: %d&amp;quot;,
deviceProp.maxThreadsPerMultiProcessor);
log_info(&amp;quot;Maximum number of warps per multiprocessor: %d&amp;quot;,
deviceProp.maxThreadsPerMultiProcessor / 32);
return 0;
&lt;/code>&lt;/pre>
&lt;p>输出结果为：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230901155126-2023-09-01.webp"
alt="20230901155126-2023-09-01" width="90%" loading="lazy">
&lt;/figure>
&lt;p>可以看到 RTX4090 最大 64 个线程束每个 SM。&lt;/p>
&lt;p>内核使用寄存器的数量会影响 SM 内线程束的数量，nvcc 的编译选项也有手动控制寄存器的使用。也可以通过调整线程块内线程的多少来提高占用率，当然要合理不能太极端：&lt;/p>
&lt;ul>
&lt;li>小的线程块：每个线程块中线程太少，会在所有资源没用完就达到了线程束的最大要求&lt;/li>
&lt;li>大的线程块：每个线程块中太多线程，会导致每个 SM 中每个线程可用的硬件资源较少。&lt;/li>
&lt;/ul>
&lt;p>一个确定网格和线程块大小的基本准则如下：&lt;/p>
&lt;ol>
&lt;li>保持每个块中线程数量是线程束大小（32）的倍数&lt;/li>
&lt;li>避免块太小：每个块至少要有 128 或 256 个线程&lt;/li>
&lt;li>根据内核资源的需求调整块大小&lt;/li>
&lt;li>块的数量要远远多于 SM 的数量，从而在设备中可以显示有足够的并行&lt;/li>
&lt;li>通过实验得到最佳执行配置和资源使用情况&lt;/li>
&lt;/ol>
&lt;p>尽管在每种情况下会遇到不同的硬件限制，但它们都会导致计算资源未被充分利用，阻碍隐藏指令和内存延迟的并行的建立。占用率唯一注重的是在每个 SM 中并发线程或线程束的数量。然而，充分的占用率不是性能优化的唯一目标。内核一旦达到一定级别的占用率，进一步增加占用率可能不会改进性能。为了提高性能，可以调整很多其他因素。&lt;/p>
&lt;h2 id="6-同步">6. 同步&lt;/h2>
&lt;p>栅栏同步是一个原语，它在许多并行编程语言中都很常见。在 CUDA 中，同步可以在两个级别执行：&lt;/p>
&lt;ul>
&lt;li>线程块内同步&lt;/li>
&lt;li>系统级别&lt;/li>
&lt;/ul>
&lt;p>块级别的就是同一个块内的线程会同时停止在某个设定的位置，用&lt;/p>
&lt;pre>&lt;code class="language-c">__syncthreads();
&lt;/code>&lt;/pre>
&lt;p>这个函数完成，这个函数只能同步同一个块内的线程，不能同步不同块内的线程，想要同步不同块内的线程，就只能让核函数执行完成，控制程序交换主机，这种方式来同步所有线程。当__syncthreads 被调用时，在同一个线程块中每个线程都必须等待直至该线程块中所有其他线程都已经达到这个同步点。线程产生的所有全局内存和共享内存访问，将会在栅栏后对线程块中所有其他的线程可见。该函数可以协调同一个块中线程之间的通信，但它强制线程束空闲，从而可能对性能产生负面影响。&lt;/p>
&lt;p>在不同的块之间没有线程同步。块间同步，唯一安全的方法是在每个内核执行结束端使用全局同步点；也就是说，在全局同步之后，终止当前的核函数，开始执行新的核函数。&lt;/p>
&lt;p>不同块中的线程不允许相互同步，因此 GPU 可以以任意顺序执行块。这使得 CUDA 程序在大规模并行 GPU 上是可扩展的。&lt;/p>
&lt;h2 id="7-可扩展性">7. 可扩展性&lt;/h2>
&lt;p>对于任何并行应用程序而言，可扩展性是一个理想的特性。可扩展性意味着为并行应用程序提供了额外的硬件资源，相对于增加的资源，并行应用程序会产生加速。例如，若一个 CUDA 程序在两个 SM 中是可扩展的，则与在一个 SM 中运行相比，在两个 SM 中运行会使运行时间减半。一个可扩展的并行程序可以高效地使用所有的计算资源以提高性能。可扩展性意味着增加的计算核心可以提高性能。串行代码本身是不可扩展的，因为在成千上万的内核上运行一个串行单线程应用程序，对性能是没有影响的。并行代码有可扩展的潜能，但真正的可扩展性取决于算法设计和硬件特性。&lt;/p>
&lt;p>能够在可变数量的计算核心上执行相同的应用程序代码的能力被称为&lt;strong>透明可扩展性&lt;/strong>。一个透明的可扩展平台拓宽了现有应用程序的应用范围，并减少了开发人员的负担，因为它们可以避免新的或不同的硬件产生的变化。可扩展性比效率更重要。一个可扩展但效率很低的系统可以通过简单添加硬件核心来处理更大的工作负载。一个效率很高但不可扩展的系统可能很快会达到可实现性能的上限。&lt;/p>
&lt;p>CUDA 内核启动时，线程块分布在多个 SM 中。网格中的线程块以并行或连续或任意的顺序被执行。这种独立性使得 CUDA 程序在任意数量的计算核心间可以扩展。&lt;/p>
&lt;p>下图展示了 CUDA 架构可扩展性的一个例子。左侧的 GPU 有两个 SM， 可以同时执行两个块；右侧的 GPU 有 4 个 SM ，可以同时执行 4 个块。不修改任何代码，一个应用程序可以在不同的 GPU 配置上运行，并且所需的执行时间根据可用的资源而改变。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230901163747-2023-09-01.webp"
alt="20230901163747-2023-09-01" width="90%" loading="lazy">
&lt;/figure>
&lt;h2 id="参考资料">参考资料&lt;/h2>
&lt;p>[1] CUDA C 编程权威指南，机械工业出版社，（美）程润伟（John Cheng） 等著&lt;/p></description></item><item><title>SSE 与 AVE 向量化编程</title><link>https://cuterwrite.top/p/simd/</link><pubDate>Sat, 12 Aug 2023 08:00:00 +0000</pubDate><guid>https://cuterwrite.top/p/simd/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/173b9c0b3728e5e9e05d12a4f6dda9a2a7560722.jpg@1256w_1776h_!web-article-pic.webp" alt="Featured image of post SSE 与 AVE 向量化编程" />&lt;h1 id="sse-与-ave-向量化编程">SSE 与 AVE 向量化编程&lt;/h1>
&lt;h2 id="一-向量化编程简介">一、 向量化编程简介&lt;/h2>
&lt;p>近年来，CPU 已经达到了一些物理和功率限制，因此在 GHz 方面，CPU 速度并没有显著提高。随着计算需求的不断增加，CPU 设计人员决定用三种解决方案来解决这个问题：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>增加更多核心&lt;/strong>。通过这种方式，操作系统可以在不同的内核之间分配正在运行的应用程序。此外，程序还可以创建多个线程来最大化核心使用率&lt;/li>
&lt;li>&lt;strong>将向量化操作应用到每个核心&lt;/strong>。该解决方案允许 CPU 对数据向量执行相同的指令。这只能在应用程序级别完成&lt;/li>
&lt;li>&lt;strong>多条指令的无序执行&lt;/strong>。如果现代 CPU 是独立的，那么它们最多可以同时执行四条指令。&lt;/li>
&lt;/ul>
&lt;p>向量寄存器始于 1997 年的 MMX 指令集。MMX 指令集具有 80 位的寄存器。之后发布了 SSE 指令集（从 SSE1 到 SEE4.2 有多个版本），具有 128 位寄存器。2011 年，英特尔发布了采用 AVX 指令集（256 位寄存器）的 Sandy Bridge 架构。2016 年，首款 AVX-512 CPU 发布，采用 512 位寄存器（最多 16x 32 位浮点矢量）。&lt;/p>
&lt;p>本文将重点介绍 SSE 和 AVX 指令集，因为它们通常出现在最近的处理器中。AVX-512 不在讨论范围内，但只需将 256 位寄存器更改为 512 位对应寄存器(ZMM 寄存器)，即可将本文中的所有示例应用于 AVX-512。&lt;/p>
&lt;h3 id="1-sseave-寄存器">1. SSE/AVE 寄存器&lt;/h3>
&lt;p>SSE 和 AVX 各有 16 个寄存器。在 SSE 中，它们被称为 XMM0-XMM15，而在 AVX 中，它们被称为 YMM0-YMM15。XMM 寄存器长度为 128 位，而 YMM 为 256 位。&lt;/p>
&lt;p>SSE 增加了三个类型定义： &lt;code>__m128&lt;/code> 、 &lt;code>__m128d&lt;/code> 和 &lt;code>__m128i&lt;/code> 。分别为浮点型、双精度型(D)和整型(I)。&lt;/p>
&lt;p>AVE 增加了三个类型定义： &lt;code>__m256&lt;/code> 、 &lt;code>__m256d&lt;/code> 和 &lt;code>__m256i&lt;/code> 。分别为浮点型、双精度型(D)和整型(I)。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230810233117.webp"
alt="20230810233117" width="90%" loading="lazy">
&lt;/figure>
&lt;div class="notice notice-warning" >
&lt;div class="notice-title">&lt;svg t="1705945674099" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="16953" width="200" height="200">&lt;path d="M512 106.666667a405.333333 405.333333 0 1 0 405.333333 405.333333A405.333333 405.333333 0 0 0 512 106.666667z m120.533333 489.6a25.621333 25.621333 0 0 1 0 36.266666 25.749333 25.749333 0 0 1-36.266666 0L512 548.266667l-84.266667 84.266666a25.749333 25.749333 0 0 1-36.266666 0 25.621333 25.621333 0 0 1 0-36.266666L475.733333 512l-84.266666-84.266667a25.642667 25.642667 0 0 1 36.266666-36.266666L512 475.733333l84.266667-84.266666a25.642667 25.642667 0 0 1 36.266666 36.266666L548.266667 512z" fill="#ffffff" p-id="16954">&lt;/path>&lt;/svg>&lt;/div>&lt;p>XMM 和 YMM 是重叠的：XMM 寄存器被视为相应 YMM 寄存器的下半部分。这可能会在混合使用 SSE 和 AVX 代码时带来一些性能问题。&lt;/p>&lt;/div>
&lt;p>浮点数据类型（如__m128、__m128d、__m256 和__m256d）在 GCC 编译器中被视为具有相同数据结构的类型。因此，GCC 允许以数组的形式访问这些数据类型的组件。即：下面代码是合法的。&lt;/p>
&lt;pre>&lt;code class="language-c">__m256 myvar = _mm256_set1_ps(6.665f); // Set all vector values to a single float
myvar[0] = 2.22f; // This is valid in GCC compiler
float f = (3.4f + myvar[0]) * myvar[7]; // This is valid in GCC compiler
&lt;/code>&lt;/pre>
&lt;p>例如，对于__m128 类型的变量，可以通过索引来访问其中的四个单精度浮点数组件。对于__m128d 类型的变量，可以通过索引来访问其中的两个双精度浮点数组件。类似地，对于__m256 和__m256d 类型的变量，可以通过索引来访问其中的八个单精度浮点数或四个双精度浮点数组件。&lt;/p>
&lt;p>而在 GCC 编译器中，__m128i 和__m256i 是用于处理整数向量的数据类型。它们被定义为联合体（union），可以表示不同长度的整数向量。然而，由于联合体的特性，访问其中的具体数据成员可能会有一些困难。为了从整数向量中提取单个数据值，可以使用 &lt;code>_mm_extract_epiXX()&lt;/code> 函数。这些函数允许从整数向量中提取指定位置的数据值，并将其作为标量返回。 &lt;code>_mm_extract_epiXX()&lt;/code> 函数中的 XX 表示整数向量的位宽，例如， &lt;code>_mm_extract_epi32()&lt;/code> 用于从 32 位整数向量中提取单个 32 位整数值。&lt;/p>
&lt;h3 id="2-ave-操作例子">2. AVE 操作例子&lt;/h3>
&lt;p>执行 AVX 指令的过程如下：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230810234918.webp"
alt="20230810234918" width="90%" loading="lazy">
&lt;/figure>
&lt;p>所有操作同时进行。就性能而言，在 AVX 中对浮点数执行单个 &lt;code>Add&lt;/code> 的消耗与在 AVX 中对 8 个浮点数执行 &lt;code>VAdd&lt;/code> 的消耗近似。在&lt;a class="link" href="https://www.agner.org/optimize/instruction_tables.pdf" target="_blank" rel="noopener" >Agner Fog&amp;rsquo;s instruction tables
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
中，可以获得更多有关指令延迟和吞吐量的信息。在 Sandy Bridge 架构上，&lt;code>VADDPS/D&lt;/code> 的延迟为 3，吞吐量为 1，就像 &lt;code>FADD(P)&lt;/code> 一样。&lt;/p>
&lt;h3 id="3-先决条件">3. 先决条件&lt;/h3>
&lt;p>SSE/AVX 需要目标机器具备相应的硬件支持。因此，为了确保程序在目标机器上能够正常运行，需要满足这些指令集扩展的先决条件。本文中的示例代码为了简化构建过程并确保程序在当前机器上正常运行，使用&lt;code>-march=native&lt;/code> 编译选项。这个选项会自动检测当前机器的 CPU 能力，并使用相应的指令集扩展。这样可以充分利用目标机器的硬件能力，提高程序的性能和效率。&lt;/p>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>注意：编译后的二进制文件在没有 AVX 功能的计算机上将失败。如果需要适应不同 CPU 的二进制代码，则需要利用 CPU Flag 并调用不同的函数，或者针对不同的指令集生成不同的二进制代码。&lt;/p>&lt;/div>
&lt;p>由于操作系统、编译器和 CPU 都必须允许 SSE/AVX 扩展。我们可以运行以下脚本来检测系统功能：&lt;/p>
&lt;pre>&lt;code class="language-shell">#!/bin/bash
#CPU flag detection
echo -e &amp;quot;\e[32m&amp;gt;&amp;gt;&amp;gt; Getting CPU flag capabilities and number of cores\e[0m&amp;quot;
cat /proc/cpuinfo | egrep &amp;quot;(flags|model name|vendor)&amp;quot; | sort | uniq -c
#Compiler capabilities. -march=native is required!
echo -e &amp;quot;\e[32m&amp;gt;&amp;gt;&amp;gt; Getting GCC capabilities\e[0m&amp;quot;
gcc -march=native -dM -E - &amp;lt; /dev/null | egrep &amp;quot;SSE|AVX&amp;quot; | sort
#OS kernel version
echo -e &amp;quot;\e[32m&amp;gt;&amp;gt;&amp;gt; Getting OS Kernel Version\e[0m&amp;quot;
uname -a
&lt;/code>&lt;/pre>
&lt;p>在 CPU Flag 中，我们可以看到 SSE 和 AVX 的支持。我们将搜索 avx 标志。这表明 CPU 兼容 AVX。如果有 avx2，则表示 CPU 允许 AVX2 扩展。AVX 足以支持 8x32 位浮点矢量。AVX2 为整数增加了 256 位向量（例如 8x32 位整数）。尽管如此，256 位整数向量的执行速度似乎与两个 128 位向量相同，因此与 SSE 128 位整数向量相比，性能并没有显著提高。&lt;/p>
&lt;p>在 GCC 的输出中，我们可以看到 #define __AVX__ 1 等。这表明 GCC 允许使用 AVX 指令集扩展。&lt;/p>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>记住始终使用 -march=native 或 -mavx， 如果运行 GCC 时没有使用正确的 march，就不会得到 &lt;strong>AVX&lt;/strong> 标志！ 默认的 GCC 参数是通用的，如果没有该标记，即使 CPU 支持 AVX，也无法启用 AVX。&lt;/p>&lt;/div>
&lt;p>最后，我们需要再次检查 Linux 内核是否为 2.6.30 或更高版本。理想的内核是 4.4.0 或更高版本。&lt;/p>
&lt;p>有了所有这些先决条件，我们就可以开始编写第一个 AVX 向量程序了。&lt;/p>
&lt;h2 id="二自动向量化">二、自动向量化&lt;/h2>
&lt;h3 id="1-gcc-自动向量化-flag">1. GCC 自动向量化 flag&lt;/h3>
&lt;p>GCC 是一种高级编译器，使用优化标志 -O3 或 -ftree-vectorize 时，编译器会搜索循环向量化（需要指定-mavx flag）。在源代码保持不变的情况下，GCC 编译出来的代码会完全不同。&lt;/p>
&lt;p>除非启用某些标志，否则 GCC 不会记录任何有关自动向量化的内容。如果需要自动向量化结果的详细信息，可以使用以下编译器 flag&lt;/p>
&lt;ul>
&lt;li>&lt;code>-fopt-info-vec&lt;/code> 或 &lt;code>-fopt-info-vec-optimized&lt;/code>：编译器将记录哪些循环（按行号）正在进行向量化优化。&lt;/li>
&lt;li>&lt;code>-fopt-info-vec-missed&lt;/code>：关于未被向量化的循环的详细信息，以及许多其他详细信息。&lt;/li>
&lt;li>&lt;code>-fopt-info-vec-note&lt;/code>：关于所有循环和正在进行的优化的详细信息。&lt;/li>
&lt;li>&lt;code>-fopt-info-vec-all&lt;/code>：所有以上的选项放在一起。&lt;/li>
&lt;/ul>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>注意：还有类似的 -fopt-info-[options]-optimized 标志用于其他编译器优化，如内联： -fopt-info-inline-optimized&lt;/p>&lt;/div>
&lt;p>在以下示例中，我们将使用 -O3 和 -fopt-info-vec-optimized 启用 GCC 自动向量化。当然也可以更改编译器标志以查看不同的日志记录选项。&lt;/p>
&lt;pre>&lt;code class="language-cpp">// autovector.cpp
// compile: g++ -fopt-info-vec-optimized -o autovector autovector.cpp
#pragma GCC optimize(&amp;quot;O3&amp;quot;, &amp;quot;unroll-loops&amp;quot;, &amp;quot;omit-frame-pointer&amp;quot;, &amp;quot;inline&amp;quot;) // 优化选项
#pragma GCC option(&amp;quot;arch=native&amp;quot;, &amp;quot;tune=native&amp;quot;, &amp;quot;no-zero-upper&amp;quot;) // 启用 AVX
#pragma GCC target(&amp;quot;avx&amp;quot;) // 启用 AVX
#include &amp;lt;bits/stdc++.h&amp;gt;
#include &amp;lt;x86intrin.h&amp;gt; // AVX/SSE 指令集
int main()
{
const int N = 200000;
const int numTests = 10000;
float a[N], b[N], c[N], result[N];
auto start = std::chrono::high_resolution_clock::now();
// 数据初始化
for (int i = 0; i &amp;lt; N; ++i)
{
a[i] = ((float)i) + 0.1335f;
b[i] = 1.50f * ((float)i) + 0.9383f;
c[i] = 0.33f * ((float)i) + 0.1172f;
}
for (int i = 0; i &amp;lt; numTests; ++i)
{
for (int j = 0; j &amp;lt; N; ++j)
{
result[j] = a[j] + b[j] - c[j] + 3 * (float)i;
}
}
auto end = std::chrono::high_resolution_clock::now();
auto duration = std::chrono::duration_cast&amp;lt;std::chrono::microseconds&amp;gt;(end - start).count();
assert(result[2] == (2.0f + 0.1335f) + (1.50f * 2.0f + 0.9383f) - (0.33f * 2.0f + 0.1172f) +
3 * (float)(numTests - 1));
std::cout &amp;lt;&amp;lt; &amp;quot;CG&amp;gt; message -channel \&amp;quot;results\&amp;quot; Time used: &amp;quot; &amp;lt;&amp;lt; duration
&amp;lt;&amp;lt; &amp;quot;s, N * numTests=&amp;quot; &amp;lt;&amp;lt; (N * numTests) &amp;lt;&amp;lt; std::endl;
return 0;
}
&lt;/code>&lt;/pre>
&lt;p>如果一切正常，将看到编译器测试结果：&lt;/p>
&lt;pre>&lt;code class="language-text">autovector.cpp:15:23: optimized: loop vectorized using 32 byte vectors
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>将编译选项更改为 &lt;code>-fopt-info-vec-all&lt;/code>，可以看到更多的信息，包括向量化的循环的行号。&lt;/li>
&lt;li>在 autovector.cpp 第 1 行，将 O3 改为 O2 , 然后重新运行。将不会看到 loop vectorized ，而且非向量化编译会比向量化编译慢。&lt;/li>
&lt;/ul>
&lt;h3 id="2-循环向量化的要求">2. 循环向量化的要求&lt;/h3>
&lt;p>并非所有循环都能进行向量化。要进行向量化，对循环有一些严格的要求。&lt;/p>
&lt;ul>
&lt;li>一旦循环开始，循环计数就不能改变。这意味着，循环的终点可以是一个动态变量，可以随意增加或减少其值，但一旦循环开始，它就必须保持不变。&lt;/li>
&lt;li>使用 break 或 continue 句子会有一些限制。有时编译器会很聪明地让它起作用，但在某些情况下，循环不会被向量化。&lt;/li>
&lt;li>在循环内调用外部函数有一些限制&lt;/li>
&lt;li>循环不应该有数据依赖关系。&lt;/li>
&lt;li>条件句 (if/Else) 可以在不改变控制流的情况下使用，并且只用于有条件地将 A 或 B 值加载到 C 变量中。选择 A 或 B 是在编译器中使用掩码完成的，因此它同时计算分支 A 和 B ，而 C 将存储一个或另一个值：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-c"> if ( s &amp;gt;= 0 ) {
x[i] = (-b[i]+s)/(2.0f*a[i]);
y[i] = (-b[i]-s)/(2.0f*a[i]);
}
else {
x[i] = 0.0f;
y[i] = 0.0f;
}
&lt;/code>&lt;/pre>
&lt;p>这是一个可向量循环。控制流从未改变，x[i] 和 y[i] 值总是设置为其中一个或另一个值。&lt;/p>
&lt;p>有关自动向量化的更多信息，请阅读&lt;a class="link" href="https://software.intel.com/sites/default/files/m/4/8/8/2/a/31848-CompilerAutovectorizationGuide.pdf" target="_blank" rel="noopener" >Intel C++编译器的矢量化
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。该文档虽然面向 Intel 编译器，但它提供了有关自动向量化的有趣而完整的信息。&lt;/p>
&lt;p>自动向量化的好处是，它是自动完成的。编译器会尝试向量化循环，开发人员不需要做任何事情。但是有时(尤其是在高性能计算应用中)需要微调循环和向量化，通过使用手动 AVX 向量化来确保最大吞吐量。&lt;/p>
&lt;h2 id="三sseavx-的使用">三、SSE/AVX 的使用&lt;/h2>
&lt;p>支持 SSE/AVX 的 CPU 具有用于操作 XMM 和 YMM 寄存器的汇编指令。但在大多数编译器中，通过使用内置函数简化了这一过程，因此开发人员不需要直接使用汇编。&lt;/p>
&lt;h3 id="1-内置函数">1. 内置函数&lt;/h3>
&lt;p>编译器将汇编指令封装为函数，使用它们就像调用带有正确参数的函数一样简单。有时，如果 CPU 不支持指令集，些内置函数就会被模拟。&lt;/p>
&lt;p>SSE/AVX 内置函数使用以下命名约定：&lt;/p>
&lt;pre>&lt;code class="language-cpp">_&amp;lt;vector_size&amp;gt;_&amp;lt;intrin_op&amp;gt;_&amp;lt;suffix&amp;gt;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>&amp;lt;vector_size&amp;gt;&lt;/code> 是指向量的大小。对于 128 位的 SSE， 它为 &lt;code>mm&lt;/code>，对于 256 位的 AVX/AVX2，它为 &lt;code>mm256&lt;/code>，对于 512 位的 AVX512， 它为 &lt;code>mm512&lt;/code> 。&lt;/li>
&lt;li>&lt;code>&amp;lt;intrin_op&amp;gt;&lt;/code> 是指内置函数的名称，例如 &lt;code>add&lt;/code> 或 &lt;code>sub&lt;/code>，&lt;code>mul&lt;/code> 等 。&lt;/li>
&lt;li>&lt;code>&amp;lt;suffix&amp;gt;&lt;/code> 是指内置函数的参数类型，例如 &lt;code>ps&lt;/code> 表示 float ，&lt;code>pd&lt;/code> 表示 double ，&lt;code>epi8&lt;/code> 表示 int8_t，&lt;code>epi32&lt;/code> 表示 int32_t , &lt;code>epu16&lt;/code> 表示 uint16_t 等。&lt;/li>
&lt;/ul>
&lt;p>你可以在&lt;a class="link" href="https://software.intel.com/sites/landingpage/IntrinsicsGuide" target="_blank" rel="noopener" >Intel Intrinsics Guide
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
中找到所有内置函数，它是 SSE/AVX 中提供的任何内置函数的完整参考。此外，还有一份 &lt;a class="link" href="https://db.in.tum.de/~finis/x86-intrin-cheatsheet-v2.2.pdf?lang=en" target="_blank" rel="noopener" >x86 内置函数 Cheet Sheet
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
，但由于内容更为复杂，阅读起来比较困难。&lt;/p>
&lt;h3 id="2-sseavx-没有提供的内置函数">2. SSE/AVX 没有提供的内置函数&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>缺少整数除法&lt;/strong>：由于某些原因，SSE 和 AVX 缺少整数除法运算符。有一些方法可以克服这一点：&lt;/p>
&lt;ul>
&lt;li>在线性代码中通过计算除法来完成操作。首先，从向量中取出单个数据，然后进行除法运算，最后将结果再次存储回向量中。然而，这种方法速度较慢。&lt;/li>
&lt;li>将整数向量转换为浮点数，将它们相除，然后再次转换为整数。&lt;/li>
&lt;li>对于编译时的已知除数，有一些魔法数（magic number）可以将常量除法转换为乘法运算。可以参考&lt;a class="link" href="https://libdivide.com/" target="_blank" rel="noopener" >libdivide
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/li>
&lt;li>对于 2 的幂除法，使用位移操作。除以整数 2 等于右移。只有当所有向量都被相同的 2 的幂整除时，才能进行右移操作。不过对有符号数进行右移时要注意！需要使用符号位移。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>缺少三角函数&lt;/strong>：内置函数中没有三角函数。可能的解决办法是用线性代码计算（对每个向量值逐一计算），或创建近似函数。泰勒级数和 Remez 近似函数的效果很好。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>缺少随机数生成器&lt;/strong>：此外，没有随机数生成器。但是从线性版本重新创建一个好的伪随机生成器是很简单的。只需确定伪随机数生成器中使用的位即可。填充向量首选 32 位或 64 位 RNG。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="3-性能损失">3. 性能损失&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>数据对齐&lt;/strong>： 旧的 CPU 架构不能使用向量化，除非数据在内存中与向量大小一致。其他一些 CPU 可以使用未对齐的数据，但性能会有所损失。在最近的处理器中，这种影响似乎可以忽略不计。但为了安全起见，如果不增加过多的开销，对齐数据可能是个好主意。有关数据对齐的资料，可参考&lt;a class="link" href="https://lemire.me/blog/2012/05/31/data-alignment-for-speed-myth-or-reality/" target="_blank" rel="noopener" >Data alignment for speed: myth or reality?
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;ul>
&lt;li>在 GCC 中，可以使用以下变量属性进行数据对齐： &lt;code>__attribute__((aligned(16)))&lt;/code>、&lt;code>__attribute__((aligned(32)))&lt;/code>&lt;/li>
&lt;li>最简单的变量对齐声明：&lt;code>#define ALIGN __attribute__((aligned(32)))&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>SSE/AVX 转换损失&lt;/strong>： 将传统的 SSE 库与新的 AVX 架构混合使用还有一个大问题。由于 XMM 和 YMM 共享低 128 位，在 AVX 和 SSE 之间转换可能导致高 128 位出现未定义的值。为了解决这个问题，编译器需要保存高 128 位，清除它，执行旧的 SSE 操作，然后恢复旧值。但是这显著增加了 AVX 操作的开销，导致性能下降。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="notice notice-tip" >
&lt;div class="notice-title">&lt;svg t="1705945832245" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="19409" width="200" height="200">&lt;path d="M512 74.666667C270.933333 74.666667 74.666667 270.933333 74.666667 512S270.933333 949.333333 512 949.333333 949.333333 753.066667 949.333333 512 753.066667 74.666667 512 74.666667z m238.933333 349.866666l-2.133333 2.133334-277.333333 277.333333c-10.666667 10.666667-29.866667 12.8-42.666667 2.133333L426.666667 704l-149.333334-149.333333c-12.8-12.8-12.8-32 0-44.8 10.666667-10.666667 29.866667-12.8 42.666667-2.133334l2.133333 2.133334 125.866667 125.866666 253.866667-253.866666c10.666667-10.666667 29.866667-12.8 42.666666-2.133334l2.133334 2.133334c12.8 12.8 12.8 32 4.266666 42.666666z" fill="#ffffff" p-id="19410">&lt;/path>&lt;/svg>&lt;/div>&lt;p>注意：这个问题并不意味着不能同时使用__m128 和__m256 而不影响性能。AVX 有一个针对__m128 的新指令集，带有 VEX 前缀。这些新的 VEX 指令与__M256 指令相结合没有任何问题。当非 VEX __m128 指令与 __m256 指令结合使用时，就会产生转换代价。当使用旧的 SSE 库链接到新的启用 AVX 的程序时，就会发生这种情况。&lt;/p>&lt;/div>
&lt;ul>
&lt;li>
&lt;p>为了避免转换损失，编译器可以使用 &lt;code>-mvzeroupper&lt;/code> 参数自动添加对 &lt;code>VZEROUPPER&lt;/code> (清除高 128 位)或 &lt;code>VZEROALL&lt;/code>（清除所有 YMM 寄存器）的调用，程序员也可以手动添加。如果不使用外部 SSE 库，且确定所有代码都启用了 VEX 并在编译时启用了 AVX 扩展，则可以使用 &lt;code>-mvzeroupper&lt;/code> 参数指示编译器避免添加 &lt;code>VZEROUPPER&lt;/code> 调用： &lt;code>-mno-vzeroupper&lt;/code>。更多关于 SSE/AVX 转换损失的资料，可参考&lt;a class="link" href="https://software.intel.com/en-us/articles/avoiding-avx-sse-transition-penalties" target="_blank" rel="noopener" >Avoiding AVX-SSE Transition Penalties
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
和 &lt;a class="link" href="https://stackoverflow.com/questions/41303780/why-is-this-sse-code-6-times-slower-without-vzeroupper-on-skylake" target="_blank" rel="noopener" >Why is this SSE code 6 times slower without VZEROUPPER on Skylake?
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>数据移动成本&lt;/strong>：在 AVX 寄存器中来回移动数据的成本很高。在某些情况下，如果有一些数据存储在线性结构中，那么将这些数据发送到 AVX 向量、执行一些操作并恢复这些数据的成本要比简单地执行线性计算高。因此，开发时必须考虑到数据加载和卸载的开销。请记住，在某些情况下，这将成为性能瓶颈。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="4-avx-使用例子计算平方根">4. AVX 使用例子：计算平方根&lt;/h3>
&lt;ul>
&lt;li>下面程序是对浮点数的 SQRT 计算进行向量化，显式使用 &lt;code>__m256&lt;/code> 数据类型来存储浮点数，从而减少数据加载的开销。&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-cpp">// vectorized_sqrt.cpp
// compile: g++ -o vectorized_sqrt vectorized_sqrt.cpp
#pragma GCC optimize(&amp;quot;O3&amp;quot;, &amp;quot;unroll-loops&amp;quot;, &amp;quot;omit-frame-pointer&amp;quot;, &amp;quot;inline&amp;quot;) // 优化选项
#pragma GCC option(&amp;quot;arch=native&amp;quot;, &amp;quot;tune=native&amp;quot;, &amp;quot;no-zero-upper&amp;quot;) // 启用 AVX
#pragma GCC target(&amp;quot;avx&amp;quot;) // 启用 AVX
#include &amp;lt;bits/stdc++.h&amp;gt;
#include &amp;lt;x86intrin.h&amp;gt; // AVX/SSE 指令集
const int N = 64000000;
const int V = N / 8;
float linear[N];
// 禁用自动向量化
__attribute__((optimize(&amp;quot;no-tree-vectorize&amp;quot;))) inline void normal_sqrt()
{
for (int i = 0; i &amp;lt; N; ++i)
{
linear[i] = sqrtf(linear[i]);
}
}
__m256 ALIGN vectorized[V];
inline void avx_sqrt()
{
for (int i = 0; i &amp;lt; V; ++i)
{
vectorized[i] = _mm256_sqrt_ps(vectorized[i]);
}
}
#define TIME \
std::chrono::duration_cast&amp;lt;std::chrono::duration&amp;lt;double&amp;gt;&amp;gt;( \
std::chrono::high_resolution_clock::now() - now) \
.count()
int main(int argc, char **argv)
{
// 数据初始化
for (int i = 0; i &amp;lt; N; ++i)
{
linear[i] = ((float)i) + 0.1335f;
}
for (int i = 0; i &amp;lt; V; ++i)
{
for (int v = 0; v &amp;lt; 8; ++v)
{
vectorized[i][v] = ((float)(i * 8 + v)) + 0.1335f;
}
}
// normal_sqrt benchmark
auto now = std::chrono::high_resolution_clock::now();
for (int i = 0; i &amp;lt; 20; ++i)
{
normal_sqrt();
}
auto linear_time = TIME;
std::cerr &amp;lt;&amp;lt; &amp;quot;Normal sqrtf: &amp;quot; &amp;lt;&amp;lt; linear_time &amp;lt;&amp;lt; std::endl;
// AVX sqrt benchmark
now = std::chrono::high_resolution_clock::now();
for (int i = 0; i &amp;lt; 20; ++i)
{
avx_sqrt();
}
auto avx_time = TIME;
std::cerr &amp;lt;&amp;lt; &amp;quot;AVX sqrtf: &amp;quot; &amp;lt;&amp;lt; avx_time &amp;lt;&amp;lt; std::endl;
// Check Values
for (int i = 0; i &amp;lt; V; ++i)
{
for (int v = 0; v &amp;lt; 8; ++v)
{
if (abs(linear[i * 8 + v] - vectorized[i][v]) &amp;gt; 0.00001f)
{
std::cerr &amp;lt;&amp;lt; &amp;quot;Error: AVX sqrtf is not equal to normal sqrtf!&amp;quot; &amp;lt;&amp;lt; std::endl;
std::cerr &amp;lt;&amp;lt; &amp;quot;linear[&amp;quot; &amp;lt;&amp;lt; i * 8 + v &amp;lt;&amp;lt; &amp;quot;] = &amp;quot; &amp;lt;&amp;lt; linear[i * 8 + v] &amp;lt;&amp;lt; std::endl;
std::cerr &amp;lt;&amp;lt; &amp;quot;vectorized[&amp;quot; &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &amp;quot;][&amp;quot; &amp;lt;&amp;lt; v &amp;lt;&amp;lt; &amp;quot;] = &amp;quot; &amp;lt;&amp;lt; vectorized[i][v]
&amp;lt;&amp;lt; std::endl;
return -1;
}
}
}
std::cout &amp;lt;&amp;lt; &amp;quot;Linear to AVX improvement : &amp;quot; &amp;lt;&amp;lt; (linear_time / avx_time * 100) &amp;lt;&amp;lt; &amp;quot;%&amp;quot;
&amp;lt;&amp;lt; std::endl;
return 0;
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>可能会看到 600% 或更高的性能提升。也就是说，一旦加载了数据，AVX 的运行速度将是普通 sqrtf 的 7 倍。理论极限是 800%，但很少能达到。一般来说可以预期平均提高 300% 到 600%。运行结果如下：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-text">Normal sqrtf: 1.51901
AVX sqrtf: 0.374871
Linear to AVX improvement : 405.209%
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>可以看到：运行速度提高了 405%。&lt;/li>
&lt;/ul>
&lt;h2 id="四c中的-sseavx-框架">四、C++中的 SSE/AVX 框架&lt;/h2>
&lt;h3 id="1-内置函数的复杂性">1. 内置函数的复杂性&lt;/h3>
&lt;p>直接使用内置函数会使代码编写和维护变得复杂。问题在于内置函数名很长，比如算术运算用函数符号书写：&lt;code>add(a,b)&lt;/code> 而不是 &lt;code>a + b&lt;/code>。导致下面的代码很难阅读：&lt;/p>
&lt;pre>&lt;code class="language-cpp">x = _mm256_div_ps(_mm256_add_ps(b, _mm256_sqrt_ps(_mm256_sub_ps(_mm256_mul_ps(b, b),
_mm256_mul_ps(_mm256_mul_ps(a, c),
_mm256_set1_ps(4.0f))))) , _mm256_mul_ps(a,_mm256_set1_ps(2.0f)));
&lt;/code>&lt;/pre>
&lt;p>而以下封装版本的可读性非常好：&lt;/p>
&lt;pre>&lt;code class="language-cpp">x = (b + sqrt(b * b - a * c * 4.0f)) / (a * 2.0f);
&lt;/code>&lt;/pre>
&lt;h3 id="2-用于-simd-计算的-c框架">2. 用于 SIMD 计算的 C++框架&lt;/h3>
&lt;p>现有的一些框架在新的类中封装了向量数据类型。然后，它们重载算术、逻辑和赋值运算符，以简化计算。其中，可以使用这两个框架：&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="http://www.agner.org/optimize/#vectorclass" target="_blank" rel="noopener" >Agner Fog&amp;rsquo;s C++ vector class library
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
： 内容完整，定期更新。而且还包含了三角函数。&lt;/li>
&lt;li>&lt;a class="link" href="https://gain-performance.com/ume/" target="_blank" rel="noopener" >Unified Multicore Environment
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
：一个较新的库&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/xtensor-stack/xsimd" target="_blank" rel="noopener" >xsimd Wrapper
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
：一个比较好用的 C++ Wrapper。&lt;/li>
&lt;/ul>
&lt;p>不过，这些库的体积都比较大，在代码大小有限（小于 100kb）的情况下，可以使用以下简单的封装版本，只需要关注一两种类型。&lt;/p>
&lt;a href="https://github.com/marchete/Course-SSE-and-AVX-Vectorization-ES/tree/master/projects/avx/framework" target="_blank" class="card-github fetch-waiting no-styling"
repo="marchete/Course-SSE-and-AVX-Vectorization-ES" id="repo-CBVfn2VvTSlUKFl7-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-CBVfn2VvTSlUKFl7-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">marchete&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">Course-SSE-and-AVX-Vectorization-ES&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-CBVfn2VvTSlUKFl7-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-CBVfn2VvTSlUKFl7-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-CBVfn2VvTSlUKFl7-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-CBVfn2VvTSlUKFl7-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-CBVfn2VvTSlUKFl7-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-CBVfn2VvTSlUKFl7-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/marchete\/Course-SSE-and-AVX-Vectorization-ES', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-CBVfn2VvTSlUKFl7-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-CBVfn2VvTSlUKFl7-language').innerText = data.language;
document.getElementById('repo-CBVfn2VvTSlUKFl7-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-CBVfn2VvTSlUKFl7-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-CBVfn2VvTSlUKFl7-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-CBVfn2VvTSlUKFl7-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-CBVfn2VvTSlUKFl7-license').classList.add = "no-license"
};
document.getElementById('repo-CBVfn2VvTSlUKFl7-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for marchete\/Course-SSE-and-AVX-Vectorization-ES.")
}).catch(err => {
const c = document.getElementById('repo-CBVfn2VvTSlUKFl7-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for marchete\/Course-SSE-and-AVX-Vectorization-ES.")
})
&lt;/script>
&lt;ul>
&lt;li>除了内置函数，该封装版本还封装了一些特殊的函数：&lt;/li>
&lt;li>&lt;strong>Blend-based functions&lt;/strong>：blend 是根据掩码有条件地加载向量值的过程，这类函数用于混合两个向量的函数。
&lt;ul>
&lt;li>&lt;code>if_select(mask,value_true,value_false)&lt;/code> ：根据掩码对向量进行有条件加载。如果掩码为真，则返回 value_true，否则返回 value_false。&lt;/li>
&lt;li>&lt;code>if_add(mask,value,add_when_true)&lt;/code> ：条件加法。返回 &lt;code>value + (mask? add_when_true:0)&lt;/code> ，对于每个向量分量。&lt;/li>
&lt;li>&lt;code>if_sub, if_mul, if_div&lt;/code> ：与 if_add 类似，只是算术运算方式不同。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Horizontal functions&lt;/strong>：Horizontal 表示这些函数通过计算某些逻辑值或算术值，在单个向量变量内运行。
&lt;ul>
&lt;li>&lt;code>horizontal_or(mask)&lt;/code> ：如果掩码中的任何向量分量为 true。返回布尔值。&lt;/li>
&lt;li>&lt;code>horizontal_add(vector)&lt;/code> ：返回向量的所有分量的总和。返回值是一个数字(浮点型、双精度型或整型，具体取决于向量类型)。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="五masking-与-conditional-load">五、Masking 与 Conditional Load&lt;/h2>
&lt;h3 id="1-向量中的掩码">1. 向量中的掩码&lt;/h3>
&lt;p>掩码是向量之间逻辑运算的结果。它与布尔运算有许多相似之处（它们是对单个数字或其他布尔运算的逻辑运算结果），但在内部，每个掩码组件必须全部为 0 位或全部为 1 位。&lt;/p>
&lt;p>让我们比较具有大于运算符的两个 AVX 浮点向量：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230811173734.webp"
alt="20230811173734" width="90%" loading="lazy">
&lt;/figure>
&lt;p>输入是两个带有浮点分量的向量。逻辑运算的输出也是一个带浮点分量的向量，但其值的位数被设置为全 0 或全 1。全 1 表示 &amp;ldquo;真&amp;rdquo;，全 0 表示 &amp;ldquo;假&amp;rdquo;。对于浮点数，全 1 的值打印为-nan，对于整数，则打印为-1。存储的实际值并不重要。我们只需要知道它保存的是真值和假值。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>逻辑运算符的结果(&amp;gt;、&amp;lt;、==、&amp;amp;&amp;amp;、||等&lt;/strong>：以逻辑&amp;amp;&amp;amp;运算符为例：&lt;/p>
&lt;ul>
&lt;li>&lt;code>vector &amp;amp;&amp;amp; vector = mask&lt;/code>&lt;/li>
&lt;li>&lt;code>mask &amp;amp;&amp;amp; mask == mask&lt;/code>&lt;/li>
&lt;li>&lt;code>vector &amp;amp;&amp;amp; mask == ?????&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>最后一种情况，可能会有意想不到的结果，这就像试图做 3 &amp;gt; false，也许在 C++ 中这是可行的，但在逻辑意义上这是不正确的。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>注意：与布尔运算不同，并非零以外的任何数字都是 TRUE。只有所有位都设置为 1 的矢量成分才被视为 TRUE。不要使用其他值作为掩码。否则会失败，或得到意想不到的结果。&lt;/p>&lt;/div>
&lt;h3 id="2-条件加载">2. 条件加载&lt;/h3>
&lt;p>掩码可用于有条件地将值加载到向量中。比如可以使用掩码来有条件地控制值向量的加载：&lt;code>if_select(mask,value_true,value_false)&lt;/code> 可以表示为：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230811174342.webp"
alt="20230811174342" width="90%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>当掩码设置为 &lt;code>FALSE&lt;/code> 时，数据从 &lt;code>value_false&lt;/code> 向量加载；当设置为 &lt;code>TRUE&lt;/code> 时，数据从 &lt;code>value_true&lt;/code> 向量加载。这个概念简单而有效。&lt;/li>
&lt;/ul>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>代码示例 1：使用掩码和 SIMD-Framework 中的 v8f.h 实现条件加载。（主要使用 if_select(mask,value_true,value_false) 方法，该函数是 _mm256_blendv_ps 的封装）&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-cpp">#pragma GCC optimize(&amp;quot;O3&amp;quot;, &amp;quot;unroll-loops&amp;quot;, &amp;quot;omit-frame-pointer&amp;quot;, &amp;quot;inline&amp;quot;)
#pragma GCC option(&amp;quot;arch=native&amp;quot;, &amp;quot;tune=native&amp;quot;, &amp;quot;no-zeroupper&amp;quot;)
#pragma GCC target(&amp;quot;avx&amp;quot;)
#include &amp;lt;bits/stdc++.h&amp;gt;
#include &amp;lt;x86intrin.h&amp;gt;
#include &amp;quot;v8f.h&amp;quot;
using namespace std;
inline v8f testConditions(const v8f &amp;amp;value)
{
return if_select(value &amp;gt; 3.0f || (value &amp;lt;= -3.7f &amp;amp;&amp;amp; value &amp;gt; -15.0f), sqrt(2.0f * value + 1.5f),
(-2.0f * value - 8.7f));
}
inline bool validate(const v8f &amp;amp;test, const v8f &amp;amp;vector)
{
for (int j = 0; j &amp;lt; 8; ++j)
{
float value = test[j];
float expected;
if (value &amp;gt; 3.0f || (value &amp;lt;= -3.7f &amp;amp;&amp;amp; value &amp;gt; -15.0f))
{
expected = sqrt(2.0f * value + 1.5f);
}
else
{
expected = (-2.0f * value - 8.7f);
}
if (abs(expected - vector[j]) &amp;gt; 0.00001f)
{
cout &amp;lt;&amp;lt; &amp;quot;Assert Error:&amp;quot; &amp;lt;&amp;lt; expected &amp;lt;&amp;lt; &amp;quot; &amp;quot; &amp;lt;&amp;lt; vector[j] &amp;lt;&amp;lt; endl;
return false;
}
}
return true;
}
int main()
{
int validTests = 0;
int TotalTests = 1000;
for (int i = 0; i &amp;lt; TotalTests; ++i)
{
float offset = -500.0f + (1000.0f * i) / TotalTests;
v8f test(1.4f, 3.3f, -12.5f, -33.4f, 7.9f, -70.2f, 15.1f, 22.6f);
test += offset;
v8f result = testConditions(test);
if (validate(test, result))
{
++validTests;
}
}
cout &amp;lt;&amp;lt; &amp;quot;Valid Tests:&amp;quot; &amp;lt;&amp;lt; validTests &amp;lt;&amp;lt; &amp;quot;/&amp;quot; &amp;lt;&amp;lt; TotalTests &amp;lt;&amp;lt; &amp;quot; (&amp;quot;
&amp;lt;&amp;lt; (100 * validTests / TotalTests) &amp;lt;&amp;lt; &amp;quot;%)&amp;quot; &amp;lt;&amp;lt; endl;
if (validTests != TotalTests)
{
return -1;
}
return 0;
}
&lt;/code>&lt;/pre>
&lt;h3 id="3-性能">3. 性能&lt;/h3>
&lt;p>使用掩码的条件加载不是真正的分支，因此它们不会有误预测，并且 CPU 可以更好地利用无序执行。但这是有代价的。因为它们是无分支的，并且所有条件执行都是通过掩码操作完成的，所以总是计算和执行这两个分支。如果要对 &lt;code>value_false&lt;/code> 进行非常复杂的计算，那么即使只有 0.00001% 的时间会发生，也会一直进行计算。如果代码中有些部分很少需要，但计算成本很高，这可能会导致性能问题。在下一章数据流控制中，可以通过控制数据流的方法，根据某些条件提前退出循环。&lt;/p>
&lt;h2 id="六数据流控制">六、数据流控制&lt;/h2>
&lt;h3 id="1-共享数据流问题">1. 共享数据流问题&lt;/h3>
&lt;p>在线性编程中，创建条件分支 if、switch、continue 和 break 来控制数据流没有任何问题。你只需创建一个无限循环，并在条件满足时跳出循环即可。但是一个向量不仅有一个条件结果，而且同时有 N 个条件结果。向量的一部分可以准备退出循环(因为向量数据已达到退出条件)，但其余数据在退出之前仍有活动工作要做。&lt;/p>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>如果向量分量已经计算完成，请冻结它以避免对其进行任何进一步的计算。具体做法是在任何数值赋值中屏蔽已完成的分量。未完成的向量分量会不断更新，但已完成的分量不会。因此，如果我有一个 8x 浮点矢量，而分量 0、1、4 和 7 已达到结束状态，我就需要在每次数据加载时加上一个掩码[false,false,true,true,false,true,false]&lt;/p>&lt;/div>
&lt;h3 id="2-避免执行计算开销很大的分支">2. 避免执行计算开销很大的分支&lt;/h3>
&lt;p>要节省 CPU 时间，最简单的方法是检查掩码内的所有值是否相同，要么全部为 &amp;ldquo;true&amp;rdquo;，要么全部为 &amp;ldquo;false&amp;rdquo;。当掩码内的所有值都相同时，我们就得到了一个简单的布尔值，要么为真，要么为假。这可以用来跳过部分代码，或使用普通的条件分支：if、switch、continue 和 break 等。&lt;/p>
&lt;p>在 SIMD-Framework 中，使用的是 &lt;code>horizontal_or(mask)&lt;/code>函数（封装了&lt;code>_xxx_testz_xx&lt;/code>）。该函数检查掩码内是否有任何值为真，如果存在真值则返回 true，否则返回 false。&lt;/p>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>代码示例 2：使用 horizontal_or(mask) 函数判断掩码内是否有任何值为真，减少分支计算&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-cpp">v8f result(0.0f);
for (int i = 0; i &amp;lt; 2000; i++)
{
v8f test(1.4f, 3.3f, -12.5f, -33.4f, 7.9f, -70.2f, 15.1f, 22.6f);
test += ((float)i) / 100.0f;
if (horizontal_or(test &amp;gt;= 38.0f))
{
result += if_select(test &amp;gt;= 38.0f, slowFunction(i), test);
}
else
{
// 全为 false，不需要执行 slowFunction，直接加上 test 向量即可
result += test;
}
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>通过使用 &lt;code>horizontal_or&lt;/code> ，还可以提前跳出循环。自动向量化无法实现这种优化，但手动向量化可以，而且是首选。&lt;/li>
&lt;/ul>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>代码示例 3：使用 horizontal_or 提前跳出循环，该程序需要同时进行 8 次并行模拟，以 200 个回合为限，计算最大连击得分。一旦在任何一次并行模拟中得分超过 1700 分，就结束模拟，并返回最大得分（一个浮点数值，不是包含所有得分的整个向量，只是最大值）和获得该得分的回合。&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-cpp">#pragma GCC optimize(&amp;quot;O3&amp;quot;, &amp;quot;unroll-loops&amp;quot;, &amp;quot;omit-frame-pointer&amp;quot;, &amp;quot;inline&amp;quot;)
#pragma GCC option(&amp;quot;arch=native&amp;quot;, &amp;quot;tune=native&amp;quot;, &amp;quot;no-zeroupper&amp;quot;)
#pragma GCC target(&amp;quot;avx&amp;quot;)
#include &amp;lt;bits/stdc++.h&amp;gt;
#include &amp;lt;x86intrin.h&amp;gt;
#include &amp;quot;v8f.h&amp;quot;
using namespace std;
int validateResult(const int &amp;amp;turn, const float &amp;amp;bestScore)
{
cout &amp;lt;&amp;lt; &amp;quot;Turn:&amp;quot; &amp;lt;&amp;lt; turn &amp;lt;&amp;lt; &amp;quot; bestScore:&amp;quot; &amp;lt;&amp;lt; std::setprecision(10) &amp;lt;&amp;lt; bestScore &amp;lt;&amp;lt; endl;
if (turn != 133)
{
cout &amp;lt;&amp;lt; &amp;quot;ERROR, Expected turn exit at 133 != &amp;quot; &amp;lt;&amp;lt; turn &amp;lt;&amp;lt; endl;
return -1;
}
if (bestScore != 1707.318481f)
{
cout &amp;lt;&amp;lt; &amp;quot;ERROR, Expected a bestScore of 1707.318481f != &amp;quot; &amp;lt;&amp;lt; std::setprecision(10)
&amp;lt;&amp;lt; bestScore &amp;lt;&amp;lt; endl;
return -1;
}
return 0;
}
int main()
{
int turn = 0;
v8f Scores(1.0f, 3.0f, 7.0f, 13.4f, 22.7f, 0.01f, 4.556f, 9.7f);
for (turn = 0; turn &amp;lt; 200; ++turn)
{
Scores += ((float)(turn) / 15.0f);
if (turn == 40)
{
Scores *= Scores / 15.0f + 2.0f;
}
if (turn == 70)
{
Scores += if_select(Scores &amp;lt; 430.0f, 850.0f, 120.0f);
}
// 利用 horizontal_or 提前退出循环
if (horizontal_or(Scores &amp;gt;= 1700.0f))
{
break;
}
}
cout &amp;lt;&amp;lt; &amp;quot;Scores: &amp;quot; &amp;lt;&amp;lt; Scores &amp;lt;&amp;lt; endl;
float bestScore = 0.0f;
// 遍历获取最大分量
for (int i = 0; i &amp;lt; 8; i++)
{
float score = get(Scores, i);
if (bestScore &amp;lt; score)
{
bestScore = score;
}
}
return validateResult(turn, bestScore);
}
&lt;/code>&lt;/pre>
&lt;h2 id="七数据对齐">七、数据对齐&lt;/h2>
&lt;p>数据对齐是一种强制编译器在特定字节边界的内存中创建数据对象的方法。这样做的目的是为了提高从处理器加载和存储数据的效率。无需赘述，当数据可以在特定字节边界的内存地址之间移动时，处理器就可以高效地移动数据。对于支持英特尔® AVX-512 指令的英特尔® 处理器来说，当数据起始地址位于 64 字节边界时，内存移动效果最佳。因此，需要强制编译器创建起始地址为 64 字节模的数据对象。&lt;/p>
&lt;p>除了在对齐边界上创建数据（使基指针对齐）外，编译器还能在已知数据访问（包括基指针和索引）对齐 64 字节时执行优化。在通常情况下，如果没有用户的帮助，编译器并不知道循环内部的数据是对齐的。这可能迫使编译器在生成代码时采取保守做法，从而影响性能。因此还必须通过编译指示(C/C++)或指令(Fortran)、选项(如 Fortran 中的-Align array64byte)以及子句/属性的组合来通知编译器进行对齐，以便英特尔编译器能够生成最佳代码。&lt;/p>
&lt;p>总而言之，需要两个步骤：&lt;/p>
&lt;ol>
&lt;li>数据对齐&lt;/li>
&lt;li>在性能关键区域(使用数据的区域)中使用 pragma/directives/clauses 来告诉编译器内存访问是对齐的&lt;/li>
&lt;/ol>
&lt;h3 id="1-数据对齐">1. 数据对齐&lt;/h3>
&lt;p>调整数据以提高应用性能非常重要。这通常意味着两点：&lt;/p>
&lt;ol>
&lt;li>在为数组（或指针）分配空间时对齐基指针&lt;/li>
&lt;li>确保每个向量化循环（对于每个线程）的起始索引具有良好的对齐属性&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>对齐静态数组（基指针）&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>静态数组的对齐十分简单，不过在 Windows 上与 Linux 上的声明有所区别，以 64 字节边界上静态声明 1000 元素单精度浮点数组为例&lt;/li>
&lt;li>在 Windows 上，使用 &lt;code>__declspec(align(64))&lt;/code> 修饰符：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-c">__declspec(align(64)) float a[1000];
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>在 Linux 上，使用 &lt;code>__attribute__((aligned(64)))&lt;/code> 修饰符：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-c">float a[1000] __attribute__((aligned(64)));
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>对齐动态数据&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>动态数据的对齐相对复杂，需要使用特殊的内存分配函数，如 &lt;code>_mm_malloc&lt;/code> 和 &lt;code>_mm_free&lt;/code> 来替代 &lt;code>malloc&lt;/code> 和 &lt;code>free&lt;/code> 函数。其中，这些函数的第二个参数是对齐参数（以字节为单位），比如 &lt;code>mm_malloc(p, 64)&lt;/code> 返回的数据将是 64 字节对齐的。&lt;/li>
&lt;/ul>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>对于动态分配的 C/C++ 数组，仅仅在创建时使用 mm_malloc 对齐数据是不够的（这是一个必要条件），还需要在相关循环之前添加一个__assume_aligned(a, 64) 形式的子句。如果没有这一步，编译器将无法检测使用此类数组进行访问时的最佳对齐方式。&lt;/p>&lt;/div>
&lt;ul>
&lt;li>在 C++17 中，还可以使用 &lt;code>std::aligned_alloc&lt;/code> 函数来分配对齐的内存，但是这个函数只能在 C++17 中使用，而且只能在 Linux 上使用。使用方式如下：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-cpp">float *a = std::static_cast&amp;lt;float *&amp;gt;std::aligned_alloc(64, 1000 * sizeof(float));
// 使用完毕后，需要释放内存
std::free(a);
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>对齐循环索引&lt;/strong>&lt;/p>
&lt;p>对于内存访问形式为 &lt;strong>a[i+n1]&lt;/strong> 的循环，必须满足特定的对齐要求。具体来说，用户必须确保 （i-loop 的下界 + n1）是 16 的倍数（假设数据类型为 float）。&lt;/p>
&lt;p>此外，除非在编译时信息可以在静态情况下获得（例如访问形式为 &lt;strong>x[i]&lt;/strong> ，并且所有线程的循环下界都是常数 0，或者在循环内部存在形式为 &lt;strong>b[i+16*k]&lt;/strong> 的访问），用户还必须通知编译器关于这个对齐要求。否则，这一步还需要在循环前添加一个 &lt;strong>__assume(n1%16==0)&lt;/strong> 或者 &lt;strong>#pragma vector aligned&lt;/strong> 的语句（ &lt;strong>仅限于 Windows 平台&lt;/strong> ）。以下是一个不满足数据对齐要求的例子：&lt;/p>
&lt;pre>&lt;code class="language-cpp">#define N 1000
float a[N] __attribute__((aligned(64)));
void process_array()
{
for (int i = 0; i &amp;lt; N; i++)
{
float result = a[i + 4]; // 访问 a[i+n1]，其中 n1 = 4
// 其它计算操作...
}
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>如果我们要确保内存访问的性能最佳，我们需要确保 i 和 n1 的组合是对齐的，以便在向量化指令集中能够更有效地执行。在上面的代码中，循环的下界是 i 的初始值 0 ，所以 (0 + 4) 不是 16 的倍数。为了满足对齐要求，我们需要进行调整，并且通知编译器这个对齐属性。&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-cpp">#define N 1000
float a[N] __attribute__((aligned(64)));
void process_array(int n1)
{
for (int i = 0; i &amp;lt; N; i += 16) // 调整循环的步长。
{
__assume((n1 % 16) == 0);
float result = a[i + n1]; // 访问 a[i+n1]
// 其它计算操作...
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="2-通知编译器数据对齐">2. 通知编译器数据对齐&lt;/h3>
&lt;p>既然已经对齐了数据，那么在程序中实际使用数据时，就有必要告知编译器这些数据是对齐的。例如，将数据作为参数传递给性能关键的函数或子程序时，编译器如何知道参数是对齐的还是未对齐的？例如，数据通常在一个源文件中声明，但在许多其他源文件中使用。因此，这一信息必须由用户提供，因为编译器往往没有关于参数的信息。&lt;/p>
&lt;p>有两种方法可以告知编译器数据对齐情况。 一种方法是使用 &lt;strong>OpenMP SIMD ALIGNED&lt;/strong> 子句通知编译器在使用数据时的数据对齐情况。另一种方法则是使用英特尔专有子句在代码中指定数据对齐方式。&lt;/p>
&lt;p>编译器要为 i 循环内的（浮点数组）内存访问（如 &lt;strong>a[i+n1]&lt;/strong> 和 &lt;strong>X[i]&lt;/strong>）生成对齐的加载/存储，就必须知道：&lt;/p>
&lt;ol>
&lt;li>基数指针（a 和 X）已对齐。对于静态数组，可以使用上面讨论的技术实现对齐，例如使用 &lt;code>__declspec(align(64))&lt;/code> 。对于动态分配的数组，仅仅在创建时使用 &lt;code>mm_malloc&lt;/code> 或 &lt;code>aligned_alloc&lt;/code> 对齐数据是不够的，还需要如下所示的子句 &lt;code>__assume_aligned(a, 64)&lt;/code> 。&lt;/li>
&lt;li>编译器必须知道（i-loop 的下界 + n1）是 16 的倍数（假设数据类型为 float）。如果循环下界为 0 ，那么所需的信息就是 n1 是 16 的倍数。一种方法是添加一个 &lt;strong>__assume(n1%16==0)&lt;/strong> 形式的子句。&lt;/li>
&lt;/ol>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>代码示例 4： 在 Windows 上使用 __assume_aligned 和 __assume 指令来告知编译器数据对齐情况。&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-cpp">// compile options: -O3 -xcore-avx512 -qopt-report-phase=vec -qopt-report=5 -qopt-report-file=stdout -restrict -c
// 该编译指令将生成一个名为 stdout 的文件，其中包含有关向量化的信息。
// restrict 关键字：提示编译器：在该指针的生命周期内，其指向的对象不会被别的指针所引用。
#define N 1000
__declspec(align(64)) float X[N], X2[N];
void foo(float * restrict a, int n, int n1, int n2)
{
__assume_aligned(a, 64);
__assume((n1 % 16) == 0);
__assume((n2 % 16) == 0);
for (int i = 0; i &amp;lt; n; i++)
{
X[i] += a[i] + a[i + n1] + a[i - n1] + a[i + n2] + a[i - n2];
}
for (int i = 0; i &amp;lt; n; i++)
{
X2[i] += X[i] * a[i];
}
}
&lt;/code>&lt;/pre>
&lt;h2 id="八总结">八、总结&lt;/h2>
&lt;p>本文主要介绍了 SIMD 的基本概念，以及 SIMD 的优化思路，最后通过一些简单的示例代码，介绍了 SIMD 的使用方法。主要内容如下：&lt;/p>
&lt;ul>
&lt;li>在代码中使用 SSE 和 AVX 指令的硬件和软件要求。&lt;/li>
&lt;li>可用的向量数据类型。&lt;/li>
&lt;li>有关如何检查自动向量化使用情况的信息，以及有关可自动向量化的循环的提示。&lt;/li>
&lt;li>C++中的 SSE/AVX 框架。&lt;/li>
&lt;li>掩码和条件加载。&lt;/li>
&lt;li>数据流控制。&lt;/li>
&lt;li>数据对齐。&lt;/li>
&lt;/ul>
&lt;p>SIMD 的优势和劣势：&lt;/p>
&lt;p>&lt;strong>优势&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>与线性代码相比，潜在的性能提升 300%到 600%。&lt;/li>
&lt;li>与在 GPU 级别进行向量化编程的 CUDA 相似。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>劣势&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>性能取决于运行硬件。&lt;/li>
&lt;li>当存在大量数据加载和卸载时，性能不佳。&lt;/li>
&lt;li>数据流会变得很难控制，而且向量内每个值的执行时间都会影响整个向量的执行时间。在所有值都满足退出条件之前，不能提前退出。&lt;/li>
&lt;li>编码复杂。&lt;/li>
&lt;li>缺乏内置函数： 三角函数、随机数、整数除法等。&lt;/li>
&lt;/ul>
&lt;p>总的来说，SIMD 的优势远大于劣势，SIMD 的使用可以大大提高程序的性能，但是需要注意的是，SIMD 的使用需要编码复杂，而且需要硬件支持，所以在使用 SIMD 之前，需要对程序进行分析，判断是否有必要使用 SIMD。&lt;/p>
&lt;h2 id="参考资料">参考资料&lt;/h2>
&lt;p>[1] &lt;a class="link" href="https://www.codeproject.com/Articles/874396/Introduction-to-SIMD-instructions" target="_blank" rel="noopener" >Introduction to SIMD instructions
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;p>[2] &lt;a class="link" href="https://www.agner.org/optimize/instruction_tables.pdf" target="_blank" rel="noopener" >Agner Fog&amp;rsquo;s instruction tables
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;p>[3] &lt;a class="link" href="https://software.intel.com/sites/default/files/m/4/8/8/2/a/31848-CompilerAutovectorizationGuide.pdf" target="_blank" rel="noopener" >Intel C++编译器的矢量化
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;p>[4] &lt;a class="link" href="https://software.intel.com/sites/landingpage/IntrinsicsGuide" target="_blank" rel="noopener" >Intel Intrinsics Guide
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;p>[5] &lt;a class="link" href="https://db.in.tum.de/~finis/x86-intrin-cheatsheet-v2.2.pdf?lang=en" target="_blank" rel="noopener" >x86 内置函数 Cheet Sheet
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;p>[6] &lt;a class="link" href="https://libdivide.com/" target="_blank" rel="noopener" >libdivide
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;p>[7] &lt;a class="link" href="https://software.intel.com/en-us/articles/avoiding-avx-sse-transition-penalties" target="_blank" rel="noopener" >Avoiding AVX-SSE Transition Penalties
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;p>[8] &lt;a class="link" href="https://stackoverflow.com/questions/41303780/why-is-this-sse-code-6-times-slower-without-vzeroupper-on-skylake" target="_blank" rel="noopener" >Why is this SSE code 6 times slower without VZEROUPPER on Skylake?
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;p>[9] &lt;a class="link" href="https://www.intel.com/content/www/us/en/developer/articles/technical/data-alignment-to-assist-vectorization.html" target="_blank" rel="noopener" >Intel Data Alignment Guide
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p></description></item><item><title>MPI 与并行计算（五）：MPI 扩展</title><link>https://cuterwrite.top/p/mpi-tutorial/5/</link><pubDate>Thu, 20 Jul 2023 03:00:00 +0000</pubDate><guid>https://cuterwrite.top/p/mpi-tutorial/5/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/9a5806864623b04c918b9d8bee35c49fc2790c52.jpg@1256w_828h_!web-article-pic.avif" alt="Featured image of post MPI 与并行计算（五）：MPI 扩展" />&lt;h1 id="mpi-与并行计算五mpi-扩展">MPI 与并行计算（五）：MPI 扩展&lt;/h1>
&lt;h2 id="1-动态进程">1. 动态进程&lt;/h2>
&lt;p>MPI-1 假定所有的进程都是静态的，运行时不能增加和删除进程。MPI-2 引入了动态进程的概念：&lt;/p>
&lt;ul>
&lt;li>MPI-1 不定义如何创建进程和如何建立通信。MPI-2 中的动态进程机制以可移植的方式(平台独立)提供了这种能力。&lt;/li>
&lt;li>动态进程有利于将 PVM 程序移植到 MPI 上。并且还可能支持一些重要的应用类型， 如 Client/Server 和 Process farm。&lt;/li>
&lt;li>动态进程允许更有效地使用资源和负载平衡。例如，所用节点数可以按需要减少和增加。&lt;/li>
&lt;li>支持容错。当一个节点失效时，可以在另一个节点上创建一个新进程运行该节点上的进程的工作。&lt;/li>
&lt;/ul>
&lt;p>在 MPI-1 中 一个 MPI 程序一旦启动，一直到该 MPI 程序结束，进程的个数是固定的，在程序运行过程中是不可能动态改变的。在 MPI-2 中，允许在程序运行过程中动态改变进程的数目，并提供了动态进程创建和管理的各种调用。&lt;/p>
&lt;p>&lt;strong>组间通信域&lt;/strong>在动态进程管理中处于核心的地位，只有掌握了它的基本概念，才能准确把握和使用进程的动态特性和动态进程之间的通信。&lt;/p>
&lt;p>在 MPI-2 中，对点到点通信和组通信都给出了使用组间通信域时的确切含义。在语法上，不管是使用组内还是组间通信域，二者没有任何区别，但其语义是不同的。&lt;/p>
&lt;ol>
&lt;li>对于构成组间通信域的两个进程组，调用进程把自己所在的组看作是本地组，而把另一个组称为远地组，使用组间通信域的一个特点是本地组进程发送的数据被远地组进程接收而本地组接收的数据必然来自远地组。&lt;/li>
&lt;li>在使用组间通信域的点到点通信中，发送语句指定的目的进程是远地组中的进程编号，接收进程指出的源进程编号也是远地组的进程编号。&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>如图所示为组间通信域上的点到点通信
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230720171153.webp"
alt="20230720171153" width="90%" loading="lazy">
&lt;/figure>
&lt;/li>
&lt;/ul>
&lt;p>对于组通信，如果使用组间通信域，则其含义分不同的形式而有所不同：对于多对多通信，本地进程组的所有进程向远地进程组的所有进程发送数据，同时本地进程组的所有进程从远地进程组的所有进程接收数据，如图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230720171308.webp"
alt="20230720171308" width="90%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>此外，组间通信域上的一对多通信或多对一通信如图所示：
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230720171356.webp"
alt="20230720171356" width="90%" loading="lazy">
&lt;/figure>
&lt;/li>
&lt;/ul>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 1：动态进程的创建和通信&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">// dynamic.c
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;mpi.h&amp;gt;
int main(int argc, char *argv[])
{
int rank, size, color, new_rank, new_size;
MPI_Comm new_comm;
MPI_Init(&amp;amp;argc, &amp;amp;argv);
MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;rank);
MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;size);
color = rank / 2; // 0, 0, 1, 1, 2, 2, 3, 3
MPI_Comm_split(MPI_COMM_WORLD, color, rank, &amp;amp;new_comm);
MPI_Comm_rank(new_comm, &amp;amp;new_rank);
MPI_Comm_size(new_comm, &amp;amp;new_size);
printf(&amp;quot;rank = %d, size = %d, new_rank = %d, new_size = %d\n&amp;quot;, rank, size, new_rank, new_size);
MPI_Finalize();
return 0;
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>在 16 个进程中，每两个进程一组，共 8 组，每组的进程编号相同，运行结果如下：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-text">root@ubuntu:~# mpicc dynamic.c -o dynamic
root@ubuntu:~# mpirun -n 16 ./dynamic
rank = 0, size = 16, new_rank = 0, new_size = 2
rank = 1, size = 16, new_rank = 1, new_size = 2
rank = 2, size = 16, new_rank = 0, new_size = 2
rank = 3, size = 16, new_rank = 1, new_size = 2
rank = 4, size = 16, new_rank = 0, new_size = 2
rank = 5, size = 16, new_rank = 1, new_size = 2
rank = 6, size = 16, new_rank = 0, new_size = 2
rank = 7, size = 16, new_rank = 1, new_size = 2
rank = 8, size = 16, new_rank = 0, new_size = 2
rank = 9, size = 16, new_rank = 1, new_size = 2
rank = 10, size = 16, new_rank = 0, new_size = 2
rank = 11, size = 16, new_rank = 1, new_size = 2
rank = 12, size = 16, new_rank = 0, new_size = 2
rank = 13, size = 16, new_rank = 1, new_size = 2
rank = 14, size = 16, new_rank = 0, new_size = 2
rank = 15, size = 16, new_rank = 1, new_size = 2
&lt;/code>&lt;/pre>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 2：更复杂的动态进程的创建和通信&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-cpp">#include &amp;lt;mpi.h&amp;gt;
#include &amp;lt;cmath&amp;gt;
#include &amp;lt;fstream&amp;gt;
#include &amp;lt;iostream&amp;gt;
int world_rank, world_size;
MPI_Comm custom_comm1, custom_comm2, custom_comm3, tmp;
void splitting()
{
int color;
MPI_Comm *new_comm;
// 1- First splitting here.
// With only one call to MPI_Comm_split you should be able to split processes 0-3 in
// custom_comm1 and processes 4-6 in custom_comm2
color = MPI_UNDEFINED;
new_comm = &amp;amp;tmp;
if (world_rank &amp;gt;= 0 &amp;amp;&amp;amp; world_rank &amp;lt;= 3)
{
color = 0;
new_comm = &amp;amp;custom_comm1;
}
if (world_rank &amp;gt;= 4 &amp;amp;&amp;amp; world_rank &amp;lt;= 6)
{
color = 1;
new_comm = &amp;amp;custom_comm2;
}
MPI_Comm_split(MPI_COMM_WORLD, color, world_rank, new_comm);
// 2- Second splitting here
// Now put processes 0 and 4 in custom_comm3
color = MPI_UNDEFINED;
new_comm = &amp;amp;tmp;
if (world_rank == 0 || world_rank == 4)
{
color = 2;
new_comm = &amp;amp;custom_comm3;
}
MPI_Comm_split(MPI_COMM_WORLD, color, world_rank, new_comm);
}
int main(int argc, char **argv)
{
MPI_Init(&amp;amp;argc, &amp;amp;argv);
MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;world_rank);
MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;world_size);
splitting();
if (world_rank &amp;gt;= 0 &amp;amp;&amp;amp; world_rank &amp;lt;= 3)
{
int row_rank;
int row_size;
MPI_Comm_rank(custom_comm1, &amp;amp;row_rank);
MPI_Comm_size(custom_comm1, &amp;amp;row_size);
std::cout &amp;lt;&amp;lt; &amp;quot;custom_comm1: &amp;quot; &amp;lt;&amp;lt; row_rank &amp;lt;&amp;lt; &amp;quot;/&amp;quot; &amp;lt;&amp;lt; row_size &amp;lt;&amp;lt; std::endl;
}
if (world_rank &amp;gt;= 4 &amp;amp;&amp;amp; world_rank &amp;lt;= 6)
{
int row_rank;
int row_size;
MPI_Comm_rank(custom_comm2, &amp;amp;row_rank);
MPI_Comm_size(custom_comm2, &amp;amp;row_size);
std::cout &amp;lt;&amp;lt; &amp;quot;custom_comm2: &amp;quot; &amp;lt;&amp;lt; row_rank &amp;lt;&amp;lt; &amp;quot;/&amp;quot; &amp;lt;&amp;lt; row_size &amp;lt;&amp;lt; std::endl;
}
if (world_rank == 0 || world_rank == 4)
{
int row_rank;
int row_size;
MPI_Comm_rank(custom_comm3, &amp;amp;row_rank);
MPI_Comm_size(custom_comm3, &amp;amp;row_size);
std::cout &amp;lt;&amp;lt; &amp;quot;custom_comm3: &amp;quot; &amp;lt;&amp;lt; row_rank &amp;lt;&amp;lt; &amp;quot;/&amp;quot; &amp;lt;&amp;lt; row_size &amp;lt;&amp;lt; std::endl;
}
MPI_Finalize();
return 0;
}
&lt;/code>&lt;/pre>
&lt;h2 id="2-远程存储访问remote-memory-accessrma">2. 远程存储访问（Remote Memory Access，RMA）&lt;/h2>
&lt;ul>
&lt;li>在 MPI-2 中增加远程存储访问的能力，主要是为了使 MPI 在编写特定算法和通信模型的并行程序时更加自然和简洁。因为在许多情况下，都需要一个进程对另外一个进程的存储区域进行直接访问。&lt;/li>
&lt;li>MPI-2 对远程存储的访问主要是通过&lt;strong>窗口&lt;/strong>来进行的，为了进行远程存储访问，首先需要定义一个窗口，该窗口开在各个进程的一段本地进程存储空间，其目的是为了让其它的进程可以通过这一窗口来访问本地的数据。&lt;/li>
&lt;li>定义好窗口之后，就可以通过窗口来访问远程存储区域的数据了。MPI-2 提供了三种基本的访问形式，即读、写和累计，读操作只是从远端的窗口获取数据，并不对远端数据进行任何修改；写操作将本地的内容写入远端的窗口，它修改远端窗口的内容；累计操作就更复杂一些，它将远端窗口的数据和本地的数据进行某种指定方式的运算之后，再将运算的结果写入远端窗口。&lt;/li>
&lt;li>MPI-2 就是通过读、写和累计三种操作来实现对远程存储的访问和更新的。除了基本的窗口操作之外 MPI-2 还提供了窗口管理功能 用来实现对窗口操作的同步管理。MPI-2 对窗口的同步管理有三种方式 ：
&lt;ul>
&lt;li>栅栏方式 fence：在这种方式下，对窗口的操作必须放在一对栅栏语句之间，这样可以保证当栅栏语句结束之后，其内部的窗口操作可以正确完成。&lt;/li>
&lt;li>握手方式：在这种方式下，调用窗口操作的进程需要将具体的窗口调用操作放在以 MPI_WIN_START 开始，以 MPI_WIN_COMPLETE 结束的调用之间。相应的,被访问的远端进程需要以一对调用 MPI_WIN_POST 和 MPI_WIN_WAIT 与之相适应。MPI_WIN_POST 允许其它的进程对自己的窗口进行访问，而 MPI_WIN_WAIT 调用结束之后可以保证对本窗口的调用操作全部完成。MPI_WIN_START 申请对远端进程窗口的访问，&lt;strong>只有当远端窗口执行了 MPI_WIN_POST 操作之后才可以访问远端窗口&lt;/strong>，MPI_WIN_COMPLETE 完成对远端窗口访问操作。&lt;/li>
&lt;li>锁方式：在这种方式下，不同的进程通过对特定的窗口加锁来实现互斥访问。当然用户根据需要可以使用共享的锁，这是就可以允许使用共享锁的进程对同一窗口同时访问。远端存储的访问窗口是具体的实现形式，通过窗口操作实现来实现单边通信，通过对窗口的管理操作来实现对窗口操作的同步控制。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">窗口操作&lt;/th>
&lt;th style="text-align:left">说明&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_create&lt;/td>
&lt;td style="text-align:left">创建窗口&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_free&lt;/td>
&lt;td style="text-align:left">释放窗口&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_fence&lt;/td>
&lt;td style="text-align:left">栅栏同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_start&lt;/td>
&lt;td style="text-align:left">握手同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_complete&lt;/td>
&lt;td style="text-align:left">握手同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_post&lt;/td>
&lt;td style="text-align:left">握手同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_wait&lt;/td>
&lt;td style="text-align:left">握手同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_lock&lt;/td>
&lt;td style="text-align:left">锁同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_unlock&lt;/td>
&lt;td style="text-align:left">锁同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_test&lt;/td>
&lt;td style="text-align:left">锁同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_lock_all&lt;/td>
&lt;td style="text-align:left">锁同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_unlock_all&lt;/td>
&lt;td style="text-align:left">锁同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_flush&lt;/td>
&lt;td style="text-align:left">锁同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_flush_all&lt;/td>
&lt;td style="text-align:left">锁同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_flush_local&lt;/td>
&lt;td style="text-align:left">锁同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_flush_local_all&lt;/td>
&lt;td style="text-align:left">锁同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_shared_query&lt;/td>
&lt;td style="text-align:left">查询窗口&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>小结：窗口是远程存储访问中的重要概念，其实 MPI-2 的远程存储访问就是各进程将自己的一部分内存区域开辟成其它所有进程都可以访问的窗口，从而使其它的进程实现对自己数据的远程访问，窗口操作是相对简单的，&lt;strong>对窗口访问的同步控制&lt;/strong>是需要注意的问题。&lt;/li>
&lt;/ul>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 3：远程存储访问&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">// rma.c
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;mpi.h&amp;gt;
int main(int argc, char *argv[])
{
int rank, size, i, j, *buf, *winbuf;
MPI_Win win;
MPI_Init(&amp;amp;argc, &amp;amp;argv);
MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;rank);
MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;size);
buf = (int *)malloc(size * sizeof(int));
MPI_Win_create(buf, size * sizeof(int), sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &amp;amp;win);
for (i = 0; i &amp;lt; size; i++)
buf[i] = 0;
MPI_Win_fence(0, win);
if (rank == 0)
{
for (i = 0; i &amp;lt; size; i++)
buf[i] = i;
}
MPI_Win_fence(0, win);
if (rank == 1)
{
for (i = 0; i &amp;lt; size; i++)
printf(&amp;quot;buf[%d] = %d\n&amp;quot;, i, buf[i]);
}
MPI_Win_free(&amp;amp;win);
MPI_Finalize();
return 0;
}
&lt;/code>&lt;/pre>
&lt;h2 id="3-并行-iompi-io">3. 并行 I/O（MPI-IO）&lt;/h2>
&lt;p>MPI-1 没有对并行文件 I/O 给出任何定义，原因在于并行 I/O 过于复杂，很难找到一个统一的标准。但是，I/O 是很多应用不可缺少的部分，MPI-2 在大量实践的基础上，提出了一个并行 I/O 的标准接口。MPI-2 提供的关于并行文件 I/O 的调用十分丰富，根据读写定位方法的不同，可以分为三种：&lt;/p>
&lt;ol>
&lt;li>指定显示的偏移：这种调用没有文件指针的概念 每次读写操作都必须明确指定读写文件的位置。&lt;/li>
&lt;li>各进程拥有独立的文件指针：这种方式的文件操作不需要指定读写的位置每一个进程都有一个相互独立的文件指针，读写的起始位置就是当前指针的位置。读写完成后文件指针自动移到下一个有效数据的位置。这种方式的文件操作需要每一个进程都定义各自在文件中的&lt;strong>文件视图（view）&lt;/strong>，文件视图（view）数据是文件连续或不连续的一部分，各个进程对文件视图（view）的操作就如同是对一个打开的独立的连续文件的操作一样。&lt;/li>
&lt;li>共享文件指针：在这种情况下，每一个进程对文件的操作都是从当前共享文件指针的位置开始，操作结束后共享文件指针自动转移到下一个位置。共享指针位置的变化对所有进程都是可见的，各进程使用的是同一个文件指针。任何一个进程对文件的读写操作都会引起其它所有进程文件指针的改变。&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230720201633.webp"
alt="20230720201633" width="90%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>MPI-IO 文件访问过程
&lt;ul>
&lt;li>在进行 I/O 之前，必须要通过调用 MPI_File_open 打开文件&lt;/li>
&lt;li>每个进程都需要定义文件指针用来控制文件访问&lt;/li>
&lt;li>I/O 操作完成后，必须通过调用 MPI_File_close 来关闭文件&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>并行文件的基本操作
&lt;ul>
&lt;li>打开：&lt;code>MPI_File_open(comm, filename, amode, info, fh)&lt;/code>
&lt;ul>
&lt;li>comm：组内通信域&lt;/li>
&lt;li>filename：文件名&lt;/li>
&lt;li>amode：打开模式&lt;/li>
&lt;li>info：传递给运行时的信息&lt;/li>
&lt;li>fh：返回的文件句柄
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">文件访问模式&lt;/th>
&lt;th style="text-align:left">含义&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">MPI_MODE_RDONLY&lt;/td>
&lt;td style="text-align:left">只读&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_MODE_RDWR&lt;/td>
&lt;td style="text-align:left">读写&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_MODE_WRONLY&lt;/td>
&lt;td style="text-align:left">只写&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_MODE_CREATE&lt;/td>
&lt;td style="text-align:left">若文件不存在则创建&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_MODE_EXCL&lt;/td>
&lt;td style="text-align:left">创建不存在的新文件，若文件已存在则报错&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_MODE_DELETE_ON_CLOSE&lt;/td>
&lt;td style="text-align:left">关闭文件时删除文件&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_MODE_UNIQUE_OPEN&lt;/td>
&lt;td style="text-align:left">文件只能被一个进程打开&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_MODE_SEQUENTIAL&lt;/td>
&lt;td style="text-align:left">文件只能被顺序访问&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_MODE_APPEND&lt;/td>
&lt;td style="text-align:left">追加方式打开，初始文件指针指向文件末尾&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>关闭：&lt;code>MPI_File_close(fh)&lt;/code>
&lt;ul>
&lt;li>fh：文件句柄&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>删除：&lt;code>MPI_File_delete(filename, info)&lt;/code>
&lt;ul>
&lt;li>filename：文件名&lt;/li>
&lt;li>info：传递给运行时的信息&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>修改文件大小：&lt;code>MPI_File_set_size(fh, size)&lt;/code>
&lt;ul>
&lt;li>fh：文件句柄&lt;/li>
&lt;li>size：新的文件大小(字节)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>查看文件大小：&lt;code>MPI_File_get_size(fh, size)&lt;/code>
&lt;ul>
&lt;li>fh：文件句柄&lt;/li>
&lt;li>size：文件大小(字节)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>预申请空间：&lt;code>MPI_File_preallocate(fh, size)&lt;/code>
&lt;ul>
&lt;li>fh：文件句柄&lt;/li>
&lt;li>size：预申请的空间大小(字节)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 4：并行 I/O - 指定显示偏移并行读&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">#include &amp;lt;mpi.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;sys/stat.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
int main(int argc, char **argv)
{
int rank, size;
MPI_File fh;
MPI_Status status;
MPI_Init(&amp;amp;argc, &amp;amp;argv);
MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;rank);
MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;size);
char *filename = &amp;quot;testfile&amp;quot;;
struct stat st;
stat(filename, &amp;amp;st);
int filesize = st.st_size;
int bufsize = filesize / size;
MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &amp;amp;fh);
MPI_Offset offset = rank * bufsize;
if (rank == size - 1)
{
bufsize += filesize % size;
}
char* buf = (char*)malloc(bufsize * sizeof(char));
printf(&amp;quot;Buf size: %d\n&amp;quot;, bufsize);
MPI_File_read_at(fh, offset, buf, bufsize, MPI_CHAR, &amp;amp;status);
printf(&amp;quot;Process %d read: %s\n&amp;quot;, rank, buf);
MPI_File_close(&amp;amp;fh);
MPI_Finalize();
free(buf);
return 0;
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>tesfile 文件内容为：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-text">ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz01234567891
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>运行结果为：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-text">root@ubuntu:~# mpicc read.c -o read
root@ubuntu:~# mpirun -n 2 ./read
Buf size: 125
Process 1 read: ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz01234567891
Buf size: 124
Process 0 read: ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789
&lt;/code>&lt;/pre>
&lt;h2 id="4-正确地使用-mpi-io">4. 正确地使用 MPI-IO&lt;/h2>
&lt;ul>
&lt;li>正确使用 MPI-IO
&lt;ul>
&lt;li>根据 I/O 需求，每个应用都有其特定的 I/O 访问模式&lt;/li>
&lt;li>对于不同的 I/O 系统，同样的 I/O 访问模式也可以使用不同的 I/O 函数和 I/O 方式实现&lt;/li>
&lt;li>通常 MPI-IO 中 I/O 访问模式的实现方式可分为 4 级：&lt;strong>level0-level3&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>以分布式数组访问为例&lt;/li>
&lt;li>level0：每个进程对本地数组的一行发出一个独立的读请求（就像在 unix 中一样）
&lt;pre>&lt;code class="language-c">MPI_File_open(..., file, ..., &amp;amp;fh);
for (i = 0; i &amp;lt; n_local_rows; i++)
{
MPI_File_seek(fh, ...);
MPI_File_read(fh, &amp;amp;(A[i][0]), ...);
}
MPI_File_close(&amp;amp;fh);
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>level1：类似于 level 0，但每个过程都使用集合 I/O 函数
&lt;pre>&lt;code class="language-c">MPI_File_open(MPI_COMM_WORLD, file, ...,&amp;amp;fh);
for (i = 0; i &amp;lt; n_local_rows; i++)
{
MPI_File_seek(fh, ...);
MPI_File_read_all(fh, &amp;amp;(A[i][0]), ...);
}
MPI_File_close(&amp;amp;fh);
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>level2：每个进程创建一个派生数据类型来描述非连续访问模式，定义一个文件视图，并调用独立的 I/O 函数
&lt;pre>&lt;code class="language-c">MPI_Type_create_subarray(...,&amp;amp;subarray, ...);
MPI_Type_commit(&amp;amp;subarray);
MPI_File_open(..., file, ..., &amp;amp;fh);
MPI_File_set_view(fh, ..., subarray, ...);
MPI_File_read(fh, A, ...);
MPI_File_close(&amp;amp;fh);
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>level3：类似于 level 0，但每个过程都使用集合 I/O 函数
&lt;pre>&lt;code class="language-c">MPI_Type_create_subarray(...,&amp;amp;subarray, ...);
MPI_Type_commit(&amp;amp;subarray);
MPI_File_open(MPI_COMM_WORLD, file,...,&amp;amp;fh);
MPI_File_set_view(fh, ..., subarray, ...);
MPI_File_read_all(fh, A, ...);
MPI_File_close(&amp;amp;fh);
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul>
&lt;h2 id="5-总结">5. 总结&lt;/h2>
&lt;ul>
&lt;li>MPI-IO 有许多功能，可以帮助用户获得高性能 I/O
&lt;ul>
&lt;li>支持非连续性访问
&lt;ul>
&lt;li>派生数据类型&lt;/li>
&lt;li>文件视图&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>集合 I/O&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>用户应该根据应用程序 I/O 特性来选择适合的 I/O 访问模式实现&lt;/li>
&lt;li>同时，MPI-IO 不是实现并行 I/O 的唯一选择。目前已有一些更高级的库可代替 MPI-IO
&lt;ul>
&lt;li>HDF5、netCDF&amp;hellip;&amp;hellip;&lt;/li>
&lt;li>这些库都是基于 MPI-IO 实现&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item></channel></rss>