<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>MPI on cuterwrite</title><link>https://cuterwrite.top/tags/mpi/</link><description>Recent content in MPI on cuterwrite</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>cuterwrite</copyright><lastBuildDate>Thu, 01 Feb 2024 01:01:01 +0000</lastBuildDate><atom:link href="https://cuterwrite.top/tags/mpi/index.xml" rel="self" type="application/rss+xml"/><item><title>编译安装 UCX 1.15.0 与 OpenMPI 5.0.0：详尽指南</title><link>https://cuterwrite.top/p/openmpi-with-ucx/</link><pubDate>Thu, 01 Feb 2024 01:01:01 +0000</pubDate><guid>https://cuterwrite.top/p/openmpi-with-ucx/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/cropped_2024010204-2024-02-03.webp" alt="Featured image of post 编译安装 UCX 1.15.0 与 OpenMPI 5.0.0：详尽指南" />&lt;h1 id="编译安装-ucx-1150-与-openmpi-500详尽指南">编译安装 UCX 1.15.0 与 OpenMPI 5.0.0：详尽指南&lt;/h1>
&lt;h2 id="一环境准备">一、环境准备&lt;/h2>
&lt;p>首先，请确保您的系统满足以下基本要求：&lt;/p>
&lt;ol>
&lt;li>操作系统：支持 Linux（如 Ubuntu 20.04 LTS）或其他类 Unix 操作系统。&lt;/li>
&lt;li>开发工具包：安装必要的构建工具和库，例如 &lt;code>build-essential&lt;/code> ，&lt;code>libnuma-dev&lt;/code> ，&lt;code>pkg-config&lt;/code> 等。&lt;/li>
&lt;li>内核版本：对于最佳性能，建议使用最新稳定版内核。&lt;/li>
&lt;li>需要支持 RDMA 的硬件环境或虚拟环境。&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">sudo apt-get update
sudo apt-get install -y build-essential libnuma-dev pkg-config
&lt;/code>&lt;/pre>
&lt;h2 id="二编译安装-ucx-1150">二、编译安装 UCX 1.15.0&lt;/h2>
&lt;ol>
&lt;li>下载 UCX 源码包：&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">wget https://github.com/openucx/ucx/releases/download/v1.15.0/ucx-1.15.0.tar.gz
tar -xzvf ucx-1.15.0.tar.gz
cd ucx-1.15.0
&lt;/code>&lt;/pre>
&lt;ol start="2">
&lt;li>配置 UCX 编译选项：&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">mkdir build &amp;amp;&amp;amp; cd build
../configure --prefix=/root/software/ucx/1.5.0
&lt;/code>&lt;/pre>
&lt;p>您可以根据实际需求添加更多配置选项，比如指定特定的网卡类型或者启用特定的功能。&lt;/p>
&lt;ol start="3">
&lt;li>编译并安装：&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">make -j 8
make install
&lt;/code>&lt;/pre>
&lt;ol start="4">
&lt;li>UCX 架构说明&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>UCX 1.15.0 的架构如下图所示：&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/Architecture-2024-02-03.webp"
alt="Architecture-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>组件&lt;/th>
&lt;th>角色&lt;/th>
&lt;th>说明&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>UCP&lt;/td>
&lt;td>Protocol&lt;/td>
&lt;td>实现高级抽象，如标记匹配、流、连接协商和建立、多轨以及处理不同的内存类型。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UCT&lt;/td>
&lt;td>Transport&lt;/td>
&lt;td>实现低级通信原语，如活动消息、远程内存访问和原子操作。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UCM&lt;/td>
&lt;td>Memory&lt;/td>
&lt;td>通用的数据结构、算法和系统实用程序的集合。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UCP&lt;/td>
&lt;td>Protocol&lt;/td>
&lt;td>截获内存注册缓存使用的内存分配和释放事件。&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="三编译安装-openmpi-500">三、编译安装 OpenMPI 5.0.0&lt;/h2>
&lt;ol>
&lt;li>下载 OpenMPI 源码包：&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">wget https://download.open-mpi.org/release/open-mpi/v5.0/openmpi-5.0.0.tar.gz
tar -xzvf openmpi-5.0.0.tar.gz
cd openmpi-5.0.0
&lt;/code>&lt;/pre>
&lt;ol start="2">
&lt;li>配置 OpenMPI 编译选项，指定使用 UCX 作为传输层：&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">mkdir build &amp;amp;&amp;amp; cd build
../configure --without-hcoll \
--enable-python-bindings \
--enable-mpirun-prefix-by-default \
--prefix=/root/software/openmpi/5.0.0-ucx-1.15.0 \
--with-ucx=/root/software/ucx/1.15.0 \
--enable-mca-no-build=btl-uct
&lt;/code>&lt;/pre>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;ul>
&lt;li>对于 OpenMPI 4.0 及更高版本，&lt;code>btl_uct&lt;/code> 组件可能存在编译错误。该组件对于使用 UCX 来说并不重要；因此可以通过 &lt;code>--enable-mca-no-build=btl-uct&lt;/code> 禁用：&lt;/li>
&lt;li>&lt;code>--enable-python-bindings&lt;/code> 选项用于启用 Python 绑定。&lt;/li>
&lt;li>&lt;code>--enable-mpirun-prefix-by-default&lt;/code> 选项用于在使用 &lt;code>mpirun&lt;/code> 启动 MPI 程序时自动添加 &lt;code>--prefix&lt;/code> 选项。&lt;/li>
&lt;li>&lt;code>--without-hcoll&lt;/code> 选项用于禁用 HCOLL 组件。不设置编译时会报错 &lt;code>cannot find -lnuma&lt;/code> 与 &lt;code>cannot find -ludev&lt;/code> 的错误。&lt;/li>
&lt;/ul>&lt;/div>
&lt;p>最后配置选项如下：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/ompi-config-2024-02-03.webp"
alt="ompi-config-2024-02-03" width="auto" loading="lazy"/>
&lt;/figure>
&lt;ol start="3">
&lt;li>编译并安装：&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">make -j 8
make install
&lt;/code>&lt;/pre>
&lt;h2 id="四验证安装与设置环境变量">四、验证安装与设置环境变量&lt;/h2>
&lt;p>安装完成后，可以通过运行简单的 MPI 程序来验证 UCX 和 OpenMPI 是否成功集成：&lt;/p>
&lt;pre>&lt;code class="language-bash">mpirun -np 2 --mca pml ucx --mca btl ^vader,tcp,openib,uct [-x UCX_NET_DEVICES=mlx5_0:1] hostname
&lt;/code>&lt;/pre>
&lt;p>（如果在 root 上运行则需要加上 &lt;code>--allow-run-as-root&lt;/code> 选项，如果有 RDMA 设备可以设置 &lt;code>-x UCX_NET_DEVICES&lt;/code> ）&lt;/p>
&lt;p>如果一切正常，您会看到两台主机名的输出。为了方便使用，可以将 OpenMPI 的 bin 目录等添加到系统 PATH 环境变量中：&lt;/p>
&lt;pre>&lt;code class="language-bash">vim ~/.bashrc
export PATH=/root/software/openmpi/5.0.0-ucx-1.15.0/bin:$PATH
export LD_LIBRARY_PATH=/root/software/openmpi/5.0.0-ucx-1.15.0/lib:$LD_LIBRARY_PATH
export CPATH=/root/software/openmpi/5.0.0-ucx-1.15.0/include:$CPATH
export MANPATH=/root/software/openmpi/5.0.0-ucx-1.15.0/share/man:$MANPATH
source ~/.bashrc
&lt;/code>&lt;/pre>
&lt;h2 id="五ucx-性能测试">五、UCX 性能测试&lt;/h2>
&lt;p>发送方：&lt;/p>
&lt;pre>&lt;code class="language-bash">ucx_perftest -c 0 -d mlx5_0:1
&lt;/code>&lt;/pre>
&lt;p>接收方：&lt;/p>
&lt;pre>&lt;code class="language-bash">ucx_perftest -c 1 -d mlx5_0:1 &amp;lt;server_hostname&amp;gt; -t tag_lat
&lt;/code>&lt;/pre>
&lt;p>总之，通过以上步骤，我们已经成功地从源代码编译并安装了 UCX 1.15.0 和 OpenMPI 5.0.0，并将其整合为一个高效稳定的高性能计算环境。在实际应用中，可以根据具体需求进一步优化配置以获得更优性能。&lt;/p></description></item><item><title>MPI 与并行计算（五）：MPI 扩展</title><link>https://cuterwrite.top/p/mpi-tutorial/5/</link><pubDate>Thu, 20 Jul 2023 03:00:00 +0000</pubDate><guid>https://cuterwrite.top/p/mpi-tutorial/5/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/9a5806864623b04c918b9d8bee35c49fc2790c52.jpg@1256w_828h_!web-article-pic.avif" alt="Featured image of post MPI 与并行计算（五）：MPI 扩展" />&lt;h1 id="mpi-与并行计算五mpi-扩展">MPI 与并行计算（五）：MPI 扩展&lt;/h1>
&lt;h2 id="1-动态进程">1. 动态进程&lt;/h2>
&lt;p>MPI-1 假定所有的进程都是静态的，运行时不能增加和删除进程。MPI-2 引入了动态进程的概念：&lt;/p>
&lt;ul>
&lt;li>MPI-1 不定义如何创建进程和如何建立通信。MPI-2 中的动态进程机制以可移植的方式(平台独立)提供了这种能力。&lt;/li>
&lt;li>动态进程有利于将 PVM 程序移植到 MPI 上。并且还可能支持一些重要的应用类型， 如 Client/Server 和 Process farm。&lt;/li>
&lt;li>动态进程允许更有效地使用资源和负载平衡。例如，所用节点数可以按需要减少和增加。&lt;/li>
&lt;li>支持容错。当一个节点失效时，可以在另一个节点上创建一个新进程运行该节点上的进程的工作。&lt;/li>
&lt;/ul>
&lt;p>在 MPI-1 中 一个 MPI 程序一旦启动，一直到该 MPI 程序结束，进程的个数是固定的，在程序运行过程中是不可能动态改变的。在 MPI-2 中，允许在程序运行过程中动态改变进程的数目，并提供了动态进程创建和管理的各种调用。&lt;/p>
&lt;p>&lt;strong>组间通信域&lt;/strong>在动态进程管理中处于核心的地位，只有掌握了它的基本概念，才能准确把握和使用进程的动态特性和动态进程之间的通信。&lt;/p>
&lt;p>在 MPI-2 中，对点到点通信和组通信都给出了使用组间通信域时的确切含义。在语法上，不管是使用组内还是组间通信域，二者没有任何区别，但其语义是不同的。&lt;/p>
&lt;ol>
&lt;li>对于构成组间通信域的两个进程组，调用进程把自己所在的组看作是本地组，而把另一个组称为远地组，使用组间通信域的一个特点是本地组进程发送的数据被远地组进程接收而本地组接收的数据必然来自远地组。&lt;/li>
&lt;li>在使用组间通信域的点到点通信中，发送语句指定的目的进程是远地组中的进程编号，接收进程指出的源进程编号也是远地组的进程编号。&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>如图所示为组间通信域上的点到点通信
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230720171153.webp"
alt="20230720171153" width="90%" loading="lazy"/>
&lt;/figure>
&lt;/li>
&lt;/ul>
&lt;p>对于组通信，如果使用组间通信域，则其含义分不同的形式而有所不同：对于多对多通信，本地进程组的所有进程向远地进程组的所有进程发送数据，同时本地进程组的所有进程从远地进程组的所有进程接收数据，如图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230720171308.webp"
alt="20230720171308" width="90%" loading="lazy"/>
&lt;/figure>
&lt;ul>
&lt;li>此外，组间通信域上的一对多通信或多对一通信如图所示：
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230720171356.webp"
alt="20230720171356" width="90%" loading="lazy"/>
&lt;/figure>
&lt;/li>
&lt;/ul>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 1：动态进程的创建和通信&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">// dynamic.c
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;mpi.h&amp;gt;
int main(int argc, char *argv[])
{
int rank, size, color, new_rank, new_size;
MPI_Comm new_comm;
MPI_Init(&amp;amp;argc, &amp;amp;argv);
MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;rank);
MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;size);
color = rank / 2; // 0, 0, 1, 1, 2, 2, 3, 3
MPI_Comm_split(MPI_COMM_WORLD, color, rank, &amp;amp;new_comm);
MPI_Comm_rank(new_comm, &amp;amp;new_rank);
MPI_Comm_size(new_comm, &amp;amp;new_size);
printf(&amp;quot;rank = %d, size = %d, new_rank = %d, new_size = %d\n&amp;quot;, rank, size, new_rank, new_size);
MPI_Finalize();
return 0;
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>在 16 个进程中，每两个进程一组，共 8 组，每组的进程编号相同，运行结果如下：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-text">root@ubuntu:~# mpicc dynamic.c -o dynamic
root@ubuntu:~# mpirun -n 16 ./dynamic
rank = 0, size = 16, new_rank = 0, new_size = 2
rank = 1, size = 16, new_rank = 1, new_size = 2
rank = 2, size = 16, new_rank = 0, new_size = 2
rank = 3, size = 16, new_rank = 1, new_size = 2
rank = 4, size = 16, new_rank = 0, new_size = 2
rank = 5, size = 16, new_rank = 1, new_size = 2
rank = 6, size = 16, new_rank = 0, new_size = 2
rank = 7, size = 16, new_rank = 1, new_size = 2
rank = 8, size = 16, new_rank = 0, new_size = 2
rank = 9, size = 16, new_rank = 1, new_size = 2
rank = 10, size = 16, new_rank = 0, new_size = 2
rank = 11, size = 16, new_rank = 1, new_size = 2
rank = 12, size = 16, new_rank = 0, new_size = 2
rank = 13, size = 16, new_rank = 1, new_size = 2
rank = 14, size = 16, new_rank = 0, new_size = 2
rank = 15, size = 16, new_rank = 1, new_size = 2
&lt;/code>&lt;/pre>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 2：更复杂的动态进程的创建和通信&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-cpp">#include &amp;lt;mpi.h&amp;gt;
#include &amp;lt;cmath&amp;gt;
#include &amp;lt;fstream&amp;gt;
#include &amp;lt;iostream&amp;gt;
int world_rank, world_size;
MPI_Comm custom_comm1, custom_comm2, custom_comm3, tmp;
void splitting()
{
int color;
MPI_Comm *new_comm;
// 1- First splitting here.
// With only one call to MPI_Comm_split you should be able to split processes 0-3 in
// custom_comm1 and processes 4-6 in custom_comm2
color = MPI_UNDEFINED;
new_comm = &amp;amp;tmp;
if (world_rank &amp;gt;= 0 &amp;amp;&amp;amp; world_rank &amp;lt;= 3)
{
color = 0;
new_comm = &amp;amp;custom_comm1;
}
if (world_rank &amp;gt;= 4 &amp;amp;&amp;amp; world_rank &amp;lt;= 6)
{
color = 1;
new_comm = &amp;amp;custom_comm2;
}
MPI_Comm_split(MPI_COMM_WORLD, color, world_rank, new_comm);
// 2- Second splitting here
// Now put processes 0 and 4 in custom_comm3
color = MPI_UNDEFINED;
new_comm = &amp;amp;tmp;
if (world_rank == 0 || world_rank == 4)
{
color = 2;
new_comm = &amp;amp;custom_comm3;
}
MPI_Comm_split(MPI_COMM_WORLD, color, world_rank, new_comm);
}
int main(int argc, char **argv)
{
MPI_Init(&amp;amp;argc, &amp;amp;argv);
MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;world_rank);
MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;world_size);
splitting();
if (world_rank &amp;gt;= 0 &amp;amp;&amp;amp; world_rank &amp;lt;= 3)
{
int row_rank;
int row_size;
MPI_Comm_rank(custom_comm1, &amp;amp;row_rank);
MPI_Comm_size(custom_comm1, &amp;amp;row_size);
std::cout &amp;lt;&amp;lt; &amp;quot;custom_comm1: &amp;quot; &amp;lt;&amp;lt; row_rank &amp;lt;&amp;lt; &amp;quot;/&amp;quot; &amp;lt;&amp;lt; row_size &amp;lt;&amp;lt; std::endl;
}
if (world_rank &amp;gt;= 4 &amp;amp;&amp;amp; world_rank &amp;lt;= 6)
{
int row_rank;
int row_size;
MPI_Comm_rank(custom_comm2, &amp;amp;row_rank);
MPI_Comm_size(custom_comm2, &amp;amp;row_size);
std::cout &amp;lt;&amp;lt; &amp;quot;custom_comm2: &amp;quot; &amp;lt;&amp;lt; row_rank &amp;lt;&amp;lt; &amp;quot;/&amp;quot; &amp;lt;&amp;lt; row_size &amp;lt;&amp;lt; std::endl;
}
if (world_rank == 0 || world_rank == 4)
{
int row_rank;
int row_size;
MPI_Comm_rank(custom_comm3, &amp;amp;row_rank);
MPI_Comm_size(custom_comm3, &amp;amp;row_size);
std::cout &amp;lt;&amp;lt; &amp;quot;custom_comm3: &amp;quot; &amp;lt;&amp;lt; row_rank &amp;lt;&amp;lt; &amp;quot;/&amp;quot; &amp;lt;&amp;lt; row_size &amp;lt;&amp;lt; std::endl;
}
MPI_Finalize();
return 0;
}
&lt;/code>&lt;/pre>
&lt;h2 id="2-远程存储访问remote-memory-accessrma">2. 远程存储访问（Remote Memory Access，RMA）&lt;/h2>
&lt;ul>
&lt;li>在 MPI-2 中增加远程存储访问的能力，主要是为了使 MPI 在编写特定算法和通信模型的并行程序时更加自然和简洁。因为在许多情况下，都需要一个进程对另外一个进程的存储区域进行直接访问。&lt;/li>
&lt;li>MPI-2 对远程存储的访问主要是通过&lt;strong>窗口&lt;/strong>来进行的，为了进行远程存储访问，首先需要定义一个窗口，该窗口开在各个进程的一段本地进程存储空间，其目的是为了让其它的进程可以通过这一窗口来访问本地的数据。&lt;/li>
&lt;li>定义好窗口之后，就可以通过窗口来访问远程存储区域的数据了。MPI-2 提供了三种基本的访问形式，即读、写和累计，读操作只是从远端的窗口获取数据，并不对远端数据进行任何修改；写操作将本地的内容写入远端的窗口，它修改远端窗口的内容；累计操作就更复杂一些，它将远端窗口的数据和本地的数据进行某种指定方式的运算之后，再将运算的结果写入远端窗口。&lt;/li>
&lt;li>MPI-2 就是通过读、写和累计三种操作来实现对远程存储的访问和更新的。除了基本的窗口操作之外 MPI-2 还提供了窗口管理功能 用来实现对窗口操作的同步管理。MPI-2 对窗口的同步管理有三种方式 ：
&lt;ul>
&lt;li>栅栏方式 fence：在这种方式下，对窗口的操作必须放在一对栅栏语句之间，这样可以保证当栅栏语句结束之后，其内部的窗口操作可以正确完成。&lt;/li>
&lt;li>握手方式：在这种方式下，调用窗口操作的进程需要将具体的窗口调用操作放在以 MPI_WIN_START 开始，以 MPI_WIN_COMPLETE 结束的调用之间。相应的,被访问的远端进程需要以一对调用 MPI_WIN_POST 和 MPI_WIN_WAIT 与之相适应。MPI_WIN_POST 允许其它的进程对自己的窗口进行访问，而 MPI_WIN_WAIT 调用结束之后可以保证对本窗口的调用操作全部完成。MPI_WIN_START 申请对远端进程窗口的访问，&lt;strong>只有当远端窗口执行了 MPI_WIN_POST 操作之后才可以访问远端窗口&lt;/strong>，MPI_WIN_COMPLETE 完成对远端窗口访问操作。&lt;/li>
&lt;li>锁方式：在这种方式下，不同的进程通过对特定的窗口加锁来实现互斥访问。当然用户根据需要可以使用共享的锁，这是就可以允许使用共享锁的进程对同一窗口同时访问。远端存储的访问窗口是具体的实现形式，通过窗口操作实现来实现单边通信，通过对窗口的管理操作来实现对窗口操作的同步控制。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">窗口操作&lt;/th>
&lt;th style="text-align:left">说明&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_create&lt;/td>
&lt;td style="text-align:left">创建窗口&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_free&lt;/td>
&lt;td style="text-align:left">释放窗口&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_fence&lt;/td>
&lt;td style="text-align:left">栅栏同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_start&lt;/td>
&lt;td style="text-align:left">握手同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_complete&lt;/td>
&lt;td style="text-align:left">握手同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_post&lt;/td>
&lt;td style="text-align:left">握手同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_wait&lt;/td>
&lt;td style="text-align:left">握手同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_lock&lt;/td>
&lt;td style="text-align:left">锁同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_unlock&lt;/td>
&lt;td style="text-align:left">锁同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_test&lt;/td>
&lt;td style="text-align:left">锁同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_lock_all&lt;/td>
&lt;td style="text-align:left">锁同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_unlock_all&lt;/td>
&lt;td style="text-align:left">锁同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_flush&lt;/td>
&lt;td style="text-align:left">锁同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_flush_all&lt;/td>
&lt;td style="text-align:left">锁同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_flush_local&lt;/td>
&lt;td style="text-align:left">锁同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_flush_local_all&lt;/td>
&lt;td style="text-align:left">锁同步&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Win_shared_query&lt;/td>
&lt;td style="text-align:left">查询窗口&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>小结：窗口是远程存储访问中的重要概念，其实 MPI-2 的远程存储访问就是各进程将自己的一部分内存区域开辟成其它所有进程都可以访问的窗口，从而使其它的进程实现对自己数据的远程访问，窗口操作是相对简单的，&lt;strong>对窗口访问的同步控制&lt;/strong>是需要注意的问题。&lt;/li>
&lt;/ul>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 3：远程存储访问&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">// rma.c
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;mpi.h&amp;gt;
int main(int argc, char *argv[])
{
int rank, size, i, j, *buf, *winbuf;
MPI_Win win;
MPI_Init(&amp;amp;argc, &amp;amp;argv);
MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;rank);
MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;size);
buf = (int *)malloc(size * sizeof(int));
MPI_Win_create(buf, size * sizeof(int), sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &amp;amp;win);
for (i = 0; i &amp;lt; size; i++)
buf[i] = 0;
MPI_Win_fence(0, win);
if (rank == 0)
{
for (i = 0; i &amp;lt; size; i++)
buf[i] = i;
}
MPI_Win_fence(0, win);
if (rank == 1)
{
for (i = 0; i &amp;lt; size; i++)
printf(&amp;quot;buf[%d] = %d\n&amp;quot;, i, buf[i]);
}
MPI_Win_free(&amp;amp;win);
MPI_Finalize();
return 0;
}
&lt;/code>&lt;/pre>
&lt;h2 id="3-并行-iompi-io">3. 并行 I/O（MPI-IO）&lt;/h2>
&lt;p>MPI-1 没有对并行文件 I/O 给出任何定义，原因在于并行 I/O 过于复杂，很难找到一个统一的标准。但是，I/O 是很多应用不可缺少的部分，MPI-2 在大量实践的基础上，提出了一个并行 I/O 的标准接口。MPI-2 提供的关于并行文件 I/O 的调用十分丰富，根据读写定位方法的不同，可以分为三种：&lt;/p>
&lt;ol>
&lt;li>指定显示的偏移：这种调用没有文件指针的概念 每次读写操作都必须明确指定读写文件的位置。&lt;/li>
&lt;li>各进程拥有独立的文件指针：这种方式的文件操作不需要指定读写的位置每一个进程都有一个相互独立的文件指针，读写的起始位置就是当前指针的位置。读写完成后文件指针自动移到下一个有效数据的位置。这种方式的文件操作需要每一个进程都定义各自在文件中的&lt;strong>文件视图（view）&lt;/strong>，文件视图（view）数据是文件连续或不连续的一部分，各个进程对文件视图（view）的操作就如同是对一个打开的独立的连续文件的操作一样。&lt;/li>
&lt;li>共享文件指针：在这种情况下，每一个进程对文件的操作都是从当前共享文件指针的位置开始，操作结束后共享文件指针自动转移到下一个位置。共享指针位置的变化对所有进程都是可见的，各进程使用的是同一个文件指针。任何一个进程对文件的读写操作都会引起其它所有进程文件指针的改变。&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230720201633.webp"
alt="20230720201633" width="90%" loading="lazy"/>
&lt;/figure>
&lt;ul>
&lt;li>MPI-IO 文件访问过程
&lt;ul>
&lt;li>在进行 I/O 之前，必须要通过调用 MPI_File_open 打开文件&lt;/li>
&lt;li>每个进程都需要定义文件指针用来控制文件访问&lt;/li>
&lt;li>I/O 操作完成后，必须通过调用 MPI_File_close 来关闭文件&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>并行文件的基本操作
&lt;ul>
&lt;li>打开：&lt;code>MPI_File_open(comm, filename, amode, info, fh)&lt;/code>
&lt;ul>
&lt;li>comm：组内通信域&lt;/li>
&lt;li>filename：文件名&lt;/li>
&lt;li>amode：打开模式&lt;/li>
&lt;li>info：传递给运行时的信息&lt;/li>
&lt;li>fh：返回的文件句柄
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">文件访问模式&lt;/th>
&lt;th style="text-align:left">含义&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">MPI_MODE_RDONLY&lt;/td>
&lt;td style="text-align:left">只读&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_MODE_RDWR&lt;/td>
&lt;td style="text-align:left">读写&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_MODE_WRONLY&lt;/td>
&lt;td style="text-align:left">只写&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_MODE_CREATE&lt;/td>
&lt;td style="text-align:left">若文件不存在则创建&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_MODE_EXCL&lt;/td>
&lt;td style="text-align:left">创建不存在的新文件，若文件已存在则报错&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_MODE_DELETE_ON_CLOSE&lt;/td>
&lt;td style="text-align:left">关闭文件时删除文件&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_MODE_UNIQUE_OPEN&lt;/td>
&lt;td style="text-align:left">文件只能被一个进程打开&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_MODE_SEQUENTIAL&lt;/td>
&lt;td style="text-align:left">文件只能被顺序访问&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_MODE_APPEND&lt;/td>
&lt;td style="text-align:left">追加方式打开，初始文件指针指向文件末尾&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>关闭：&lt;code>MPI_File_close(fh)&lt;/code>
&lt;ul>
&lt;li>fh：文件句柄&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>删除：&lt;code>MPI_File_delete(filename, info)&lt;/code>
&lt;ul>
&lt;li>filename：文件名&lt;/li>
&lt;li>info：传递给运行时的信息&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>修改文件大小：&lt;code>MPI_File_set_size(fh, size)&lt;/code>
&lt;ul>
&lt;li>fh：文件句柄&lt;/li>
&lt;li>size：新的文件大小(字节)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>查看文件大小：&lt;code>MPI_File_get_size(fh, size)&lt;/code>
&lt;ul>
&lt;li>fh：文件句柄&lt;/li>
&lt;li>size：文件大小(字节)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>预申请空间：&lt;code>MPI_File_preallocate(fh, size)&lt;/code>
&lt;ul>
&lt;li>fh：文件句柄&lt;/li>
&lt;li>size：预申请的空间大小(字节)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 4：并行 I/O - 指定显示偏移并行读&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">#include &amp;lt;mpi.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;sys/stat.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
int main(int argc, char **argv)
{
int rank, size;
MPI_File fh;
MPI_Status status;
MPI_Init(&amp;amp;argc, &amp;amp;argv);
MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;rank);
MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;size);
char *filename = &amp;quot;testfile&amp;quot;;
struct stat st;
stat(filename, &amp;amp;st);
int filesize = st.st_size;
int bufsize = filesize / size;
MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &amp;amp;fh);
MPI_Offset offset = rank * bufsize;
if (rank == size - 1)
{
bufsize += filesize % size;
}
char* buf = (char*)malloc(bufsize * sizeof(char));
printf(&amp;quot;Buf size: %d\n&amp;quot;, bufsize);
MPI_File_read_at(fh, offset, buf, bufsize, MPI_CHAR, &amp;amp;status);
printf(&amp;quot;Process %d read: %s\n&amp;quot;, rank, buf);
MPI_File_close(&amp;amp;fh);
MPI_Finalize();
free(buf);
return 0;
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>tesfile 文件内容为：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-text">ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz01234567891
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>运行结果为：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-text">root@ubuntu:~# mpicc read.c -o read
root@ubuntu:~# mpirun -n 2 ./read
Buf size: 125
Process 1 read: ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz01234567891
Buf size: 124
Process 0 read: ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789
&lt;/code>&lt;/pre>
&lt;h2 id="4-正确地使用-mpi-io">4. 正确地使用 MPI-IO&lt;/h2>
&lt;ul>
&lt;li>正确使用 MPI-IO
&lt;ul>
&lt;li>根据 I/O 需求，每个应用都有其特定的 I/O 访问模式&lt;/li>
&lt;li>对于不同的 I/O 系统，同样的 I/O 访问模式也可以使用不同的 I/O 函数和 I/O 方式实现&lt;/li>
&lt;li>通常 MPI-IO 中 I/O 访问模式的实现方式可分为 4 级：&lt;strong>level0-level3&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>以分布式数组访问为例&lt;/li>
&lt;li>level0：每个进程对本地数组的一行发出一个独立的读请求（就像在 unix 中一样）
&lt;pre>&lt;code class="language-c">MPI_File_open(..., file, ..., &amp;amp;fh);
for (i = 0; i &amp;lt; n_local_rows; i++)
{
MPI_File_seek(fh, ...);
MPI_File_read(fh, &amp;amp;(A[i][0]), ...);
}
MPI_File_close(&amp;amp;fh);
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>level1：类似于 level 0，但每个过程都使用集合 I/O 函数
&lt;pre>&lt;code class="language-c">MPI_File_open(MPI_COMM_WORLD, file, ...,&amp;amp;fh);
for (i = 0; i &amp;lt; n_local_rows; i++)
{
MPI_File_seek(fh, ...);
MPI_File_read_all(fh, &amp;amp;(A[i][0]), ...);
}
MPI_File_close(&amp;amp;fh);
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>level2：每个进程创建一个派生数据类型来描述非连续访问模式，定义一个文件视图，并调用独立的 I/O 函数
&lt;pre>&lt;code class="language-c">MPI_Type_create_subarray(...,&amp;amp;subarray, ...);
MPI_Type_commit(&amp;amp;subarray);
MPI_File_open(..., file, ..., &amp;amp;fh);
MPI_File_set_view(fh, ..., subarray, ...);
MPI_File_read(fh, A, ...);
MPI_File_close(&amp;amp;fh);
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>level3：类似于 level 0，但每个过程都使用集合 I/O 函数
&lt;pre>&lt;code class="language-c">MPI_Type_create_subarray(...,&amp;amp;subarray, ...);
MPI_Type_commit(&amp;amp;subarray);
MPI_File_open(MPI_COMM_WORLD, file,...,&amp;amp;fh);
MPI_File_set_view(fh, ..., subarray, ...);
MPI_File_read_all(fh, A, ...);
MPI_File_close(&amp;amp;fh);
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul>
&lt;h2 id="5-总结">5. 总结&lt;/h2>
&lt;ul>
&lt;li>MPI-IO 有许多功能，可以帮助用户获得高性能 I/O
&lt;ul>
&lt;li>支持非连续性访问
&lt;ul>
&lt;li>派生数据类型&lt;/li>
&lt;li>文件视图&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>集合 I/O&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>用户应该根据应用程序 I/O 特性来选择适合的 I/O 访问模式实现&lt;/li>
&lt;li>同时，MPI-IO 不是实现并行 I/O 的唯一选择。目前已有一些更高级的库可代替 MPI-IO
&lt;ul>
&lt;li>HDF5、netCDF&amp;hellip;&amp;hellip;&lt;/li>
&lt;li>这些库都是基于 MPI-IO 实现&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>MPI 与并行计算（四）：数据类型</title><link>https://cuterwrite.top/p/mpi-tutorial/4/</link><pubDate>Thu, 20 Jul 2023 01:00:00 +0000</pubDate><guid>https://cuterwrite.top/p/mpi-tutorial/4/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230720120242.webp" alt="Featured image of post MPI 与并行计算（四）：数据类型" />&lt;h1 id="mpi-与并行计算四数据类型">MPI 与并行计算（四）：数据类型&lt;/h1>
&lt;h2 id="1-预定义数据类型">1. 预定义数据类型&lt;/h2>
&lt;p>MPI 支持异构计算(Heterogeneous Computing)，它指在不同计算机系统上运行程序，每台计算可能有不同生产厂商，不同操作系统。
MPI 通过提供预定义数据类型来解决异构计算中的互操作性问题，建立它与具体语言的对应关系。&lt;/p>
&lt;ul>
&lt;li>MPI 中预定义的数据类型如下：&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">MPI 数据类型(C 语言绑定)&lt;/th>
&lt;th style="text-align:left">C 语言数据类型&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">MPI_CHAR&lt;/td>
&lt;td style="text-align:left">char&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_SHORT&lt;/td>
&lt;td style="text-align:left">short&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_INT&lt;/td>
&lt;td style="text-align:left">int&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_LONG&lt;/td>
&lt;td style="text-align:left">long&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_UNSIGNED_CHAR&lt;/td>
&lt;td style="text-align:left">unsigned char&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_UNSIGNED_SHORT&lt;/td>
&lt;td style="text-align:left">unsigned short&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_UNSIGNED&lt;/td>
&lt;td style="text-align:left">unsigned&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_UNSIGNED_LONG&lt;/td>
&lt;td style="text-align:left">unsigned long&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_FLOAT&lt;/td>
&lt;td style="text-align:left">float&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_DOUBLE&lt;/td>
&lt;td style="text-align:left">double&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_LONG_DOUBLE&lt;/td>
&lt;td style="text-align:left">long double&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_BYTE&lt;/td>
&lt;td style="text-align:left">无&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_PACKED&lt;/td>
&lt;td style="text-align:left">无&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>但是，对于点对点通信，仅仅使用包含一系列相同基本数据类型的缓冲区是不够的。我们经常要传递包含不同数据类型值的信息（例如一个整数变量 count，然后是一串实数）；并且我们经常要发送不连续的数据（例如，矩阵的一个子块）。&lt;/p>
&lt;p>OpenMPI 为发送非连续数据提供 pack/unpack 函数。用户在发送数据前要明确地将数据打包到连续的缓冲区中，并在接收数据后将其从连续的缓冲区中解包。虽然使用这些函数可以实现非连续数据的发送，但是这种方式不够灵活，而且效率低下。不过为了与以前的库或代码兼容，下面提供了这两个函数的使用方法。&lt;/p>
&lt;pre>&lt;code class="language-cpp">int MPI_Pack(const void* inbuf, int incount, MPI_Datatype datatype, void *outbuf, int outsize, int *position, MPI_Comm comm)
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>inbuf&lt;/code>：输入缓冲区的起始地址&lt;/li>
&lt;li>&lt;code>incount&lt;/code>：输入缓冲区中数据的个数&lt;/li>
&lt;li>&lt;code>datatype&lt;/code>：输入缓冲区中数据的类型&lt;/li>
&lt;li>&lt;code>outbuf&lt;/code>：输出缓冲区的起始地址&lt;/li>
&lt;li>&lt;code>outsize&lt;/code>：输出缓冲区的大小&lt;/li>
&lt;li>&lt;code>position&lt;/code>：输出缓冲区中的位置&lt;/li>
&lt;li>&lt;code>comm&lt;/code>：通信域&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-cpp">int MPI_Unpack(const void* inbuf, int insize, int *position, void *outbuf, int outcount, MPI_Datatype datatype, MPI_Comm comm)
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>inbuf&lt;/code>：输入缓冲区的起始地址&lt;/li>
&lt;li>&lt;code>insize&lt;/code>：输入缓冲区的大小&lt;/li>
&lt;li>&lt;code>position&lt;/code>：输入缓冲区中的位置&lt;/li>
&lt;li>&lt;code>outbuf&lt;/code>：输出缓冲区的起始地址&lt;/li>
&lt;li>&lt;code>outcount&lt;/code>：输出缓冲区中数据的个数&lt;/li>
&lt;li>&lt;code>datatype&lt;/code>：输出缓冲区中数据的类型&lt;/li>
&lt;li>&lt;code>comm&lt;/code>：通信域&lt;/li>
&lt;/ul>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 1：Pack/Unpack&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;quot;mpi.h&amp;quot;
#define MASTER 0
#define STRLEN 25
int main(int argc, char* argv[])
{
int rank;
int size;
int position;
char message[BUFSIZ];
float value; //VALUE TO SEND
char name[STRLEN]; //ASSIGNED NAME
int param; //ADDITIONAL PARAM
MPI_Init( &amp;amp;argc, &amp;amp;argv );
MPI_Comm_size( MPI_COMM_WORLD, &amp;amp;size );
MPI_Comm_rank( MPI_COMM_WORLD, &amp;amp;rank );
if (rank == MASTER) {
value = 10;
sprintf(name, &amp;quot;My Name&amp;quot;);
param = 1;
position = 0;
/* now let's pack all those values into a single message */
MPI_Pack(&amp;amp;value, 1, MPI_FLOAT, message, BUFSIZ,
&amp;amp;position, MPI_COMM_WORLD);
/* position has been incremented to first free byte in the message.. */
MPI_Pack(name, STRLEN, MPI_CHAR, message, BUFSIZ,
&amp;amp;position, MPI_COMM_WORLD);
/* position has been incremented again.. */
MPI_Pack(&amp;amp;param, 1, MPI_INT, message, BUFSIZ,
&amp;amp;position, MPI_COMM_WORLD);
MPI_Send(message, BUFSIZ, MPI_PACKED, 1, 1, MPI_COMM_WORLD);
}
else {
MPI_Recv(message, BUFSIZ, MPI_PACKED, 0, 1, MPI_COMM_WORLD, NULL);
position = 0;
MPI_Unpack(message, BUFSIZ, &amp;amp;position, &amp;amp;value, 1,
MPI_FLOAT, MPI_COMM_WORLD);
/* Note that we must know the length of string to expect here! */
MPI_Unpack(message, BUFSIZ, &amp;amp;position, name, STRLEN,
MPI_CHAR, MPI_COMM_WORLD);
MPI_Unpack(message, BUFSIZ, &amp;amp;position, &amp;amp;param, 1,
MPI_INT, MPI_COMM_WORLD);
printf(&amp;quot;rank %d:\t%d %.1f %s\n&amp;quot;, rank, param, value, name);
}
MPI_Finalize();
return EXIT_SUCCESS;
}
&lt;/code>&lt;/pre>
&lt;h2 id="2-派生数据类型">2. 派生数据类型&lt;/h2>
&lt;ul>
&lt;li>MPI 提供了全面而强大的 &lt;strong>构造函数(Constructor Function)&lt;/strong> 来定义派生数据类型。派生数据类型是一种抽象的数据结构，可以用来描述数据的组织形式，而不是数据本身。&lt;/li>
&lt;li>派生数据类型可以用类型图来描述，这是一种通用的类型描述方法，它是一系列二元组&amp;lt;基类型，偏移&amp;gt;的集合，可以表示成如下格式：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-text">&amp;lt;基类型 1，偏移 1&amp;gt;，&amp;lt;基类型 2，偏移 2&amp;gt;，...，&amp;lt;基类型 n，偏移 n&amp;gt;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>
&lt;p>在派生数据类型中，基类型可以是任何 MPI 预定义数据类型，也可以是其它的派生数据类型，即支持数据类型的嵌套定义。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>如图，阴影部分是基类型所占用的空间，其它空间可以是特意留下的，也可以是为了方便数据对齐。
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230720143120.webp"
alt="20230720143120" width="90%" loading="lazy"/>
&lt;/figure>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>基类型指出了该类型图中包括哪些基本的数据类型，而偏移则指出该基类型在整个类型图中的起始位置，基类型可以是预定义类型或派生类型，偏移可正可负，没有递增或递减的顺序要求，而一个类型图中包括的所有基类型的集合称为某类型的类型表，表示为：&lt;/p>
&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-text">类型表={基类型 1，基类型 2，...，基类型 n}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>将类型图和一个数据缓冲区的基地址结合起来 可以说明一个通信缓冲区内的数据分布情况&lt;/li>
&lt;li>预定义数据类型是通用数据类型的特例，比如 MPI_INT 是一个预先定义好了的数据类型句柄，其类型图为&lt;code>{(int, 0)}&lt;/code>，有一个基类型入口项 int 和偏移 0，其它的基本数据类型与此相似，数据类型的&lt;strong>跨度&lt;/strong>被定义为&lt;strong>该数据类型的类型图中从第一个基类型到最后一个基类型间的距离&lt;/strong>&lt;/li>
&lt;li>即如果某一个类型的类型图为:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-text">typemap={(type0,disp0),...,(typen-1,dispn-1)},
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>则该类型图的下界定义为：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-text">lb(typemap)=min{dispj}, j=0,...,n-1
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>该类型图的上界定义为：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-text">ub(typemap)=max{dispj+sizeof(typej)}, j=0,...,n-1
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>该类型图的跨度定义为：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-text">extent(typemap)=ub(typemap)-lb(typemap) + e
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>
&lt;p>由于不同的类型有不同的对齐位置的要求 e(extent)就是&lt;strong>能够使类型图的跨度满足该类型的类型表中的所有的类型都能达到下一个对齐要求所需要的最小非负整数值&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>假设&lt;code>type={(double, 0), (char, 8)}&lt;/code>，进一步假设 double 型的值必须严格分配到地址为 8 的倍数的存储空间，则该数据类型的 extent 是 16（(从 9 循环到下一个 8 的倍数)，一个由一个字符后面紧跟一个双精度值的数据类型,其 extent 也是 16&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在 MPI 中，派生数据类型的构造函数有如下几种：&lt;/p>
&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">函数名&lt;/th>
&lt;th style="text-align:left">含义&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">MPI_Type_contiguous&lt;/td>
&lt;td style="text-align:left">定义由相同数据类型的元素组成的类型&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Type_vector&lt;/td>
&lt;td style="text-align:left">定义由成块的元素组成的类型，块之间具有相同间隔&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Type_indexed&lt;/td>
&lt;td style="text-align:left">定义由成块的元素组成的类型，块长度和偏移由参数指定&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Type_struct&lt;/td>
&lt;td style="text-align:left">定义由不同数据类型的元素组成的类型&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Type_commit&lt;/td>
&lt;td style="text-align:left">提交一个派生数据类型&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MPI_Type_free&lt;/td>
&lt;td style="text-align:left">释放一个派生数据类型&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>（1）最简单的数据类型构造函数是 &lt;code>MPI_Type_contiguous&lt;/code> ，它允许将数据类型复制到连续位置。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230814005632.webp"
alt="20230814005632" width="90%" loading="lazy"/>
&lt;/figure>
&lt;pre>&lt;code class="language-cpp">int MPI_Type_contiguous(int count, MPI_Datatype oldtype,MPI_Datatype *newtype)
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>count&lt;/code>: 重复的次数&lt;/li>
&lt;li>&lt;code>oldtype&lt;/code>: 基本数据类型&lt;/li>
&lt;li>&lt;code>newtype&lt;/code>: 派生数据类型&lt;/li>
&lt;/ul>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 2： MPI_Type_contiguous 的使用&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">#include &amp;quot;mpi.h&amp;quot;
#include &amp;lt;stdio.h&amp;gt;
int main(int argc, char *argv[])
{
int myrank;
MPI_Status status;
MPI_Datatype type;
int buffer[100];
MPI_Init(&amp;amp;argc, &amp;amp;argv);
MPI_Type_contiguous( 100, MPI_CHAR, &amp;amp;type );
MPI_Type_commit(&amp;amp;type);
MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;myrank);
if (myrank == 0)
{
MPI_Send(buffer, 1, type, 1, 123, MPI_COMM_WORLD);
}
else if (myrank == 1)
{
MPI_Recv(buffer, 1, type, 0, 123, MPI_COMM_WORLD, &amp;amp;status);
}
MPI_Finalize();
return 0;
}
&lt;/code>&lt;/pre>
&lt;p>（2）函数 &lt;code>MPI_Type_vector&lt;/code> 是一个更通用的构造函数，它允许将数据类型复制到由等间距块组成的位置。每个块都是通过连接相同数量的旧数据类型副本来获得的。块之间的间距是旧数据类型范围的倍数。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230814005859.webp"
alt="20230814005859" width="90%" loading="lazy"/>
&lt;/figure>
&lt;pre>&lt;code class="language-cpp">int MPI_Type_vector(int count, int blocklength, int stride,MPI_Datatype oldtype, MPI_Datatype *newtype)
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>count&lt;/code>: 重复的次数&lt;/li>
&lt;li>&lt;code>blocklength&lt;/code>: 每个块中的元素数&lt;/li>
&lt;li>&lt;code>stride&lt;/code>: 旧数据类型的跨度&lt;/li>
&lt;li>&lt;code>oldtype&lt;/code>: 基本数据类型&lt;/li>
&lt;li>&lt;code>newtype&lt;/code>: 派生数据类型&lt;/li>
&lt;/ul>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 3： MPI_Type_vector 的使用&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-cpp">#include &amp;quot;mpi.h&amp;quot;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#define SIZE 4
/*Sendind each colum to a processor*/
int main (int argc, char *argv[])
{
int numtasks, rank, source=0, dest, tag=1, i;
float a[SIZE][SIZE] =
{1.0, 2.0, 3.0, 4.0,
5.0, 6.0, 7.0, 8.0,
9.0, 10.0, 11.0, 12.0,
13.0, 14.0, 15.0, 16.0};
float b[SIZE];
MPI_Status stat;
MPI_Datatype columntype;
MPI_Init(&amp;amp;argc,&amp;amp;argv);
MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;rank);
MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;numtasks);
MPI_Type_vector(SIZE/*num of element in a column*/,
1 /*one element for row*/,
SIZE /*take an element each 4*/, MPI_FLOAT, &amp;amp;columntype);
MPI_Type_commit(&amp;amp;columntype);
if (numtasks == SIZE) {
if (rank == 0) {
for (i=0; i&amp;lt;numtasks; i++)
MPI_Send(&amp;amp;a[0][i], 1, columntype, i, tag, MPI_COMM_WORLD);
}
MPI_Recv(b, SIZE, MPI_FLOAT, source, tag, MPI_COMM_WORLD, &amp;amp;stat);
printf(&amp;quot;rank= %d b= %3.1f %3.1f %3.1f %3.1f\n&amp;quot;,
rank,b[0],b[1],b[2],b[3]);
}
else
printf(&amp;quot;Must specify %d processors. Terminating.\n&amp;quot;,SIZE);
MPI_Type_free(&amp;amp;columntype);
MPI_Finalize();
}
&lt;/code>&lt;/pre>
&lt;p>（3）函数 &lt;code>MPI_Type_index&lt;/code> 允许将旧数据类型复制到一系列块中(每个块是旧数据类型的串联)，其中每个块可以包含不同数量的副本，并且具有不同的位移。所有块位移都是旧类型范围的倍数。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230814010438.webp"
alt="20230814010438" width="90%" loading="lazy"/>
&lt;/figure>
&lt;pre>&lt;code class="language-c">int MPI_Type_indexed(int count, const int array_of_blocklengths[],const int array_of_displacements[], MPI_Datatype oldtype,MPI_Datatype *newtype)
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>count&lt;/code>: 重复的次数&lt;/li>
&lt;li>&lt;code>array_of_blocklengths&lt;/code>: 每个块中的元素数&lt;/li>
&lt;li>&lt;code>array_of_displacements&lt;/code>: 每个块的偏移量&lt;/li>
&lt;li>&lt;code>oldtype&lt;/code>: 基本数据类型&lt;/li>
&lt;li>&lt;code>newtype&lt;/code>: 派生数据类型&lt;/li>
&lt;/ul>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 4： MPI_Type_indexed 的使用&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-cpp"> #include &amp;quot;mpi.h&amp;quot;
#include &amp;lt;stdio.h&amp;gt;
#define NELEMENTS 6
main(int argc, char *argv[]) {
int numtasks, rank, source=0, dest, tag=1, i;
int blocklengths[2], displacements[2];
float a[16] = {1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0};
float b[NELEMENTS];
MPI_Status stat;
MPI_Datatype indextype; // required variable
MPI_Init(&amp;amp;argc,&amp;amp;argv);
MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;rank);
MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;numtasks);
blocklengths[0] = 4; /*take 4 elements from the array*/
blocklengths[1] = 2; /*take 2 elemnets from the array*/
displacements[0] = 5;/*start from the element index 5 the first block that is 6.0 */
displacements[1] = 12;/*start from the element index 12 the first block that is 13.0 */
// create indexed derived data type
MPI_Type_indexed(2, blocklengths, displacements, MPI_FLOAT, &amp;amp;indextype);
MPI_Type_commit(&amp;amp;indextype);
if (rank == 0) {
for (i=0; i&amp;lt;numtasks; i++)
// task 0 sends one element of indextype to all tasks
MPI_Send(a, 1, indextype, i, tag, MPI_COMM_WORLD);
}
// all tasks receive indextype data from task 0
MPI_Recv(b, NELEMENTS, MPI_FLOAT, source, tag, MPI_COMM_WORLD, &amp;amp;stat);
printf(&amp;quot;rank= %d b= %3.1f %3.1f %3.1f %3.1f %3.1f %3.1f\n&amp;quot;,
rank,b[0],b[1],b[2],b[3],b[4],b[5]);
// free datatype when done using it
MPI_Type_free(&amp;amp;indextype);
MPI_Finalize();
}
&lt;/code>&lt;/pre>
&lt;p>（4）&lt;code>MPI_Type_create_struct&lt;/code> 是最通用的类型构造函数。允许程序员定义由组件数据类型的完全定义的映射形成的新数据类型。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230814010602.webp"
alt="20230814010602" width="90%" loading="lazy"/>
&lt;/figure>
&lt;pre>&lt;code class="language-c">int MPI_Type_create_struct(int count, const int array_of_blocklengths[],const MPI_Aint array_of_displacements[],const MPI_Datatype array_of_types[], MPI_Datatype *newtype
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>count&lt;/code>: 重复的次数&lt;/li>
&lt;li>&lt;code>array_of_blocklengths&lt;/code>: 每个块中的元素数&lt;/li>
&lt;li>&lt;code>array_of_displacements&lt;/code>: 每个块的偏移量&lt;/li>
&lt;li>&lt;code>array_of_types&lt;/code>: 每个块的数据类型&lt;/li>
&lt;li>&lt;code>newtype&lt;/code>: 派生数据类型&lt;/li>
&lt;/ul>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 5：MPI_Type_create_struct 的使用&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-cpp">#include &amp;quot;mpi.h&amp;quot;
#include &amp;lt;stdio.h&amp;gt;
#define NELEM 25
main(int argc, char *argv[])
{
int numtasks, rank, source = 0, dest, tag = 1, i;
typedef struct
{
float x, y, z;
float velocity;
int n, type;
} Particle;
Particle p[NELEM], particles[NELEM];
MPI_Datatype particletype, oldtypes[2]; // required variables
int blockcounts[2];
// MPI_Aint type used to be consistent with syntax of
// MPI_Type_extent routine
MPI_Aint offsets[2], lb, extent;
MPI_Status stat;
MPI_Init(&amp;amp;argc, &amp;amp;argv);
MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;rank);
MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;numtasks);
// setup description of the 4 MPI_FLOAT fields x, y, z, velocity
offsets[0] = 0;
oldtypes[0] = MPI_FLOAT;
blockcounts[0] = 4;
// setup description of the 2 MPI_INT fields n, type
// need to first figure offset by getting size of MPI_FLOAT
MPI_Type_get_extent(MPI_FLOAT, &amp;amp;lb, &amp;amp;extent);
offsets[1] = 4 * extent;
oldtypes[1] = MPI_INT;
blockcounts[1] = 2;
// define structured type and commit it
MPI_Type_create_struct(2, blockcounts, offsets, oldtypes, &amp;amp;particletype);
MPI_Type_commit(&amp;amp;particletype);
// task 0 initializes the particle array and then sends it to each task
if (rank == 0)
{
for (i = 0; i &amp;lt; NELEM; i++)
{
particles[i].x = i * 1.0;
particles[i].y = i * -1.0;
particles[i].z = i * 1.0;
particles[i].velocity = 0.25;
particles[i].n = i;
particles[i].type = i % 2;
}
for (i = 0; i &amp;lt; numtasks; i++)
MPI_Send(particles, NELEM, particletype, i, tag, MPI_COMM_WORLD);
}
// all tasks receive particletype data
MPI_Recv(p, NELEM, particletype, source, tag, MPI_COMM_WORLD, &amp;amp;stat);
printf(&amp;quot;rank= %d %3.2f %3.2f %3.2f %3.2f %d %d\n&amp;quot;, rank, p[3].x, p[3].y, p[3].z,
p[3].velocity, p[3].n, p[3].type);
// free datatype when done using it
MPI_Type_free(&amp;amp;particletype);
MPI_Finalize();
}
&lt;/code>&lt;/pre>
&lt;p>在这里，偏移量有一个问题。手动计算偏移量可能比较麻烦。虽然这种情况越来越少，但有些类型的大小会因系统/操作系统而异，因此硬编码可能会带来麻烦。一种更简洁的方法是使用标准库中的 offsetof 宏（在 C 语言中必须包含 stddef.h，在 C++ 语言中必须包含 cstddef）。它会返回一个 size_t（可隐式转换为 MPI_Aint），与该属性的偏移量相对应。于是可以将偏移量表定义为：&lt;/p>
&lt;pre>&lt;code class="language-cpp">MPI_Aint displacements[2] = {offsetof(Particle, x), offsetof(Particle, n)};
&lt;/code>&lt;/pre></description></item><item><title>MPI 与并行计算（三）：集合通信</title><link>https://cuterwrite.top/p/mpi-tutorial/3/</link><pubDate>Wed, 19 Jul 2023 15:00:00 +0000</pubDate><guid>https://cuterwrite.top/p/mpi-tutorial/3/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230720002052.webp" alt="Featured image of post MPI 与并行计算（三）：集合通信" />&lt;h1 id="mpi-与并行计算三集合通信">MPI 与并行计算（三）：集合通信&lt;/h1>
&lt;h2 id="1-定义">1. 定义&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>集合通信（Collective Communication）&lt;/strong>：是一个进程组中的所有进程都参加的全局通信操作。&lt;/li>
&lt;li>特点：
&lt;ul>
&lt;li>通信空间中的所有进程都参与通信操作&lt;/li>
&lt;li>每一个进程都需要调用该操作函数&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>数据移动类型&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Broadcast&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/bcast_p2p.gif"
alt="bcast_p2p" width="auto" loading="lazy"/>
&lt;/figure>
&lt;ul>
&lt;li>Scatter&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/scatter.webp"
alt="scatter" width="auto" loading="lazy"/>
&lt;/figure>
&lt;ul>
&lt;li>Gather&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/gather.webp"
alt="gather" width="auto" loading="lazy"/>
&lt;/figure>
&lt;ul>
&lt;li>AllGather&lt;/li>
&lt;li>Alltoall&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230720003014.webp"
alt="20230720003014" width="auto" loading="lazy"/>
&lt;/figure>
&lt;h2 id="2-集合通信实现的功能">2. 集合通信实现的功能&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>集合通信一般实现三个功能：通信、聚合和同步&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">类型&lt;/th>
&lt;th style="text-align:center">函数名&lt;/th>
&lt;th style="text-align:center">含义&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">通信&lt;/td>
&lt;td style="text-align:center">MPI_Bcast&lt;/td>
&lt;td style="text-align:center">一对多广播同样的消息&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">通信&lt;/td>
&lt;td style="text-align:center">MPI_Gather&lt;/td>
&lt;td style="text-align:center">多对一收集各个进程的消息&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">通信&lt;/td>
&lt;td style="text-align:center">MPI_Gatherv&lt;/td>
&lt;td style="text-align:center">MPI_Gather 的一般化&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">通信&lt;/td>
&lt;td style="text-align:center">MPI_Allgather&lt;/td>
&lt;td style="text-align:center">全局收集&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">通信&lt;/td>
&lt;td style="text-align:center">MPI_Allgatherv&lt;/td>
&lt;td style="text-align:center">MPI_Allgather 的一般化&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">通信&lt;/td>
&lt;td style="text-align:center">MPI_Scatter&lt;/td>
&lt;td style="text-align:center">一对多散播不同的消息&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">通信&lt;/td>
&lt;td style="text-align:center">MPI_Scatterv&lt;/td>
&lt;td style="text-align:center">MPI_Scatter 的一般化&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">通信&lt;/td>
&lt;td style="text-align:center">MPI_Alltoall&lt;/td>
&lt;td style="text-align:center">多对多全局交换消息&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">通信&lt;/td>
&lt;td style="text-align:center">MPI_Alltoallv&lt;/td>
&lt;td style="text-align:center">MPI_Alltoall 的一般化&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">聚合&lt;/td>
&lt;td style="text-align:center">MPI_Reduce&lt;/td>
&lt;td style="text-align:center">多对一规约&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">聚合&lt;/td>
&lt;td style="text-align:center">MPI_Allreduce&lt;/td>
&lt;td style="text-align:center">MPI_Reduce 的一般化&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">聚合&lt;/td>
&lt;td style="text-align:center">MPI_Scan&lt;/td>
&lt;td style="text-align:center">多对多扫描&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">聚合&lt;/td>
&lt;td style="text-align:center">MPI_Reduce_scatter&lt;/td>
&lt;td style="text-align:center">MPI_Reduce 的一般化&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">同步&lt;/td>
&lt;td style="text-align:center">MPI_Barrier&lt;/td>
&lt;td style="text-align:center">路障同步&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/li>
&lt;li>
&lt;p>通信：集合通信，按照通信方向的不同，又可以分为三种：一对多通信，多对一通信和多对多通信。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>一对多通信：一个进程向其它所有的进程发送消息，这个负责发送消息的进程叫做 Root 进程。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>多对一通信：一个进程负责从其它所有的进程接收消息，这个接收的进程也叫做 Root 进程。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>多对多通信：每一个进程都向其它所有的进程发送或者接收消息。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="3-一对多通信广播">3. 一对多通信：广播&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>广播&lt;/strong>是一对多通信的典型例子，其调用格式为：&lt;code>MPI_Bcast(void *buffer, int count, MPI_Datatype datatype, int root, MPI_Comm comm)&lt;/code>&lt;/li>
&lt;/ul>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 1：广播&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">// bcast.c
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;mpi.h&amp;gt;
int main(int argc, char *argv[])
{
int rank, size;
int data;
MPI_Init(&amp;amp;argc, &amp;amp;argv);
MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;rank);
MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;size);
if (rank == 0)
{
data = 123;
}
MPI_Bcast(&amp;amp;data, 1, MPI_INT, 0, MPI_COMM_WORLD);
printf(&amp;quot;Process %d got data %d\n&amp;quot;, rank, data);
MPI_Finalize();
return 0;
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>结果：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-shell">root@ubuntu:~# mpicc bcast.c -o bcast
root@ubuntu:~# mpirun -n 2 ./bcast
Process 0 got data 123
Process 1 got data 123
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>广播的特点
&lt;ul>
&lt;li>标号为 Root 的进程发送相同的消息给通信域 Comm 中的所有进程。&lt;/li>
&lt;li>消息的内容如同点对点通信一样由三元组&amp;lt;Address, Count, Datatype&amp;gt;标识。&lt;/li>
&lt;li>对 Root 进程来说，这个三元组既定义了发送缓冲也定义了接收缓冲。对其它进程来说，这个三元组只定义了接收缓冲&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="4-多对一通信收集">4. 多对一通信：收集&lt;/h2>
&lt;ul>
&lt;li>收集是多对一通信的典型例子，其调用格式为：&lt;code>MPI_Gather(void *sendbuf, int sendcount, MPI_Datatype sendtype, void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm)&lt;/code>&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230720004519.webp"
alt="20230720004519" width="90%" loading="lazy"/>
&lt;/figure>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 2：收集&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">// gather.c
#include &amp;lt;mpi.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
int main(int argc, char *argv[])
{
int rank, size;
// 分布变量
int data[2];
int *buf;
MPI_Init(&amp;amp;argc, &amp;amp;argv);
MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;rank);
MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;size);
data[0] = rank * 2 + 1;
data[1] = rank * rank * 3 + 2;
if (rank == 0)
{
// 开辟接收缓存区
buf = malloc(2 * size * sizeof(int));
}
MPI_Gather(data, 2, MPI_INT, buf, 2, MPI_INT, 0, MPI_COMM_WORLD);
if (rank == 0)
{
for (int i = 0; i &amp;lt; 2 * size; i++)
{
printf(&amp;quot;%d &amp;quot;, buf[i]);
}
printf(&amp;quot;\n&amp;quot;);
free(buf);
}
MPI_Finalize();
return 0;
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>结果：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-shell">root@ubuntu:~# mpicc gather.c -o gather
root@ubuntu:~# mpirun -n 2 ./gather
1 2 3 5
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>收集的特点
&lt;ul>
&lt;li>在收集操作中，Root 进程从进程域 Comm 的所有进程(包括它自已)接收消息。&lt;/li>
&lt;li>这 n 个消息按照进程的标识 rank 排序进行拼接，然后存放在 Root 进程的接收缓冲中。&lt;/li>
&lt;li>接收缓冲由三元组&amp;lt;RecvAddress, RecvCount, RecvDatatype&amp;gt;标识，发送缓冲由三元组&amp;lt;SendAddress, SendCount, SendDatatype&amp;gt;标识，所有非 Root 进程忽略接收缓冲。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="5-一对多通信散播">5. 一对多通信：散播&lt;/h2>
&lt;ul>
&lt;li>散播是一个一对多操作，其调用格式为：&lt;code>MPI_Scatter(void *sendbuf, int sendcount, MPI_Datatype sendtype, void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm)&lt;/code>&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230720005711.webp"
alt="20230720005711" width="90%" loading="lazy"/>
&lt;/figure>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 3：散播&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">// scatter.c
#include &amp;lt;mpi.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
int main(int argc, char *argv[])
{
int rank, size;
int *buf;
int data[2];
MPI_Init(&amp;amp;argc, &amp;amp;argv);
MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;rank);
MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;size);
if (rank == 0)
{
buf = malloc(2 * size * sizeof(int));
for (int i = 0; i &amp;lt; 2 * size; i++)
{
buf[i] = i;
}
}
MPI_Scatter(buf, 2, MPI_INT, data, 2, MPI_INT, 0, MPI_COMM_WORLD);
printf(&amp;quot;rank = %d, data = %d %d\n&amp;quot;, rank, data[0], data[1]);
if (rank == 0)
{
free(buf);
}
MPI_Finalize();
return 0;
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>结果：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-shell">root@ubuntu:~# mpicc scatter.c -o scatter
root@ubuntu:~# mpirun -n 2 ./scatter
rank = 0, data = 0 1
rank = 1, data = 2 3
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>散播的特点
&lt;ul>
&lt;li>Scatter 执行与 Gather 相反的操作。&lt;/li>
&lt;li>Root 进程给所有进程(包括它自已)发送一个不同的消息，这 n (n 为进程域 comm 包括的进程个数)个消息在 Root 进程的发送缓冲区中按进程标识的顺序有序地存放。&lt;/li>
&lt;li>每个接收缓冲由三元组&amp;lt;RecvAddress, RecvCount, RecvDatatype&amp;gt;标识，所有的非 Root 进程忽略发送缓冲。对 Root 进程，发送缓冲由三元组&amp;lt;SendAddress,SendCount, SendDatatype&amp;gt;标识。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="6-聚合">6. 聚合&lt;/h2>
&lt;ul>
&lt;li>集合通信的聚合功能使得 MPI 进行通信的同时完成一定的计算。&lt;/li>
&lt;li>MPI 聚合的功能分三步实现：
&lt;ul>
&lt;li>首先是通信的功能，即消息根据要求发送到目标进程，目标进程也已经收到了各自需要的消息&lt;/li>
&lt;li>然后是对消息的处理，即执行计算功能&lt;/li>
&lt;li>最后把处理结果放入指定的接收缓冲区&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>MPI 提供了两种类型的聚合操作: 归约(Reduce)和扫描(Scan)。&lt;/li>
&lt;/ul>
&lt;h2 id="7-同步">7. 同步&lt;/h2>
&lt;ul>
&lt;li>同步功能用来协调各个进程之间的进度和步伐 。目前 MPI 的实现中支持一个同步操作，即&lt;strong>路障同步(Barrier)&lt;/strong>。&lt;/li>
&lt;li>路障同步的调用格式为：&lt;code>MPI_Barrier(MPI_Comm comm)&lt;/code>
&lt;ul>
&lt;li>在路障同步操作&lt;code>MPI_Barrier(Comm)&lt;/code>中，通信域 Comm 中的所有进程相互同步。&lt;/li>
&lt;li>在该操作调用返回后，可以保证组内所有的进程都已经执行完了调用之前的所有操作，可以开始该调用后的操作。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="8-规约">8. 规约&lt;/h2>
&lt;p>MPI_REDUCE 将组内每个进程输入缓冲区中的数据按给定的操作 op 进行运算，并将其结果返回到序列号为 root 的进程的输出缓冲区中，输入缓冲区由参数 sendbuf、count 和 datatype 定义，输出缓冲区由参数 recvbuf count 和 datatype 定义，要求两者的元素数目和类型都必须相同，因为所有组成员都用同样的参数 count、datatype、op、root 和 comm 来调用此例程 故而所有进程都提供长度相同、元素类型相同的输入和输出缓冲区，每个进程可能提供一个元素或一系列元素 组合操作依次针对每个元素进行。&lt;/p>
&lt;p>操作 op 始终被认为是可结合的 并且所有 MPI 定义的操作被认为是可交换的，用户自定义的操作被认为是可结合的，但可以不是可交换的。MPI 中已经定义好的一些操作,它们是为函数&lt;code>MPI_Reduce&lt;/code> 和一些其他的相关函数,如&lt;code>MPI_Allreduce&lt;/code>、&lt;code>MPI_Reduce_scatter&lt;/code> 和&lt;code>MPI_Scan&lt;/code> 而定义的 这些操作用来设定相应的 op。&lt;/p>
&lt;p>MPI 预定的归约操作如下:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">操作&lt;/th>
&lt;th style="text-align:center">含义&lt;/th>
&lt;th style="text-align:center">操作&lt;/th>
&lt;th style="text-align:center">含义&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">MPI_MAX&lt;/td>
&lt;td style="text-align:center">最大值&lt;/td>
&lt;td style="text-align:center">MPI_MIN&lt;/td>
&lt;td style="text-align:center">最小值&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">MPI_SUM&lt;/td>
&lt;td style="text-align:center">求和&lt;/td>
&lt;td style="text-align:center">MPI_PROD&lt;/td>
&lt;td style="text-align:center">求积&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">MPI_LAND&lt;/td>
&lt;td style="text-align:center">逻辑与&lt;/td>
&lt;td style="text-align:center">MPI_BAND&lt;/td>
&lt;td style="text-align:center">按位与&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">MPI_LOR&lt;/td>
&lt;td style="text-align:center">逻辑或&lt;/td>
&lt;td style="text-align:center">MPI_BOR&lt;/td>
&lt;td style="text-align:center">按位或&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">MPI_LXOR&lt;/td>
&lt;td style="text-align:center">逻辑异或&lt;/td>
&lt;td style="text-align:center">MPI_BXOR&lt;/td>
&lt;td style="text-align:center">按位异或&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">MPI_MAXLOC&lt;/td>
&lt;td style="text-align:center">最大值和位置&lt;/td>
&lt;td style="text-align:center">MPI_MINLOC&lt;/td>
&lt;td style="text-align:center">最小值和位置&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 4：计算 pi 值&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">// reduce.c
#include &amp;lt;math.h&amp;gt;
#include &amp;lt;mpi.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
double f(double);
double f(double x)
{
return 4.0 / (1.0 + x * x);
}
int main(int argc, char **argv)
{
int done = 0, n, myid, numprocs, i;
double PI25DT = 3.141592653589793238462643;
double mypi, pi, h, sum, x;
double startwtime = 0.0, endwtime;
int namelen;
char process_name[MPI_MAX_PROCESSOR_NAME];
MPI_Init(&amp;amp;argc, &amp;amp;argv);
MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;numprocs);
MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;myid);
MPI_Get_processor_name(process_name, &amp;amp;namelen);
fprintf(stdout, &amp;quot;Process %d of %d is on %s\n&amp;quot;, myid, numprocs, process_name);
n = 0;
if (myid == 0)
{
fprintf(stdout, &amp;quot;Enter the number of intervals: (0 quits) &amp;quot;);
fflush(stdout);
scanf(&amp;quot;%d&amp;quot;, &amp;amp;n);
startwtime = MPI_Wtime();
}
/* 将 n 广播给所有进程 */
MPI_Bcast(&amp;amp;n, 1, MPI_INT, 0, MPI_COMM_WORLD);
/* 矩形宽度 */
h = 1.0 / (double)n;
/* 矩形面积初值 */
sum = 0.0;
/* 每个进程计算自己的部分 */
for (i = myid + 1; i &amp;lt;= n; i += numprocs)
{
x = h * ((double)i - 0.5);
sum += f(x);
}
/* 各个进程并行计算得到的和 */
mypi = h * sum;
MPI_Reduce(&amp;amp;mypi, &amp;amp;pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
/* 将部分和累加得到最终结果 */
if (myid == 0)
{
printf(&amp;quot;pi is approximately %.16f, Error is %.16f\n&amp;quot;, pi, fabs(pi - PI25DT));
endwtime = MPI_Wtime();
printf(&amp;quot;wall clock time = %f\n&amp;quot;, endwtime - startwtime);
fflush(stdout);
}
MPI_Finalize();
return 0;
}
&lt;/code>&lt;/pre></description></item><item><title>MPI 与并行计算（二）：点到点通信</title><link>https://cuterwrite.top/p/mpi-tutorial/2/</link><pubDate>Wed, 19 Jul 2023 13:00:00 +0000</pubDate><guid>https://cuterwrite.top/p/mpi-tutorial/2/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230719212254.webp" alt="Featured image of post MPI 与并行计算（二）：点到点通信" />&lt;h1 id="mpi-与并行计算二点到点通信">MPI 与并行计算（二）：点到点通信&lt;/h1>
&lt;h2 id="1-mpi-的通信模式">1. MPI 的通信模式&lt;/h2>
&lt;ul>
&lt;li>通信模式：指的是缓冲管理以及发送方和接收方之间的同步方式。&lt;/li>
&lt;li>MPI 支持四种通信模式：标准通信模式、缓冲通信模式、就绪通信模式和同步通信模式&lt;/li>
&lt;/ul>
&lt;h2 id="2-标准通信模式mpi_send-和-mpi_recv">2. 标准通信模式：MPI_Send 和 MPI_Recv&lt;/h2>
&lt;ul>
&lt;li>由 MPI 决定是否缓冲消息&lt;/li>
&lt;li>没有足够的系统缓冲区时或出于性能的考虑， MPI 可能进行直接拷贝： 仅当相应的接收开始后，发送语句才能返回&lt;/li>
&lt;li>MPI 缓冲消息：发送语句地相应的接收语句完成前返回&lt;/li>
&lt;li>发送的结束 == 消息已从发送方发出，而不是滞留在发送方的系统缓冲区中&lt;/li>
&lt;li>非本地的：发送操作的成功与否依赖于接收操作&lt;/li>
&lt;li>理论上要求有接收进程的 recv 调用配合，发送函数是&lt;code>MPI_Send()&lt;/code>。&lt;/li>
&lt;/ul>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 1：标准通信模式&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-cpp">#include &amp;lt;mpi.h&amp;gt;
#include &amp;lt;iostream&amp;gt;
#include &amp;lt;thread&amp;gt;
#define BUF_SIZE 10
int main(int argc, char *argv[])
{
int myid, numprocs;
int other;
int sb[BUF_SIZE];
int rb[BUF_SIZE];
// 初始化 MPI 环境
MPI_Init(&amp;amp;argc, &amp;amp;argv);
MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;myid);
MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;numprocs);
MPI_Status status;
for (int i = 0; i &amp;lt; BUF_SIZE; i++)
{
sb[i] = myid + i;
}
if (myid == 0)
{
other = 1;
}
else if (myid == 1)
{
other = 0;
}
if (myid == 0)
{
std::cout &amp;lt;&amp;lt; &amp;quot;process &amp;quot; &amp;lt;&amp;lt; myid &amp;lt;&amp;lt; &amp;quot; tring send...&amp;quot; &amp;lt;&amp;lt; std::endl;
MPI_Send(sb, BUF_SIZE, MPI_INT, other, 1, MPI_COMM_WORLD);
std::cout &amp;lt;&amp;lt; &amp;quot;process &amp;quot; &amp;lt;&amp;lt; myid &amp;lt;&amp;lt; &amp;quot; tring receiving...&amp;quot; &amp;lt;&amp;lt; std::endl;
MPI_Recv(rb, BUF_SIZE, MPI_INT, other, 1, MPI_COMM_WORLD, &amp;amp;status);
}
else if (myid == 1)
{
// sleep 10s, 与缓冲通信模式相比，这里的发送和接收操作是阻塞的
std::this_thread::sleep_for(std::chrono::seconds(10));
std::cout &amp;lt;&amp;lt; &amp;quot;process &amp;quot; &amp;lt;&amp;lt; myid &amp;lt;&amp;lt; &amp;quot; tring receiving...&amp;quot; &amp;lt;&amp;lt; std::endl;
MPI_Recv(rb, BUF_SIZE, MPI_INT, other, 1, MPI_COMM_WORLD, &amp;amp;status);
std::cout &amp;lt;&amp;lt; &amp;quot;process &amp;quot; &amp;lt;&amp;lt; myid &amp;lt;&amp;lt; &amp;quot; tring send...&amp;quot; &amp;lt;&amp;lt; std::endl;
MPI_Send(sb, BUF_SIZE, MPI_INT, other, 1, MPI_COMM_WORLD);
}
std::cout &amp;lt;&amp;lt; &amp;quot;Hello World! Process &amp;quot; &amp;lt;&amp;lt; myid &amp;lt;&amp;lt; &amp;quot; of &amp;quot; &amp;lt;&amp;lt; numprocs &amp;lt;&amp;lt; std::endl;
std::cout &amp;lt;&amp;lt; &amp;quot;Send buffer: &amp;quot; &amp;lt;&amp;lt; std::endl;
for (int i = 0; i &amp;lt; BUF_SIZE; i++)
{
std::cout &amp;lt;&amp;lt; sb[i] &amp;lt;&amp;lt; &amp;quot; &amp;quot;;
}
std::cout &amp;lt;&amp;lt; std::endl;
std::cout &amp;lt;&amp;lt; &amp;quot;Receive buffer: &amp;quot; &amp;lt;&amp;lt; std::endl;
for (int i = 0; i &amp;lt; BUF_SIZE; i++)
{
std::cout &amp;lt;&amp;lt; rb[i] &amp;lt;&amp;lt; &amp;quot; &amp;quot;;
}
std::cout &amp;lt;&amp;lt; std::endl;
MPI_Finalize();
return 0;
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>运行结果：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-shell">root@ubuntu:~# mpicxx -o mpi mpi.cpp
root@ubuntu:~# mpirun -n 2 ./mpi
process 0 tring send...
process 0 tring receiving...
process 1 tring receiving...
process 1 tring send...
Hello World! Process 1 of 2
Send buffer:
1 2 3 4 5 6 7 8 9 10
Receive buffer:
0 1 2 3 4 5 6 7 8 9
Hello World! Process 0 of 2
Send buffer:
0 1 2 3 4 5 6 7 8 9
Receive buffer:
1 2 3 4 5 6 7 8 9 10
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>这就是点对点通信的基本用法，通过发送和接收操作，进程之间可以进行数据交换和协调工作。&lt;/li>
&lt;/ul>
&lt;h2 id="3-缓冲通信模式">3. 缓冲通信模式&lt;/h2>
&lt;ul>
&lt;li>前提: 用户显示地指定用于缓冲消息的系统缓冲区&lt;code>MPI_Buffer_attach(*buffer, *size)&lt;/code> 。&lt;/li>
&lt;li>发送是本地的: 完成不依赖于与其匹配的接收操作。 发送的结束仅表明消息进入系统的缓冲区中，发送缓冲区可以重用，而对接收方的情况并不知道。&lt;/li>
&lt;li>缓冲模式在相匹配的接收未开始的情况下，总是将送出的消息放在缓冲区内，这样发送者可以很快地继续计算,然后由系统处理放在缓冲区中的消息。&lt;/li>
&lt;li>占用内存，一次内存拷贝。&lt;/li>
&lt;li>函数调用形式为：&lt;code>MPI_Bsend()&lt;/code>。B 表示缓冲，缓冲通信模式主要用于解开阻塞通信的发送与接收之间的耦合。&lt;/li>
&lt;li>作用总结：通常情况下，MPI 发送和接收操作是阻塞的，即发送操作会等待接收方准备好接收，接收操作会等待发送方发送数据。但是，MPI 提供了一种称为缓冲区（buffering）的机制，可以使&lt;strong>发送操作立即返回，而不需要等待接收方准备好&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 2：缓冲通信模式&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-cpp">#include &amp;lt;mpi.h&amp;gt;
#include &amp;lt;iostream&amp;gt;
#include &amp;lt;thread&amp;gt;
int main(int argc, char **argv)
{
int myid, numprocs;
// 初始化
MPI_Init(&amp;amp;argc, &amp;amp;argv);
MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;myid);
MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;numprocs);
int s1, s2;
// 取缓冲区的上界，以字节为单位
MPI_Pack_size(7, MPI_CHAR, MPI_COMM_WORLD, &amp;amp;s1);
MPI_Pack_size(2, MPI_DOUBLE, MPI_COMM_WORLD, &amp;amp;s2);
int buffer_size = 2 * MPI_BSEND_OVERHEAD + s1 + s2;
char *buffer = new char[buffer_size];
// 装配一个用于通信的缓冲区
MPI_Buffer_attach(buffer, buffer_size);
char msg1[7] = &amp;quot;Hello&amp;quot;;
double msg2[2] = {1.0, 2.0};
char rmsg1[7];
double rmsg2[2];
if (myid == 0)
{
MPI_Bsend(msg1, 7, MPI_CHAR, 1, 1, MPI_COMM_WORLD);
MPI_Bsend(msg2, 2, MPI_DOUBLE, 1, 2, MPI_COMM_WORLD);
std::cout &amp;lt;&amp;lt; &amp;quot;Send msg1: &amp;quot; &amp;lt;&amp;lt; msg1 &amp;lt;&amp;lt; std::endl;
std::cout &amp;lt;&amp;lt; &amp;quot;Send msg2: &amp;quot; &amp;lt;&amp;lt; msg2[0] &amp;lt;&amp;lt; &amp;quot; &amp;quot; &amp;lt;&amp;lt; msg2[1] &amp;lt;&amp;lt; std::endl;
}
else if (myid == 1)
{
// sleep 10s
std::this_thread::sleep_for(std::chrono::seconds(10));
MPI_Recv(rmsg1, 7, MPI_CHAR, 0, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
MPI_Recv(rmsg2, 2, MPI_DOUBLE, 0, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
std::cout &amp;lt;&amp;lt; &amp;quot;Receive msg1: &amp;quot; &amp;lt;&amp;lt; rmsg1 &amp;lt;&amp;lt; std::endl;
std::cout &amp;lt;&amp;lt; &amp;quot;Receive msg2: &amp;quot; &amp;lt;&amp;lt; rmsg2[0] &amp;lt;&amp;lt; &amp;quot; &amp;quot; &amp;lt;&amp;lt; rmsg2[1] &amp;lt;&amp;lt; std::endl;
}
MPI_Buffer_detach(&amp;amp;buffer, &amp;amp;buffer_size);
free(buffer);
MPI_Finalize();
return 0;
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>运行结果：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-shell">root@ubuntu:~# mpicxx -o mpi mpi.cpp
root@ubuntu:~# mpirun -n 2 ./mpi
Send msg1: Hello
Send msg2: 1 2
Receive msg1: Hello
Receive msg2: 1 2
&lt;/code>&lt;/pre>
&lt;h2 id="4-就绪通信模式">4. 就绪通信模式&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>发送请求仅当有匹配的接收后才能发出，否则出错&lt;/strong>。在就绪模式下，系统默认与其相匹配的接收已经调用。 接收必须先于发送。&lt;/li>
&lt;li>它不可以不依赖于接收方的匹配的接收请求而任意发出&lt;/li>
&lt;li>其函数调用形式为：&lt;code>MPI_RSend()&lt;/code>。R 表示就绪，仅当对方的接收操作启动并准备就绪时，才可以发送数据。&lt;/li>
&lt;/ul>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 3：就绪通信模式&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-cpp">#include &amp;lt;mpi.h&amp;gt;
#include &amp;lt;iostream&amp;gt;
int main(int argc, char **argv)
{
int myid, numprocs;
// 初始化
MPI_Init(&amp;amp;argc, &amp;amp;argv);
MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;myid);
MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;numprocs);
MPI_Status status;
int buffer[10];
if (myid == 0)
{
for (int i = 0; i &amp;lt; 10; i++)
{
buffer[i] = -1;
}
MPI_Recv(buffer, 10, MPI_INT, 1, 1, MPI_COMM_WORLD, &amp;amp;status);
for (int i = 0; i &amp;lt; 10; i++)
{
if (buffer[i] != i)
{
std::cout &amp;lt;&amp;lt; &amp;quot;error&amp;quot; &amp;lt;&amp;lt; std::endl;
break;
}
}
}
else if (myid == 1)
{
for (int i = 0; i &amp;lt; 10; i++)
{
buffer[i] = i;
}
MPI_Rsend(buffer, 10, MPI_INT, 0, 1, MPI_COMM_WORLD);
}
MPI_Finalize();
return 0;
}
&lt;/code>&lt;/pre>
&lt;h2 id="5-同步通信模式">5. 同步通信模式&lt;/h2>
&lt;ul>
&lt;li>本质特征：收方接收该消息的缓冲区已准备好， 不需要附加的系统缓冲区&lt;/li>
&lt;li>任意发出：发送请求可以不依赖于收方的匹配的接收请求而任意发出&lt;/li>
&lt;li>成功结束： 仅当收方已发出接收该消息的请求后才成功返回，否则将阻塞。意味着：
&lt;ul>
&lt;li>发送方缓冲区可以重用&lt;/li>
&lt;li>收方已发出接收请求&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>是非本地的&lt;/li>
&lt;li>函数调用形式为：&lt;code>MPI_Ssend()&lt;/code>。S 表示同步。&lt;/li>
&lt;/ul>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 4：同步通信模式&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-cpp">#include &amp;lt;mpi.h&amp;gt;
#include &amp;lt;iostream&amp;gt;
int main(int argc, char **argv)
{
int myid, numprocs;
// 初始化
MPI_Init(&amp;amp;argc, &amp;amp;argv);
MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;myid);
MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;numprocs);
MPI_Status status;
int buffer[10];
if (myid == 0)
{
for (int i = 0; i &amp;lt; 10; i++)
{
buffer[i] = -1;
}
MPI_Recv(buffer, 10, MPI_INT, 1, 1, MPI_COMM_WORLD, &amp;amp;status);
std::cout &amp;lt;&amp;lt; &amp;quot;process &amp;quot; &amp;lt;&amp;lt; myid &amp;lt;&amp;lt; &amp;quot; receiving...&amp;quot; &amp;lt;&amp;lt; std::endl;
for (int i = 0; i &amp;lt; 10; i++)
{
if (buffer[i] != i)
{
std::cout &amp;lt;&amp;lt; &amp;quot;error&amp;quot; &amp;lt;&amp;lt; std::endl;
break;
}
}
}
else if (myid == 1)
{
for (int i = 0; i &amp;lt; 10; i++)
{
buffer[i] = i;
}
std::cout &amp;lt;&amp;lt; &amp;quot;process &amp;quot; &amp;lt;&amp;lt; myid &amp;lt;&amp;lt; &amp;quot; sending...&amp;quot; &amp;lt;&amp;lt; std::endl;
MPI_Ssend(buffer, 10, MPI_INT, 0, 1, MPI_COMM_WORLD);
}
MPI_Finalize();
return 0;
}
&lt;/code>&lt;/pre>
&lt;h2 id="6-阻塞通信与非阻塞通信">6. 阻塞通信与非阻塞通信&lt;/h2>
&lt;p>阻塞通信调用时，整个程序只能执行通信相关的内容，而无法执行计算相关的内容；非阻塞调用的初衷是尽量让通信和计算重叠进行，提高程序整体执行效率。&lt;/p>
&lt;p>非阻塞通信调用返回意味着通信开始启动；而非阻塞通信完成则需要调用其他的接口来查询。&lt;/p>
&lt;ul>
&lt;li>非阻塞通信的调用接口&lt;/li>
&lt;li>非阻塞通信的完成查询接口&lt;/li>
&lt;/ul>
&lt;p>非阻塞通信的&lt;strong>发送&lt;/strong>和&lt;strong>接受&lt;/strong>过程都需要同时具备以上两个要素：调用与完成。（1）”调用“按照通信方式的不同（标准、缓存、同步、就绪），有各种函数接口；（2）”完成“是重点，因为程序员需要知道非阻塞调用是否执行完成了，来做下一步的操作。&lt;/p>
&lt;p>MPI 为“完成”定义了一个内部变量 MPI_Request request，每个 request 与一个在非阻塞调用发生时与该调用发生关联（这里的调用包括发送和接收）。“完成”不区分通信方式的不同，统一用 MPI_Wait 系列函数来完成。&lt;/p>
&lt;ol>
&lt;li>&lt;code>MPI_Wait(MPI_Request *request)&lt;/code>，均等着 request 执行完毕了，再往下进行&lt;/li>
&lt;li>对于非重复非阻塞通信，&lt;code>MPI_Wait&lt;/code> 系列函数调用的返回，还意味着 request 对象被释放了，程序员不用再显式释放 request 变量。&lt;/li>
&lt;li>对于重复非阻塞通信，&lt;code>MPI_Wait&lt;/code> 系列函数调用的返回，意味着将于 request 对象关联的非阻塞通信处于不激活状态，并不释放 request&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>MPI_Wait 会迫使进程进入“阻塞模式”。发送过程将简单地等待请求完成。如果进程在 MPI_Isend 之后立即等待，则 Send 与调用 MPI_Send 相同。等待 MPI_WAIT 和 MPI_WAITANY 有两种方式&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-cpp">int MPI_Wait(MPI_Request *request, MPI_Status *status);
int MPI_Waitany(int count, MPI_Request array_of_requests[], int *index, MPI_Status *status);
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>
&lt;p>前者 MPI_WAIT 只是等待给定请求的完成。请求一完成，就会返回一个状态为 MPI_STATUS 的实例。后者&lt;code>MPI_Waitany&lt;/code> 等待一系列请求中的第一个完成的请求继续。一旦请求完成，INDEX 的值被设置为存储 ARRAY_OF_REQUESTS 中已完成请求的索引。该调用还存储已完成请求的状态。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>对于阻塞通信，如果不想跟踪此信息，可以将指向 MPI_STATUS 实例的指针替换为 MPI_STATUS_IGNORE。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>正如我们之前看到的，等待会阻止进程，直到请求（或某个请求）被满足。测试则是检查请求是否可以完成。如果可以，请求将自动完成并传输数据。关于等待，有两种测试等待：MPI_Test 和 MPI_Testany。它们的调用方式如下&lt;/p>
&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-cpp">int MPI_Test(MPI_Request *request, int *flag, MPI_Status *status);
int MPI_Testany(int count, MPI_Request array_of_requests[], int *index, int *flag, MPI_Status *status);
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>让我们从 MPI_Test 开始。至于 MPI_Wait，参数 request 和 status 并不神秘。请记住，测试是非阻塞的，因此在任何情况下，调用后进程都会继续执行。变量 flag 的作用是告诉你请求是否在测试过程中完成。如果 flag != 0 表示请求已完成&lt;/li>
&lt;li>MPI_Testany 现在应该是完全显而易见的。如果有任何请求可完成，它会将 FLAG 设置为非零值。如果是这样的话，状态和索引也被赋予一个值&lt;/li>
&lt;/ul>
&lt;h2 id="7-非阻塞的发送和接收">7. 非阻塞的发送和接收&lt;/h2>
&lt;ul>
&lt;li>&lt;code>int MPI_Isend(void *buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm, MPI_Request *request)&lt;/code>
&lt;ul>
&lt;li>buf: 发送缓冲区的起始地址&lt;/li>
&lt;li>count: 发送数据的个数&lt;/li>
&lt;li>datatype: 发送数据的类型&lt;/li>
&lt;li>dest: 目标进程的 rank&lt;/li>
&lt;li>tag: 消息标签&lt;/li>
&lt;li>comm: 通信子&lt;/li>
&lt;li>request: 非阻塞通信完成对象&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>MPI_Ibsend/MPI_Issend/MPI_Irsend&lt;/code>: 缓冲/同步/就绪通信的非阻塞发送&lt;/li>
&lt;li>&lt;code>int MPI_Irecv(void *buf, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Request *request)&lt;/code>&lt;/li>
&lt;/ul>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 5：非阻塞通信&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-cpp">void play_non_blocking_scenario()
{
MPI_Request request;
MPI_Status status;
int request_finished = 0;
// Initialising buffer :
for (int i = 0; i &amp;lt; buffer_count; ++i)
{
buffer[i] = (rank == 0 ? i * 2 : 0);
}
MPI_Barrier(MPI_COMM_WORLD);
// Starting the chronometer
double time = -MPI_Wtime();
if (rank == 0)
{
sleep(3);
// 1- Initialise the non-blocking send to process 1
// [...]
MPI_Isend(buffer, buffer_count, MPI_INT, 1, 0, MPI_COMM_WORLD, &amp;amp;request);
double time_left = 6000.0;
while (time_left &amp;gt; 0.0)
{
usleep(1000); // We work for 1ms
// 2- Test if the request is finished (only if not already finished)
// [...]
if (!request_finished)
{
MPI_Test(&amp;amp;request, &amp;amp;request_finished, &amp;amp;status);
}
// 1ms left to work
time_left -= 1000.0;
}
// 3- If the request is not yet complete, wait here.
// [...]
if (!request_finished)
{
MPI_Wait(&amp;amp;request, &amp;amp;status);
}
// Modifying the buffer for second step
for (int i = 0; i &amp;lt; buffer_count; ++i)
{
buffer[i] = -i;
}
// 4- Prepare another request for process 1 with a different tag
// [...]
MPI_Isend(buffer, buffer_count, MPI_INT, 1, 1, MPI_COMM_WORLD, &amp;amp;request);
time_left = 3000.0;
while (time_left &amp;gt; 0.0)
{
usleep(1000); // We work for 1ms
// 5- Test if the request is finished (only if not already finished)
// [...]
if (!request_finished)
{
MPI_Wait(&amp;amp;request, &amp;amp;status);
}
// 1ms left to work
time_left -= 1000.0;
}
// 6- Wait for it to finish
// [...]
if (!request_finished)
{
MPI_Wait(&amp;amp;request, &amp;amp;status);
}
}
else
{
// Work for 5 seconds
sleep(5);
// 7- Initialise the non-blocking receive from process 0
// [...]
MPI_Irecv(buffer, buffer_count, MPI_INT, 0, 0, MPI_COMM_WORLD, &amp;amp;request);
// 8- Wait here for the request to be completed
// [...]
MPI_Wait(&amp;amp;request, &amp;amp;status);
print_buffer();
// Work for 3 seconds
sleep(3);
// 9- Initialise another non-blocking receive
// [...]
MPI_Irecv(buffer, buffer_count, MPI_INT, 0, 1, MPI_COMM_WORLD, &amp;amp;request);
// 10- Wait for it to be completed
// [...]
MPI_Wait(&amp;amp;request, &amp;amp;status);
print_buffer();
}
// Stopping the chronometer
time += MPI_Wtime();
// This line gives us the maximum time elapsed on each process.
double final_time;
MPI_Reduce(&amp;amp;time, &amp;amp;final_time, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);
if (rank == 0)
{
std::cout &amp;lt;&amp;lt; &amp;quot;Total time for non-blocking scenario : &amp;quot; &amp;lt;&amp;lt; final_time &amp;lt;&amp;lt; &amp;quot;s&amp;quot; &amp;lt;&amp;lt; std::endl;
}
}
&lt;/code>&lt;/pre>
&lt;h2 id="8-探测-probe">8. 探测 Probe&lt;/h2>
&lt;ul>
&lt;li>探测实际上非常有用，它有很多用途，例如获取即将接收的元素数量、接收进程的 ID 和标记，或者是否真的接收到了任何信息。&lt;/li>
&lt;li>用于探测的函数有两个：MPI_Probe 和 MPI_IProbe。第一个是阻塞调用，而第二个则不是。现在，MPI_Probe 只会给出与收到的下一条消息相关的 MPI_Status 值，该消息对应于某个标签和 ID。如果想探测任何类型或来自任何来源的消息接收情况，可以使用 MPI_ANY_SOURCE 和 MPI_ANY_TAG。然后，可以将生成的 MPI_Status 对象与其他函数结合使用，以获取更多信息。&lt;/li>
&lt;/ul>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 6：探测&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-cpp">void probing_process(int &amp;amp;int_sum, float &amp;amp;float_sum) {
MPI_Status status;
// 1- Probe the incoming message
MPI_Probe(MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &amp;amp;status);
// 2- Get the tag and the source
int tag = status.MPI_TAG;
int source = status.MPI_SOURCE;
// Printing the message
std::cout &amp;lt;&amp;lt; &amp;quot;Received a message from process &amp;quot; &amp;lt;&amp;lt; source &amp;lt;&amp;lt; &amp;quot; with tag &amp;quot; &amp;lt;&amp;lt; tag &amp;lt;&amp;lt; std::endl;
// 3- Add to int_sum or float_sum depending on the tag of the message
if (tag == 0) {
int other;
MPI_Recv(&amp;amp;other, 1, MPI_INT, source, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
int_sum += other;
} else if (tag == 1) {
float other;
MPI_Recv(&amp;amp;other, 1, MPI_FLOAT, source, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
float_sum += other;
}
}
&lt;/code>&lt;/pre>
&lt;h2 id="9-总结">9. 总结&lt;/h2>
&lt;p>理解各种模式通信过程的行为，关键是弄清楚各个模式对缓冲使用的方式。简而言之，各个模式使用缓冲的特点可总结为：标准的 Send 实际利用了 MPI 环境提供的默认缓冲区；Bsend 实际相当于将 MPI 环境提供的 buffer 放在用户空间管理；Rsend 实际相当于不要缓冲区，但发送端不能提前等待；Ssend 实际也相当于不要缓冲区，但允许等待；异步方式下各个模式工作原理也是类似的，只不过可将其理解为 MPI 环境会另起一个线程在后台做实际的消息传输，通过 Wait、Test 等机制与 MPI 进程的主线程进行通信和同步。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">通信模式&lt;/th>
&lt;th style="text-align:center">阻塞型&lt;/th>
&lt;th style="text-align:center">非阻塞型&lt;/th>
&lt;th style="text-align:center">缓冲方式&lt;/th>
&lt;th style="text-align:center">发送方等待&lt;/th>
&lt;th style="text-align:center">接收方等待&lt;/th>
&lt;th style="text-align:center">是否本地&lt;/th>
&lt;th style="text-align:center">是否阻塞&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">标准通信&lt;/td>
&lt;td style="text-align:center">MPI_Send&lt;/td>
&lt;td style="text-align:center">MPI_Isend&lt;/td>
&lt;td style="text-align:center">MPI 环境提供的默认缓冲区&lt;/td>
&lt;td style="text-align:center">是&lt;/td>
&lt;td style="text-align:center">是&lt;/td>
&lt;td style="text-align:center">是&lt;/td>
&lt;td style="text-align:center">是&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">缓冲通信&lt;/td>
&lt;td style="text-align:center">MPI_Bsend&lt;/td>
&lt;td style="text-align:center">MPI_Ibsend&lt;/td>
&lt;td style="text-align:center">用户空间管理&lt;/td>
&lt;td style="text-align:center">否&lt;/td>
&lt;td style="text-align:center">是&lt;/td>
&lt;td style="text-align:center">是&lt;/td>
&lt;td style="text-align:center">是&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">就绪通信&lt;/td>
&lt;td style="text-align:center">MPI_Rsend&lt;/td>
&lt;td style="text-align:center">MPI_Irsend&lt;/td>
&lt;td style="text-align:center">无&lt;/td>
&lt;td style="text-align:center">否&lt;/td>
&lt;td style="text-align:center">是&lt;/td>
&lt;td style="text-align:center">否&lt;/td>
&lt;td style="text-align:center">是&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">同步通信&lt;/td>
&lt;td style="text-align:center">MPI_Ssend&lt;/td>
&lt;td style="text-align:center">MPI_Issend&lt;/td>
&lt;td style="text-align:center">无&lt;/td>
&lt;td style="text-align:center">否&lt;/td>
&lt;td style="text-align:center">是&lt;/td>
&lt;td style="text-align:center">否&lt;/td>
&lt;td style="text-align:center">是&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item><item><title>MPI 与并行计算（一）：并行环境及编程模型</title><link>https://cuterwrite.top/p/mpi-tutorial/1/</link><pubDate>Tue, 18 Jul 2023 00:00:00 +0000</pubDate><guid>https://cuterwrite.top/p/mpi-tutorial/1/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230719012753.webp" alt="Featured image of post MPI 与并行计算（一）：并行环境及编程模型" />&lt;h1 id="mpi-与并行计算一并行环境及编程模型">MPI 与并行计算（一）：并行环境及编程模型&lt;/h1>
&lt;h2 id="1-什么是-mpi">1. 什么是 MPI&lt;/h2>
&lt;p>Massage Passing Interface：是消息传递函数库的标准规范，由 MPI 论坛开发，支持 Fortran 和 C。&lt;/p>
&lt;ol>
&lt;li>一种新的库描述，不是一种语言。共有上百个函数调用接口，在 Fortran 和 C 语言中可以直接对这些函数进行调用。&lt;/li>
&lt;li>MPI 是一种标准或规范的代表，而不是特指某一个对它的具体实现。迄今为止所有的并行计算机制造商都提供对 MPI 的支持。
&lt;ul>
&lt;li>Intel MPI&lt;/li>
&lt;li>OpenMPI&lt;/li>
&lt;li>mpich&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>MPI 是一种消息传递编程模型，并成为这种编程模型的代表和事实上的标准。&lt;/li>
&lt;/ol>
&lt;h2 id="2-mpi-的发展过程">2. MPI 的发展过程&lt;/h2>
&lt;ul>
&lt;li>MPI 1.1：1995
&lt;ul>
&lt;li>MPICH:是 MPI 最流行的非专利实现,由 Argonne 国家实验室和密西西比州立大学联合开发,具有更好的可移植性.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>MPI 1.2~2.0：动态进程, 并行 I/O, 支持 F90 和 C++(1997)&lt;/li>
&lt;/ul>
&lt;h2 id="3-为什么要用-mpi">3. 为什么要用 MPI&lt;/h2>
&lt;ul>
&lt;li>高可移植性：MPI 已在 IBM PC 机上、 MS Windows 上、所有主要的 Unix 工作站上和所有主流的并行机上得到实现。使用 MPI 作消息传递的 C 或 Fortran 并行程序可不加改变地运行在 IBMPC、 MS Windows、 Unix 工作站、以及各种并行机上。&lt;/li>
&lt;/ul>
&lt;h2 id="4-并行编程模式">4. 并行编程模式&lt;/h2>
&lt;ul>
&lt;li>隐式并行：借助编译器和运行时环境的支持发掘程序的并行性，对串行程序进行并行化。&lt;/li>
&lt;li>数据并行：数据并行依靠所处理的数据集合无关性，借助数据划分来驱动程序之间的并行执行。&lt;/li>
&lt;li>消息传递：消息传递模型可以通过如下的几个概念加以定义：
&lt;ul>
&lt;li>一组仅有本地内存空间的进程&lt;/li>
&lt;li>进程之间通过发送和接收消息进行通信&lt;/li>
&lt;li>进程之间需要使用协同操作完成数据传递，如发送操作必须要求有与之配对的接收操作&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>共享变量：并行代码分别驻留在不同的处理器上，通过读写公共存储器中的共享变量进行同步和通信，一般适合在多核系统，SMP 系统上运行。分布式存储系统可以在运行时库支持下通过自定义机制以共享变量的方式运行。&lt;/li>
&lt;/ul>
&lt;h2 id="5-mpi-的工作模式">5. MPI 的工作模式&lt;/h2>
&lt;ol>
&lt;li>运行方式：以串行方式编写，运行时分别执行不同的块。&lt;/li>
&lt;li>资源分配：所有程序元素只要没有进行显示区分，无论是代码、函数、全局变量还是局部变量，都默认的由全部进程共同拥有，所有进程看到的虽然是相同的名字，但在“物理”上却彼此无关。&lt;/li>
&lt;li>显示区分：就是指程序员需在程序设计阶段通过显示的条件判断来指定在不同进程上运行不同的代码块。这正是 MPI 程序的特点，也恰好是难点之一。&lt;/li>
&lt;/ol>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>一个典型的 MPI 程序代码模式如下：&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">// Inititalization code block
if (rank == process1) {
// define works to be carried out by process1
} else if (rank == process2) {
// define works to be carried out by process2
} else if (rank == process3) {
// define works to be carried out by process3
} else if (rank == process4) {
// define works to be carried out by process4
}...else if (rank == processn) {
// define works to be carried out by processn
} else {
// define works to be carried out by all processes
}
&lt;/code>&lt;/pre>
&lt;h2 id="6-mpi-消息传递通信的基本概念">6. MPI 消息传递通信的基本概念&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>消息：一个消息可以比做一个封信。需要定义消息的内容以及消息的发送与接收者。前者称为消息的缓冲(Message Buffer)，后者称为消息信封(Message Envelop)。在 MPI 中，消息缓冲由三元组&amp;lt;起始地址，数据个数，数据类型&amp;gt;来标识，而消息信封则是由三元组&amp;lt;源/目标进程，消息标签，通讯域&amp;gt;来标识。如下为&lt;code>MPI_Send&lt;/code> 的消息缓冲和消息信封。
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/blog/20230718183355.webp"
alt="20230718183355" width="auto" loading="lazy"/>
&lt;/figure>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>缓冲区：MPI 环境定义了 3 种缓冲区：应用缓冲区，系统缓冲区和用户向系统注册的缓冲区。&lt;/p>
&lt;ul>
&lt;li>应用缓冲区：保存将要发送或接收的消息内容即上述的消息缓冲。&lt;/li>
&lt;li>系统缓冲区：MPI 环境为通信所准备的存储空间。&lt;/li>
&lt;li>用户缓冲区：指用户向系统注册的缓冲区，用户使用某些 API(如&lt;code>MPI_Bsend&lt;/code>)时，在程序中显示申请的存储空间，然后注册到 MPI 环境中供通信所用。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>通信子：MPI 环境管理进程及通信的基本设施。&lt;code>MPI_COMM_WORLD&lt;/code> 就是 MPI 环境启动时默认创建的通信子。对某个进程的操作必须放在通信子内方可有效。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>进程号：进程号即进程的&lt;code>rank&lt;/code>，指在某个通信子内某个进程号 rank 为 num。在一个通信子内，每一个进程都有它&lt;!-- raw HTML omitted -->唯一&lt;!-- raw HTML omitted -->的 num，这个标识号是在进程初始化时由系统分配，从 0 开始编号。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>进程组：定义一个通信子，也就指定了一组共享该空间的进程，这些进程组成了该通信子的进程组(group)。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>通信协议：MPI 环境依据实现的策略不同，可能采用如下一种或几种协议。&lt;/p>
&lt;ul>
&lt;li>立即通信协议，总是假定目标进程具有保存消息数据的能力。&lt;/li>
&lt;li>集中通信协议，在目标准备好之后，才可以执行发送动作。&lt;/li>
&lt;li>短消息协议，消息数据与信封封装在一起发送。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="7-mpi-程序编译运行">7. MPI 程序编译、运行&lt;/h2>
&lt;ul>
&lt;li>MPI 环境安装：
&lt;ul>
&lt;li>更新 apt 源：&lt;code>sudo apt-get update&lt;/code>&lt;/li>
&lt;li>安装 build-essential：&lt;code>sudo apt-get install -y build-essential&lt;/code>&lt;/li>
&lt;li>安装 mpich: &lt;code>sudo apt-get install -y mpich&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>MPI 程序编译：mpicc
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;/th>
&lt;th style="text-align:center">Intel MPI&lt;/th>
&lt;th style="text-align:center">Mpich/OpenMPI&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">Fortran&lt;/td>
&lt;td style="text-align:center">mpiifort&lt;/td>
&lt;td style="text-align:center">mpi90&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">C&lt;/td>
&lt;td style="text-align:center">mpiicc&lt;/td>
&lt;td style="text-align:center">mpicc&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">C++&lt;/td>
&lt;td style="text-align:center">mpiicpc&lt;/td>
&lt;td style="text-align:center">mpicxx&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/li>
&lt;/ul>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>mpicc 编译示例&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-bash">mpicc -o mpi mpi.c
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>MPI 程序运行：mpirun
&lt;ul>
&lt;li>使用 mpirun 来运行 mpi 程序（intel mpi、mpich、openmpi 等）&lt;/li>
&lt;li>用法：mpirun -n 进程数 可执行文件名&lt;/li>
&lt;li>示例：&lt;code>mpirun -n 2 ./example&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>第一个 MPI 程序示例 - Hello World&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">// mpi.c
#include &amp;lt;mpi.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
int main(int argc, char **argv)
{
MPI_Init(&amp;amp;argc, &amp;amp;argv);
printf(&amp;quot;Hello World!\n&amp;quot;);
MPI_Finalize();
return 0;
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>编译与运行 mpi.c&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">mpicc -o mpi mpi.c
mpirun -n 2 ./mpi
&lt;/code>&lt;/pre>
&lt;div class="notice notice-warning" >
&lt;div class="notice-title">&lt;svg t="1705945674099" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="16953" width="200" height="200">&lt;path d="M512 106.666667a405.333333 405.333333 0 1 0 405.333333 405.333333A405.333333 405.333333 0 0 0 512 106.666667z m120.533333 489.6a25.621333 25.621333 0 0 1 0 36.266666 25.749333 25.749333 0 0 1-36.266666 0L512 548.266667l-84.266667 84.266666a25.749333 25.749333 0 0 1-36.266666 0 25.621333 25.621333 0 0 1 0-36.266666L475.733333 512l-84.266666-84.266667a25.642667 25.642667 0 0 1 36.266666-36.266666L512 475.733333l84.266667-84.266666a25.642667 25.642667 0 0 1 36.266666 36.266666L548.266667 512z" fill="#ffffff" p-id="16954">&lt;/path>&lt;/svg>&lt;/div>&lt;p>注意： root 用户运行 mpirun 时，需要加上&amp;ndash;allow-run-as-root 参数，否则会报错。&lt;/p>&lt;/div>
&lt;h2 id="8-mpi-的四个基本接口">8. MPI 的四个基本接口&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">接口名&lt;/th>
&lt;th style="text-align:center">功能&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">MPI_Init(&amp;amp;argc, &amp;amp;argv)&lt;/td>
&lt;td style="text-align:center">初始化 MPI 环境，MPI 系统将通过 argc, argv 得到命令行参数&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;myrank)&lt;/td>
&lt;td style="text-align:center">缺省的通信子为 MPI_COMMON_WORLD，获得进程所在缺省通信子的编号，赋值给 myrank&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;nprocs)&lt;/td>
&lt;td style="text-align:center">获得缺省通信子中进程的个数，赋值给 nprocs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">MPI_Finalize()&lt;/td>
&lt;td style="text-align:center">一般放在程序最后一行，如果没有此行，MPI 程序将不会终止&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>MPI 初始化：MPI_Init
&lt;ul>
&lt;li>int MPI_Init(int *argc, char ***argv)&lt;/li>
&lt;li>MPI_Init 是 MPI 程序的第一个调用，它完成 MPI 程序的所有初始化工作。所有的 MPI 程序的第一条可执行语句都是这条语句。&lt;/li>
&lt;li>启动 MPI 环境,标志并行代码的开始.&lt;/li>
&lt;li>要求 main 必须带参数运行,否则出错&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>MPI 结束：MPI_Finalize
&lt;ul>
&lt;li>int MPI_Finalize(void)&lt;/li>
&lt;li>MPI_FINALIZE 是 MPI 程序的最后一个调用，它结束 MPI 程序的运行，它是 MPI 程序的最后一条可执行语句，否则程序的运行结果是不可预知的。&lt;/li>
&lt;li>标志并行代码的结束,结束除主进程外其它进程。&lt;/li>
&lt;li>之后串行代码仍可在主进程(rank = 0)上运行(如果必须)。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>进程号：MPI_Comm_rank
&lt;ul>
&lt;li>int MPI_Comm_rank(MPI_Comm comm, int *rank)&lt;/li>
&lt;li>MPI_COMM_RANK 是 MPI 程序中的一个重要函数，它返回调用进程在通信子中的进程号，即 rank。&lt;/li>
&lt;li>通信子：MPI_COMM_WORLD&lt;/li>
&lt;li>进程号：rank&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>进程数：MPI_Comm_size
&lt;ul>
&lt;li>int MPI_Comm_size(MPI_Comm comm, int *size)&lt;/li>
&lt;li>MPI_COMM_SIZE 是 MPI 程序中的一个重要函数，它返回通信子中的进程数，即 size。&lt;/li>
&lt;li>通信子：MPI_COMM_WORLD&lt;/li>
&lt;li>进程数：size&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 2：打印进程 ID 和进程数&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">// mpi.c
#include &amp;lt;mpi.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
int main(int argc, char **argv)
{
int myrank, nprocs;
MPI_Init(&amp;amp;argc, &amp;amp;argv);
MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;myrank);
MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;nprocs);
printf(&amp;quot;Hello World! I'm %d of %d\n&amp;quot;, myrank, nprocs);
MPI_Finalize();
return 0;
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>结果：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-txt">root@ubuntu:~# mpicc -o mpi mpi.c
root@ubuntu:~# mpirun -n 2 ./mpi
Hello World! I'm 0 of 2
Hello World! I'm 1 of 2
&lt;/code>&lt;/pre></description></item></channel></rss>