[{"content":"\r#\rImplement Local RAG Service: Integrate Open WebUI, Ollama, and Qwen2.5\r#\rIntroduction\rWhen building information retrieval and generative AI applications, the Retrieval-Augmented Generation (RAG) model is increasingly favored by developers for its powerful ability to retrieve relevant information from a knowledge base and generate accurate answers. However, to implement an end-to-end local RAG service, not only is an appropriate model required, but also the integration of a robust user interface and an efficient inference framework.\nWhen building a local RAG service, using the easily deployable Docker method can greatly simplify model management and service integration. Here, we rely on the user interface and model inference service provided by Open WebUI, and then introduce the bge-m3 embedding model through Ollama to achieve document vectorization-based retrieval functionality, thereby helping Qwen2.5 generate more accurate answers.\nIn this article, we will discuss how to quickly start Open WebUI through Docker, synchronize Ollama\u0026rsquo;s RAG capabilities, and combine the Qwen2.5 model to achieve an efficient document retrieval and generation system.\n#\rProject Overview\rThis project will use the following key tools:\nOpen WebUI: Provides a web interface for user interaction with the model. Ollama: Used for managing embedding and large language model inference tasks. Among them, the bge-m3 model in Ollama will be used for document retrieval, and Qwen2.5 will be responsible for answer generation. Qwen2.5: The model part uses the Qwen 2.5 series launched by Alibaba, providing natural language generation for retrieval-augmented generation services. In order to implement the RAG service, we need the following steps:\nDeploy Open WebUI as the user interaction interface. Configure Ollama for efficient scheduling of the Qwen2.5 series models. Use the embedding model named bge-m3 configured by Ollama to implement retrieval vectorization. #\rDeploy Open WebUI\rOpen WebUI provides a simple Docker-based solution, allowing users to launch the web interface directly via Docker without manually configuring numerous dependencies.\nFirst, make sure that Docker\ris installed on the server. If it is not installed, you can quickly install it using the following command:\ncurl https://get.docker.com | sh\rThen create a directory to save the Open WebUI data, so the data will not be lost after the project is updated:\nsudo mkdir -p /DATA/open-webui\rNext, we can start Open WebUI with the following command:\ndocker run -d -p 3000:8080 \\\r--add-host=host.docker.internal:host-gateway \\\r-v /DATA/open-webui:/app/backend/data \\\r--name open-webui \\\r--restart always \\\rghcr.io/open-webui/open-webui:main\rIf you want to run Open WebUI with Nvidia GPU support, you can use the following command:\ndocker run -d -p 3000:8080 \\\r--gpus all \\\r--add-host=host.docker.internal:host-gateway \\\r-v /DATA/open-webui:/app/backend/data \\\r--name open-webui \\\r--restart always \\\rghcr.io/open-webui/open-webui:cuda\rHere we expose the Open WebUI service on port 3000 of the machine, which can be accessed via a browser at http://localhost:3000 (for remote access, use the public IP and open port 3000). /DATA/open-webui is the data storage directory, you can adjust this path as needed.\nOf course, besides the Docker installation method, you can also install Open WebUI via pip, source code compilation, Podman, and other methods. For more installation methods, please refer to the Open WebUI official documentation\r.\n#\rBasic Settings\rEnter the account information to be registered, set a strong password!!! The first registered user will be automatically set as the system administrator, so please ensure you are the first registered user.\nClick the avatar in the lower left corner, select the Admin Panel Click Settings in the panel Disable allowing new user registrations (optional) Click Save in the lower right corner #\rConfigure Ollama and Qwen2.5\r#\rDeploy Ollama\rInstall Ollama on the local server. Currently, Ollama offers multiple installation methods. Please refer to Ollama\u0026rsquo;s official documentation\rto download and install the latest version 0.3.11 (Qwen2.5 is only supported starting from this version). For installation details, you can refer to an article I wrote earlier: Ollama: From Beginner to Advanced\r.\nStart the Ollama service (not needed if started via Docker, but the 11434 port must be exposed):\nollama serve\rAfter the Ollama service starts, you can connect to the Ollama service by visiting http://localhost:11434.\nOllama Library\rprovides semantic vector models (bge-m3) as well as major text generation models (including Qwen2.5). Next, we will configure Ollama to meet the needs of this project for document retrieval and question-answer generation.\n#\rDownload Qwen2.5 model\rTo install Qwen2.5 through Ollama, you can directly run the ollama pull command in the command line to download the Qwen2.5 model. For example, to download the 72B model of Qwen2.5, you can use the following command:\nollama pull qwen2.5:72b\rThis command will fetch the Qwen2.5 model from Ollama\u0026rsquo;s model repository and prepare the runtime environment.\nQwen2.5 offers multiple model sizes, including 72B, 32B, 14B, 7B, 3B, 1.5B, 0.5B, etc. You can choose the appropriate model based on your needs and GPU memory size. I am using a server with 4x V100, so I can directly choose the 72B model. If you require faster token generation speed and can tolerate a slight performance loss, you can use the q4_0 quantized version qwen2.5:72b-instruct-q4_0; if you can tolerate slower token generation speed, you can use qwen2.5:72b-instruct-q5_K_M. For a server with 4x V100, although the q5_K_M model\u0026rsquo;s token generation is noticeably laggy, I still chose the q5_K_M model to test the performance of Qwen2.5.\nFor personal computers with less video memory, it is recommended to use the 14B or 7B model, which can be downloaded using the following command:\nollama pull qwen2.5:14b\rOr\nollama pull qwen2.5:7b\rIf you have started both Open WebUI and Ollama services, you can also download the model in the admin panel.\n#\rDownload bge-m3 model\rDownload the bge-m3 model in Ollama, which is used for document vectorization. Run the following command in the command line to download the model (or download it in the Open WebUI interface):\nollama pull bge-m3:latest\rUp to this point, we have completed the configuration of Ollama. Next, we will configure the RAG service in Open WebUI.\n#\rRAG Integration and Configuration\r#\rConfigure Ollama\u0026rsquo;s RAG interface in Open WebUI\r#\rAccess Open WebUI management interface\rAfter starting Open WebUI, you can directly access the service address through a web browser, log in to your administrator account, and then enter the administrator panel.\n#\rSet up Ollama interface\rIn the Open WebUI admin panel, click Settings, you will see the option for external connections, ensure that the address for the Ollama API is host.docker.internal:11434, then click the verify connection button on the right to confirm whether the Ollama service is properly connected.\n#\rSet up semantic vector model\rIn the Open WebUI admin panel, click Settings, then click Documents, and follow these steps:\nSet the semantic vector model engine to Ollama. Set the semantic vector model to bge-m3:latest. The remaining settings can be kept as default. Here, I set the maximum file upload size to 10MB, the maximum number of uploads to 3, Top K to 5, block size and block overlap to 1500 and 100 respectively, and enabled PDF image processing. Click the bottom right corner to save. #\rTest RAG Service\rNow, you have implemented a complete local RAG system. You can enter any natural language question in the main interface of Open WebUI, then upload the corresponding document. The system will call the semantic vector model to vectorize the document, then use the Qwen2.5 model to retrieve the document, generate an answer, and return it to the user.\nIn the Open WebUI user chat interface, upload the document you want to retrieve, then enter your question and click send. Open WebUI will call Ollama\u0026rsquo;s bge-m3 model for document vectorization processing, and then call the Qwen2.5 model for question and answer generation.\nHere I uploaded a simple txt file (text generated by GPT), the content is as follows:\n# 奇幻森林的冒险\r## 引言\r在一个遥远的王国边界，有一片神秘的奇幻森林，传说中栖息着许多奇异的生物和古老的魔法。很少有人敢于进入，因为进入森林的人都没有再回来过。故事的主人公是一个年轻的冒险者，他名叫艾文。\r## 第一章：艾文的决定\r艾文是一个热爱冒险和探索的年轻人，他从小就听过很多关于奇幻森林的故事。尽管家人和朋友都劝他不要去，但他坚定地认为，自己注定要揭开这片森林的秘密。一天清晨，他收拾好行囊，带着勇气和好奇心，向森林进发。\r### 1.1 出发前的准备\r在出发前，艾文去了城里最有名的图书馆，查阅了关于奇幻森林的资料。他发现，有一本古老的手稿记录了进入森林的路线，以及如何避开其中一些危险的生物。艾文将这本手稿复印在自己的笔记本上，准备在需要的时候参考。\r### 1.2 第一次穿越\r艾文刚进入森林就感觉到这里的气息与外界完全不同。空气中弥漫着浓郁的花香，还有隐隐约约的奇怪声音。穿越森林的第一天，艾文没有遇到什么危险，但他能感觉到，有什么东西在暗中观察他。\r## 第二章：神秘生物\r第二天，艾文继续深入森林。然而，他没走多远，就遇到了一只奇异的生物。这是一只会发光的小鹿，全身散发着柔和的蓝色光芒。起初，艾文感到惊讶和畏惧，但这只小鹿却没有攻击他的意思，还带着他走向一个隐秘的洞穴。\r### 2.1 洞穴中的秘密\r在洞穴中，艾文发现了一块古老的石板，石板上刻有一些奇怪的符号。小鹿似乎知道这些符号的含义，带着艾文一步一步地解读。原来，这些符号记载着一种强大的魔法，可以帮助他在森林中找到失落的宝藏。\r### 2.2 获得帮助\r艾文决定接受小鹿的帮助，解开这些符号的秘密。他们在洞穴中度过了几天，艾文学会了如何利用森林中的资源制作药剂和武器。通过这些，他在森林中的生存能力大大提高。\r## 第三章：最终的试炼\r在小鹿的指引下，艾文终于来到了森林的深处，那里有一个古老的祭坛。据说，只有最勇敢的冒险者才能通过祭坛的试炼，获得最终的宝藏。\r### 3.1 面对恐惧\r祭坛周围布满了各种陷阱和幻觉。艾文必须面对自己内心深处的恐惧，才能通过这些障碍。最终，他用智慧和勇气克服了一切，获得了进入祭坛的资格。\r### 3.2 发现宝藏\r在祭坛的中心，艾文发现了一颗闪闪发光的宝石。据传，这颗宝石拥有改变命运的力量。艾文拿起宝石，感受到了其中的强大力量。他知道，这不仅仅是一件珍宝，还有可能是破解奇幻森林秘密的关键。\r## 结论\r艾文成功地揭开了奇幻森林的部分秘密，成为了传说中的英雄。他的冒险故事也激励了更多年轻的冒险者，带着勇气和智慧，踏上探索未知世界的旅程。\rThen three questions were asked separately(in Chinese):\n艾文在森林中遇到的奇异生物是什么？ 艾文在洞穴中找到的古老石板上刻的是什么？ 艾文在祭坛中心发现了什么宝藏？ The following image is the answer:\n#\rSummary\rWith the help of Open WebUI and Ollama, we can easily build an efficient and intuitive local RAG system. By using the bge-m3 semantic vector model for text vectorization, combined with the Qwen2.5 generation model, users can efficiently interact with document retrieval and enhanced generation tasks within a unified web interface. This not only protects data privacy but also significantly enhances the localization capabilities of generative AI.\n","date":"2024-09-20T22:44:00Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_116903285_p0_master1200.webp","permalink":"https://cuterwrite.top/en/p/integrate-open-webui-ollama-qwen25-local-rag/","title":"Implementing Local RAG Service: Integrating Open WebUI, Ollama, and Qwen2.5"},{"content":"\r#\rArm Matrix Acceleration: Scalable Matrix Extension SME\r#\r1. SME Introduction\rScalable Matrix Extension SME is built on the basis of Scalable Vector Extensions (SVE and SVE2) and adds the capability to efficiently handle matrices. The main features include:\nCalculate the SVE vector\u0026rsquo;s outer product Matrix tile storage Loading, storing, inserting, and extracting tile vectors (including dynamic transposition) Streaming SVE mode The table below summarizes the main features of SME, SVE, and SVE2:\nSME SVE SVE2 Streaming SVE Mode NEON DSP++ Scalable Vector Dynamic Matrix Transpose Multi-Precision Arithmetic Per-Lane Predication Vector Cross Product Match Detection and Histogram Gather-load and Scatter-store Load, store, insert, and extract matrix vectors Non-temporal scatter/gather Predict vectorization Bitwise Permute ML Extension (FP16 + DOT) AE, SHA3, SM4, Crypto V8.6 BF16, FP and Int8 support SME has defined the following new features:\nNew architecture state, can be used to store two-dimensional matrix tile Streaming SVE mode, supports SVE2 instructions where the execution vector length matches the tile length. New instruction to accumulate (or decrement) the outer product of two vectors into a matrix tile. New load, store, and move instructions: Vectors can be written to a row or column of a matrix tile, or a row or column of a matrix tile can be read into a vector. Similar to SVE2, SME is also an extension that supports scalable vector length, enabling vector length agnosticism (VLA), per-lane predication, predication-driven loop control, and management functions.\n#\r2. Streaming SVE mode\rSME introduced the Streaming SVE mode, which implements a subset of the SVE2 instruction set and adds new SME-specific instructions.\nStreaming SVE mode supports high-throughput streaming data processing for large datasets, and streaming data usually has simple loop control flow and limited conditionality.\nIn Non-streaming SVE mode, the complete SVE2 instruction set is supported, typically handling complex data structures and complex judgments.\nStreaming SVE Mode and Non-streaming SVE Mode Most SME instructions are only available in Streaming SVE mode. The streaming vector length (SVL) in Streaming SVE mode may differ from the non-streaming vector length (NSVL).\nThe expectation is: SVL should be longer than or equal to NSVL, that is, SVL \u0026gt;= NSVL. For example, the length of NSVL can be 128-bit, while the length of SVL can be 512-bit.\nThe SVL of SME can be 128-bit, 256-bit, 512-bit, 1024-bit, or 2048-bit. SVL needs to be a power of 2, and NSVL needs to be a multiple of 128.\nSimilar to SVE2, the software can control the SMCR_ELx.LEN register bit to set the effective SVL length that EL1, EL2, EL3 want to use (it can be set shorter than the SVL supported by the hardware).\nFor more information on the Streaming SVE mode, refer to section B1.4.6 of the Arm Architecture Reference Manual (A-profile architecture).\n#\r3. Switch between Non-streaming and Streaming SVE modes\rIf the CPU hardware implementation supports both Streaming SVE mode of SME and Non-streaming SVE mode of SVE2, applications can dynamically switch between these two operation modes based on their needs.\nProvide an independent operating mode for SME, allowing CPU hardware implementations to offer different vector lengths for the same application. For example, a CPU hardware implementation can choose to support a longer Streaming SVE mode vector length and optimize the hardware for stream operations suitable for high throughput.\nApplications can easily switch dynamically between Streaming SVE mode and Non-streaming SVE mode. The PSTATE.{SM, ZA} bits introduced by SME can enable and disable Streaming SVE mode and SME ZA storage:\nSM: Enable and disable Streaming SVE mode ZA: Enable and disable ZA storage access You can use the MSR/MRS instructions to operate the Streaming Vector Control Register (SVCR) to set and read the PSTATE.{SM, ZA} bits, with specific operations as follows:\nMSR SVCRSM, #\u0026lt;imm\u0026gt; MSR SVCRSM, # MSR SVCRZA, #\u0026lt;imm\u0026gt; MSR SVCRSMZA, #\u0026lt;imm\u0026gt; The SMSTART instruction is an alias for the MSR instruction that sets PSTATE.SM and PSTATE.ZA.\nSMSTART: Simultaneously enable Streaming SVE mode and ZA storage access SMSTART SM: Enable Streaming SVE mode SMSTART ZA: Enable ZA storage access The SMSTOP instruction is an alias for the MSR instruction that clears PSTATE.SM and PSTATE.ZA.\nSMSTOP: Simultaneously disable Streaming SVE mode and ZA storage access SMSTOP SM: Disable Streaming SVE mode SMSTOP ZA: Disable ZA storage access The diagram below shows how the application switches between Streaming SVE mode and Non-streaming SVE mode:\nApplication switching Streaming SVE mode and Non-streaming SVE mode For more information on switching between Streaming SVE mode and Non-Streaming SVE mode using SMSTART and SMSTOP, please refer to sections C6.2.327 and C6.2.328 of the Arm Architecture Reference Manual on A-profile architecture.\n#\r4. SME Architecture Status\rSimilar to SVE2, in Streaming SVE mode, it has Z0-Z31 vector registers and P0-P15 predicate registers.\nThe lowest numbered SVE vector register Zn also holds fixed-length Vn, Qn, Dn, Sn, Hn, and Bn registers.\nWhen entering Streaming SVE mode (PSTATE.SM changes from 0 to 1) or exiting Streaming SVE mode (PSTATE.SM changes from 1 to 0), all these registers will be zeroed.\nMost non-streaming SVE2 instructions can be used in Streaming SVE mode, but may use different vector lengths (streaming mode uses VSL length, non-streaming mode uses NVSL length). The RDSVL instruction can be used to read the current effective vector length VL.\n// Read multiple of Streaming SVE vector register size to Xd\rRDSVL \u0026lt;Xd\u0026gt;, #\u0026lt;imm\u0026gt;\rBecause SME supports Vector Length Agnostic (VLA), in Streaming SVE mode, software rarely needs to explicitly read the SVL vector length. In Non-streaming SVE mode, the RDSVL instruction is usually used to determine the value of SVL.\n#\r5. ZA array\rThe newly introduced ZA (Z Array, ZA Storage) in SME is a two-dimensional (2D) square array with a size of SVL x SVL. It is called Z Array because the length of its rows and columns is consistent with the Zn registers in Streaming SVE mode.\nZA array For example: If the vector length in Streaming SVE mode is 256-bit, i.e., the length of the Zn register is 256-bit, then the size of ZA is 256/8 bytes x 256/8 bytes.\nThe ZA array can be accessed in the following way:\nZA array vector access ZA tiles ZA tile slices #\r5.1 ZA array vector access\rA row of the ZA array can be accessed as a vector of SVL length, and this vector can contain elements with data type lengths of 8-bit, 16-bit, 32-bit, 64-bit, or 128-bit, such as 32-bit fp32 floating-point numbers.\nZA.B[N], ZA.H[N], ZA.S[N], ZA.D[N], ZA.Q[N]\rAmong them, B, H, S, D, Q represent 8-bit, 16-bit, 32-bit, 64-bit, 128-bit, respectively.\nThe number of ZA array vectors is the same as the number of bytes in SVL. For example, if SLV is 256-bit, then the number of ZA array vectors is 32, and the range of N is from 0 to 31.\nTo support context switching, SME introduces new LDR and STR instructions for loading and storing a ZA array vector from memory.\nLDR ZA[\u0026lt;Wv\u0026gt;, \u0026lt;imm\u0026gt;], [\u0026lt;Xn|SP\u0026gt;{, #\u0026lt;imm\u0026gt;, MUL VL}]\rSTR ZA[\u0026lt;Wv\u0026gt;, \u0026lt;imm\u0026gt;], [\u0026lt;Xn|SP\u0026gt;{, #\u0026lt;imm\u0026gt;, MUL VL}]\r#\r5.2 ZA tiles\rZA tile is a square two-dimensional submatrix within ZA. The width of a ZA tile is always SVL, which is the same as the width of the ZA array.\nHow many usable ZA tiles ZA can be divided into is determined by the size of the data type of the elements:\nElement Data Type Size Tile Quantity Tile Name 8-bit 1 ZA0.B 16-bit 2 ZA0.H-ZA1.H 32-bit 4 ZA0.S-ZA3.S 64-bit 8 ZA0.D-ZA7.D 128-bit 16 ZA0.Q-ZA15.Q When the element data type is 8-bit, ZA can only be accessed as a ZA tile (ZA0.B). When the element data type is 16-bit, ZA can be accessed as 2 ZA tiles (ZA0.H and ZA1.H). When the element data type is 32-bit, ZA can be accessed as 4 ZA tiles (ZA0.S to ZA3.S). When the element data type is 64-bit, ZA can be accessed as 8 ZA tiles (ZA0.D to ZA7.D). When the element data type is 128-bit, ZA can be accessed as 16 ZA tiles (ZA0.Q to ZA15.Q). For example, if SVL is 256-bit and the element data type size is 8-bit, then ZA can be considered as ZA0.B, or it can be seen as 32 vectors (32 rows, each row size is 32 x 8-bit, i.e., 32 elements per row).\nIf SVL is 256-bit and the element data type size is 16-bit, then ZA can be considered as 2 ZA tiles (ZA0.H and ZA1.H), with each tile considered as 16 vectors (16 rows, each row size is 16 x 16-bit, i.e., 16 elements per row).\nThe advantage of doing this is to fully utilize ZA storage. In practical applications, for example, when the SVL is 256-bit, the element data type size is 32-bit, and the size of ZA is 256-bit x 256-bit, to perform an outer product operation on vectors in two Z registers, the outer product result is a 2D array of 8 x 8 floating-point numbers. This outer product only requires 1/4 of the storage space of ZA. By dividing ZA into 4 ZA tiles, ZA storage can be fully utilized.\n#\r5.3 ZA tile slices\rA ZA tile can be accessed as a whole or in the form of individual ZA tile slices.\nWhen accessed as a whole, instructions can be accessed using the name of the tile:\nZA0.B, ZA0.H-ZA1.H, ZA0.S-ZA3.S, ZA0.D-ZA7.D or ZA0.Q-ZA15.Q\rA ZA tile slice is a one-dimensional array composed of continuous elements in the horizontal or vertical direction of its ZA tile, that is, a row or a column in the ZA tile.\nAccessing a vector of a ZA tile is reading and writing a ZA tile slice:\nHorizontal or vertical ZA tile slice access is indicated by the H or V suffix following the ZA tile name. A specific ZA tile slice is represented by an index, indicated by the slice index [N] following the ZA tile name. For example, if the SVL is 128 bits and the element data type size is 8-bit, then its horizontal and vertical ZA tile slice can be represented as shown in the figure below:\nFor example, if the SVL is 128 bits and the element data type size is 16-bit, then its horizontal and vertical ZA tile slice can be represented as shown in the figure below:\nIn order to improve the efficiency of hardware access to ZA tile and ZA tile slices, the ZA tile slices of a ZA tile are interleaved.\nThe image below shows an example of this interleaved arrangement. In this example, SVL is 256 bits, and the element data type size is 16 bits. This means that ZA can be viewed as two ZA tiles (ZA0H and ZA1H) and has interleaved horizontal tile slices:\nThe figure below shows a mixed view of the horizontal and vertical ZA tile slice sizes for different element data types:\nThe left columns show the different processing methods for each row of the ZA memory.\nSet SIZE as the size of vector elements, where SIZE is 1, 2, 4, 8, 16, representing data types B, H, S, D, or Q, respectively.\nSet NUM_OF_ELEMENTS as the number of elements in the vector, i.e., bytes_of(SVL)/SIZE.\nHorizontal tile slice, ZAnH.\u0026lt;B|H|S|D|Q\u0026gt;[m] accesses a vector that contains a whole row (m x SIZE + n) in ZA storage. The vector contains elements of data type B, H, S, D, or Q.\nVertical tile slice, ZAnV.\u0026lt;B|H|S|D|Q\u0026gt;[m] accesses a vector that contains the entire column (m x SIZE) in ZA storage. This vector contains elements of data type B, H, S, D, or Q.\nZAnV.[m] accesses a vector containing column (m x SIZE) and row elements (i x SIZE + n), where i ranges from 0 to NUM_OF_ELEMENTS-1. This vector contains elements of data types B, H, S, D, or Q.\nBe careful with overlapping when applying mixed element data type sizes and horizontal and vertical tile slices.\nFor more information on ZA Array, ZA array vectors, tile, and tile slices, refer to sections B1.4.8 to B1.4.12 of the Arm Architecture Reference Manual for the A-profile architecture.\n#\r6. Instructions supported in Steaming SVE mode\rSome instructions have limitations in Streaming SVE mode:\nSome SVE/SVE2 instructions become illegal to execute Gather-load and Scatter-store instructions Use the SVE2 instruction of the First Fault register Most NEON instructions become UNDEFINED For more information about instructions affected by the Streaming SVE mode, please refer to the document \u0026ldquo;Arm Architecture Reference Manual.\u0026rdquo;\nSME has added several new instructions, including:\nMatrix outer product and accumulate or subtract instructions, including FMOPA, UMOPA, and BFMOPA. SVE2 vector registers (Z0-Z31) serve as the row and column inputs for outer product operations. ZA storage stores the output results of the two-dimensional matrix tile. Instructions for performing addition operations with the SVE2 Z vector and the rows or columns of ZA Instruction for clearing ZA tiles Added some instructions that can be used in both Streaming and Non-streaming modes. #\r7. SME Directive\rThe main SME commands for operating ZA storage include:\nCalculate the cross product of two vectors, and then accumulate or decrement, and place the result into an instruction of a ZA tile. Instructions to store or load SVE vectors (Z registers) into or from rows or columns of the ZA tile In the horizontal or vertical direction, an SVE vector and ZA tile addition instruction An instruction to add a multiple of the vector length in Streaming SVE mode to a scalar register #\r7.1 Outer Product and Accumulate or Subtract Instructions\rIn order to help understand outer product and accumulate or subtract instructions, let\u0026rsquo;s see how to use the outer product operation to perform matrix multiplication.\nCalculating the outer product of two vectors a and b will yield a result matrix C containing the outer product:\nNow consider the matrix multiplication operation of two matrices a and b:\nThis matrix multiplication can be achieved by calculating two outer product operations and accumulating the two resulting matrices (which is the commonly used handwritten calculation method), as shown in the diagram below:\nSME introduced efficient outer product and accumulate or subtract instructions for the following data types:\n8-bit, 16-bit integers FP16, BF16, FP32, and FP64 floating point numbers These instructions calculate the outer product of two vectors in two Z vector registers (Zn and Zm), accumulate or subtract the resulting array with the existing data in a ZA tile (ZAda), and store the result in the same ZA tile (ZAda). Each source vector is independently predicated by the corresponding control predicate registers (Pn and Pm).\nOutput Array Input Vector Description Example INT32 INT8, INT8 Store the sum of the outer products of four INT8s into each INT32 element SMOPA or SMOPS or UMOPA or UMOPS: Signed or unsigned integer outer product sum, and accumulate or subtract. For example: UMOPS \u0026lt;ZAda\u0026gt;.S, \u0026lt;Pn\u0026gt;/M, \u0026lt;Pm\u0026gt;/M, \u0026lt;Zn\u0026gt;.B, \u0026lt;Zm\u0026gt;.B INT32 INT16, INT16 Store the sum of the outer product of two INT16 in each INT32 element SMOPA or SMOPS or UMOPA or UMOPS: Signed or unsigned integer outer product sum, and accumulate or subtract. For example: UMOPS \u0026lt;ZAda\u0026gt;.S, \u0026lt;Pn\u0026gt;/M, \u0026lt;Pm\u0026gt;/M, \u0026lt;Zn\u0026gt;.H, \u0026lt;Zm\u0026gt;.H INT64 INT16, INT16 If FEAT_SME_I16I64 is implemented, the sum of the outer products of four INT16s is stored in each INT64 element SMOPA or SMOPS or UMOPA or UMOPS: signed or unsigned integer outer product sum, and accumulate or subtract. For example: UMOPS \u0026lt;ZAda\u0026gt;.D, \u0026lt;Pn\u0026gt;/M, \u0026lt;Pm\u0026gt;/M, \u0026lt;Zn\u0026gt;.H, \u0026lt;Zm\u0026gt;.H FP32 BF16, BF16 Store the sum of two BF16 outer products into each FP32 element BFMOPA or BFMOPS: BFloat16 outer product sum, with accumulation or subtraction. For example: BFMOPS \u0026lt;ZAda\u0026gt;.S, \u0026lt;Pn\u0026gt;/M, \u0026lt;Pm\u0026gt;/M, \u0026lt;Zn\u0026gt;.H, \u0026lt;Zm\u0026gt;.H FP32 FP16, FP16 Store the sum of two FP16 outer products into each FP32 element FMOPA or FMOPS: Half-precision floating-point outer product sum, and accumulate or subtract. For example: FMOPS \u0026lt;ZAda\u0026gt;.S, \u0026lt;Pn\u0026gt;/M, \u0026lt;Pm\u0026gt;/M, \u0026lt;Zn\u0026gt;.H, \u0026lt;Zm\u0026gt;.H FP32 FP32, FP32 Simple FP32 outer product FMOPA or FMOPS: Floating-point outer product and accumulate or subtract. For example: FMOPS \u0026lt;ZAda\u0026gt;.S, \u0026lt;Pn\u0026gt;/M, \u0026lt;Pm\u0026gt;/M, \u0026lt;Zn\u0026gt;.S, \u0026lt;Zm\u0026gt;.S FP64 FP64, FP64 If FEAT_SME_F64F64 is implemented, perform a simple FP64 outer product FMOPA or FMOPS: Floating point outer product and accumulate or subtract. For example: FMOPS \u0026lt;ZAda\u0026gt;.D, \u0026lt;Pn\u0026gt;/M, \u0026lt;Pm\u0026gt;/M, \u0026lt;Zn\u0026gt;.D, \u0026lt;Zm\u0026gt;.D #\r7.1.1 FP32, FP64 outer product and accumulate or subtract instructions\rInstructions where the input vectors and output arrays have the same data type (FP32, FP64) are relatively simple.\nThe following example demonstrates FP32 type outer product with accumulation or subtraction instructions.\nFMOPA \u0026lt;ZAda\u0026gt;.S, \u0026lt;Pn\u0026gt;/M, \u0026lt;Pm\u0026gt;/M, \u0026lt;Zn\u0026gt;.S, \u0026lt;Zm\u0026gt;.S\rFMOPS \u0026lt;ZAda\u0026gt;.S, \u0026lt;Pn\u0026gt;/M, \u0026lt;Pm\u0026gt;/M, \u0026lt;Zn\u0026gt;.S, \u0026lt;Zm\u0026gt;.S\rIn this example, assuming the SVL vector length is 128, Zn.S and Zm.S contain vectors composed of 4 FP32 numbers, this instruction calculates the outer product of Zn.S and Zm.S, the result of the outer product is the gray matrix in the figure, then accumulates or subtracts this outer product result with the existing values in the ZA tile ZAda.S, and stores the result in the same ZA tile.\n#\r7.1.2 FP16, BF16, INT16, INT8, I16I64 type outer product and accumulate or subtract instructions\rBecause these instructions will expand the data type of the calculation results, these operations are not as straightforward as the previous FP32 and FP64 type instructions.\nBF16 instruction calculates the outer product of two BF16s, expands the result type to FP32, and then destructively adds or subtracts the result with the target tile. INT8 instructions compute the sum of the outer product of four INT8s, expanding the result type to INT32, and then perform destructive addition or subtraction of the result with the target tile. INT16 instruction calculates the outer product sum of two INT16s, expands the result type to INT32, and then performs a destructive add or subtract with the target tile. FP16 instructions calculate the sum of the outer product of two FP16s, expand the result type to FP32, and then perform destructive addition or subtraction of the result with the target tile. If FEAT_SME_I16I64 is implemented, the I16I64 instruction calculates the sum of the outer products of four INT16s, expands the result type to INT64, and then destructively adds or subtracts the result with the target tile. The following example demonstrates the operation of the INT8 UMOPA instruction with an SVL vector length of 128:\nUMOPA \u0026lt;ZAda\u0026gt;.S, \u0026lt;Pn\u0026gt;/M, \u0026lt;Pm\u0026gt;/M, \u0026lt;Zn\u0026gt;.B, \u0026lt;Zm\u0026gt;.B\rEach input register (Zn.B, Zm.B) is treated as a matrix containing 4x4 elements, which can be seen as blocks composed of 4 consecutive elements (as marked by the red lines in the diagram) that have been transposed.\nIn this example, because the SVL vector length is 128-bit:\nThe first source vector Zn.B contains a 4x4 submatrix of unsigned 8-bit integers. The second source vector Zm.B, contains a 4x4 submatrix of unsigned 8-bit integers. UMOPA instruction calculates the sum of the 4x4 expanded 32-bit integer outer product, then destructively accumulates the integers in the target tile (ZAda). More generally, the UMOPA instruction multiplies submatrices from the first source vector with submatrices from the second source vector. Each source vector contains a submatrix of unsigned 8-bit integers of size (SVL/32) x 4. The resulting (SVL/32) x (SVL/32) expanded 32-bit integer outer product is then destructively added to a 32-bit integer target tile.\nThe following example demonstrates the operation of a BF16 BFMOPA with an SVL of 128-bit:\nBFMOPA \u0026lt;ZAda\u0026gt;.S, \u0026lt;Pn\u0026gt;/M, \u0026lt;Pm\u0026gt;/M, \u0026lt;Zn\u0026gt;.H, \u0026lt;Zm\u0026gt;.H\rIn this example, because the SVL vector length is 128-bit:\nThe first source vector Zn.H, contains a 4x2 submatrix of BF16 integers, which is expanded into single-precision floating-point numbers. The second source vector Zm.H, contains a 2x4 submatrix of a BF16 integer, which is expanded into a single-precision floating-point number. BMOPA instruction calculates the sum of a 4x4 single-precision outer product, and then destructively accumulates it with the single-precision floating-point numbers in the target tile (ZAda). More generally speaking, the BFMOPA instruction expands the type of the (SVL/32) x2 BF16 submatrix stored in the first source vector to single precision, expands the type of the 2x (SVL/32) BF16 submatrix stored in the second source vector to single precision, and multiplies these two submatrices. Then, the resulting (SVL/32) x (SVL/32) single-precision outer product is destructively added to a single-precision target tile.\nThe following table shows the number of MACs (Multiply-Accumulate) for the corresponding data type performed by an outer product and accumulate or subtract instruction for several data types and SVL lengths:\n128-bit 256-bit 512-bit FP32 16 64 256 FP64 4 16 64 INT8 64 256 1024 INT16 32 128 512 BF16 32 128 512 FP16 32 128 512 #\r7.2 SME Instructions with Predication\rEach source vector can be independently predicated by its corresponding control predicate register:\nOuter product and accumulate or subtract instructions use Pn/M and Pn/M (without /Z form): Inactive source elements are treated as having a value of 0. Slice move command uses Pg/M: The Inactive elements in the target slice remain unchanged. Tile slice load instruction uses Pg/Z: Inactive elements in the target tile slice are set to 0. Tile slice store instruction uses Pg: Inactive elements that will not be written to memory. Predication makes it easier to handle cases where the dimensions of the matrix are not a multiple of SVL.\nFor example, the instructions in the image below:\nThe input vector Z0 is predicated by P0, Z1 is predicated by P1.\nIn this example:\nSVL vector length is 512-bit. The Z register contains a vector of 16 FP32 numbers. The last two elements in P0 are inactive. The last element in P1 is inactive. This instruction updates (16-2) x (16-1) FP32 elements in ZA0.S, because Pn/M is used, the remaining elements in ZA0.S remain unchanged.\nThe figure below shows more examples of predicated outer products with accumulation or subtraction. The underlined text in the figure indicates the parts of the calculation affected by inactive predicate elements.\n#\r7.3 ZA tile and addition operation with a Z vector\rSME includes instructions to add a vector to the rows or columns of a ZA tile, and these instructions also support predication.\nInstruction Description ADDHA Add the source vector to each horizontal slice of the ZA tile ADDVA Add the source vector to each vertical slice of the ZA tile For example:\nADDHA ZA0.S, P0/M, P1/M, Z1.S\rWill perform the following actions:\nThis ADDHA instruction adds each element of the source vector Z1 to the corresponding active element of each horizontal slice of the ZA0.S tile.\nElements in a Tile are predicated by a pair of governing predicates. An element in a horizontal slice can be considered active under the following conditions:\nIt is TRUE for the element corresponding to the second governing predicate, and It corresponds to TRUE at the row number of the first governing predicate\u0026rsquo;s horizontal slice, and the inactive elements in the target tile remain unchanged. #\r7.4 Tile load, store, move instructions\rSME tile load, store, move instructions can:\nRead data from memory and place it into a row or column of the ZA tile Write the row or column of the ZA tile into memory Move the row of the ZA tile to the SVE Z vector register Move the SVE Z vector register to a ZA tile row or column #\r7.4.1 Tile slice load and store instructions\rThe LD1B, LD1H, LD1S, LD1D, and LD1Q instructions load consecutive memory values into a ZA tile slice with 8-bit, 16-bit, 32-bit, 64-bit, or 128-bit elements, respectively.\nThe ST1B, ST1H, ST1S, ST1D, and ST1Q instructions store a ZA tile slice containing 8-bit, 16-bit, 32-bit, 64-bit, or 128-bit elements, respectively, into contiguous memory.\nThese instructions also support predication, for example:\nLD1B ZA0H.B[W0, #imm], P0/Z, [X1, X2]\rThis LD1B instruction performs a predicated continuous byte read, reading data from memory at address (X1+X2) into the horizontal tile slice in ZA0 at row number (W0+imm). Inactive elements in the target tile slice are set to 0.\nST1H ZA1V.H[W0, #imm], P2, [X1, X2, LSL #1]\rThis ST1H instruction executes a predicated continuous halfword store operation, storing the vertical tile slice in ZA1 with the column number (W0+imm) to the memory address (X1+X2*2), and elements that are inactive in the tile slice are not written to memory.\n#\r7.4.2 Tile slice move instruction\rThe MOV instruction (alias for the MOVA instruction) moves the value of a Z vector register to a ZA tile slice, or moves the value from a ZA tile slice to a Z vector register. This instruction operates on a single horizontal or vertical tile slice of a ZA tile with a specified element size. The row number/column number of the slice is specified by the slice\u0026rsquo;s retrieval register plus an immediate offset. Inactive elements in the target slice remain unchanged.\nFor example:\nMOV ZA0H.B[W0, #imm], P0/M, Z0.B\rOr\nMOVA ZA0H.B[W0, #imm], P0/M, Z0.B\rThis instruction moves the values in vector register Z0.B to the horizontal ZA tile slice ZA0H.B[W0,#imm], using P0 as the predication register. Inactive elements in the target tile slice remain unchanged.\n#\r7.5 ZA array vector load/store instructions\rSME LDR instruction reads data from memory into a ZA array vector, SME STR instruction stores the values from a ZA array vector into memory. These instructions do not have predication functionality. They are primarily for saving/restoring ZA storage during software context switching. SME LDR/STR instructions can also be used in Non-streaming SVE mode when PSTATE.ZA is enabled. For example, the ZA array vector in the following STR instruction is specified by a vector selection register Wv (scalar register W) plus an optional immediate number (Wv+Imm). The address for accessing memory is: a scalar register as the base, plus the same optional immediate offset multiplied by the current vector length in bytes.\nSTR ZA[\u0026lt;Wv\u0026gt;, \u0026lt;imm\u0026gt;], [\u0026lt;Xn|SP\u0026gt;{, #\u0026lt;imm\u0026gt;, MUL VL}]\r#\r7.6 ZA tile clear instruction\rSME ZERO instruction can clear a group of 64-bit ZA tile:\nZERO { \u0026lt;mask\u0026gt;}\rThe ZERO instruction can zero out up to 8 ZA tiles named ZA0.D to ZA8.D. The tiles to be zeroed are specified by the mask in the instruction, while the remaining tiles remain unchanged.\nThis instruction can also be used in Non-streaming SVE mode when PSTATE.ZA is enabled.\nIf you want to clear the entire ZA array, you can use an instruction alias, ZERO {ZA}.\n#\r7.7 New SVE2 Instructions\rThe SME architecture extension has added some new SVE2 instructions, which can also be used in PE that implements SVE2 when in Non-streaming SVE mode. These instructions include:\nSelect a predicate register or an all-false Predicate select instruction Reverse 64-bit double word element instruction Signed/Unsigned clamp to smaller/larger value vector instructions The following introduces the Predicate select instruction.\n#\r7.7.1 PSEL Instruction\rPSEL instruction selects a predicate register or all-false to the target predicate register, as follows:\nPSEL \u0026lt;Pd\u0026gt;, \u0026lt;Pn\u0026gt;, \u0026lt;Pm\u0026gt;.\u0026lt;T\u0026gt;[\u0026lt;Wv\u0026gt;, \u0026lt;imm\u0026gt;]\rIf the element specified in the second source predicate register (Pm) is True, this instruction places the content of the first source predicate register (Pn) into the destination predicate register (Pd), otherwise, it sets the value of the destination predicate register to all false. For example, the following instruction, assuming the value of W12 is 0:\nPSEL P0, P1, P2.B[W12, #0]\rThe [0]th element of the second source predicate register [W12+0] is False, so the target register P0 is set to all 0 (all-false), as shown in the figure below:\nNow look at the following instruction, still assuming the value of W12 is 0, but this time the immediate offset is 1:\nPSEL P0, P1, P2.B[W12, #1]\rThe [1] element of the second source predicate register [W12+1] is True, therefore select the value of the first source predicate register to the destination register P0, as shown in the diagram below:\n#\rReferences\rArm Scalable Matrix Extension (SME) Introduction\rArm Scalable Matrix Extension (SME) Instructions\r","date":"2024-08-13T22:44:00Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-14_117622407_p0_master1200.webp","permalink":"https://cuterwrite.top/en/p/arm-sme-for-performance/","title":"Arm Matrix Acceleration: Scalable Matrix Extension SME"},{"content":"\r#\rARM Performance Optimization: Scalable Vector Extension SVE\r#\r1. SVE Introduction\rAfter the Neon architecture extension with a fixed 128-bit vector length instruction set, Arm designed the Scalable Vector Extension (SVE) as the next-generation SIMD extension for AArch64. SVE introduces the scalable concept, allowing flexible vector length implementations and providing a range of possible values in CPU implementations. The vector length can vary from a minimum of 128 bits to a maximum of 2048 bits, in increments of 128 bits. The SVE design ensures that the same application can run on different SVE-supporting implementations without recompiling the code. SVE enhances the architecture\u0026rsquo;s applicability to high-performance computing (HPC) and machine learning (ML) applications, which require very large amounts of data processing. SVE2 is a superset of SVE and Neon. SVE2 allows the use of more functional domains in data-level parallelism. SVE2 inherits the concepts, vector registers, and operation principles of SVE. SVE and SVE2 define 32 scalable vector registers. Chip partners can choose an appropriate vector length design implementation, with hardware varying between 128 bits and 2048 bits (in increments of 128 bits). The advantage of SVE and SVE2 is that only one vector instruction set uses scalable variables.\nThe SVE design philosophy allows developers to write and build software once, and then run the same binary on different AArch64 hardware with various SVE vector length implementations. The portability of the binary means developers do not need to know the vector length implementation of their system. This eliminates the need to rebuild the binary, making the software easier to port. In addition to scalable vectors, SVE and SVE2 also include:\nper-lane predication Gather Load/Scatter Store Speculative Vectorization These features help vectorize and optimize loops when dealing with large datasets.\nThe main difference between SVE2 and SVE lies in the functional coverage of the instruction set. SVE is specifically designed for HPC and ML applications. SVE2 extends the SVE instruction set to enable accelerated data processing in areas beyond HPC and ML. The SVE2 instruction set can also accelerate common algorithms used in the following applications:\nComputer Vision Multimedia LTE Basic Processing Genomics In-memory database Web Service General software SVE and SVE2 both support collecting and processing large amounts of data. SVE and SVE2 are not extensions of the Neon instruction set. Instead, SVE and SVE2 are redesigned to offer better data parallelism than Neon. However, the hardware logic of SVE and SVE2 covers the implementation of Neon hardware. When a microarchitecture supports SVE or SVE2, it also supports Neon. To use SVE and SVE2, the software running on that microarchitecture must first support Neon.\n#\r2. SVE Architecture Basics\rThis section introduces the basic architectural features shared by SVE and SVE2. Like SVE, SVE2 is also based on scalable vectors. In addition to the existing register file provided by Neon, SVE and SVE2 add the following registers:\n32 scalable vector registers, Z0-Z31 16 scalable Predicate registers, P0-P15 1 First Fault Predicate register, FFR Scalable Vector System Control Register, ZCR_ELx #\r2.1 Scalable Vector Registers\rScalable vector registers Z0-Z31 can be implemented in microarchitecture as 128-2048 bits. The lowest 128 bits are shared with Neon\u0026rsquo;s fixed 128-bit vectors V0-V31.\nThe image below shows scalable vector registers Z0-Z31:\nScalable Vector Registers Z0-Z31 Scalable Vector:\nCan accommodate 64, 32, 16, and 8-bit elements Supports integer, double precision, single precision, and half precision floating-point elements The vector length can be configured for each exception level (EL) #\r2.2 Scalable Predicate Register\rIn order to control which active elements participate in operations, Predicate registers (abbreviated as P registers) are used as masks in many SVE instructions, which also provides flexibility for vector operations. The figure below shows the scalable Predicate registers P0-P15:\nScalable Predicate Registers P0-P15 The P register is typically used as a bitmask for data manipulation:\nEach P register is 1/8 the length of a Z register P0-P7 are used for loading, storing, and arithmetic operations P8-P15 used for loop management FFR is a special P register set by the first-fault vector load and store instructions, used to indicate the success of load and store operations for each element. FFR is designed to support speculative memory access, making vectorization easier and safer in many cases. #\r2.3 Scalable Vector System Control Register\rThe figure below shows the Scalable Vector System Control Register ZCR_ELx:\nScalable Vector System Control Register ZCR_Elx Scalable Vector System Control Register indicates SVE implementation features:\nZCR_Elx.LEN field is used for the vector length of the current and lower anomaly levels. Most bits are currently reserved for future use. #\r2.4 SVE Assembly Syntax\rThe SVE assembly syntax format consists of an opcode, destination register, P register (if the instruction supports a Predicate mask), and input operands. The following instruction example will detail this format.\nExample 1:\nLDFF1D {\u0026lt;Zt\u0026gt;.D}, \u0026lt;Pg\u0026gt;/Z, [\u0026lt;Xn|SP\u0026gt;, \u0026lt;Zm\u0026gt;.D, LSL #3]\rAmong them:\n\u0026lt;Zt\u0026gt; is the Z register, Z0-Z31 \u0026lt;Zt\u0026gt;.D and \u0026lt;Zm\u0026gt;.D specify the element type of the target and operand vectors, without needing to specify the number of elements. \u0026lt;Pg\u0026gt; is the P register, P0-P15 \u0026lt;Pg\u0026gt;/Z is to zero the P register. \u0026lt;Zm\u0026gt; specifies the offset for the Gather Load address mode. Example 2:\nADD \u0026lt;Zdn\u0026gt;.\u0026lt;T\u0026gt;, \u0026lt;Pg\u0026gt;/M, \u0026lt;Zdn\u0026gt;.\u0026lt;T\u0026gt;, \u0026lt;Zm\u0026gt;.\u0026lt;T\u0026gt;\rAmong them:\n\u0026lt;Pg\u0026gt;/M is the merge P register. \u0026lt;Zdn\u0026gt; is both the destination register and one of the input operands. The instruction syntax shows \u0026lt;Zdn\u0026gt; in both places for convenience. In the assembly encoding, for simplification, they are only encoded once. Example 3:\nORRS \u0026lt;Pd\u0026gt;.B, \u0026lt;Pg\u0026gt;.Z, \u0026lt;Pn\u0026gt;.B, \u0026lt;Pm\u0026gt;.B\rS is the new interpretation of the P register condition flags NZCV. \u0026lt;Pg\u0026gt; controls the P register to act as a bitmask in the example operation. #\r2.5 SVE Architecture Features\rSVE includes the following key architectural features:\nper-lane predication In order to allow flexible operations on selected elements, SVE introduces 16 P registers, P0-P15, to indicate valid operations on vector active channels. For example:\nADD Z0.D, P0/M, Z0.D, Z1.D\rAdd the active elements Z0 and Z1 and place the result in Z0. P0 indicates which elements of the operands are active and inactive. The M following P0 stands for Merging, meaning the inactive elements of Z0 will retain their initial values after the ADD operation. If Z follows P0, the inactive elements will be zeroed, and the inactive elements of the destination register will be zeroed after the operation.\nPer-lane predication merging If \\Z is used, the inactive elements will be zeroed, and the inactive elements of the target register will be zeroed after the operation. For example\nCPY Z0.B, P0/Z, #0xFF\rIndicates that the signed integer 0xFF will be copied to the active channel of Z0, while the inactive channels will be cleared.\nPer-lane predication zeroing Not all instructions have the Predicate option. Additionally, not all Predicate operations have both merge and zeroing options. You must refer to the AArch64 SVE Supplement\rto understand the specification details of each instruction.\nGather Load and Scatter Store The addressing modes in SVE allow vectors to be used as base addresses and offsets in Gather Load and Scatter Store instructions, which enables access to non-contiguous memory locations. For example:\nLD1SB Z0.S, P0/Z, [Z1.S] // Gather Load signed bytes from memory addresses generated by the 32-bit vector base address Z1 into the active 32-bit elements of Z0.\rLD1SB Z0.D, P0/Z, [X0, Z1.D] // Gather Load signed bytes from memory addresses generated by the 64-bit scalar base address X0 plus the vector index in Z1.D into the active elements of Z0.\rThe following example shows the load operation LD1SB Z0.S, P0/Z, [Z1.S], where P0 contains all true elements, and Z1 contains scattered addresses. After loading, the least significant byte of each element in Z0.S will be updated with data fetched from scattered memory locations.\nGather-load and Scatter-store Example Loop control and management of the P register driver As a key feature of SVE, the P register not only flexibly controls individual elements of vector operations but also enables P register-driven loop control. P register-driven loop control and management make loop control efficient and flexible. This feature eliminates the overhead of extra loop heads and tails for processing partial vectors by registering active and inactive element indices in the P register. P register-driven loop control and management mean that in the subsequent loop iterations, only active elements will perform the intended operations. For example:\nWHILEL0 P0.S, x8, x9 // Generate a predicate in P0, starting from the lowest numbered element, true when the incremented value of the first unsigned scalar operand X8 is less than the second scalar operand X9, then false until the highest numbered element.\rB.FIRST Loop_start // B.FIRST (equivalent to B.MI) or B.NFRST (equivalent to B.PL) is usually used to branch based on the test result of the above instruction, determining whether the first element of P0 is true or false as the condition to end or continue the loop.\rExample of loop control and management driven by P register Vector partitioning for speculation in software management Speculative loading can pose challenges for memory reading of traditional vectors, if errors occur in certain elements during the reading process, it is difficult to reverse the load operation and track which elements failed to load. Neon does not allow speculative loading. To allow speculative loading of vectors (e.g., LDRFF), SVE introduces the first-fault vector load instruction. To allow vector access across invalid pages, SVE also introduces the FFR register. When using the first-fault vector load instruction to load into an SVE vector, the FFR register updates with the success or failure result of each element\u0026rsquo;s load. When a load error occurs, FFR immediately registers the corresponding element, registers the remaining elements as 0 or false, and does not trigger an exception. Typically, the RDFFR instruction is used to read the FFR status. The RDFFR instruction ends iteration when the first element is false. If the first element is true, the RDFFR instruction continues iteration. The length of FFR is the same as the P vector. This value can be initialized using the SETFFR instruction. The following example uses LDFF1D to read data from memory, and FFR is updated accordingly:\nLDFF1D Z0.D, P0/Z, [Z1.D, #0] // Use the first-fault behavior to gather doublewords from the memory address generated by vector base address Z1 plus 0, loading into the active elements of Z0. Inactive elements do not read device memory or trigger a fault, and are set to zero in the destination vector. A successful load from valid memory sets the corresponding element in the FFR to true. The first-fault load sets the corresponding element and the remaining elements in the FFR to false or 0.\rExample of Vector Partitioning for Software-Managed Speculation Extended floating point and horizontal reduction In order to allow efficient reduction operations in vectors and meet different precision requirements, SVE enhances floating-point and horizontal reduction operations. These instructions may have a sequential (low to high) or tree-based (pairwise) floating-point reduction order, where the order of operations may lead to different rounding results. These operations require a trade-off between reproducibility and performance. For example:\nFADDA D0, P0/M, D1, Z2.D // Perform a floating-point addition strict-order reduction from the low to high elements of the source vector, accumulating the result into the SIMD\u0026amp;FP scalar register. This example instruction adds D1 to all active elements of Z2.D and stores the result into scalar register D0. Vector elements are processed in strict order from low to high, with scalar source D1 providing the initial value. Inactive elements in the source vector are ignored. FADDV performs a recursive pairwise reduction and stores the result into the scalar register.\rExtended Floating-point and Horizontal Reductions Example #\r3. New Features of SVE2\rThis section introduces the features added by SVE2 to the Arm AArch64 architecture. To achieve scalable performance, SVE2 is built on SVE, allowing vectors to reach up to 2048 bits.\nIn SVE2, many instructions that replicate existing instructions in Neon have been added, including:\nConverted Neon integer operations, for example, Signed Absolute Difference Accumulate (SAB) and Signed Halving Add (SHADD). Converted Neon extensions, narrowing and paired operations, for example, Unsigned Add Long - Bottom (UADDLB) and Unsigned Add Long - Top (UADDLT). The order of element processing has changed. SVE2 processes interleaved even and odd elements, while Neon processes the low half and high half elements of narrow or wide operations. The diagram below illustrates the difference between Neon and SVE2 processing:\nComparison of Transformed Neon Narrow or Wide Operations Complex number operations, such as complex integer multiplication-accumulation with rotation (CMLA). Multi-precision arithmetic, used for large integer arithmetic and cryptography, for example, carry-in long addition - bottom (ADCLB), carry-in long addition - top (ADCLT) and SM4 encryption and decryption (SM4E). For backward compatibility, the latest architecture requires Neon and VFP. Although SVE2 includes some features of SVE and Neon, SVE2 does not preclude the presence of Neon on the chip.\nSVE2 supports optimization for emerging applications beyond the HPC market, such as in machine learning (ML) (UDOT instructions), computer vision (TBL and TBX instructions), baseband networks (CADD and CMLA instructions), genomics (BDEP and BEXT instructions), and servers (MATCH and NMATCH instructions).\nSVE2 enhances the overall performance of general-purpose processors in handling large volumes of data, without the need for additional off-chip accelerators.\n#\r4. Using SVE programming\rThis section introduces software tools and libraries that support SVE2 application development. This section also explains how to develop applications for targets that support SVE2, run the application on hardware that supports SVE2, and simulate the application on any Armv8-A hardware.\n#\r4.1 Software and Library Support\rTo build SVE or SVE2 applications, you must choose a compiler that supports SVE and SVE2 features.\nGNU tools version 8.0+ supports SVE. Arm Compiler for Linux\rVersion 18.0+ supports SVE, Version 20.0+ supports SVE and SVE2. Both GNU and Arm Compiler for Linux compilers support optimizing C/C++/Fortran code. LLVM (open-source Clang) version 5 and above includes support for SVE, and version 9 and above includes support for SVE2. To find out which SVE or SVE2 features are supported by each version of the LLVM tools, please refer to the LLVM toolchain SVE support page\r. Arm Performance Libraries\rare highly optimized for mathematical routines and can be linked to your applications. Arm Performance Libraries version 19.3+ supports SVE\u0026rsquo;s math library.\nArm Compiler for Linux is part of Arm Allinea Studio, including Arm C/C++ Compiler, Arm Fortran Compiler, and Arm Performance Libraries.\n#\r4.2 How to Program Using SVE2\rThere are several methods to write or generate SVE and SVE2 code. In this section, we will explore some of these methods.\nTo write or generate SVE and SVE2 code, you can:\nWrite SVE assembly code Programming with SVE intrinsics Automatic vectorization Use SVE optimization library Let\u0026rsquo;s take a closer look at these four options.\n#\r4.2.1 Write SVE assembly code\rYou can write SVE instructions as inline assembly in C/C++ code, or as a complete function in assembly source code. For example:\n```assembly\r.globl subtract_arrays // -- Begin function\r.p2align 2\r.type subtract_arrays, @function\rsubtract_arrays: // @subtract_arrays\r.cfi_startproc\r// %bb.0:\rorr w9, wzr, #0x400\rmov x8, xzr\rwhilelo p0.s, xzr, x9\r.LBB0_1: // =\u0026gt;This Inner Loop Header: Depth=1\rld1w { z0.s }, p0/z, [x1, x8, lsl #2]\rld1w { z1.s }, p0/z, [x2, x8, lsl #2]\rsub z0.s, z0.s, z1.s\rst1w { z0.s }, p0, [x0, x8, lsl #2]\rincw x8\rwhilelo p0.s, x8, x9\rb.mi .LBB0_1\r// %bb.2:\rret\r.Lfunc_end0:\r.size subtract_arrays, .Lfunc_end0-subtract_arrays\r.cfi_endproc\rIf you write functions that mix high-level language and assembly language, you must be familiar with the Application Binary Interface (ABI)\rstandards updated for SVE. The Arm Architecture Procedure Call Standard (AAPCS)\rspecifies data types and register allocation, and is most relevant to assembly programming. AAPCS requires:\nZ0-Z7 and P0-P3 are used to pass scalable vector parameters and results. Z8-Z15 and P4-P15 are callee-saved. All other vector registers (Z16-Z31) may be corrupted by the called function, and the calling function is responsible for backing up and restoring them when necessary. #\r4.2.2 Using SVE Instruction Functions (Intrinsics)\rSVE intrinsic functions are functions supported by the compiler that can be replaced with corresponding instructions. Programmers can directly call instruction functions in high-level languages such as C and C++. The ACLE (Arm C Language Extensions) for SVE defines which SVE intrinsic functions are available, their parameters, and their functionality. A compiler that supports ACLE can replace intrinsics with mapped SVE instructions during compilation. To use ACLE intrinsics, you must include the header file arm_sve.h, which contains a list of vector types and intrinsic functions (for SVE) that can be used in C/C++. Each data type describes the size and data type of the elements in the vector:\nsvint8_t svuint8_t svint16_t svuint16_t svfloat16_t svint32_t svuint32_t svfloat32_t svint64_t svuint64_t svfloat64_t For example, svint64_t represents a 64-bit signed integer vector, svfloat16_t represents a half-precision floating-point vector.\nThe following example C code has been manually optimized using SVE intrinsics:\n// intrinsic_example.c\r#include \u0026lt;arm_sve.h\u0026gt;\rsvuint64_t uaddlb_array(svuint32_t Zs1, svuint32_t Zs2)\r{\r// widening add of even elements\rsvuint64_t result = svaddlb(Zs1, Zs2);\rreturn result;\r}\rThe source code that includes the arm_sve.h header file can use SVE vector types, just like data types can be used for variable declarations and function parameters. To compile the code using the Arm C/C++ compiler and target the Armv8-A architecture that supports SVE, use:\narmclang -O3 -S -march=armv8-a+sve2 -o intrinsic_example.s intrinsic_example.c\rThis command generates the following assembly code:\n// instrinsic_example.s\ruaddlb_array: // @uaddlb_array\r.cfi_startproc\r// %bb.0:\ruaddlb z0.d, z0.s, z1.s\rret\r#\r4.2.3 Automatic Vectorization\rC/C++/Fortran compilers (for example, the native Arm Compiler for Linux\rfor the Arm platform and the GNU compiler) support vectorization of C, C++, and Fortran loops using SVE or SVE2 instructions. To generate SVE or SVE2 code, choose the appropriate compiler options. For example, one option to enable SVE2 optimization using armclang is -march=armv8-a+sve2. If you want to use the SVE version of the library, combine -march=armv8-a+sve2 with -armpl=sve.\n#\r4.2.4 Using SVE/SVE2 to Optimize Libraries\rUse libraries highly optimized for SVE/SVE2, such as Arm Performance Libraries\rand Arm Compute Libraries. Arm Performance Libraries contain highly optimized implementations of mathematical functions optimized for BLAS, LAPACK, FFT, sparse linear algebra, and libamath. To be able to link any Arm Performance Libraries function, you must install Arm Allinea Studio and include armpl.h in your code. To build applications using Arm Compiler for Linux and Arm Performance Libraries, you must specify -armpl=\u0026lt;arg\u0026gt; on the command line. If you are using GNU tools, you must include the Arm Performance Libraries installation path in the linker command line with -L\u0026lt;armpl_install_dir\u0026gt;/lib and specify the GNU option equivalent to the Arm Compiler for Linux -armpl=\u0026lt;arg\u0026gt; option, which is -larmpl_lp64. For more information, please refer to the Arm Performance Libraries Getting Started Guide.\n#\r4.3 How to run SVE/SVE2 programs\rIf you do not have access to SVE hardware, you can use models or simulators to run the code. You can choose from the following models and simulators:\nQEMU: Cross-compilation and native models, supporting modeling on Arm AArch64 platforms with SVE. Fast Models: Cross-platform models that support modeling on Arm AArch64 platforms with SVE running on x86-based hosts. Architecture Envelope Model (AEM) with SVE2 support is only available to major partners. Arm Instruction Emulator (ArmIE): Runs directly on the Arm platform. Supports SVE and supports SVE2 from version 19.2+. #\r5. ACLE Intrinsics\r#\r5.1 ACLE Introduction\rACLE (Arm C Language Extensions) is used in C and C++ code to support Arm features through intrinsics and other characteristics.\nACLE (ARM C Language Extensions) extends the C/C++ language with Arm-specific features. Predefined macros: __ARM_ARCH_ISA_A64, __ARM_BIG_ENDIAN, etc. Internal functions: __clz(uint32_t x), __cls(uint32_t x), etc. Data types: SVE, NEON, and FP16 data types. ACLE support for SVE uses ACLE for variable-length vector (VLA) programming. Almost every SVE instruction has a corresponding intrinsic function. Data type used to represent size-agnostic vectors used by SVE intrinsics. Applicable scenarios for the following users: Users who wish to manually adjust SVE code. Users who wish to adapt or manually optimize applications and libraries. Users who need low-level access to Arm targets. #\r5.2 How to use ACLE\rInclude header files arm_acle.h: Core ACLE arm_fp16.h: Add FP16 data type. The target platform must support FP16, i.e., march=armv8-a+fp16. arm_neon.h: Add NEON Intrinsics and data types. The target platform must support NEON, i.e., march=armv8-a+simd. arm_sve.h: Add SVE Intrinsics and data types. The target platform must support SVE, i.e., march=armv8-a+sve. #\r5.3 SVE ACLE\rThe first thing to do is to include the header files #include \u0026lt;arm_sve.h\u0026gt;\rVLA data type svfloat64_t, svfloat16_t, svuint32_t, etc. Naming convention: sv\u0026lt;datatype\u0026gt;\u0026lt;datasize\u0026gt;_t Prediction Merge: _m Reset: _z Uncertain: _x Data type of P register: svbool_t Use generics for function overloading, for example, the function svadd will automatically select the corresponding function based on the parameter type. Function naming convention: svbase[disambiguator][type0][type1]...[predication] base refers to basic operations, such as add, mul, sub, etc. disambiguator is used to distinguish different variants of the same basic operation. typeN specifies the type of vector and P register. predication specifies the handling method for inactive elements. For example: svfloat64_t svld1_f64, svbool_t svwhilelt_b8, svuint32_t svmla_u32_z, svuint32_t svmla_u32_m #\r5.4 Common SVE Intrinsics\rPredicate Predicate is a vector of type bool, used to control whether the corresponding position in the vector participates in the computation during the process. svbool_t pg = svwhilelt_b32(i, num) generates a predicate for (i, i + 1, i + 2, \u0026hellip;, i + vl - 1) \u0026lt; num svbool_t pg = svptrue_b32() generates a predicate that is all true Among them, b32 corresponds to processing 32-bit data (int/float), in addition, there are also intrinsics corresponding to b8, b16, b64. Memory data access svld1(pg, *base): Load contiguous vector from address base. svst1(pg, *base, vec): Store the vector vec into the address base. svld1_gather_index(pg, *base, vec_index): Load the data corresponding to the vector index from the address base. svst1_scatter_index(pg, *base, vec_index, vec): Store data from vector vec to the positions corresponding to the vector indices. Basic calculation svadd_z(pg, sv_vec1, sv_vec2) svadd_m(pg, sv_vec1, sv_vec2) svadd_x(pg, sv_vec1, sv_vec2) svadd_x(pg, sv_vec1, x) Among them, _z indicates setting the position where pg is false to zero, _m indicates retaining the original value, and _x indicates uncertainty (any value is possible). The second operand can be scalar data. svmul, svsub, svsubr, svdiv, svdivr: Among them, svsubr swaps the position of the subtrahend and the minuend compared to svsub. Others svdup_f64(double x): Generate a vector with all elements being x. svcntd(): Returns the vector length of 64-bit data: svcntb corresponds to 8 bits, svcnth corresponds to 16 bits, svcntw corresponds to 32 bits. #\r5.5 SVE Structure Intrinsics\rFor corresponding structure data, SVE provides some special intrinsics, such as: svld3, svget3, svset3, svst3, etc. These intrinsics are used for processing structure data.\nFor example, for the particle structure:\ntypedef struct {\rfloat x;\rfloat y;\rfloat z;\r} Particle;\rYou can use svld3 to load all the data in the structure as a group of 3 vectors, and then use svget3 to extract a vector from the group of 3 vectors, where the value of index 0, 1, 2 corresponds to x, y, z respectively.\nParticle *ps;\rfloat factor = 2.2;\r// Initialization part omitted\rfor (int i = 0; i \u0026lt; num; i += svcntw()) {\rsvbool_t pg = svwhilelt_b32(i, num);\rsvfloat32x3_t sv_ps = svld3(pg, (float32_t *)\u0026amp;ps[i]);\rsvfloat32_t sv_ps_x = svget3(sv_ps, 0);\rsvfloat32_t sv_ps_y = svget3(sv_ps, 1);\r// Perform calculation\rsv_ps_x = svmul_x(pg, sv_ps_x, factor);\rsv_ps_y = svmul_x(pg, sv_ps_y, factor);\r// Save results\rsv_ps = svset3(sv_ps, 0, sv_ps_x);\rsv_ps = svset3(sv_ps, 1, sv_ps_y);\rsvst3(pg, (float32_t *)\u0026amp;ps[i], sv_ps);\r}\rsvld3(pg, *base): Load all data in the structure as a group of 3 vectors; where base is the address of the 3-element structure array. svget3(tuple, index): Extract a vector from a group of 3 vectors; the value of index is 0, 1, or 2. svset3(tuple, index, vec): Set one vector in a group of 3 vectors; the value of index is 0, 1, or 2. svst3(pg, *base, vec): Store a group of 3 vectors into a structure; where base is the address of an array of structures with 3 elements. #\r5.6 SVE Condition Selection\rSVE provides methods such as svcmplt, svcompact, svcntp_b32, etc., which can select elements to retain in the vector based on conditions.\nFor example, for non-vectorized code:\nfor (int i = 0; i \u0026lt; num; i++) {\rfloat tmp = provided[i];\rif (tmp \u0026lt; mark) {\rselected[count++] = tmp;\rif (count \u0026gt;= maxSize) {\rbreak;\r}\r}\r}\rThe purpose of this code is to select elements from the provided array that are less than mark and store them in the selected array until the selected array is full.\nRewrite with SVE Intrinsic:\nfor (int i = 0; i \u0026lt; num; i += svcntw()) {\rsvbool_t pg = svwhilelt_b32(i, num);\rsvfloat32_t sv_tmp = svld1(pg, \u0026amp;provided[i]);\rsvbool_t pg_sel = svcmplt(pg, sv_tmp, mark);\rsv_tmp = svcompact(pg_sel, sv_tmp);\rsvst1(pg, \u0026amp;selected[count], sv_tmp);\rcount += svcntp_b32(pg, pg_sel);\rif (count \u0026gt;= maxSize) {\rbreak;\r}\r}\rsvcmplt(pg, vec1, vec2): Compare the size of two vectors, returning a predicate indicating the positions in vec1 that are less than vec2. svcompact(pg, sv_tmp): Compress the vector, move the data with pg as active to the lower positions of the vector in order, and set the remaining positions to zero. svcntp_b32(pg, pg2): Returns the number of active elements in pg2 This code first loads the data from the provided array into sv_tmp, then uses svcmplt to generate a predicate indicating the positions less than mark. Next, it uses svcompact to compress sv_tmp, obtaining the data less than mark, and then stores it into the selected array using svst1. Finally, it uses svcntp_b32 to count the number of active elements and update count. svcompact schematic diagram (256-bit vector) Due to the compact operation, the selected array stores new data less than mark continuously from the count position, and the remaining positions are set to zero.\nsvst1 schematic diagram (256-bit vector) #\r5.7 SVE Vectorized Loop Interleaving\rThe vectorized loop interleaving implemented by SVE Intrinsic can greatly reduce the number of times vectors are read compared to compiler auto vectorization.\nFor example, for non-vectorized code:\nfor (int j = offset; j \u0026lt; outerLen - offset; j++) {\rint m2index = (j - offset) * innerLen;\rint m1index = m2index + innerLen;\rint m0index = m1index + innerLen;\rint p1index = m0index + innerLen;\rint p2index = p1index + innerLen;\rfor (int i = 0; i \u0026lt; innerLen; i++) {\rres[m0index + i] = m2factor * field[m2index + i] +\rm1factor * field[m1index + i] +\rm0factor * field[m0index + i] +\rp1factor * field[p1index + i] +\rp2factor * field[p2index + i];\r}\r}\rAfter the compiler automatically vectorizes the code, each iteration requires reading data from five different vectors, resulting in low efficiency.\nRewrite with SVE Intrinsic:\nfor (int i = 0; i \u0026lt; innerLen; i += svcntd()) {\rsvbool_t pg = svwhilelt_b32(i, innerLen);\rint dataIndex = i;\rsvfloat64_t jm2Field = svld1(pg, \u0026amp;field[dataIndex]);\rdataIndex += innerLen;\rsvfloat64_t jm1Field = svld1(pg, \u0026amp;field[dataIndex]);\rdataIndex += innerLen;\rsvfloat64_t jm0Field = svld1(pg, \u0026amp;field[dataIndex]);\rdataIndex += innerLen;\rsvfloat64_t jp1Field = svld1(pg, \u0026amp;field[dataIndex]);\rfor (int j = offset; j \u0026lt; outerLen - offset; j += 1) {\rsvfloat64_t jp2Field = svld1(pg, \u0026amp;field[(j + offset) * innerLen + i]);\rsvfloat64_t svRes = svmul_x(pg, jm2Field, m2factor);\rsvRes = svmad_x(pg, jm1Field, m1factor, svRes);\rsvRes = svmad_x(pg, jm0Field, m0factor, svRes);\rsvRes = svmad_x(pg, jp1Field, p1factor, svRes);\rsvRes = svmad_x(pg, jp2Field, p2factor, svRes);\rsvst1(pg, \u0026amp;res[j * innerLen + 1], svRes);\rjm2Field = jm1Field;\rjm1Field = jm0Field;\rjm0Field = jp1Field;\rjp1Field = jp2Field;\r}\r}\rsvmad_x(pg, vec1, vec2, vec3): Calculates vec1 * vec2 + vec3, returns a vector. This code only needs to read one vector per iteration, greatly reducing the number of vector reads. #\rReferences\rIntroduction to SVE2\rSVE Deep Dive\rArm C Language Extensions\r","date":"2024-08-11T02:13:00Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_117622464_p0_master1200.webp","permalink":"https://cuterwrite.top/en/p/arm-sve-for-performance/","title":"Arm Performance Optimization: Scalable Vector Extension SVE"},{"content":"\r#\rLLM Ecosystem Introduction: From Model Fine-Tuning to Application Implementation\r#\rModel fine-tuning\rPre-trained LLMs typically possess broad knowledge, but fine-tuning is essential for them to excel in specific tasks. Here are some commonly used LLM fine-tuning tools:\n#\rAxolotl\rOpenAccess-AI-Collective\r/\raxolotl\rWaiting for api.github.com...\r0\r0\runkown\rWaiting...\rAxolotl is a tool designed to simplify the fine-tuning of various AI models, supporting multiple configurations and architectures.\nMain Features:\nTrain various Huggingface models, such as llama, pythia, falcon, mpt Supports fullfinetune, lora, qlora, relora, and gptq Customize configuration using simple yaml files or CLI rewrite functions Load different dataset formats, use custom formats, or built-in tokenized datasets Integrated with xformer, flash attention, rope scaling, and multi-packing Can work with a single GPU or multiple GPUs through FSDP or Deepspeed. Easily run locally or in the cloud using Docker Record the results and optional checkpoints to wandb or mlflow Quick Start: Requirements: Python \u0026gt;=3.10 and Pytorch \u0026gt;=2.1.1\ngit clone https://github.com/OpenAccess-AI-Collective/axolotl\rcd axolotl\rpip3 install packaging ninja\rpip3 install -e '.[flash-attn,deepspeed]'\rUsage:\n# preprocess datasets - optional but recommended\rCUDA_VISIBLE_DEVICES=\u0026quot;\u0026quot; python -m axolotl.cli.preprocess examples/openllama-3b/lora.yml\r# finetune lora\raccelerate launch -m axolotl.cli.train examples/openllama-3b/lora.yml\r# inference\raccelerate launch -m axolotl.cli.inference examples/openllama-3b/lora.yml \\\r--lora_model_dir=\u0026quot;./outputs/lora-out\u0026quot;\r# gradio\raccelerate launch -m axolotl.cli.inference examples/openllama-3b/lora.yml \\\r--lora_model_dir=\u0026quot;./outputs/lora-out\u0026quot; --gradio\r# remote yaml files - the yaml config can be hosted on a public URL\r# Note: the yaml config must directly link to the **raw** yaml\raccelerate launch -m axolotl.cli.train https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/examples/openllama-3b/lora.yml\rFor more detailed information, please visit the Axolotl\rproject homepage.\n#\rLlama-Factory\rhiyouga\r/\rLLaMA-Factory\rWaiting for api.github.com...\r0\r0\runkown\rWaiting...\rLlama-Factory is launched by Meta and is a framework focused on fine-tuning Llama models. It is built on top of the PyTorch ecosystem and provides efficient training and evaluation tools.\nMain Features:\nMultiple models: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Yi, Gemma, Baichuan, ChatGLM, Phi, etc. Integration Methods: (Incremental) Pre-training, (Multimodal) Instruction Supervised Fine-tuning, Reward Model Training, PPO Training, DPO Training, KTO Training, ORPO Training, etc. Multiple Precisions: 16-bit full parameter fine-tuning, frozen fine-tuning, LoRA fine-tuning, and 2/3/4/5/6/8-bit QLoRA fine-tuning based on AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ. Advanced Algorithms: GaLore, BAdam, DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ, PiSSA, and Agent fine-tuning. Practical Tips: FlashAttention-2, Unsloth, RoPE scaling, NEFTune, and rsLoRA. Experiment Monitoring: LlamaBoard, TensorBoard, Wandb, MLflow, etc. Rapid Reasoning: OpenAI-style API, browser interface, and command-line interface based on vLLM. Performance Metrics\nCompared to the P-Tuning\rfine-tuning by ChatGLM official, the LoRA fine-tuning by LLaMA Factory provides a 3.7 times speedup and achieves higher Rouge scores in advertising copy generation tasks. Combined with 4-bit quantization technology, LLaMA Factory\u0026rsquo;s QLoRA fine-tuning further reduces GPU memory consumption.\nVariable Definition\rTraining Speed: Number of samples processed per second during the training phase. (Batch size=4, truncation length=1024) Rouge Score: Rouge-2 score on the validation set of the advertising copy generation\rtask. (Batch size=4, truncation length=1024) GPU Memory: Peak GPU memory for 4-bit quantization training. (Batch size=1, truncation length=1024) We use pre_seq_len=128 in the P-Tuning of ChatGLM, and lora_rank=32 in the LoRA fine-tuning of LLaMA Factory. Quick Start\ngit clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\rcd LLaMA-Factory\rpip install -e \u0026quot;.[torch,metrics]\u0026quot;\rOptional additional dependencies: torch, torch-npu, metrics, deepspeed, bitsandbytes, hqq, eetq, gptq, awq, aqlm, vllm, galore, badam, qwen, modelscope, quality\nWhen encountering package conflicts, you can use pip install \u0026ndash;no-deps -e . to resolve them.\nWindows User Guide\rIf you want to enable Quantized LoRA (QLoRA) on the Windows platform, you need to install the precompiled bitsandbytes library, which supports CUDA 11.1 to 12.2. Please choose the appropriate release version\raccording to your CUDA version.\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl\rIf you want to enable FlashAttention-2 on the Windows platform, you need to install the precompiled flash-attn library, which supports CUDA 12.1 to 12.2. Please download and install the corresponding version according to your needs from flash-attention\r.\nAscend NPU User Guide\rWhen installing LLaMA Factory on Ascend NPU devices, you need to specify additional dependencies and use the command pip install -e \u0026quot;.[torch-npu,metrics]\u0026quot; to install them. Additionally, you need to install the Ascend CANN Toolkit and Kernels\r. Please refer to the installation tutorial\ror use the following command:\n# Please replace URL with the URL corresponding to the CANN version and device model\r# Install CANN Toolkit\rwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-toolkit_8.0.RC1.alpha001_linux-\u0026quot;$(uname -i)\u0026quot;.run\rbash Ascend-cann-toolkit_8.0.RC1.alpha001_linux-\u0026quot;$(uname -i)\u0026quot;.run --install\r# Install CANN Kernels\rwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run\rbash Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run --install\r# Set environment variables\rsource /usr/local/Ascend/ascend-toolkit/set_env.sh\rDependencies Minimum Recommended CANN 8.0.RC1 8.0.RC1 torch 2.1.0 2.1.0 torch-npu 2.1.0 2.1.0.post3 deepspeed 0.13.2 0.13.2 Please use ASCEND_RT_VISIBLE_DEVICES instead of CUDA_VISIBLE_DEVICES to specify the computation device.\nIf you encounter a situation where reasoning cannot proceed normally, try setting do_sample: false.\nDownload pre-built Docker image: 32GB\r| 64GB\rThe following three commands perform LoRA fine-tuning, inference, and merging on the Llama3-8B-Instruct model.\nllamafactory-cli train examples/train_lora/llama3_lora_sft.yaml\rllamafactory-cli chat examples/inference/llama3_lora_sft.yaml\rllamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml\rFor more detailed information, please visit the Llama-Factory\rproject homepage.\n#\rFirefly\ryangjianxin1\r/\rFirefly\rWaiting for api.github.com...\r0\r0\runkown\rWaiting...\rFirefly is an open-source large model training project that supports pre-training, instruction fine-tuning, and DPO for mainstream large models, including but not limited to Qwen2, Yi-1.5, Llama3, Gemma, Qwen1.5, MiniCPM, Llama, InternLM, Baichuan, ChatGLM, Yi, Deepseek, Qwen, Orion, Ziya, Xverse, Mistral, Mixtral-8x7B, Zephyr, Vicuna, Bloom, etc. This project supports full parameter training, LoRA, QLoRA efficient training, and supports pre-training, SFT, DPO. If your training resources are limited, we strongly recommend using QLoRA for instruction fine-tuning, as we have validated the effectiveness of this method on the Open LLM Leaderboard and achieved very good results.\nMain Features:\n📗 Supports pre-training, instruction fine-tuning, DPO, full parameter training, LoRA, QLoRA efficient training. Train different models through configuration files, allowing beginners to quickly get started with model training. 📗 Supports using Unsloth\rto accelerate training and save video memory. 📗 Supports most mainstream open-source large models, such as Llama3, Gemma, MiniCPM, Llama, InternLM, Baichuan, ChatGLM, Yi, Deepseek, Qwen, Orion, Ziya, Xverse, Mistral, Mixtral-8x7B, Zephyr, Vicuna, Bloom, aligning with the templates of each official chat model during training. 📗 Organize and open-source instruction fine-tuning datasets: firefly-train-1.1M, moss-003-sft-data, ultrachat, WizardLM_evol_instruct_V2_143k, school_math_0.25M. 📗 Open source Firefly series instruction fine-tuning model weights\r. 📗 Validated the effectiveness of the QLoRA training process on the Open LLM Leaderboard. The README of the project contains detailed usage instructions, including how to install, how to train, how to fine-tune, and how to evaluate, etc. Please visit the Firefly\rproject homepage.\n#\rXTuner\rInternLM\r/\rxtuner\rWaiting for api.github.com...\r0\r0\runkown\rWaiting...\rXTuner is an efficient, flexible, and versatile lightweight large model fine-tuning tool library.\nMain Features:\nEfficient Supports pre-training and lightweight fine-tuning of large language models (LLM) and multimodal image-text models (VLM). XTuner supports fine-tuning a 7B model with 8GB of video memory and also supports multi-node cross-device fine-tuning of larger scale models (70B+). Automatically distribute high-performance operators (such as FlashAttention, Triton kernels, etc.) to accelerate training throughput. Compatible with DeepSpeed\r🚀, easily apply various ZeRO training optimization strategies. Flexible Supports multiple large language models, including but not limited to InternLM\r, Mixtral-8x7B\r, Llama 2\r, ChatGLM\r, Qwen\r, Baichuan\r. Supports pre-training and fine-tuning of the multimodal text-image model LLaVA. The model LLaVA-InternLM2-20B\rtrained with XTuner performs excellently. Carefully designed data pipeline, compatible with any data format, both open-source data and custom data can be quickly adopted. Supports multiple fine-tuning algorithms such as QLoRA\r, LoRA\r, and full parameter fine-tuning, allowing users to make the optimal choice based on specific needs. Omnipotent Supports incremental pre-training, instruction fine-tuning, and Agent fine-tuning. Predefined numerous open-source dialogue templates, supporting conversation with open-source or trained models. The trained model can be seamlessly integrated with the deployment toolkit LMDeploy\r, the large-scale evaluation toolkit OpenCompass\r, and VLMEvalKit\r. Quick Start: Installation\rIt is recommended to use conda to first build a Python-3.10 virtual environment conda create --name xtuner-env python=3.10 -y\rconda activate xtuner-env\rInstall XTuner via pip: pip install -U xtuner\rCan also integrate DeepSpeed installation:\npip install -U 'xtuner[deepspeed]'\rInstall XTuner from source: git clone https://github.com/InternLM/xtuner.git\rcd xtuner\rpip install -e '.[all]'\rFine-tuning\rXTuner supports fine-tuning large language models. For dataset preprocessing guidelines, please refer to the documentation\r.\nStep 0, prepare the configuration file. XTuner provides multiple out-of-the-box configuration files, which users can view with the following command: xtuner list-cfg\rOr, if the provided configuration file does not meet the usage requirements, please export the provided configuration file and make the corresponding changes:\nxtuner copy-cfg ${CONFIG_NAME} ${SAVE_PATH}\rvi ${SAVE_PATH}/${CONFIG_NAME}_copy.py\rStep 1, start fine-tuning. xtuner train ${CONFIG_NAME_OR_PATH}\rFor example, we can use the QLoRA algorithm to fine-tune InternLM2.5-Chat-7B on the oasst1 dataset:\n# Single card\rxtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2\r# Multi-card\r(DIST) NPROC_PER_NODE=${GPU_NUM} xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2\r(SLURM) srun ${SRUN_ARGS} xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --launcher slurm --deepspeed deepspeed_zero2\r--deepspeed indicates using DeepSpeed\r🚀 to optimize the training process. XTuner has multiple built-in strategies, including ZeRO-1, ZeRO-2, ZeRO-3, etc. If the user wishes to disable this feature, please remove this parameter directly.\nFor more examples, please refer to the documentation\r. Step 2, convert the saved PTH model (if using DeepSpeed, it will be a folder) to a HuggingFace model:\nxtuner convert pth_to_hf ${CONFIG_NAME_OR_PATH} ${PTH} ${SAVE_PATH}\rFor more detailed information, please visit the XTuner\rproject homepage.\n#\rModel quantization\rLLM is usually large in volume and requires high computational resources. Model quantization techniques can compress model size, improve operational efficiency, and make it easier to deploy:\n#\rAutoGPTQ\rPanQiWei\r/\rAutoGPTQ\rWaiting for api.github.com...\r0\r0\runkown\rWaiting...\rAutoGPTQ is a large language model quantization toolkit based on the GPTQ algorithm, simple to use and with a user-friendly interface.\nQuick Installation\nFor CUDA 11.7: pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/\rFor CUDA 11.8: pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\rFor RoCm 5.4.2: pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/rocm542/\rFor more detailed information, please visit the project homepage of AutoGPTQ\r.\n#\rAutoAWQ\rcasper-hansen\r/\rAutoAWQ\rWaiting for api.github.com...\r0\r0\runkown\rWaiting...\rAutoAWQ is another automated model quantization tool that supports multiple quantization precisions and offers flexible configuration options, allowing adjustments based on different hardware platforms and performance requirements.\nAutoAWQ is an easy-to-use 4-bit quantization model package. Compared to FP16, AutoAWQ can increase model speed by 3 times and reduce memory requirements by 3 times. AutoAWQ implements the activation-aware weight quantization (AWQ) algorithm for quantizing LLMs. AutoAWQ was created and improved based on the original work AWQ\rfrom MIT.\nInstallation Method:\nBefore installing, ensure that CUDA \u0026gt;= 12.1 is installed (Note: The following is just the quickest installation method)\npip install autoawq\rFor more details and examples, please visit the project homepage of AutoAWQ\r.\n#\rNeural Compressor\rintel\r/\rneural-compressor\rWaiting for api.github.com...\r0\r0\runkown\rWaiting...\rNeural Compressor is a model compression toolkit developed by Intel, supporting popular model compression techniques on all major deep learning frameworks (TensorFlow, PyTorch, ONNX Runtime, and MXNet).\nInstallation Method:\npip install \u0026quot;neural-compressor\u0026gt;=2.3\u0026quot; \u0026quot;transformers\u0026gt;=4.34.0\u0026quot; torch torchvision\rFor more detailed information and examples, please visit the project homepage of Neural Compressor\r.\n#\rModel deployment\rDeploying a trained LLM to a production environment is crucial. Here are some commonly used LLM deployment tools:\n#\rvLLM\rvllm-project\r/\rvllm\rWaiting for api.github.com...\r0\r0\runkown\rWaiting...\rvLLM is a fast and easy-to-use LLM inference service library.\nMain Features:\nFast SOTA service throughput Efficiently manage attention key-value memory using PagedAttention Continuously batch process received requests Use CUDA/HIP graphs for acceleration Quantization: Supports GPTQ, AWQ, SqueezeLLM, FP8 KV cache Optimized CUDA kernel Flexible Seamless integration with popular Hugging Face models Provide high-throughput services using various decoding algorithms (including parallel sampling, beam search, etc.) Provide tensor parallel support for distributed inference Stream output Compatible with OpenAI\u0026rsquo;s application programming interface server Supports NVIDIA GPU, AMD GPU, Intel CPU, and GPU (Experimental) Support prefix caching (Experimental) Support for multiple languages Seamless Support Transformer-based models, such as Llama MoE-based model, such as Mixtral Multimodal models, such as LLaVA Quick Installation:\npip install vllm\rFor more detailed information, please refer to the vLLM\rofficial documentation.\n#\rSGL\rsgl-project\r/\rsglang\rWaiting for api.github.com...\r0\r0\runkown\rWaiting...\rSGLang is a structured generation language designed specifically for large language models (LLMs). By co-designing the front-end language and the runtime system, it makes your interactions with LLMs faster and more controllable.\nMain Features:\nFlexible front-end language: Easily write LLM applications through chainable generation calls, advanced prompts, control flow, multiple modes, concurrency, and external interaction. High-performance backend runtime: Features RadixAttention capability, which can accelerate complex LLM programs by reusing KV cache across multiple calls. It can also function as a standalone inference engine, implementing all common techniques (such as continuous batching and tensor parallelism). For more detailed information, please visit the SGL\rproject homepage.\n#\rSkyPilot\rskypilot-org\r/\rskypilot\rWaiting for api.github.com...\r0\r0\runkown\rWaiting...\rSkyPilot is a flexible cloud LLM deployment tool launched by UC Berkeley RISELab, supporting multiple cloud platforms and hardware accelerators. It can automatically select the optimal deployment plan and provide cost optimization features.\nMain Features:\nMulti-cloud support: Supports various cloud platforms such as AWS, GCP, Azure, allowing users to choose the appropriate deployment environment. Easy to expand: Queue and run multiple jobs, automatic management Easy Access to Object Storage: Easily access object storage (S3, GCS, R2) For more detailed information, please visit the SkyPilot\rproject homepage.\n#\rTensorRT-LLM\rNVIDIA\r/\rTensorRT-LLM\rWaiting for api.github.com...\r0\r0\runkown\rWaiting...\rTensorRT-LLM is a high-performance LLM inference engine launched by NVIDIA, capable of fully utilizing GPU accelerated computation and optimized for the Transformer model architecture, significantly improving inference speed.\nTensorRT-LLM provides users with an easy-to-use Python API for defining large language models (LLMs) and building TensorRT engines, which incorporate state-of-the-art optimization techniques for efficient inference execution on NVIDIA® graphics processors. TensorRT-LLM also includes components for creating Python and C++ runtimes that execute these TensorRT engines.\nFor more details, please visit the TensorRT-LLM\rproject homepage.\n#\rOpenVino\ropenvinotoolkit\r/\ropenvino\rWaiting for api.github.com...\r0\r0\runkown\rWaiting...\rOpenVINO™ is an open-source toolkit for optimizing and deploying artificial intelligence inference.\nMain Features:\nInference Optimization: Enhance the performance of deep learning in computer vision, automatic speech recognition, generative AI, natural language processing using large and small language models, and many other common tasks. Flexible Model Support: Models trained with popular frameworks such as TensorFlow, PyTorch, ONNX, Keras, and PaddlePaddle. Convert and deploy models without the need for the original framework. Broad Platform Compatibility: Reduce resource requirements and efficiently deploy across a range of platforms from edge to cloud. OpenVINO™ supports inference on CPUs (x86, ARM), GPUs (integrated and discrete GPUs supporting OpenCL), and AI accelerators (Intel NPU). Community and Ecosystem: Join an active community contributing to improving deep learning performance in various fields. For more information, please visit the project homepage of OpenVino\r.\n#\rTGI\rhuggingface\r/\rtext-generation-inference\rWaiting for api.github.com...\r0\r0\runkown\rWaiting...\rText Generation Inference (TGI) is a toolkit for deploying and serving large language models (LLMs). TGI enables high-performance text generation for the most popular open-source LLMs, including Llama, Falcon, StarCoder, BLOOM, GPT-NeoX, and others.\nTGI has implemented many features, and detailed information can be found on the project homepage of TGI\r.\n#\rLocal run\rThanks to model compression and optimization techniques, we can also run LLM on personal devices:\n#\rMLX\rml-explore\r/\rmlx\rWaiting for api.github.com...\r0\r0\runkown\rWaiting...\rMLX is a framework specifically designed to support running LLM on Apple devices, fully utilizing Metal to accelerate computation, and providing easy-to-use APIs to facilitate developers in integrating LLM into iOS applications.\nMain Features:\nSimilar Application Programming Interface: MLX\u0026rsquo;s Python API is very similar to NumPy. MLX also has fully functional C++, C, and Swift APIs, which are very similar to the Python API. MLX has higher-level packages like mlx.nn and mlx.optimizers, whose APIs are very close to PyTorch, simplifying the construction of more complex models. Composable Function Transformations: MLX supports composable function transformations for automatic differentiation, automatic vectorization, and computation graph optimization. Lazy Evaluation: In MLX, computations only materialize arrays when needed. Dynamic Graph Construction: In MLX, the computational graph is dynamically constructed. Changing the shape of function parameters does not slow down the compilation speed, and debugging is simple and intuitive. Multi-device: Operations can run on any supported device (currently CPU and GPU). Unified Memory: The unified memory model is a significant difference between MLX and other frameworks. Arrays in MLX reside in shared memory. Operations on MLX arrays can be performed on any supported device type without the need to transfer data. MLX is designed by machine learning researchers for machine learning researchers. The framework aims to be user-friendly while still efficiently training and deploying models. The design concept of the framework itself is also very simple. Our goal is to enable researchers to easily expand and improve MLX, thus quickly exploring new ideas. For more details, please visit the project homepage of MLX\r.\n#\rLlama.cpp\rLlama.cpp is a Llama model inference engine implemented in C++, capable of running efficiently on CPUs, and supports multiple operating systems and hardware platforms, allowing developers to run LLM on resource-constrained devices.\nggerganov\r/\rllama.cpp\rWaiting for api.github.com...\r0\r0\runkown\rWaiting...\rMain Features:\nCPU Inference: Optimized for CPU platforms, allowing LLM to run on devices without a GPU. Cross-platform support: Supports multiple operating systems such as Linux, macOS, Windows, making it convenient for users to use on different platforms. Lightweight Deployment: The compiled binary files are small, making it convenient for users to deploy and use. For more detailed information, please visit the project homepage of Llama.cpp\r.\n#\rOllama\rollama\r/\rollama\rWaiting for api.github.com...\r0\r0\runkown\rWaiting...\rIn the article 【Ollama: From Beginner to Advanced】\r, it is introduced that Ollama is a tool for building large language model applications. It provides a simple and easy-to-use command line interface and server, allowing you to easily download, run, and manage various open-source LLMs. Unlike traditional LLMs that require complex configuration and powerful hardware, Ollama allows you to conveniently experience the powerful features of LLMs as easily as using a mobile app.\nMain Features:\nSimple and Easy to Use: Ollama provides a simple and easy-to-use command line tool for users to download, run, and manage LLM. Multiple models: Ollama supports various open-source LLMs, including Qwen2, Llama3, Mistral, etc. Compatible with OpenAI Interface: Ollama supports the OpenAI API interface, making it easy to switch existing applications to Ollama. For more details, please visit the project homepage of Ollama\r.\n#\rAgent and RAG framework\rCombining LLM with external data and tools can build more powerful applications. Here are some commonly used Agent and RAG frameworks:\n#\rLlamaIndex\rrun-llama\r/\rllama_index\rWaiting for api.github.com...\r0\r0\runkown\rWaiting...\rLlamaIndex (GPT Index) is a data framework for LLM applications. Building applications with LlamaIndex typically requires using LlamaIndex core and a selected set of integrations (or plugins). There are two ways to build applications with LlamaIndex in Python:\nLauncher: llama-index ( https://pypi.org/project/llama-index/)\r. Python starter package, includes core LlamaIndex and some integrations. Customization: llama-index-core ( https://pypi.org/project/llama-index-core/)\r. Install the core LlamaIndex and add the necessary LlamaIndex integration packages for your application on LlamaHub. There are currently over 300 LlamaIndex integration packages that can seamlessly collaborate with the core, allowing you to build using your preferred LLM, embeddings, and vector storage databases. The LlamaIndex Python library is named as such, so import statements containing core mean that the core package is being used. Conversely, those statements without core mean that the integration package is being used.\n# typical pattern\rfrom llama_index.core.xxx import ClassABC # core submodule xxx\rfrom llama_index.xxx.yyy import (\rSubclassABC,\r) # integration yyy for submodule xxx\r# concrete example\rfrom llama_index.core.llms import LLM\rfrom llama_index.llms.openai import OpenAI\r#\rCrewAI\rjoaomdmoura\r/\rcrewAI\rWaiting for api.github.com...\r0\r0\runkown\rWaiting...\rCrewAI is a framework for building AI Agents that can integrate LLM with other tools and APIs to accomplish more complex tasks, such as automating web operations, generating code, and more.\nMain Features:\nRole-Based Agent Design: You can customize agents using specific roles, goals, and tools. Delegation between Autonomous Agents: Agents can autonomously delegate tasks to other agents and query information from each other, thereby improving problem-solving efficiency. Flexible task management: Customizable tools can be used to define tasks and dynamically assign tasks to agents. Process-Driven: The system is process-centered, currently supporting sequential task execution and hierarchical processes. In the future, it will also support more complex processes, such as negotiation and autonomous processes. Save output as file: Allows saving the output of a single task as a file for later use. Parse output to Pydantic or Json: It is possible to parse the output of a single task into a Pydantic model or Json format for easy subsequent processing and analysis. Support for Open Source Models: You can use OpenAI or other open source models to run your agent team. For more information on configuring agent and model connections, including how to connect to a locally running model, see Connecting crewAI to Large Language Models\r. For more detailed information, please visit the project homepage of CrewAI\r.\n#\rOpenDevin\ropendevin\r/\ropendevin\rWaiting for api.github.com...\r0\r0\runkown\rWaiting...\rOpenDevin is an autonomous software engineer platform powered by artificial intelligence and LLMs.\nOpenDevin agents collaborate with human developers to write code, fix bugs, and release features.\nFor more information, please visit the project homepage of OpenDevin\r.\n#\rModel evaluation\rIn order to select a suitable LLM and evaluate its performance, we need to conduct model evaluation:\n#\rLMSys\rLMSys Org is an open research organization founded by students and faculty from the University of California, Berkeley, in collaboration with the University of California, San Diego, and Carnegie Mellon University.\nThe goal is to make large models accessible to everyone by jointly developing open models, datasets, systems, and evaluation tools. Train large language models and make their applications widely available, while also developing distributed systems to accelerate their training and inference process.\nCurrently, the LMSys Chatbot Area is one of the most recognized large model rankings, acknowledged by many companies and research institutions.\nLeaderboard address: https://arena.lmsys.org/\r#\rOpenCompass\rOpenCompass is an LLM evaluation platform that supports various models (Llama3, Mistral, InternLM2, GPT-4, LLaMa2, Qwen, GLM, Claude, etc.) on over 100 datasets.\nopen-compass\r/\ropencompass\rWaiting for api.github.com...\r0\r0\runkown\rWaiting...\r#\rOpen LLM Leaderboard\rOpen LLM Leaderboard is a continuously updated LLM ranking list that ranks different models based on multiple evaluation metrics, making it convenient for developers to understand the latest model performance and development trends.\nLeaderboard address: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\r#\rSummary\rThe LLM ecosystem is thriving, covering all aspects from model training to application implementation. With continuous technological advancements, it is believed that LLM will play a more important role in more fields, bringing us a more intelligent application experience.\n","date":"2024-07-05T22:46:00Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_119269220_p0_master1200.webp","permalink":"https://cuterwrite.top/en/p/llm-ecosystem/","title":"Introduction to LLM Ecosystem: From Model Fine-tuning to Application Implementation"},{"content":"\r#\rMemory Window of RDMA\rThis article welcomes non-commercial reprints, please indicate the source when reprinting.\nStatement: For collection only, for easier reading.\n― Savir, Zhihu Column: 14. RDMA Memory Window This article is the 14th in the \u0026ldquo;RDMA Talk\u0026rdquo; column. Welcome to repost, please indicate the source when reposting.\nIn the article 【RDMA Memory Region】\r, we introduced Memory Region, which is a special memory area registered by the user: on one hand, its contents will not be swapped to the hard disk, and on the other hand, the RDMA network card records its address translation relationship, allowing the hardware to find the corresponding physical address after obtaining the virtual address specified by the user in the WR.\nIn this article, we will explain the concept of Memory Window, which is a more flexible memory management unit based on Memory Region. Besides the concept of MW, this article will also provide a more detailed introduction to some memory-related concepts in the RDMA field, such as L_Key/R_Key, etc. It is recommended to read this article in conjunction with 【RDMA Memory Region】\rfor better understanding, and it is suggested that readers review it first.\n#\rWhat is Memory Window\rMemory Window, abbreviated as MW, can be translated into Chinese as 内存窗口. It is an RDMA resource requested by the user to allow a remote node to access the local memory area. Each MW is bound (referred to as bind) to an already registered MR, but compared to MR, it can provide more flexible permission control. MW can be roughly understood as a subset of MR, and many MWs can be divided from one MR, each MW can set its own permissions. The relationship between MW and MR is shown in the following diagram:\nThe relationship between MR and MW #\rMemory access permission control\rTo explain why MW is designed, let\u0026rsquo;s first discuss the access control involved in both MR and MW.\n#\rMR/MW permissions configuration\rThe permissions here refer to the local/remote node, for the read/write permissions of the local memory, they form four combinations:\nLocal End Remote End Read Local Read Remote Read Write Local Write Remote Write Apart from these four types of permissions, there are also Atomic permissions, etc., which are not within the scope of this article.\nAmong the four types of permissions in the table, the lowest is Local Read, which is a permission that users must grant to MR/MW because if a piece of memory is inaccessible to local users, it loses its meaning. Additionally, there is a restriction: if an MR needs to be configured with Remote Write or the not-yet-introduced Remote Atomic permissions, it must also be configured with Local Write permissions. Under this constraint, each MR or MW can configure permissions as needed. For example, if an MR we registered needs to allow remote nodes to write data but not read, we enable the Remote Write permission and disable the Remote Read permission. In this way, when the HCA (network card) receives a WRITE request initiated by the peer for a certain address within the range of this MR, it can allow it; however, when the HCA receives a READ operation from the peer on this MR, it will reject the request and return an error message to the peer.\n#\rMemory Key\rThe above access permission configuration cannot prevent malicious users from accessing local or remote memory. For example, if a node grants Remote Write permission to a memory region, wouldn\u0026rsquo;t any remote node (process) be able to write to this region as long as it provides the correct address information? Therefore, the IB specification designed the Memory Key, which can be simply understood as a key mechanism for accessing MR. Only with the correct key can one open the door to MR/MW.\nKey is a string of numbers, consisting of two parts: a 24-bit Index and an 8-bit Key:\nComposition of L_Key/R_Key Among them, Index is used by HCA for quick indexing to local virtual-to-physical address translation tables and other MR-related information, while Key is used to verify the legality of the entire field to prevent unauthorized users from arbitrarily passing the Index.\nMemory Key is divided into two types according to their usage, Local Key and Remote Key:\n#\rL_Key\rLocal Key, associated with an MR, is used for HCA to access local memory. When a process on the local side attempts to use memory of an already registered MR, the HCA will verify the L_Key it passes. It uses the index in the L_Key to look up the address translation table, translates the virtual address into a physical address, and then accesses the memory.\nIn the article 【RDMA Shared Receive Queue】\r, we described sge, which consists of a starting address, length, and key. When users fill out a WR, if they need the HCA to access the local memory, they need to describe the memory block through a linked list of sge (sgl). Here, the key in the sge is filled with L_Key, which are key1 and key3 in the diagram below, representing the L_Key of MR1 and MR2, respectively. Without L_Key, any local user process could direct the hardware to access the contents of other locally registered MRs, and the hardware would find it difficult to efficiently translate virtual addresses to physical addresses.\nThe function of L_Key #\rR_Key\rRemote Key, associated with an MR or MW, is used for a remote node to access local memory. When a remote node attempts to access local memory, on one hand, the local HCA will verify whether the R_Key is valid, and on the other hand, it will use the index in the R_Key to check the address translation table, translating the virtual address into a physical address and then accessing the memory.\nFor any RDMA operation (i.e., Write/Read/Atomic), the user must carry the remote memory region\u0026rsquo;s R_Key in the WR.\nThe Function of R_Key The IB specification ensures that MR can be accessed correctly and safely according to the user\u0026rsquo;s expectations through the two mechanisms mentioned above. We use a metaphor to summarize the content related to MR/MW permission control:\nA equipped their room (MR) with two keys (Memory Key), one for personal use (L_Key), and the other key (R_Key) was sent to B (can be via any communication method). B can open the door when A is not home (the local CPU does not perceive the remote node\u0026rsquo;s RDMA operations on local memory) using the key (R_Key). After opening the door, B might only be able to view the room\u0026rsquo;s arrangement through glass (A only granted remote read permission for this MR), or enter the room and find it completely dark, unable to see anything, but can place items in the room (A only granted remote write permission for this MR), and of course, it is also possible that there\u0026rsquo;s no glass and the lights are on (remote read and write permissions were granted simultaneously).\n#\rWhy have MW\rIn short, the purpose of designing MW is to control remote memory access permissions more flexibly.\nIn the article 【RDMA 之 Memory Region】\r, we introduced the process of user registering MR, which requires transitioning from user mode to kernel mode, calling the function provided by the kernel to pin the memory (to prevent paging), and then creating a virtual-physical address mapping table and issuing it to the hardware.\nBecause MR is managed by the kernel, if a user wants to modify the information of an existing MR, for example, if I want to revoke the remote write permission of a certain MR, leaving only the remote read permission; or if I want to invalidate an R_Key that was previously authorized to a remote node, the user needs to use the Reregister MR interface to make modifications. This interface is equivalent to first Deregister MR and then Register MR. The above process requires transitioning to kernel mode to complete, and this process is time-consuming.\nUnlike MR, which requires permission modification through the control path, MW can be dynamically bound to an already registered MR through the data path (i.e., directly issuing WR to the hardware from user space) after creation, and simultaneously set or change its access permissions. This process is much faster than re-registering MR.\nIn order for a piece of memory to be capable of RDMA WRITE/READ operations by a remote node, we have two methods: registering an MR and registering an MW and then binding it to an already registered MR. Both will generate an R_Key to provide to the remote node. The first method has simpler preparation steps but is less flexible, and once registered, modifications are relatively troublesome. The second method involves additional steps of registering an MW and binding the MW to an MR compared to the first method, but it allows for convenient and quick control over remote access permissions.\n#\rThe relationship between MW and MR permissions\rPerhaps some readers might think, when configuring their permissions during MR application, and when MW is bound to MR, their permissions are also configured, what is the relationship between these two permissions? The IB specification has a dedicated section on this in 10.6.7.2.2:\nWhen binding a Memory Window, a Consumer can request any combination of remote access rights for the Window. However, if the associated Region does not have local write access enabled and the Consumer requests remote write or remote atomic access for the Window, the Channel Interface must return an error either at bind time or access time.\nIn summary, if you want to configure remote write or remote atomic operation (Atomic) permissions for MW, then the MR it is bound to must have local write permissions. In other cases, the permissions of the two do not interfere with each other: remote users using MW must follow the permission configuration of MW; remote users using MR must follow the permission configuration of MR.\n#\rUser Interface\rAs usual, when it comes to user interfaces, we classify them according to control paths and data paths:\n#\rControl path\rMW supports addition, deletion, and search, but cannot be directly modified:\n#\rCreate - Allocate MW\rApply for MW, mainly to create the software structure related to MW and prepare the hardware. The user needs to specify the type of MW introduced later in the text. This interface will generate a handle for the Memory Window, which the user can use to refer to this MW in the future.\nNote that at this time MW is not bound to MR and is in a state that cannot be accessed remotely.\n#\rDelete - Deallocate MW\rUnregister MW. It\u0026rsquo;s easy to understand, just destroy the related resources.\n#\rQuery - Query MW\rQuery MW information, including R_Key and its status, MW type, and PD, etc.\nIt needs to be emphasized again that although this Verbs is described in the IB specification, the related API has not been implemented in the RDMA software stack. There are quite a few Verbs interfaces in similar situations. The RDMA software stack is based on practicality, and interfaces without user demand are generally not implemented.\n#\rData path\rMW has a unique set of interfaces in the data path, divided into Bind and Invalidate categories:\n#\rBind\rBind(ing) means \u0026ldquo;binding,\u0026rdquo; which refers to associating an MW with a specified range of an already registered MR and configuring certain read and write permissions. The result of binding will generate an R_key, which the user can pass to a remote node for remote access. Note that an MW can be bound multiple times, and multiple MWs can be bound to a single MR. If an MR still has bound MWs, then this MR cannot be deregistered.\nBind\u0026#39;s Software and Hardware Interaction There are two ways to bind: one is to call the Post Send interface to issue a Bind MW WR, and the other is to call the Bind MW interface.\nPost Send Bind MW WR In the previous text, we discussed that compared to MR, the biggest advantage of MW is the ability to quickly configure permissions from the data path. Post Send Bind MW WR operation refers to the user issuing a WR to the SQ through the post send interface (such as ibv_post_send()), where the operation type of this WR (such as SEND/RDMA WRITE/RDMA READ) is specified as BIND MW. Additionally, the WR carries information about the permissions and the range of the MR to be bound. Unlike other WRs, after issuing a Bind MW WR, the hardware does not send any packets but instead binds the MW to the specified MR.\nThis method is only applicable to Type 2 MW introduced later.\nBind MW Although this is an independent interface, it is actually an additional layer encapsulated outside Post Send Bind MW WR. The user provides the relevant information for MW binding, including permissions and the information of the MR to be bound. The driver is responsible for assembling and issuing the WR to the hardware. After the interface succeeds, the newly generated R_Key will be returned to the user.\nThis method is only applicable to Type 1 MW introduced later.\nThe relationship between the above two operations is as follows:\nThe relationship between two types of Bind operations #\rInvalidation\rInvalidate means invalidation, referring to the operation where a user sends a WR with an Invalidate opcode to the hardware to invalidate an R_Key.\nIt is important to emphasize that the object of the Invalidate operation is the R_Key, not the MW itself. The effect after Invalidate is that the remote user can no longer use this R_Key to access the corresponding MW, but the MW resource still exists, and new R_Keys can still be generated for remote use in the future.\nThe Invalidate operation can only be used for Type 2 MW introduced below.\nAccording to the different initiators of the Invalidate operation, it can be further divided into two types:\nLocal Invalidate Invalid local operation. If a higher-level user wants to revoke the R_Key permissions of a certain remote user without reclaiming MW resources, they can issue a Local Invalidate operation to the SQ. After the hardware receives it, it will modify the configuration of the corresponding MR. After successful execution, if the remote user holding this R_Key attempts to perform RDMA operations on the MW, the local hardware will reject it and return an error.\nBecause it is a local operation, the hardware will not send a message to the link after receiving this WR.\nSoftware and Hardware Interaction of Local Invalidate Operation Remote Invalidate Remote invalid operation. When a remote user no longer uses an R_Key, they can proactively send a message to allow the local end to reclaim this R_Key. The remote user issues a WR with this operation code to the SQ, and once the hardware receives it, it will assemble a message and send it to the local end. After the local hardware receives the remote\u0026rsquo;s Remote Invalidate operation, it will set the corresponding R_Key to an unusable state. Just like Local Invalidate, thereafter the remote end will not be able to use this R_Key to perform RDMA operations on the corresponding MW.\nRemote Invalidate operation\u0026#39;s software and hardware interaction #\rType of MW\rAccording to different implementations and application scenarios, the IB specification classifies MW:\n#\rType 1\rType 1 MW is associated with a PD and a QP, and it is not bound to a QP, so it will not affect the destruction of a QP under the same PD.\nThe key field of the R_Key for Type 1 MW is controlled by the driver and hardware. Here, \u0026ldquo;controlled\u0026rdquo; means that the key is allocated by the driver and hardware, not by the upper-level user. This is also the reason mentioned earlier that Type 1 MW cannot perform the Invalidate operation. If a user of Type 1 MW wants to invalidate an R_Key, they can bind this MW again through the Bind MW interface. The hardware or driver will automatically allocate a new key field for the R_Key, and the original R_Key will become invalid.\nIn addition, if a user temporarily wants to unbind an MW from any MR but still wants to retain the related resources instead of destroying this MW, they can achieve this by calling the Bind MW interface and setting the MW length to 0.\nThe IB specification allows multiple Type 1 MWs to be bound to the same MR, and their ranges can overlap.\n#\rType 2\rType 2 MW grants users greater freedom, with the key field segment of the R_Key controlled by the user, allowing them to allocate it as they wish. As mentioned earlier, users perform binding through the Post Send Bind MW WR operation, and this process does not return an R_Key. Users must remember the index from the Allocate MW operation and combine it with their chosen 8-bit key to form the R_Key and send it to the peer.\nThe user can invalidate an R_Key through the Invalidate operation introduced earlier. If you want to assign a new R_Key to the MW, you must first invalidate the previous R_Key through the Invalidate operation.\nUnlike Type 1, Type 2\u0026rsquo;s MW does not support 0-length binding.\nThe IB specification also allows multiple Type 2s to be bound to the same MR, and the ranges can overlap.\nIn addition, based on different binding relationships, Type 2 can be further divided into two implementation methods, with their differences lying solely in the binding relationship with QP.\n#\rType 2A\rAssociated with a QP through QPN, meaning that when remote access occurs within this MW range, in addition to the R_Key, the correct QPN must also be specified. If a QP has a bound Type 2A MW, then this QP cannot be destroyed.\n#\rType 2B\rBy associating a QP with QPN and PD, there is an additional PD verification compared to Type 2A. When the remote end accesses the memory of the MW through RDMA operations, besides the QPN needing to be correct, the PD specified for the local QP must also be the same as the PD bound to this MW. Additionally, unlike Type 2A, a QP can be destroyed even if there is still a Type 2B MW binding relationship.\nThe introduction in the original IB specification is relatively scattered, so let\u0026rsquo;s briefly summarize the similarities and differences of several MWs:\nType 1 Type 2A Type 2B Correlation PD QP PD + QP R_Key\u0026rsquo;s key field ownership Driver + Hardware User User Binding Method Bind MW After binding, the previous R_Key automatically becomes invalid Post Send Bind MWWR Before binding, the previous R_Key needs to be invalidated Post Send Bind MWWR Before binding, the previous R_Key needs to be invalidated Is zero length supported Yes No No Supports Invalidate No Yes Yes Can the associated QP be destroyed - No Yes In addition, the IB specification also provides the following descriptions for the above types: HCA must implement Type 1 MW, and can optionally choose to implement either Type 2A or 2B. Type 1 and Type 2 MW can be simultaneously associated with the same MR. Since I have not encountered many applications using MW, I cannot clearly explain in which scenarios each type of MW should be used. If readers have insights on this topic, they are welcome to share and discuss.\nAlright, MW will be discussed up to here, and this concludes the introduction of common resources in RDMA technology.\nGiven that devices generally supporting RDMA are quite expensive, in the next article I will introduce how to conduct some programming experiments through software-simulated devices—namely Soft-RoCE.\n#\rIB specification related chapters\r3.5.3 Memory Keys Introduction\n9.4.1.1 Invalidate Operation\n10.6.7 Permission Management\n11.2.10.9~12 Related Verbs Introduction\n#\rReference document\r[1] IB Specification Vol 1-Release-1.4\n[2] Linux Kernel Networking - Implementation and Theory. Chapter 13\n","date":"2024-06-26T23:55:00Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116373724_p0_master1200.webp","permalink":"https://cuterwrite.top/en/p/rdma-memory-window/","title":"RDMA: Memory Window"},{"content":"\r#\rShared Receive Queue in RDMA\rThis article welcomes non-commercial reprints, please indicate the source when reprinting.\nStatement: For collection only, for easy reading.\n― Savir, Zhihu Column: 11. RDMA Shared Receive Queue We briefly introduced the concept of SRQ in 【3. Basic Elements of RDMA】\r. This article will take you through more details about SRQ.\n#\rBasic Concepts\r#\rWhat is SRQ?\rThe full name is Shared Receive Queue, literally translated as a shared receive queue. We know that the basic unit of RDMA communication is QP, and each QP consists of a send queue SQ and a receive queue RQ.\nSRQ is designed by the IB protocol to save resources for the receiver. We can share an RQ with all associated QPs, and this shared RQ is called an SRQ. When a QP associated with it wants to post a receive WQE, it is filled into this SRQ. Then, whenever the hardware receives data, it stores the data in the specified location based on the content of the next WQE in the SRQ.\n#\rWhy use SRQ\rUnder normal circumstances, the number of tasks we issue to SQ is far greater than the number of tasks issued to RQ. Why is that? Please first recall which types of operations use SQ and which use RQ.\nSEND/WRITE/READ all require the communication initiator to issue a WR to the SQ, and only the RECV operation paired with SEND requires the communication responder to issue a WR to the RQ (the Write operation with immediate value will also consume Receive WR, which we haven\u0026rsquo;t discussed yet). As we know, the SEND-RECV pair of operations is usually used for transmitting control information, while WRITE and READ are the main operations for performing large amounts of remote memory read and write operations, so naturally, the usage rate of SQ is much higher than that of RQ.\nEach queue is an entity, occupying memory and on-chip storage space of the network card. In commercial scenarios, the number of QPs can reach hundreds of thousands or even higher, which places high demands on memory capacity. Memory is bought with hard-earned money, and SRQ is a mechanism designed by the IB protocol to save user memory.\nLet\u0026rsquo;s take a look at the official explanation in the agreement for why SRQ is used (Section 10.2.9.1):\nWithout SRQ, an RC, UC or UD Consumer must post the number of receive WRs necessary to handle incoming receives on a given QP. If the Consumer cannot predict the incoming rate on a given QP, because, for example, the connection has a bursty nature, the Consumer must either: post a sufficient number of RQ WRs to handle the highest incoming rate for each connection, or, for RC, let message flow control cause the remote sender to back off until local Consumer posts more WRs.\n• Posting sufficient WRs on each QP to hold the possible incoming rate, wastes WQEs, and the associated Data Segments, when the Receive Queue is inactive. Furthermore, the HCA doesn’t provide a way of reclaiming these WQEs for use on other connections.\n• Letting the RC message flow control cause the remote sender to back off can add unnecessary latencies, especially if the local Consumer is unaware that the RQ is starving.\nIn simple terms, without SRQ, because the receiver of RC/UC/UD does not know how much data the other end will send and when it will arrive, it must prepare for the worst-case scenario, preparing for the possibility of receiving a large amount of data suddenly, which means issuing a sufficient number of receive WQEs to the RQ. Additionally, the RC service type can use flow control mechanisms to exert backpressure on the sender, essentially telling the other end \u0026ldquo;I don\u0026rsquo;t have enough RQ WQEs here,\u0026rdquo; so the sender will temporarily slow down or stop sending data.\nHowever, as we mentioned earlier, the first method, being prepared for the worst-case scenario, often results in a large number of RQ WQEs being idle and unused, which is a significant waste of memory. Although the second method doesn\u0026rsquo;t require issuing as many RQ WQEs, flow control comes at a cost, which is the increased communication latency.\nAnd SRQ solves the above problem by allowing many QPs to share receive WQEs (as well as memory space for storing data). When any QP receives a message, the hardware will take a WQE from the SRQ, store the received data according to its content, and then the hardware will return the completion information of the receive task to the corresponding upper-level user through the Completion Queue.\nLet\u0026rsquo;s take a look at how much memory can be saved by using SRQ compared to using a standard RQ1:\nAssume that there are N pairs of QP on the node receiving data, and each QP may receive a consecutive M messages at random times (each message consumes a WQE from an RQ),\nIf SRQ is not used, the user needs to issue N * M RQ WQEs in total. If using SRQ, the user only needs to issue K * M RQ WQEs, where K is much smaller than N. This K can be configured by the user according to the business needs. If there is a large amount of concurrent reception, then set K to a larger value; otherwise, setting K to a single digit is sufficient to handle general situations.\nWe have saved a total of (N - K) * M RQ WQEs, and RQ WQEs themselves are not very large, approximately a few KB in size, which doesn\u0026rsquo;t seem to take up much memory. However, as mentioned earlier, what is actually saved is the memory space used to store data, which is a significant amount of memory. We will use a diagram to illustrate:\nIn the above diagram, there are two RQ WQEs in the SRQ. Let\u0026rsquo;s take a look at the contents of an RQ WQE, which are composed of several SGEs (Scatter/Gather Elements). Each SGE consists of a memory address, length, and key. With a starting address and length, an SGE can point to a contiguous memory region, and multiple SGEs can represent multiple discrete contiguous memory blocks. We refer to multiple SGEs as an SGL (Scatter/Gather List). SGEs are ubiquitous in the IB software protocol stack (and indeed very common throughout Linux), allowing very large memory regions to be represented with minimal space. IB users use SGEs to specify send and receive areas.\nYou can simply estimate the size of the memory region each sge can point to. The length is a 32-bit unsigned integer, which can represent 4GB of space. Assuming an RQ WQE can hold a maximum of 256 sge, then an RQ WQE would be a total of 1TB. Of course, in reality, it cannot be that large, this is just to intuitively inform the reader of the potential memory space an RQ WQE might occupy.\n#\rSRQC\rThat is SRQ Context. Like QPC, SRQC is used to inform the hardware about attributes related to SRQ, including depth, WQE size, and other information, which will not be elaborated on in this article.\n#\rSRQN\rThat is SRQ Number. Like QP, there may be multiple SRQs in each node. To identify and distinguish these SRQs, each SRQ has a serial number, called SRQN.\n#\rPD of SRQ\rIn 【7. RDMA Protection Domain】\r, we introduced the concept of Protection Domain, which is used to isolate different RDMA resources. Each SRQ must specify its own PD, which can be the same as the PD of its associated QP, or it can be different; SRQs can also use the same PD.\nIf a packet is received while using SRQ, it will only be properly received if the MR and SRQ being accessed are under the same PD; otherwise, an immediate error will occur.\n#\rAsynchronous event\rIn the article 【10. RDMA Completion Queue】\r, we introduced that the IB protocol classifies error types into immediate errors, completion errors, and asynchronous errors based on the method of error reporting. Among them, asynchronous errors are similar to interrupts/events, so we sometimes refer to them as asynchronous events. Each HCA registers an event handling function specifically for handling asynchronous events. Upon receiving an asynchronous event, the driver performs the necessary processing and further reports it to the user.\nThere is a special asynchronous event regarding SRQ, used to promptly notify upper-level users of the SRQ status, namely the SRQ Limit Reached event.\n#\rSRQ Limit\rSRQ can set a watermark/threshold, when the number of remaining WQEs in the queue is less than the watermark, this SRQ will report an asynchronous event. It reminds the user \u0026ldquo;The WQEs in the queue are about to run out, please issue more WQEs to prevent having no place to receive new data.\u0026rdquo; This watermark/threshold is called the SRQ Limit, and the reported event is called SRQ Limit Reached.\nBecause the SRQ is shared by multiple QPs, if the depth is relatively small, it is very likely that the WQE inside will suddenly run out. Therefore, the protocol is designed with this mechanism to ensure that users can promptly intervene in situations where the WQE is insufficient.\nAfter reporting an asynchronous event, the value of SRQ Limit will be reset to 0 by the hardware (presumably to prevent continuously reporting asynchronous events to the upper layer). Of course, users can choose not to use this mechanism by simply setting the value of SRQ Limit to 0.\n#\rUser interface\r#\rControl surface\rStill the old four types—\u0026ldquo;Add, Delete, Modify, Query\u0026rdquo;:\nCreate SRQ When creating an SRQ, similar to a QP, all software and hardware resources related to the SRQ are allocated. For example, the driver will request an SRQN, allocate space for the SRQC, and fill in the configuration. When creating an SRQ, you must also specify the depth of each SRQ (how many WQEs it can store) and the maximum number of sges per WQE.\nDestroy SRQ Destroy all related software and hardware resources of SRQ.\nModify SRQ In addition to attributes such as SRQ depth, the value of SRQ Limit is also set through this interface. Because the value of the watermark is cleared every time an SRQ Limit Reached event occurs, the user needs to call Modify SRQ to reset the watermark each time.\nQuery SRQ It is usually used to query the configuration of the waterline.\n#\rData surface\r#\rPost SRQ Receive\rJust like Post Receive, it issues a receive WQE to the SRQ, which contains information about the memory block used as the receive buffer. It is important to note that the subject is SRQ and has nothing to do with QP. Currently, the user is not concerned with which QP this SRQ is associated with.\n#\rThe difference between SRQ and RQ\rIn terms of functionality, both SRQ and RQ are used to store received task requests, but due to the shared nature of SRQ, there are some differences between it and RQ.\n#\rState machine\rWe introduced in 【9. RDMA Queue Pair】\rthat QP has a complex state machine, and the sending and receiving capabilities of QP vary in different states. However, SRQ only has two states: non-error and error.\nRegardless of the state, users can issue WQEs to the SRQ. However, in an error state, the associated QP cannot receive data from this SRQ. Additionally, in an error state, users cannot query or modify the attributes of the SRQ.\nWhen a QP is in an error state, it can be returned to the RESET state through Modify QP, but for SRQ, it can only exit the error state by destroying it.\n#\rReceiving process\rFor a QP, RQ and SRQ cannot be used simultaneously, one must be chosen. If a WQE is issued to the RQ of a QP that is already associated with SRQ, an immediate error will be returned.\nLet\u0026rsquo;s compare the reception processes of SRQ and RQ. The content of this section is a key point of this article, and I believe that after reading it, readers will have a more complete understanding of the SRQ mechanism.\n#\rRQ\u0026rsquo;s receiving process\rFirst, let\u0026rsquo;s revisit the receiving process of a regular RQ (for the complete process on the sender\u0026rsquo;s side, please read 【4. RDMA Operation Types】\r):\nCreate QP.\nThrough the Post Recv interface, the user submits receive WQE to the RQ of QP2 and QP3, respectively. The WQE contains information about which memory region to place the received data.\nThe hardware receives the data.\nHardware discovery is sent to QP3, then WQE1 is taken from QP3\u0026rsquo;s RQ, and the received data is placed in the memory area specified by WQE1.\nAfter the hardware completes data storage, it generates a CQE to CQ3 associated with RQ of QP3, reporting task completion information.\nThe user retrieves WC (CQE) from CQ3, and then takes data from the specified memory area.\nThe hardware receives the data.\nThe hardware discovery is sent to QP2, then WQE1 is extracted from the RQ of QP2, and the received data is placed in the memory area specified by WQE1.\nAfter the hardware completes data storage, it generates a CQE for CQ2 associated with RQ of QP2, reporting task completion information.\nThe user retrieves WC (CQE) from CQ2, and then takes data from the specified memory area.\n#\rSRQ\u0026rsquo;s reception process\rAnd the SRQ receiving process has some differences:\nCreate SRQ1, and create QP2 and QP3, both associated with SRQ1.\nThrough the Post SRQ Recv interface, the user issues two receive WQEs to SRQ1, containing information about which memory region to place the received data.\nHardware receives data.\nThe hardware discovery is sent to QP3, extracting the first WQE from SRQ1 (now it is WQE1), and storing the received data according to the content of the WQE.\nEach WQE in the SRQ is \u0026ldquo;ownerless\u0026rdquo;, not associated with any QP. The hardware sequentially takes out the WQE according to the queue order and places the data inside.\nThe hardware discovers that the CQ associated with QP3\u0026rsquo;s RQ is CQ3, so it generates a CQE in it.\nThe user retrieves the CQE from CQ3 and takes data from the specified memory area.\nAttentive readers may ask, when a user issues a WR, each WR specifies some memory regions for storing data in the future. However, an SRQ is a pool where each WQE points to several different memory regions. After the user receives a WC in the CQ corresponding to a certain QP, how do they know where the received data has been stored?\nThere is actually wr_id information in the WC, informing the user of which WR (WQE) designated memory area the data is placed in. Since the WR is issued by the user, the user naturally knows its specific location.\nHardware received data\nThe hardware discovery is sent to QP2, and the first WQE is taken from SRQ1 (now it is WQE2), and the received data is stored according to the content of the WQE.\nThe hardware discovers that the CQ associated with QP2\u0026rsquo;s RQ is CQ2, so a CQE is generated in it.\nThe user takes out the CQE from CQ2 and retrieves data from the specified memory area.\n#\rSummary\rThis text first introduces the basic concept of SRQ, followed by its design purpose, related mechanisms, and user interface. Finally, it compares the SRQ receiving process with RQ. In actual business, the usage rate of SRQ is quite high, and it is hoped that readers can gain a deep understanding.\nLet\u0026rsquo;s stop here, thank you for reading. In the next article, I will introduce the Memory Window.\n#\rRelevant sections of the agreement\r10.2.9 The design concept of SRQ and related operations\n10.2.3 PD of SRQ and QP\n10.8.2 The relationship between QP associated with SRQ and QP not using SRQ\n10.8.5 SRQ related returns WC\n11.5.2.4 Asynchronous Events\n#\rOther references\rLinux Kernel Networking - Implementation and Theory. Chapter 13. Shared Receive Queue\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-06-26T23:34:00Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116373922_p0_master1200.webp","permalink":"https://cuterwrite.top/en/p/rdma-shared-receive-queue/","title":"RDMA: Shared Receive Queue"},{"content":"\r#\rRDMA\u0026rsquo;s Completion Queue\rThis article welcomes non-commercial reproduction, please indicate the source.\nStatement: For collection only, for easy reading\n― Savir, Zhihu Column: 10. RDMA Completion Queue We have briefly introduced CQ in previous articles, and this article will delve deeper into some of its details. Before reading this article, readers can first review this article: 【“3. RDMA Basic Elements”】\r.\n#\rBasic Concepts\rLet\u0026rsquo;s first review the function of CQ. CQ stands for Completion Queue, and its function is opposite to that of WQ (SQ and RQ). The hardware uses CQE/WC in the CQ to inform the software about the completion status of a certain WQE/WR. A reminder to readers: for upper-layer users, WC is generally used, while for drivers, it is generally referred to as CQE. This article does not distinguish between the two.\nCQE can be regarded as a \u0026ldquo;report\u0026rdquo; that specifies the execution status of a certain task, including:\nWhich task specified by which WQE of which QP was completed this time (QP Number and WR ID) What operation was performed in this task (Opcode operation type) This task executed successfully/failed, the reason for failure is XXX (Status and error code) \u0026hellip; Whenever the hardware completes processing a WQE, a CQE is generated and placed in the CQ queue. If a CQE corresponding to a WQE is not generated, then this WQE will always be considered as not yet processed. What does this mean?\nOperations involving fetching data from memory (SEND and WRITE) Before generating a CQE, the hardware may not have sent the message yet, may be in the process of sending the message, or the peer may have received the correct message. Since the memory region is allocated before sending, the upper-level software must consider this memory region still in use before receiving the corresponding CQE and cannot release all related memory resources.\nOperations involving storing data in memory (RECV and READ) Before the CQE is generated, it is possible that the hardware has not started writing data, it is possible that only half of the data has been written, or it is possible that a data verification error has occurred. Therefore, before the upper-layer software receives the CQE, the contents of the memory area used to store the received data are unreliable.\nIn summary, the user must obtain the CQE and confirm its content before considering the message sending and receiving task complete.\n#\rWhen was it generated?\rWe will explain separately according to the service type (this article only discusses RC and UD) and the operation type, because the timing and meaning of generating CQE are different in different situations. Readers are advised to review the 4th article \u0026ldquo;4. Basic RDMA Operations\u0026rdquo;\rand the 5th article \u0026ldquo;5. Basic RDMA Service Types\u0026rdquo;\r.\nReliable Service Type (RC) The previous article mentioned that reliability means that the sender is concerned that the message sent can be accurately received by the receiver, which is ensured through mechanisms such as ACK, checksum, and retransmission.\nSEND SEND operation requires hardware to fetch data from memory, then assemble it into packets to send to the other end through a physical link. For SEND, the Client side generates a CQE indicating the other end has received the data accurately, after the other end\u0026rsquo;s hardware receives and verifies the data, it will reply with an ACK packet to the sender. Only after the sender receives this ACK will a CQE be generated, thus informing the user that the task has been successfully executed. As shown in the figure, the left Client side generates the CQE for this task at the position marked by the red dot.\nRECV The RECV operation requires the hardware to place the received data into the memory area specified in the user\u0026rsquo;s WQE. After completing the checksum and data storage actions, the hardware will generate a CQE, as shown on the right side of the above figure on the server side.\nWRITE For the Client side, WRITE operation and SEND operation are the same, the hardware will fetch data from memory and wait for the peer to reply with an ACK before generating a CQE. The difference is that because WRITE is an RDMA operation, the peer CPU is not aware of it, and naturally the user is not aware of it either, so the diagram above becomes like this:\nREAD READ and RECV are somewhat similar. After the Client initiates a READ operation, the other side will reply with the data we want to read. Then, after verifying that there are no issues, the data will be placed in the specified location in the WQE. After completing the above actions, a CQE will be generated on our side. READ is also an RDMA operation, which is not perceived by the other side\u0026rsquo;s user, and naturally, no CQE is generated. In this situation, the diagram becomes like this:\nUnreliable Service Type (UD) Because unreliable service types lack retransmission and acknowledgment mechanisms, generating a CQE indicates that the hardware has already sent out the data specified by the corresponding WQE. It was previously mentioned that UD only supports SEND-RECV operations and does not support RDMA operations. Therefore, for both ends of the UD service, the timing for CQE generation is as shown in the figure below:\n#\rThe correspondence between WQ and CQ\rEach WQ must be associated with a CQ, and each CQ can be associated with multiple SQs and RQs.\nThe so-called \u0026ldquo;association\u0026rdquo; here refers to the fact that all CQEs corresponding to a WQ\u0026rsquo;s WQEs will be placed by the hardware into the bound CQ. It\u0026rsquo;s important to note that the SQ and RQ belonging to the same QP can each be associated with different CQs. As shown in the diagram below, both the SQ and RQ of QP1 are associated with CQ1, while the RQ of QP2 is associated with CQ1 and the SQ is associated with CQ2.\nBecause each WQ must be associated with a CQ, the user needs to create the CQ in advance before creating the QP, and then specify which CQ will be used by the SQ and RQ respectively.\nThe WQEs in the same WQ correspond to CQEs that are ordered\nThe hardware retrieves WQEs from a certain WQ (SQ or RQ) and processes them in a \u0026ldquo;First In, First Out\u0026rdquo; FIFO order, and when placing CQEs in the CQ associated with WRs, it also follows the order in which these WQEs were placed in the WQ. Simply put, whoever is placed in the queue first is completed first. This process is shown in the diagram below:\nIt should be noted that the use of SRQ and the RQ in RD service type are both non-order-preserving, which will not be discussed in this article.\nThe WQEs in different WQs are not ordered with respect to their corresponding CQEs.\nIn the previous text, we mentioned that a CQ might be shared by multiple WQs. In this case, the order of generation for the CQEs corresponding to these WQEs cannot be guaranteed. As shown in the figure below (the WQE number indicates the order of issuance, i.e., 1 is issued first, and 6 is issued last):\nThe above description actually also includes the situation of \u0026ldquo;WQE in SQ and RQ of the same QP, their corresponding CQE is not ordered.\u0026rdquo; This is actually quite easy to understand. SQ and RQ, one is responsible for actively initiating tasks, and the other for passively receiving tasks. They can be considered as channels in two different directions and naturally should not affect each other. Suppose the user first issues a Receive WQE and then a Send WQE for the same QP. It can\u0026rsquo;t be that if the peer doesn\u0026rsquo;t send a message to the local end, the local end cannot send a message to the peer, right?\nIn this case, since the order in which CQEs are generated is not related to the order in which WQEs are obtained, how do the upper-level application and driver know which WQE the received CQE is associated with? It\u0026rsquo;s actually quite simple, the CQE indicates the number of the WQE it corresponds to.\nAdditionally, it should be noted that even when multiple WQs share a single CQ, \u0026ldquo;WQEs in the same WQ have their corresponding CQEs ordered\u0026rdquo; is always guaranteed. This means that the CQEs corresponding to WQE 1, 3, and 4 belonging to WQ1 in the above diagram are generated in sequence, and the same applies to WQE 2, 5, and 6 belonging to WQ2.\n#\rCQC\rJust like QP, CQ is merely a queue memory space for storing CQEs. Apart from knowing the starting address, the hardware is essentially unaware of this area. Therefore, it is necessary to agree on a format with the software in advance, and then the driver will allocate memory and fill in the basic information of the CQ in this memory according to the format for the hardware to read. This memory is the CQC. The CQC contains information such as the capacity size of the CQ, the sequence number of the currently processed CQE, and so on. So by slightly modifying the QPC diagram, you can represent the relationship between CQC and CQ:\n#\rCQN\rCQ Number is the CQ\u0026rsquo;s identifier, used to distinguish different CQs. CQ does not have special reserved numbers like QP0 and QP1, which will not be further elaborated in this article.\n#\rComplete error\rThere are three types of errors in the IB protocol: immediate error, Completion Error, and Asynchronous Errors.\nImmediate error refers to \u0026ldquo;immediately stop the current operation and return an error to the upper-level user\u0026rdquo;; completion error refers to \u0026ldquo;return the error information to the upper-level user via CQE\u0026rdquo;; whereas asynchronous error refers to \u0026ldquo;report to the upper-level user through an interrupt event.\u0026rdquo; It might still be a bit abstract, so let\u0026rsquo;s give an example to illustrate under what circumstances these two types of errors might occur:\nThe user passed an illegal opcode when sending a Post Send, for example, trying to use RDMA WRITE operation during UD. Result: Immediate error generated (some manufacturers may generate a completion error in this situation)\nGenerally, in this situation, the driver will directly exit the post send process and return an error code to the upper-level user. Note that at this point, the WQE has not yet been issued to the hardware before returning.\nThe user issued a WQE with the operation type SEND, but did not receive an ACK from the other party for a long time. Result: Generation completed with error\nBecause the WQE has already reached the hardware, the hardware will generate the corresponding CQE, which contains error details of the timeout unresponse.\nMultiple WQEs were issued in user mode, so the hardware generated multiple CQEs, but the software did not retrieve the CQEs from the CQ, causing the CQ to overflow. Result: Generate asynchronous error Because the software has not fetched the CQE, it naturally will not obtain information from the CQE. At this time, the IB framework will call the event handler function registered by the software to notify the user to handle the current error.\nFrom this, it can be seen that they are all ways for the lower layer to report errors to the upper layer users, only the timing of their occurrence is different. In the IB protocol, it is specified which method should be used to report errors in different situations. For example, in the diagram below, for modifying illegal parameters during the Modify QP process, an immediate error should be returned.\nThe focus of this text is on CQ, so after introducing the error types, we will take a closer look at completion errors. Completion errors are reported by the hardware through filling error codes in the CQE. A communication process requires the participation of a requester and a responder, and the specific error causes are divided into local and remote. Let\u0026rsquo;s first take a look at the stage at which error detection is performed (the figure below is a redrawn version of Figure 118 in the IB protocol):\nThere are two error detection points for the Requester:\nLocal error detection Check the WQE in the SQ, if an error is detected, directly generate a CQE from the local error checking module to the CQ, and no data will be sent to the responder; if there is no error, send the data to the peer.\nRemote Error Detection Detect whether the response side\u0026rsquo;s ACK is abnormal. ACK/NAK is generated by the peer\u0026rsquo;s local error detection module after detection, and it contains whether there is an error on the response side and the specific type of error. Regardless of whether there is an issue with the remote error detection result, a CQE will be generated in the CQ.\nResponder\u0026rsquo;s error detection point is only one:\nLocal error detection In fact, what is detected is whether there is an issue with the peer message, which is also referred to as \u0026ldquo;local\u0026rdquo; error detection in the IB protocol. If an error is detected, it will be reflected in the ACK/NAK message sent back to the peer and will generate a CQE locally.\nIt should be noted that the generation of ACK and remote error detection mentioned above is only applicable to connection-oriented service types. Connectionless service types, such as UD type, do not care whether the peer receives it, and the receiver will not generate an ACK. Therefore, a CQE will definitely be generated after the local error detection of the Requester, regardless of whether there is a remote error.\nThen we will briefly introduce several common completion errors:\nRC service type SQ completion error Local Protection Error Local protection domain error. The data memory address specified in the local WQE is invalid for the MR, meaning the user is attempting to use data from an unregistered memory region. Remote Access Error Remote permission error. The local end does not have permission to read/write the specified remote memory address. Transport Retry Counter Exceeded Error Retransmission limit exceeded error. The peer has not responded with the correct ACK, causing multiple retransmissions from this end, exceeding the preset number of times. RC service type RQ completion error Local Access Error Local access error. Indicates that the peer attempted to write to a memory area it does not have permission to write to. Local Length Error Local length error. The local RQ does not have enough space to receive the data sent by the peer. For a complete list of error types, please refer to Section 10.10.3 of the IB protocol.\n#\rUser interface\rLike QP, we still introduce the interface provided by the IB protocol to the upper layer regarding CQ from the communication preparation phase (control plane) and the communication execution phase (data plane).\n#\rControl surface\rJust like QP, there are still the four types of \u0026ldquo;add, delete, modify, and query,\u0026rdquo; but perhaps because for CQ, the upper-layer users are resource users rather than managers, they can only read data from CQ and cannot write data. Therefore, the configurable parameter open to users is only the \u0026ldquo;CQ specification.\u0026rdquo;\nCreate CQ When creating, the user must specify the size of the CQ, i.e., how many CQEs it can store. Additionally, the user can provide a pointer to a callback function that is triggered after a CQE is generated (this will be discussed later). The kernel-mode driver will configure other related parameters and fill them into the CQC, as agreed with the hardware, to inform the hardware.\nDestroy CQ Release a CQ hardware and software resource, including CQ itself and CQC, and naturally, CQN will also become invalid.\nResize CQ The name here is slightly different because CQ only allows users to modify the size of the specifications, so Resize is used instead of Modify.\nQuery CQ Query the current specifications of CQ, as well as the callback function pointer used for notifications.\nBy comparing RDMA specifications and software protocol stacks, it can be found that many verbs interfaces are not implemented according to the specifications. Therefore, if readers find discrepancies between the software API and the protocol, there is no need to be puzzled, as RDMA technology itself is still evolving, and the software framework is in an active state of updates. If you are more concerned with programming implementation, please refer to the API documentation of the software protocol stack; if you are more concerned with academic research, please refer to the RDMA specifications.\n#\rData surface\rCQE is the medium through which hardware conveys information to software. Although the software knows under what circumstances a CQE will be generated, it does not know exactly when the hardware will place the CQE into the CQ. In the fields of communication and computing, this mode where the receiver does not know when the sender will send is called \u0026ldquo;asynchronous\u0026rdquo;. Let\u0026rsquo;s first take an example of a network card and then explain how a user can obtain a CQE (WC) through the data plane interface.\nAfter the network card receives a data packet, how to let the CPU know about this and process the packet, there are two common modes:\nInterrupt mode When the amount of data is small, or when there are frequent sporadic data exchanges, it is suitable to use the interrupt mode—meaning the CPU is usually doing other tasks, and when the network card receives a data packet, it will report an interrupt to interrupt the current task of the CPU, and the CPU will switch to handle the data packet (such as parsing the various layers of the TCP/IP protocol stack). After processing the data, the CPU jumps back to the task before the interrupt to continue execution.\nEach interrupt requires saving the context, which means saving the current values of various registers, local variables, etc., to the stack, and then restoring the context (popping from the stack) upon return. This itself incurs overhead. If the business load is heavy and the network card is constantly receiving packets, the CPU will continuously receive interrupts, and the CPU will be busy with interrupt switching, causing other tasks to not be scheduled.\nPolling mode So in addition to interrupt mode, the network card also has a polling mode, where received packets are first placed in the buffer, and the CPU periodically checks whether the network card has received data. If there is data, it takes the data from the buffer for processing; if not, it continues to handle other tasks.\nBy comparing interrupt modes, we can find that although the polling mode requires the CPU to check at intervals, which brings some overhead, using polling mode when the business is busy can greatly reduce the number of context switches for interrupts, thereby reducing the CPU\u0026rsquo;s burden.\nThe current network cards generally use a combination of interrupt and polling, which dynamically switches based on business load.\nIn the RDMA protocol, a CQE is equivalent to a data packet received by the network card, and the RDMA hardware passes it to the CPU for processing. The RDMA framework defines two types of interfaces for the upper layer, namely poll and notify, corresponding to polling and interrupt modes.\n#\rPoll completion queue\rVery straightforward, poll means polling. After the user calls this interface, the CPU will periodically check if there are fresh CQEs in the CQ. If there are, it will extract this CQE (note that once extracted, the CQE is \u0026ldquo;consumed\u0026rdquo;), parse the information within, and return it to the upper-level user.\n#\rSolicitud de notificación de finalización\rLiterally translated, it is a request completion notification. After the user calls this interface, it is equivalent to registering an interrupt with the system. This way, when the hardware places a CQE into the CQ, it will immediately trigger an interrupt to the CPU. The CPU will then stop its current work to retrieve the CQE, process it, and return it to the user.\nSimilarly, which of these two interfaces to use depends on the user\u0026rsquo;s requirements for real-time performance and the actual busyness of the business.\nThank you for reading, that concludes the introduction to CQ. In the next article, I plan to discuss SRQ in detail.\n#\rRelevant sections of the agreement\r9.9 CQ Error Detection and Recovery\n10.2.6 The relationship between CQ and WQ\n10.10 Error Types and Their Handling\n11.2.8 CQ Related Control Plane Interface\n11.4.2 CQ related data surface interface\n#\rOther references\r[1] Linux Kernel Networking - Implement and Theory. Chapter 13. Completion Queue\n","date":"2024-06-26T23:11:00Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116903369_p0_master1200.webp","permalink":"https://cuterwrite.top/en/p/rdma-completion-queue/","title":"RDMA: Completion Queue"},{"content":"\r#\rQueue Pair of RDMA\rThis article welcomes non-commercial reposting, please indicate the source.\nStatement: For collection only, for convenient reading\n― Savir, Zhihu Column: 9. Basic RDMA Service Types #\rQueue Pair\rWe have previously provided a brief introduction to the concept of QP in the article \u0026ldquo;3. Basic Elements of RDMA\u0026rdquo;\r. This article will delve deeper into some details about QP.\n#\rReview of Basic Concepts\rFirst, let\u0026rsquo;s briefly review the basic knowledge about QP:\nAccording to the description in the IB protocol, QP is a virtual interface between hardware and software. QP is a queue structure that sequentially stores tasks (WQE) issued by software to hardware. The WQE contains information such as where to retrieve data, how long the data is, and to which destination it should be sent.\nConcept of QP Each QP is independent and isolated from each other through PD, so a QP can be regarded as a resource exclusively used by a certain user, and a user can also use multiple QPs simultaneously.\nQP has many types of services, including RC, UD, RD, and UC, etc. All source QPs and destination QPs must be of the same type to interact with each other.\nAlthough the IB protocol refers to QP as a \u0026ldquo;virtual interface,\u0026rdquo; it is tangible:\nOn the hardware side, a QP is a storage space containing several WQEs. The IB network card reads the contents of the WQEs from this space and accesses the memory to store or retrieve data according to the user\u0026rsquo;s expectations. As for whether this storage space is memory space or on-chip storage space of the IB network card, the IB protocol does not impose restrictions, and each manufacturer has its own implementation.\nIn software, QP is a data structure maintained by the driver of the IB network card, which contains the address pointer of the QP and some related software attributes.\n#\rQPC\rIn the article \u0026ldquo;5. RDMA Basic Service Types\u0026rdquo;\r, we mentioned that QPC stands for Queue Pair Context, which is used to store properties related to QP. The driver does store the software properties of QP, so if we can store QP properties in software, why do we still use QPC?\nThis is because QPC is mainly for hardware viewing and is also used to synchronize QP information between software and hardware.\nWe have mentioned that the entity of a QP on hardware is merely a segment of storage space, and the hardware knows nothing beyond the starting address and size of this space, not even the service type of this QP. There is also a lot of other important information, such as a QP containing several WQEs. How does the hardware know how many there are and which one it should currently process?\nAll of the above information can be structured into a data structure by the software, and memory space can be allocated for it. However, the software only sees virtual addresses, and these memory spaces are physically discrete; the hardware does not know where this data is stored. Therefore, the software needs to pre-allocate a large contiguous space through the operating system, namely QPC, to present this information to the hardware. The network card and its accompanying driver program have pre-agreed on what content is included in the QPC, how much space each content occupies, and in what order they are stored. This way, the driver and hardware can read and write the status and other information of the QP through this QPC space.\nThe concept of QPC As shown in the figure above, the hardware actually only needs to know the address 0x12350000 of the QPC, because it can parse the contents of the QPC to determine the position of the QP, the QP sequence number, the QP size, and other information. Consequently, it can locate the QP and determine which WQE to process. Different manufacturers may have some variations in implementation, but the general principle is like this.\nThere are many Context concepts in the IB software stack, in addition to QPC, there are also Device Context, SRQC, CQC, EQC (Event Queue Context), etc. Their functions are similar to QPC, all used to record and synchronize the related attributes of certain resources.\n#\rQP Number\rReferred to as QPN, which is the number of each QP. The IB protocol specifies using $2^{24}$ bits to represent QPN, meaning each node can simultaneously use up to $2^{24}$ QPs, which is already a very large number and almost impossible to exhaust. Each node maintains its own set of QPNs independently, meaning QPs with the same number can exist on different nodes.\nThe concept of QPN itself is very simple, but there are two special reserved numbers that require extra attention:\n#\rQP0\rQP with ID 0 is used for the Subnet Management Interface (SMI), which is used to manage all nodes in the subnet. To be honest, I haven\u0026rsquo;t figured out the purpose of this interface yet, so let\u0026rsquo;s put it aside for now.\n#\rQP1\rQP numbered 1 is used for the General Service Interface (GSI), which is a set of management services, the most well-known of which is CM (Communication Management). It is a method used to exchange necessary information before formally establishing a connection between the communication nodes. Its details will be elaborated in a later article.\nThis is the reason why QP0 and QP1 did not appear in the diagram about QP in our previous article. All other QPs besides these two are regular QPs. When a user creates a QP, the driver or hardware will assign a QPN to this new QP, and generally, QPNs are assigned sequentially like 2, 3, 4. After a QP is destroyed, its QPN will be reclaimed and allocated to other newly created QPs at an appropriate time.\n#\rUser interface\rWe classify and introduce user interfaces from the control plane and data plane perspectives. The control plane refers to the user\u0026rsquo;s configuration of a certain resource, which is generally done before the actual data transmission; whereas the data plane naturally involves operations during the actual data transmission process.\n#\rControl surface\rReaders who have encountered algorithms should all understand that the nodes of a linked list involve four operations: \u0026ldquo;add, delete, modify, and search.\u0026rdquo; The nodes of a linked list are a memory area and a type of software resource.\n\u0026ldquo;Increase\u0026rdquo; means requesting a piece of memory from the operating system to store data. The system will allocate a space in memory and mark it as \u0026ldquo;in use by process XX,\u0026rdquo; and other unauthorized processes will not be able to overwrite or even read this memory space.\n\u0026ldquo;Delete\u0026rdquo; means notifying the operating system that I am no longer using this space, and it can be marked as \u0026ldquo;unused\u0026rdquo; and made available for other processes to use.\n\u0026ldquo;Modify\u0026rdquo; means to write, i.e., to change the contents of this memory area.\n\u0026ldquo;Query\u0026rdquo; means read, that is, to obtain the content of this memory area.\nQP, as one of the most important resources in RDMA technology, is no different from a linked list in its lifecycle:\nOperation Linked List Node QP Increase struct ListNode *node = malloc(sizeof(struct ListNode *)); Create QP Delete free(node); Destroy QP Modify node-\u0026gt;val = xxx; Modify QP Check xxx = node-\u0026gt;val; Query QP These four operations are actually the Verbs (RDMA\u0026rsquo;s API for upper-layer applications) that provide several interfaces to upper-layer users on the control plane:\n#\rCreate QP\rCreate a QP\u0026rsquo;s hardware and software resources, including the QP itself and the QPC. When the user creates it, they will input a series of initialization attributes, including the service type of the QP, the number of WQEs that can be stored, and other information.\n#\rDestroy QP\rRelease all software and hardware resources of a QP, including the QP itself and the QPC. After destroying the QP, the user will no longer be able to index this QP through QPN.\n#\rModify QP\rModify certain attributes of a QP, such as the state of the QP, the MTU of the path, etc. This modification process includes both the modification of software data structures and the modification of the QPC.\n#\rQuery QP\rQuery the current status and some attributes of a QP. The data queried comes from the driver and the content of the QPC.\nThese four operations all have corresponding Verbs interfaces, similar to ibv_create_qp() form, which we can directly call when writing the APP. More details about the upper-level API will be introduced later.\n#\rData surface\rIn terms of data, a QP actually has only two interfaces to the upper layer, used to fill in send and receive requests in the QP. Here, \u0026ldquo;send\u0026rdquo; and \u0026ldquo;receive\u0026rdquo; do not refer to sending and receiving data, but rather the \u0026ldquo;initiator\u0026rdquo; (Requestor) and \u0026ldquo;responder\u0026rdquo; (Responser) in a communication process.\nIn behavior, the software fills a WQE (called WR at the application layer) into the QP, requesting the hardware to perform an action. Therefore, both behaviors are called \u0026ldquo;Post XXX Request,\u0026rdquo; meaning issuing an XXX request.\n#\rSend Request\rTo emphasize again, Post Send itself does not mean that the operation type of this WQE is Send, but indicates that this WQE belongs to the initiator of the communication. The WQE/WR filled into the QP in this process can be a Send operation, RDMA Write operation, or RDMA Read operation, etc.\nThe user needs to prepare the data buffer, destination address, and other information in advance, then call the interface to pass the WR to the driver, and the driver will fill the WQE into the QP.\n#\rPost Receive Request\rThe usage scenarios for Post Recv are relatively fewer, generally only executed on the receiving end of the Send-Recv operation. The receiving end needs to prepare the buffer for receiving data in advance and inform the hardware of the buffer address and other information in the form of a WQE.\n#\rQP state machine\rSpeaking of the state of QP, we have to bring out the following image (taken from section 10.3.1 of the IB protocol):\nQP State Machine The so-called state machine describes the different states of an object and the conditions that trigger transitions between states. Designing a state machine for an object can make the lifecycle of this object very clear, and in implementation, it will also make the logic more coherent.\nFor QP, the IB specification also designs several states for it. The functions of a QP in different states vary. For example, only after entering the Ready to Send state can the QP perform Post Send data operations. State transitions between normal states (in green) are actively triggered by the user through the Modify QP user interface introduced above; whereas error states (in red) often automatically transition after an error occurs. When a QP is in an error state, it cannot perform normal operations and needs to be reconfigured to a normal state by the upper layer through Modify QP.\nIn the above diagram, we only focus on the part of QP. EE (End-to-End Context) is a concept specifically used for RD service types, which we will not cover for now. We enter this state diagram through the Create QP interface and exit this state diagram through the Destroy QP interface.\nQP has the following states, we will only introduce some important points:\n#\rRST (Reset)\rReset state. When a QP is created through Create QP, it is in this state. The related resources have already been allocated, but this QP cannot do anything at the moment. It cannot receive WQEs issued by the user, nor can it receive messages from a QP on the peer end.\n#\rINIT（Initialized）\rInitialized state. In this state, the user can issue Receive WR to this QP via Post Receive, but the received messages will not be processed and will be silently discarded; if the user issues a Post Send WR, an error will occur.\n#\rRTR（Ready to Receive）\rReady to receive status. Based on the INIT state, RQ can function normally, meaning it can move data to the specified memory location according to the instructions in the received message\u0026rsquo;s WQE. In this state, SQ still cannot function.\n#\rRTS (Ready to Send)\rReady to send status. Based on RTR, SQ can work normally, meaning the user can perform Post Send, and the hardware will also send the data according to the content of SQ. Before entering this state, QP must have already established a connection with the peer.\n#\rSQD (Send Queue Drain)\rSQ emptying state. As the name suggests, this state will process all the existing unprocessed WQEs in the SQ queue. At this time, the user can still submit new WQEs, but these WQEs will be processed only after all the old WQEs have been processed.\n#\rSQEr (Send Queue Error)\rSQ error state. When a Send WR encounters a completion error (i.e., an error reported to the driver by the hardware through CQE), it causes the QP to enter this state.\n#\rERR (Error)\rError state. If an error occurs in other states, they may enter this state. In the Error state, the QP will stop processing WQE, and any WQE that is halfway processed will also stop. The upper layer needs to switch the QP back to the initial RST state after fixing the error.\n#\rSummary\rThis article first reviews some important basic concepts of QP, then explains QPC, QPN, and other concepts closely related to QP, and finally introduces the interfaces commonly used by users to operate QP and the QP state machine. I believe that after reading this article, readers will have a deeper understanding of QP.\nIn fact, as a core concept of RDMA, there is a lot of content regarding QP, and this article cannot cover everything. I will gradually complete the related content in future articles. For example, the concept of QKey will be explained in detail in subsequent articles dedicated to various Keys.\nAlright, this is the end of the article. Thank you for reading. A preview of the next article will provide a detailed explanation of CQ.\n#\rRelevant sections of the agreement\r3.5.1 10.2.4 Basic Concepts of QP\n10.3 QP State Machine\n10.2.5 Software interfaces related to QP\n11.4 Post Send Post Recv\n","date":"2024-06-25T02:21:00Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116342820_p0_master1200.webp","permalink":"https://cuterwrite.top/en/p/rdma-queue-pair/","title":"RDMA: Queue Pair"},{"content":"\r#\rImplement Hugo PWA based on Workbox\rRecently added PWA functionality to a blog built on Hugo\r, significantly improving loading speed and user experience, even enabling offline access. As for how to achieve this, you need to understand Progressive Web Apps (PWA).\n#\rWhat is PWA?\rProgressive Web Apps (PWA) leverage modern Web APIs and traditional progressive enhancement strategies to create cross-platform web applications. These applications are ubiquitous, feature-rich, and provide users with an experience comparable to native apps.\nAdvantages of PWA:\n⚡️ Faster loading speed: PWA can cache important resources and load quickly even in poor network conditions. ✈️ Offline Access: PWA can cache content, allowing users to access content even when offline. 🔔 Push Notifications: Like native applications, PWAs can send push notifications to users to increase user engagement. 📱 Install to Home Screen: Users can add your application to the desktop of their computer or phone and browse your web application like a native app. The implementation principle of PWA is Service Worker. Service Worker is a special JavaScript resource that runs independently in the browser background, acting as a proxy between the web browser and the web server. It can intercept and handle network requests, cache resources, and push notifications.\nMainstream front-end frameworks Vue, React, and Angular all provide corresponding PWA plugins. As for static site generators like Hugo, we can implement PWA functionality by manually adding Workbox\r.\n#\rWorkbox\rWorkbox\ris a set of modules developed by the Google Chrome team, designed to simplify common Service Worker routing and caching operations. Each module is optimized for a specific aspect of Service Worker development. The goal of Workbox is to simplify the use of Service Workers as much as possible while providing the flexibility to meet the needs of complex applications when necessary.\nIf there is no Workbox, we need to manually write a Service Worker to listen to fetch events, cache resources, and implement offline access and other functions. Workbox provides a set of tools that can help us automatically generate a Service Worker and comes with some commonly used caching strategies, allowing us to focus more on business logic.\n#\rConfigure PWA\rIn the previous section, we learned about the concept and advantages of PWA, and how Workbox simplifies the development of Service Workers. Next, we will step by step configure PWA functionality for the Hugo blog.\n#\rRegister Service Worker\rFirst, we need to register the Service Worker on the page. Add the following code snippet to your Hugo theme\u0026rsquo;s layouts/partials/footer/custom.html file (other themes may need adjustments based on the file structure):\n\u0026lt;script\u0026gt;\r// Check that service workers are registered\rif ('serviceWorker' in navigator) {\r// Use the window load event to keep the page load performant\rwindow.addEventListener('load', () =\u0026gt; {\rnavigator.serviceWorker.register('/sw.js').then(reg =\u0026gt; {\rconsole.log('Service worker registered with scope: ', reg.scope);\r}, err =\u0026gt; {\rconsole.log('Service worker registration failed: ', err);\r});\r});\r}\r\u0026lt;/script\u0026gt;\rNote: Before registering the Service Worker, you need to first create the sw.js file, which we will complete in the next section.\nAfter completing the registration, you can view the registration status of the Service Worker in the developer tools (F12) of the browser under \u0026ldquo;Application\u0026rdquo; -\u0026gt; \u0026ldquo;Service Workers\u0026rdquo; panel.\nService Worker #\rImport Workbox\rIn the static folder of your Hugo site root directory, create the sw.js file. Then, add the following code in the sw.js file to import Workbox using CDN:\nimportScripts('https://cdn.bootcdn.net/ajax/libs/workbox-sw/7.1.0/workbox-sw.js');\r#\rCache strategy\rWorkbox provides some common caching strategies, such as CacheFirst, NetworkFirst, StaleWhileRevalidate, etc. Here we introduce some common strategies first.\n#\rCacheOnly Cache only\rCacheOnly Forced response from cache.\nworkbox.routing.registerRoute(\rnew RegExp(regex),\rnew workbox.strategies.CacheOnly()\r);\r#\rNetworkOnly Network Only\rNetworkOnly This caching strategy forces all requests to retrieve the latest data from the network, completely bypassing the cache.\nworkbox.routing.registerRoute(\rnew RegExp(regex),\rnew workbox.strategies.NetworkOnly()\r);\r#\rCacheFirst Cache Priority\rCacheFirst This caching strategy prioritizes speed, first attempting to retrieve the response from the cache to display content to the user as quickly as possible. If the required data is not in the cache, it will then make a request to the network to obtain the data.\nworkbox.routing.registerRoute(\rnew RegExp(regex),\rnew workbox.strategies.CacheFirst()\r);\r#\rNetworkFirst 优先网络\rNetworkFirst This caching strategy prioritizes using the latest data, so it will first attempt to fetch the response from the network. If the network request fails, such as when the user is offline or the network connection is unstable, it will fall back to using cached data to ensure that the user can still access the content.\nworkbox.routing.registerRoute(\rnew RegExp(regex),\rnew workbox.strategies.NetworkFirst()\r);\r#\rStaleWhileRevalidate reads the cache while initiating a network request\rStaleWhileRevalidate This caching strategy prioritizes returning cached content (if available). Even if the cached content is valid, it will initiate a network request in the background to obtain the latest data, ensuring that the user ultimately sees the most up-to-date content. Although this strategy ensures that the cache is regularly updated for the user, it also means that every request generates network traffic, which can be a waste of bandwidth even if the data hasn\u0026rsquo;t changed.\nworkbox.routing.registerRoute(\rnew RegExp(regex),\rnew workbox.strategies.StaleWhileRevalidate()\r);\r#\rStrategy Configuration\rWorkbox not only provides the aforementioned strategies but also allows customization through configuration options such as cacheName, plugins, and expiration. You can customize routing behavior by defining the plugins you want to use. For example, you can configure the cache name, cache expiration, and cacheable response status codes as follows:\nworkbox.routing.registerRoute(\rnew RegExp(regex),\rnew workbox.strategies.CacheFirst({\rcacheName: 'my-cache',\rplugins: [\rnew workbox.expiration.ExpirationPlugin({\rmaxEntries: 60,\rmaxAgeSeconds: 30 * 24 * 60 * 60, // 30 Days\r}),\rnew workbox.cacheableResponse.CacheableResponsePlugin({\rstatuses: [0, 200],\r}),\r],\r})\r);\r#\rSite Configuration\r#\rGlobal configuration\rThe following is the global cache configuration:\n// Cache version number\rlet cacheVersion = '-240619';\r// Maximum number of entries\rconst maxEntries = 100;\r#\rTwitto Configuration\rIn order to ensure that users can view comments even when offline, the Twitto Comments API uses a NetworkFirst caching strategy. This means the browser will first attempt to fetch the latest data from the network, and if the network is unavailable, it will use the data from the cache.\nworkbox.routing.registerRoute(\rnew RegExp('^https://comment\\.cuterwrite\\.top'),\rnew workbox.strategies.NetworkFirst({\rplugins: [\rnew workbox.expiration.ExpirationPlugin({\rmaxEntries: maxEntries,\rmaxAgeSeconds: 30 * 24 * 60 * 60,\r}),\rnew workbox.cacheableResponse.CacheableResponsePlugin({\rstatuses: [0, 200],\r}),\r],\r})\r);\r#\rRSS and Sitemap Configuration\rIn order to ensure that users always obtain the latest RSS and Sitemap data, these pages are configured to use only the network strategy (NetworkOnly) without caching.\nworkbox.routing.registerRoute(\rnew RegExp('^https://cuterwrite\\.top/(index|sitemap)\\.xml'),\rnew workbox.strategies.NetworkOnly()\r);\r#\rHTML Configuration\rIn order to ensure that users can quickly load pages while also obtaining the latest content, the website uses the StaleWhileRevalidate caching strategy for HTML pages. This means the browser will prioritize displaying the page from the cache while simultaneously making a request to the server in the background to fetch the latest version, which will be used on the next request.\nworkbox.routing.registerRoute(\rnew RegExp('.*\\.html'),\rnew workbox.strategies.StaleWhileRevalidate({\rcacheName: 'html-cache' + cacheVersion,\rplugins: [\rnew workbox.expiration.ExpirationPlugin({\rmaxEntries: maxEntries,\rmaxAgeSeconds: 30 * 24 * 60 * 60,\r}),\rnew workbox.cacheableResponse.CacheableResponsePlugin({\rstatuses: [0, 200],\r}),\r],\r})\r);\r#\rGoogle Fonts Configuration\rIn order to ensure the font files are updated while also utilizing caching to speed up page loading, the website uses a CacheFirst caching strategy for Google Fonts resources and sets a long cache expiration time.\nworkbox.routing.registerRoute(\rnew RegExp('.*\\.(?:woff|woff2|ttf|otf|eot)'),\rnew workbox.strategies.StaleWhileRevalidate({\rcacheName: 'google-fonts' + cacheVersion,\rplugins: [\r// Use expiration plugin to control the number and time of cache entries\rnew workbox.expiration.ExpirationPlugin({\r// Maximum number of cache entries\rmaxEntries: maxEntries,\r// Maximum cache time 30 days\rmaxAgeSeconds: 30 * 24 * 60 * 60,\r}),\r// Use cacheableResponse plugin to cache requests with status code 0\rnew workbox.cacheableResponse.CacheableResponsePlugin({\rstatuses: [0, 200],\r}),\r],\r})\r);\r#\rCDN Configuration\rIn order to maximize the use of cache to speed up page loading, the website adopts a CacheFirst caching strategy for resources from common CDNs and sets a long cache expiration time.\nworkbox.routing.registerRoute(\rnew RegExp('^https://(?:cdn\\.bootcdn\\.net|unpkg\\.com|cdn\\.jsdelivr\\.net)'),\rnew workbox.strategies.CacheFirst({\rcacheName: 'cdn' + cacheVersion,\rfetchOptions: {\rmode: 'cors',\rcredentials: 'omit',\r},\rplugins: [\rnew workbox.expiration.ExpirationPlugin({\rmaxEntries: maxEntries,\rmaxAgeSeconds: 30 * 24 * 60 * 60,\r}),\r],\r})\r);\r#\rUmani website statistics configuration\rIn order to ensure the accuracy of website statistics, the website adopts the NetworkOnly strategy for Umani website statistics requests and uses the BackgroundSyncPlugin to ensure that data is eventually sent successfully even when the network is offline.\nworkbox.routing.registerRoute(\rnew RegExp('^https://analytics\\.cuterwrite\\.top/uma'),\rnew workbox.strategies.NetworkOnly({\rplugins: [\r// Use background sync plugin to implement background synchronization\rnew workbox.backgroundSync.BackgroundSyncPlugin('Optical_Collect', {\rmaxRetentionTime: 12 * 60,\r}),\r],\r})\r);\r#\rImage Configuration\rIn order to speed up image loading and reduce the number of network requests, the website uses a CacheFirst caching strategy for image resources and sets a long cache expiration time.\nworkbox.routing.registerRoute(\rnew RegExp('^(https://cuterwrite-1302252842\\.file\\.myqcloud\\.com|https://s2\\.loli\\.net)'),\rnew workbox.strategies.CacheFirst({\rcacheName: 'image-cache' + cacheVersion,\rplugins: [\rnew workbox.expiration.ExpirationPlugin({\rmaxEntries: maxEntries,\rmaxAgeSeconds: 30 * 24 * 60 * 60,\r}),\rnew workbox.cacheableResponse.CacheableResponsePlugin({\rstatuses: [0, 200],\r}),\r],\r})\r);\r#\rSuffix match configuration\rIn order to balance loading speed and content updates, the website uses the StaleWhileRevalidate caching strategy for static files (such as images, CSS, and JavaScript files) that are not matched by the domain name.\nworkbox.routing.registerRoute(\rnew RegExp('.*\\.(?:png|jpg|jpeg|svg|gif|webp|ico)'),\rnew workbox.strategies.StaleWhileRevalidate()\r);\rworkbox.routing.registerRoute(\rnew RegExp('.*\\.(css|js)'),\rnew workbox.strategies.StaleWhileRevalidate()\r);\r#\rDefault behavior configuration\rIn order to handle requests that are not matched by any custom routing rules, the website is configured with a default caching behavior, using the NetworkFirst strategy and setting a network timeout to balance resource retrieval speed and offline availability.\nworkbox.routing.setDefaultHandler(\r// Prefer using cache, if cache is not available then use network request\rnew workbox.strategies.NetworkFirst({\rnetworkTimeoutSeconds: 3,\r})\r);\r#\rFull configuration\rsw.js\r```javascript\rimportScripts('https://cdn.bootcdn.net/ajax/libs/workbox-sw/7.1.0/workbox-sw.js');\r// Cache version number\rlet cacheVersion = '-240619';\r// Maximum number of entries\rconst maxEntries = 100;\rif (workbox) {\rconsole.log(`Yay! Workbox is loaded 🎉`);\r// Comment cache\rworkbox.routing.registerRoute(\rnew RegExp('^https://comment\\.cuterwrite\\.top'),\rnew workbox.strategies.NetworkFirst({\rplugins: [\rnew workbox.expiration.ExpirationPlugin({\rmaxEntries: maxEntries,\rmaxAgeSeconds: 30 * 24 * 60 * 60,\r}),\rnew workbox.cacheableResponse.CacheableResponsePlugin({\rstatuses: [0, 200],\r}),\r],\r})\r);\r// Do not cache rss and sitemap\rworkbox.routing.registerRoute(\rnew RegExp('^https://cuterwrite\\.top/(index|sitemap)\\.xml'),\rnew workbox.strategies.NetworkOnly()\r);\r// Cache HTML\rworkbox.routing.registerRoute(\rnew RegExp('.*\\.html'),\rnew workbox.strategies.StaleWhileRevalidate({\rcacheName: 'html-cache' + cacheVersion,\rplugins: [\rnew workbox.expiration.ExpirationPlugin({\rmaxEntries: maxEntries,\rmaxAgeSeconds: 30 * 24 * 60 * 60,\r}),\rnew workbox.cacheableResponse.CacheableResponsePlugin({\rstatuses: [0, 200],\r}),\r],\r})\r);\r// Cache Google Fonts\rworkbox.routing.registerRoute(\rnew RegExp('.*\\.(?:woff|woff2|ttf|otf|eot)'),\rnew workbox.strategies.StaleWhileRevalidate({\rcacheName: 'google-fonts' + cacheVersion,\rplugins: [\r// Use expiration plugin to control cache entry number and time\rnew workbox.expiration.ExpirationPlugin({\r// Maximum number of cache entries\rmaxEntries: maxEntries,\r// Maximum cache time 30 days\rmaxAgeSeconds: 30 * 24 * 60 * 60,\r}),\r// Use cacheableResponse plugin to cache requests with status code 0\rnew workbox.cacheableResponse.CacheableResponsePlugin({\rstatuses: [0, 200],\r}),\r],\r})\r);\r// Cache public libraries like bootcdn, unpkg, jsdelivr using regex\rworkbox.routing.registerRoute(\rnew RegExp('^https://(?:cdn\\.bootcdn\\.net|unpkg\\.com|cdn\\.jsdelivr\\.net)'),\rnew workbox.strategies.CacheFirst({\rcacheName: 'cdn' + cacheVersion,\rfetchOptions: {\rmode: 'cors',\rcredentials: 'omit',\r},\rplugins: [\rnew workbox.expiration.ExpirationPlugin({\rmaxEntries: maxEntries,\rmaxAgeSeconds: 30 * 24 * 60 * 60,\r}),\r],\r})\r);\r// Self-built UMA statistics script: https://analytics.cuterwrite.top/uma\rworkbox.routing.registerRoute(\rnew RegExp('^https://analytics\\.cuterwrite\\.top/uma'),\rnew workbox.strategies.NetworkOnly({\rplugins: [\r// Use background sync plugin for background synchronization\rnew workbox.backgroundSync.BackgroundSyncPlugin('Optical_Collect', {\rmaxRetentionTime: 12 * 60,\r}),\r],\r})\r);\r// Cache bucket images https://cuterwrite-1302252842.file.myqcloud.com/\rworkbox.routing.registerRoute(\rnew RegExp('^(https://cuterwrite-1302252842\\.file\\.myqcloud\\.com|https://s2\\.loli\\.net)'),\rnew workbox.strategies.CacheFirst({\rcacheName: 'image-cache' + cacheVersion,\rplugins: [\rnew workbox.expiration.ExpirationPlugin({\rmaxEntries: maxEntries,\rmaxAgeSeconds: 30 * 24 * 60 * 60,\r}),\rnew workbox.cacheableResponse.CacheableResponsePlugin({\rstatuses: [0, 200],\r}),\r],\r})\r);\r// Suffix matching for other static files not matched by domain\rworkbox.routing.registerRoute(\rnew RegExp('.*\\.(?:png|jpg|jpeg|svg|gif|webp|ico)'),\rnew workbox.strategies.StaleWhileRevalidate()\r);\rworkbox.routing.registerRoute(\rnew RegExp('.*\\.(css|js)'),\rnew workbox.strategies.StaleWhileRevalidate()\r);\r// Default match for remaining requests\rworkbox.routing.setDefaultHandler(\r// Prefer cache, if cache is not available, use network request\rnew workbox.strategies.NetworkFirst({\rnetworkTimeoutSeconds: 3,\r})\r);\r} else {\rconsole.log(`Boo! Workbox didn't load 😬`);\r}\r#\rmanifest.json\rCreate manifest.json file Create a manifest.json file in the static folder at the root directory of your Hugo blog, which contains metadata about your blog, such as name, icon, and display options.\n{\r\u0026quot;name\u0026quot;: \u0026quot;Your Blog Name\u0026quot;,\r\u0026quot;short_name\u0026quot;: \u0026quot;Blog Short Name\u0026quot;,\r\u0026quot;start_url\u0026quot;: \u0026quot;/\u0026quot;,\r\u0026quot;display\u0026quot;: \u0026quot;standalone\u0026quot;,\r\u0026quot;background_color\u0026quot;: \u0026quot;#ffffff\u0026quot;,\r\u0026quot;theme_color\u0026quot;: \u0026quot;#000000\u0026quot;,\r\u0026quot;icons\u0026quot;: [{\r\u0026quot;src\u0026quot;: \u0026quot;/icon-192x192.png\u0026quot;,\r\u0026quot;sizes\u0026quot;: \u0026quot;192x192\u0026quot;,\r\u0026quot;type\u0026quot;: \u0026quot;image/png\u0026quot;\r},\r{\r\u0026quot;src\u0026quot;: \u0026quot;/icon-512x512.png\u0026quot;,\r\u0026quot;sizes\u0026quot;: \u0026quot;512x512\u0026quot;,\r\u0026quot;type\u0026quot;: \u0026quot;image/png\u0026quot;\r}\r]\r}\rNote: Replace icon-192x192.png and icon-512x512.png with your own icon filenames. And make sure to place these two icon files in the static folder of your Hugo blog. If you want to modify the theme color and background color, you can modify the theme_color and background_color fields.\nLink manifest.json file In your Hugo blog\u0026rsquo;s layouts/partials/head/custom.html file, add the following code to link the manifest.json file to your website:\n\u0026lt;link rel=\u0026quot;manifest\u0026quot; href=\u0026quot;/manifest.json\u0026quot;\u0026gt;\rAfter completing the above steps, your Hugo blog will have PWA functionality, allowing users to access your site as if it were a native application.\n#\rReferences\rOffline Cookbook\rWorkbox\rWorkbox Github\r","date":"2024-06-18T22:28:00Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116406967_p0_master1200.webp","permalink":"https://cuterwrite.top/en/p/hugo-pwa/","title":"Implementing Hugo Progressive Web App Based on Workbox"},{"content":"In recent years, large language models (LLM) have become a cornerstone of the artificial intelligence field due to their powerful text generation and understanding capabilities. Commercial LLMs are often expensive and have closed-source code, limiting the exploration space for researchers and developers. Fortunately, the open-source community offers excellent alternatives like Ollama, allowing everyone to easily experience the charm of LLMs and combine them with HPC and IDE plugins to create more powerful personal assistants.\n#\rWhat is Ollama?\rOllama is a tool for building large language model applications. It offers a simple and easy-to-use command line interface and server, allowing you to easily download, run, and manage various open-source LLMs. Unlike traditional LLMs that require complex configurations and powerful hardware, Ollama allows you to conveniently experience the powerful features of LLMs as if you were using a mobile app.\n#\rAdvantages of Ollama\rOllama has the following significant advantages:\nOpen source and free: Ollama and its supported models are completely open source and free, allowing anyone to use, modify, and distribute them freely. Simple and Easy to Use: No need for complex configuration and installation processes, Ollama can be started and run with just a few commands. Rich Model: Ollama supports many popular open-source LLMs such as Llama 3, Mistral, Qwen2, and provides one-click download and switching features. Low resource consumption: Compared to commercial LLMs, Ollama has lower hardware requirements and can run smoothly even on ordinary laptops. Community Activity: Ollama has a large and active community where users can easily get help, share experiences, and participate in model development. #\rHow to use Ollama?\rUsing Ollama is very simple, just follow these steps:\nInstall Ollama: Download and install the latest version from the Ollama official website\raccording to your operating system. Start Ollama: Open the terminal or command line and enter the ollama serve command to start the Ollama server. Download Model: Find the desired model in the model library\r, then use the ollama pull command to download it, for example, ollama pull llama3:70b. Run the model: Use the ollama run command to start the model, for example ollama run llama3:70b. Start chatting: Enter your question or command in the terminal, and Ollama will generate a corresponding response based on the model. #\rInstall Ollama\r#\rmacOS\rDownload Ollama for macOS\r#\rWindows\rDownload Ollama for Windows\r#\rLinux\rcurl -fsSL https://ollama.com/install.sh | sh\r#\rDocker\r#\rCPU version\rdocker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\r#\rGPU version\rInstall Nvidia container toolkit\rRun Ollama in a Docker container docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\r#\rLaunch Ollama\rollama serve\rOutput the following information indicating that the Ollama server has successfully started (V100 machine):\n$ ollama serve\r### Omitted log output ###\rListening on [::]:11434 (version 0.1.42)\r#\rDownload model\rollama pull qwen2:72b\r#\rRun model\rollama run qwen2:72b\rFor example, after running the following command:\nYou are trained on data up to October 2023.\r#\rRun model in Docker container\rdocker exec -it ollama ollama run qwen2:72b\r#\rConfigure Ollama\rOllama provides a variety of environment variables for configuration:\nOLLAMA_DEBUG: Whether to enable debug mode, default is false. OLLAMA_FLASH_ATTENTION: Whether to flash attention, default is true. OLLAMA_HOST: Host address of the Ollama server, default is empty. OLLAMA_KEEP_ALIVE: Time to keep the connection alive, default is 5m. OLLAMA_LLM_LIBRARY: LLM library, default is empty. OLLAMA_MAX_LOADED_MODELS: Maximum number of loaded models, default is 1. OLLAMA_MAX_QUEUE: Maximum number of queues, default is empty. OLLAMA_MAX_VRAM: Maximum virtual memory, default is empty. OLLAMA_MODELS: Model directory, default is empty. OLLAMA_NOHISTORY: Whether to save history, defaults to false. OLLAMA_NOPRUNE: Whether to enable pruning, default is false. OLLAMA_NUM_PARALLEL: Number of parallels, default is 1. OLLAMA_ORIGINS: Allowed origins, default is empty. OLLAMA_RUNNERS_DIR: Runner directory, default is empty. OLLAMA_SCHED_SPREAD: Scheduling distribution, default is empty. OLLAMA_TMPDIR: Temporary file directory, defaults to empty. Here is the optimized list in the desired format: OLLAMA_DEBUG: Whether to enable debug mode, default is false. OLLAMA_FLASH_ATTENTION: Whether to flash attention, default is true. OLLAMA_HOST: Host address of the Ollama server, default is empty. OLLAMA_KEEP_ALIVE: Duration to keep the connection alive, default is 5m. OLLAMA_LLM_LIBRARY: LLM library, default is empty. OLLAMA_MAX_LOADED_MODELS: Maximum number of loaded models, default is 1. OLLAMA_MAX_QUEUE: Maximum queue number, default is empty. OLLAMA_MAX_VRAM: Maximum virtual memory, default is empty. OLLAMA_MODELS: Model directory, default is empty. OLLAMA_NOHISTORY: Whether to save history, default is false. OLLAMA_NOPRUNE: Whether to enable pruning, default is false. OLLAMA_NUM_PARALLEL: Number of parallels, default is 1. OLLAMA_ORIGINS: Allowed origins, default is empty. OLLAMA_RUNNERS_DIR: Runner directory, default is empty. OLLAMA_SCHED_SPREAD: Scheduling distribution, default is empty. OLLAMA_TMPDIR: Temporary file directory, default is empty. #\rAdvanced Usage: Deploying Ollama on an HPC Cluster\rFor large models or situations requiring higher performance, the powerful computing power of an HPC cluster can be used to run Ollama. By combining with Slurm for task management and using port mapping to expose the service locally, remote access and use can be conveniently achieved:\nConfigure the Ollama environment on the login node: Install Ollama and download the required models. Write a slurm script: Specify resource requirements (CPU, memory, GPU, etc.), and use the ollama serve command to start the model service and bind it to a specific port. #!/bin/bash\r#SBATCH --job-name=ollama\r#SBATCH -N 1\r#SBATCH -p GPU\r#SBATCH --gres=gpu:1\rmodule load CUDA\rollama serve\rSubmit slurm task: Use the sbatch command to submit the script, Slurm will allocate the task to compute nodes for execution. Local Port Mapping: Use the ssh -L command to map the compute node\u0026rsquo;s port to the local machine, for example: ssh -t -t username@login node ip -L 11434:localhost:11434 -i login node private key ssh compute node IP -L 11434:127.0.0.1:11434\rLocal Access: Access http://localhost:11434 in a browser or application to use the Ollama service. Note: Since the compute node is not connected to the internet, you need to use ollama pull on the login node in advance to download the required model. Additionally, you need to set OLLAMA_ORIGINS to * and OLLAMA_HOST to 0.0.0.0 to allow all sources to access the service.\n#\rAdvanced Usage: Local Code Completion Assistant\rOllama can not only be used for chatting and text creation but also for creating a powerful code completion assistant by combining code generation models and IDE plugins. For example, using the Codeqwen 7B model and the VS Code plugin Continue\r, efficient and convenient code completion functionality can be achieved.\nFirst, let me introduce Continue: Continue\rallows you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs. All of this can run entirely on your own laptop or have Ollama deployed on a server to remotely power code completion and chat experiences based on your needs.\n― Continue\nBefore starting, you need to install the following tools:\nContinue\r: VS Code Version\ror JetBrains Version\rOllama\rNext, using VS Code as an example, we will introduce how to use Ollama + Continue to implement code completion functionality:\n#\rCodestral 22B Model\rCodestral is capable of both code auto-completion and chat functionality. However, given that it has 22 billion parameters and lacks a production license, it requires a significant amount of video memory and is limited to research and testing use, so it may not be suitable for everyday local applications.\n#\rDownload and run the Codestral model\rollama run codestral\r#\rConfigure config.json\rIn the VS Code sidebar, click the Continue plugin icon, then click the \u0026ldquo;gear\u0026rdquo; icon at the bottom right of the panel to open the config.json file. Then copy the following configuration into the config.json file: {\r\u0026quot;models\u0026quot;: [\r{\r\u0026quot;title\u0026quot;: \u0026quot;Codestral\u0026quot;,\r\u0026quot;provider\u0026quot;: \u0026quot;ollama\u0026quot;,\r\u0026quot;model\u0026quot;: \u0026quot;codestral\u0026quot;\r}\r],\r\u0026quot;tabAutocompleteModel\u0026quot;: {\r\u0026quot;title\u0026quot;: \u0026quot;Codestral\u0026quot;,\r\u0026quot;provider\u0026quot;: \u0026quot;ollama\u0026quot;,\r\u0026quot;model\u0026quot;: \u0026quot;codestral\u0026quot;\r}\r}\r#\rDeepSeek Coder 6.7B model + Llama 3 8B model\rDepending on the machine\u0026rsquo;s VRAM size, you can utilize Ollama\u0026rsquo;s ability to run multiple models simultaneously and handle multiple concurrent requests, using DeepSeek Coder 6.7B for auto-completion, and Llama 3 8B for chatting. If your machine cannot run both at the same time, you can try them separately to decide whether you prefer the local auto-completion or the local chat experience.\n#\rDownload and run the DeepSeek Coder model\rollama run deepseek-coder:6.7b-base\r#\rDownload and run the Llama 3 model\rollama run llama3:8b\r#\rConfigure config.json\r{\r\u0026quot;models\u0026quot;: [\r{\r\u0026quot;title\u0026quot;: \u0026quot;Llama 3 8B\u0026quot;,\r\u0026quot;provider\u0026quot;: \u0026quot;ollama\u0026quot;,\r\u0026quot;model\u0026quot;: \u0026quot;llama3:8b\u0026quot;,\r\u0026quot;apiBase\u0026quot;: \u0026quot;http://127.0.0.1:11434\u0026quot;\r}\r],\r\u0026quot;tabAutocompleteModel\u0026quot;: {\r\u0026quot;title\u0026quot;: \u0026quot;DeepSeek Coder 6.7B\u0026quot;,\r\u0026quot;provider\u0026quot;: \u0026quot;ollama\u0026quot;,\r\u0026quot;model\u0026quot;: \u0026quot;deepseek-coder:6.7b-base\u0026quot;,\r\u0026quot;apiBase\u0026quot;: \u0026quot;http://127.0.0.1:11434\u0026quot;\r}\r}\r#\rCodeqwen 7B model + Qwen2 7B model\rThe Codeqwen 7B model is a model specifically designed for code completion, while the Qwen2 7B model is a general-purpose chat model. These two models can be well combined to achieve both code completion and chat functions.\n#\rDownload and run the Codeqwen model\rollama run codeqwen\r#\rDownload and run the Qwen2 model\rollama run qwen2:7b\r#\rConfigure config.json\r{\r\u0026quot;models\u0026quot;: [\r{\r\u0026quot;title\u0026quot;: \u0026quot;Codeqwen 7B\u0026quot;,\r\u0026quot;provider\u0026quot;: \u0026quot;ollama\u0026quot;,\r\u0026quot;model\u0026quot;: \u0026quot;codeqwen\u0026quot;,\r\u0026quot;apiBase\u0026quot;: \u0026quot;http://127.0.0.1:11434\u0026quot;\r}\r],\r\u0026quot;tabAutocompleteModel\u0026quot;: {\r\u0026quot;title\u0026quot;: \u0026quot;Qwen2 7B\u0026quot;,\r\u0026quot;provider\u0026quot;: \u0026quot;ollama\u0026quot;,\r\u0026quot;model\u0026quot;: \u0026quot;qwen2:7b\u0026quot;,\r\u0026quot;apiBase\u0026quot;: \u0026quot;http://127.0.0.1:11434\u0026quot;\r}\r}\r#\rOptimize chat using RAG vector retrieval\rContinue has a built-in @codebase\rcontext provider that can automatically retrieve the most relevant code snippets from the codebase. If you have set up a chat model (such as Codestral, Llama 3), then with the help of Ollama and LanceDB\r\u0026rsquo;s vectorization technology, you can achieve more efficient code retrieval and chat experience.\nHere, we use the nomic-embed-text model as the vector retrieval model:\n#\rDownload and run the Nomic Embed Text model\rollama run nomic-embed-text\r#\rConfigure config.json\rAdd the following content to the file: {\r\u0026quot;embeddingsProvider\u0026quot;: {\r\u0026quot;provider\u0026quot;: \u0026quot;ollama\u0026quot;,\r\u0026quot;model\u0026quot;: \u0026quot;nomic-embed-text\u0026quot;,\r\u0026quot;apiBase\u0026quot;: \u0026quot;http://127.0.0.1:11434\u0026quot;\r}\r}\r#\rCode completion effect\rCtrl + I: Generate code snippet based on instructions. Cursor hover auto-complete code #\rChat with Ollama\r#\rCode auto-comment\rSelect code to open the right-click menu #\rSummary\rOllama has opened the door to the world of open-source LLM, allowing everyone to easily experience the powerful features of LLM and customize applications according to their own needs. Whether for research, development, or daily use, Ollama can provide you with a platform to explore the limitless possibilities of LLM. With the continuous development of Ollama, it is believed that it will bring us more surprises and promote the application and development of LLM technology in various fields.\n","date":"2024-06-15T23:10:00Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116903387_p0_master1200.webp","permalink":"https://cuterwrite.top/en/p/ollama/","title":"Ollama: From Beginner to Advanced"},{"content":"\r#\rRDMA\u0026rsquo;s Address Handle\rThis article welcomes non-commercial reprints, please indicate the source when reprinting.\nStatement: For collection only, for easy reading\n― Savir, Zhihu Column: 8. RDMA Address Handle It has already been introduced that the basic unit of RDMA communication is the QP. Let\u0026rsquo;s consider a question: Suppose a QP on node A wants to exchange information with a QP on node B. Besides knowing the QP number—QPN—of node B, what other information is needed? It should be noted that the QPN is a number maintained independently by each node and is not unique across the entire network. For instance, if QP 3 on A wants to communicate with QP 5 on B, there is not just one QP5 in the network; many nodes might have their own QP 5. Therefore, we can naturally think of the need to find a way for each node to have a unique identifier.\nIn the traditional TCP-IP protocol stack, the well-known IP address is used to identify each node at the network layer. In the IB protocol, this identifier is called the GID (Global Identifier), which is a 128-bit sequence. This article will not discuss GID in detail and will introduce it later.\n#\rWhat is AH\rAH stands for Address Handle. I couldn\u0026rsquo;t think of a particularly suitable Chinese translation, so let\u0026rsquo;s directly translate it as \u0026ldquo;地址句柄\u0026rdquo; for now. The address here refers to a set of information used to locate a remote node. In the IB protocol, the address refers to information such as GID, port number, etc. As for the so-called handle, we can understand it as a pointer to an object.\nDoes everyone still remember that there are four basic service types in the IB protocol—RC, UD, RD, and UC, with RC and UD being the most commonly used. The characteristic of RC is that a reliable connection is established between the QPs of two nodes, and once the connection is established, it is not easily changed. The information of the peer is stored in the QP Context when creating the QP.\nAs for UD, there is no connection relationship between QPs. The user can fill in the peer\u0026rsquo;s address information in the WQE for whoever they want to send to. The user does not directly fill the peer\u0026rsquo;s address information into the WQE but prepares an \u0026ldquo;address book\u0026rdquo; in advance, specifying the peer node\u0026rsquo;s address information each time through an index, which is the AH.\nThe concept of AH can be roughly represented by the diagram below:\nAddress Handle Function Diagram For each destination node, a corresponding AH will be created at this end, and the same AH can be shared by multiple QPs.\n#\rThe role of AH\rBefore each communication of the UD service type, the user needs to first use the interface provided by the IB framework to create an AH for each possible peer node, then these AHs are driven into a \u0026ldquo;secure\u0026rdquo; area, and an index (pointer/handle) is returned to the user. When the user actually issues a WR (Work Request), they just need to pass in this index.\nThe above process is shown in the diagram below. Node A receives a task from the user to exchange data using its QP4 with Node B\u0026rsquo;s QP3 (specified through AH):\nUD service type uses AH to specify peer node The IB protocol does not explain why AH is used. I believe there are three reasons for defining the concept of AH:\nEnsure the destination address is available, improve efficiency Due to the connectionless nature of UD, users can directly specify the destination through WR in user mode. However, if users are allowed to fill in address information at will, and then hardware packages the data according to this information, it may lead to problems. For example, there is a scenario like this: a user tells the hardware through WR to send data to port Z of a node with GID X and MAC address Y. However, X, Y, and Z may not be a valid combination, or a node with GID X may not even exist in the network, and the hardware is unable to verify this content, so it can only obediently package and send the data. This results in the unnecessary sending of a data packet to an invalid destination.\nAnd preparing the address information in advance can avoid the above situation. When the user creates AH, they will enter kernel mode. If the parameters passed by the user are valid, the kernel will store this destination node information, generate a pointer, and return it to the user; if the parameters passed by the user are invalid, AH creation will fail. This process ensures that the address information is valid. The user can quickly specify the destination node through the pointer, speeding up the data interaction process.\nSome may ask, since the kernel is trusted, why not switch to kernel mode to verify the address information passed by the user when sending data? Please do not forget where one of the major advantages of RDMA technology lies—the data flow can go directly from user space to hardware, completely bypassing the kernel, thus avoiding the overhead of system calls and copying. If the legality of the address has to be checked every time data is sent, it will inevitably reduce the communication rate.\nHide underlying address details from the user When the user creates AH, they only need to provide information such as gid, port number, static rate, etc., while other address information required for communication (mainly MAC addresses) is resolved by the kernel driver through querying the system neighbor table and other methods. There is no need to expose this additional information to the user layer.\nYou can use PD to manage the destination address. In the previous text, when we introduced protection domains, we mentioned that besides QP and MR, AH is also resource partitioned by PD. Once the software entity AH is defined, we can isolate and manage all destinations reachable by QP.\nUse PD to isolate AH For example, in the image above, AH1~3 can only be used by QP3 and QP9 under the same PD, while AH4 can only be used by QP5.\n#\rRelevant sections of the agreement\rThere is not much coverage about AH in the agreement, and there is not even a chapter dedicated to introducing its concept:\n[1] 9.8.3 What components make up the destination address in UD service type: including AH, QPN, and Q_key\n[2] 10.2.2.2 Relevant Considerations for the Destination Address\n[3] 11.2.2.1 AH Related Verbs Interface\nAH is introduced here, thank you for reading. In the next article, I plan to describe more details about QP.\n","date":"2024-06-15T01:00:00Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116373922_p9_master1200.webp","permalink":"https://cuterwrite.top/en/p/rdma-address-handle/","title":"RDMA: Address Handle"},{"content":"\r#\rProtection Domain in RDMA\rThis article welcomes non-commercial reproduction, please indicate the source when reproducing.\nStatement: For collection only, for easy reading\n― Savir, Zhihu Column: 7. RDMA and Protection Domain In the previous text, we briefly introduced some of the most common resources in RDMA, including various Queues and the concept of MR, among others. MR is used to control and manage HCA\u0026rsquo;s access rights to local and remote memory, ensuring that the HCA can only read and write to memory regions that the user has registered after obtaining the correct Key. To better ensure security, the IB protocol also introduced the concept of Protection Domain (PD) to ensure mutual isolation among RDMA resources. This article will introduce the concept of PD.\n#\rWhat is PD?\rPD stands for Protection Domain. The concept of a domain is often seen, from mathematical \u0026ldquo;real number fields\u0026rdquo; and \u0026ldquo;complex number fields\u0026rdquo; to geographical \u0026ldquo;airspace\u0026rdquo; and \u0026ldquo;sea area,\u0026rdquo; representing a space/range. In RDMA, a PD is like a \u0026ldquo;container\u0026rdquo; that holds various resources (QP, MR, etc.), bringing these resources under its protection to prevent unauthorized access. Multiple protection domains can be defined within a node, and the resources contained within each PD are isolated from each other and cannot be used together.\nThe concept is still somewhat abstract, let\u0026rsquo;s take a look at what role PD plays and what specific problems it solves.\n#\rThe function of PD\rA user may create multiple QPs and multiple MRs, and each QP may establish connections with different remote QPs, as shown in the diagram below (gray arrows indicate the connection relationships between QPs):\nFigure 1: RDMA Resources Without PD Concept Since there is no binding relationship between MR and QP, this means that once a remote QP establishes a connection with a local QP and the conditions for communication are met, theoretically, as long as the remote node knows the VA and R_key (it can even keep guessing until it gets a valid pair of values), it can access the contents of an MR on the local node.\nIn general, the virtual address VA of MR and the key R_Key are difficult to guess, which already ensures a certain level of security. However, to better protect the data in memory and further isolate and divide the permissions of various resources, we have defined PD in each node, as shown in the diagram below.\nFigure 2: RDMA resources when adding the PD concept In the diagram, Node 0 has two PDs, with 3 QPs and 2 MRs divided into two groups. Additionally, Node 1 and Node 2 each have a PD containing all QPs and MRs. The resources in the two PDs on Node 0 cannot be used together, meaning QP3 and QP9 cannot access the data of MR1, and QP6 cannot access the data of MR0. If we specify the hardware to use QP3 and MR1 during data transmission, the hardware will verify that they do not belong to the same PD and return an error.\nFor the remote node, Node1 can only access Node0\u0026rsquo;s memory through QP3 connected via QP8. However, because Node0\u0026rsquo;s QP3 is \u0026ldquo;encircled\u0026rdquo; in the protection domain PD0, Node1\u0026rsquo;s QP8 can only access the memory corresponding to MR0, and cannot access the data in MR1 under any circumstances. This is restricted from two aspects:\nNode 1\u0026rsquo;s QP8 is only connected to Node 0\u0026rsquo;s QP3, and cannot access memory through Node 0\u0026rsquo;s QP6. MR1 and QP3 of Node 0 belong to different PDs, so even if QP8 of Node 1 obtains the VA and R_key of MR1, the hardware will refuse to provide service due to different PDs. As mentioned at the beginning of this article, a PD is like a container that protects some RDMA resources, isolating them from each other to enhance security. In fact, RDMA includes not only resources like QP and MR, but also Address Handle, Memory Window, etc., which are also isolated and protected by PD, as will be introduced later.\n#\rHow to use PD\rStill looking at the above diagram, we notice that Node 0 has two PDs for resource isolation, whereas Node 1 and Node 2 have only one PD containing all the resources.\nThe reason I draw it this way is to illustrate that the number of PDs divided on a node is entirely up to the user. If you want to enhance security, then each QP connected to a remote node and the MR provided for remote access should be isolated as much as possible by dividing PDs. If higher security is not a concern, then creating one PD that encompasses all resources is also acceptable.\nThe IB protocol specifies: Each node must have at least one PD, each QP must belong to a PD, and each MR must also belong to a PD.\nSo how is the inclusion relationship of PD reflected in the software? It itself has a software entity (structure) that records some information about this protection domain. Before users create resources like QP and MR, they must first create a PD through the interface of the IB framework to get its pointer/handle. Then, when creating QP and MR, this PD\u0026rsquo;s pointer/handle needs to be passed in, and PD information will be included in QP and MR. When the hardware sends and receives packets, it will verify the PD of QP and MR. I will introduce more about the software protocol stack in later articles.\nAdditionally, it is important to emphasize that PD is a local concept and only exists within the node, it is not visible to other nodes; whereas MR is visible to both the local and remote ends.\nFor everyone\u0026rsquo;s convenience in referencing and learning, I will list the protocol sections involved in the articles. I will also supplement the previous content when I have time.\n#\rPD related protocol chapter\rBasic concepts and functions of 3.5.5 PD 10.2.3 introduces the relationship between PD and some other RDMA resources, as well as the software interfaces related to PD. 10.6.3.5 Emphasize again the relationship between PD, MR, and QP. 11.2.1.5 Detailed introduction to the Verbs interface of PD, including functions, input parameters, output parameters, and return values, etc. Alright, that concludes the introduction to PD. In the following text, I will introduce the concept of Address Handle used for UD service types.\n","date":"2024-04-18T21:42:00Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/d31a474af07682028ca085f871bc5d07195413-2024-04-19.webp","permalink":"https://cuterwrite.top/en/p/rdma-protection-domain/","title":"RDMA: Protection Domain"},{"content":"\r#\rMemory Region of RDMA\rThis article welcomes non-commercial reproduction, please indicate the source.\nStatement: For collection only, for easy reading\n― Savir, Zhihu Column: 6. RDMA Memory Region We assume a scenario and also take the opportunity to review the RDMA WRITE operation process:\nAs shown in the figure below, Node A wants to write a piece of data into Node B\u0026rsquo;s memory via the IB protocol. The upper-layer application issues a WQE to the RDMA network card of the local node. The WQE contains information such as source memory address, destination memory address, data length, and key. Then the hardware retrieves the data from memory, packages it, and sends it to the remote network card. After Node B\u0026rsquo;s network card receives the data, it parses the destination memory address and writes the data into the local node\u0026rsquo;s memory.\nSo the question arises, the addresses provided by the APP are all virtual addresses (Virtual Address, referred to as VA below), which need to be converted by the MMU to obtain the real physical address (Physical Address, referred to as PA below). How does our RDMA network card obtain the PA to fetch data from memory? Even if the network card knows where to fetch the data, if a user maliciously specifies an illegal VA, wouldn\u0026rsquo;t the network card possibly be \u0026ldquo;instructed\u0026rdquo; to read and write critical memory?\nTo solve the above problem, the IB protocol proposed the concept of MR.\n#\rWhat is MR\rMR stands for Memory Region, which refers to a region designated by the RDMA software layer in memory for storing transmitted and received data. In the IB protocol, after a user requests a memory region for storing data, they must register the MR by calling the API provided by the IB framework to allow the RDMA network card to access this memory region. As can be seen from the diagram below, MR is just a special piece of memory:\nWhen describing the IB protocol, we usually refer to the RDMA hardware as HCA (Host Channel Adapter). The IB protocol defines it as \u0026ldquo;an IB device in processors and I/O units capable of generating and consuming packets.\u0026rdquo; To remain consistent with the protocol, we will refer to the hardware part as HCA in this and subsequent articles.\n#\rWhy register MR\rLet\u0026rsquo;s take a look at how MR addresses the two questions raised at the beginning of this article:\n#\r1. Register MR to achieve virtual-to-physical address translation\rWe all know that an APP can only see virtual addresses, and it will directly pass the VA to the HCA in the WQE (including both the source VA on the local end and the destination VA on the remote end). Modern CPUs have the \u0026ldquo;tool\u0026rdquo; of MMU and page tables to perform the conversion between VA and PA, while the HCA is either directly connected to the bus or connected to the bus after address translation through IOMMU/SMMU. It cannot \u0026ldquo;understand\u0026rdquo; the real physical memory address corresponding to the VA provided by the APP.\nSo during the process of registering MR, the hardware will create and fill a VA to PA mapping table in memory, so that when needed, VA can be converted to PA by looking up the table. Let\u0026rsquo;s provide a specific example to explain this process:\nNow assume that the node on the left initiates an RDMA WRITE operation to the node on the right, directly writing data into the memory area of the right node. Assume that both ends in the diagram have already completed the registration of MR, which corresponds to the \u0026ldquo;data Buffer\u0026rdquo; in the diagram, and have also created the VA-\u0026gt;PA mapping table.\nFirst, this end\u0026rsquo;s APP will issue a WQE to the HCA, informing the HCA of the virtual address of the local buffer used to store the data to be sent, as well as the virtual address of the peer data buffer that will be written to. This end HCA queries the VA-\u0026gt;PA mapping table to obtain the physical address of the data to be sent, then retrieves the data from memory, assembles the data packet, and sends it out. The remote HCA received the packet and parsed the destination VA from it. The peer HCA uses the VA-\u0026gt;PA mapping table stored in local memory to find the real physical address, verifies the permissions, and then stores the data in memory. Emphasize once again, for the right-side node, whether it\u0026rsquo;s address translation or writing to memory, it does not require any involvement of its CPU.\n#\r2. MR can control HCA\u0026rsquo;s access to memory permissions\rBecause the memory address accessed by the HCA comes from the user, if the user provides an illegal address (such as system memory or memory used by another process), HCA reading or writing to it may cause information leakage or memory corruption. Therefore, we need a mechanism to ensure that HCA can only access authorized and safe memory addresses. In the IB protocol, during the preparation stage for data interaction, the APP needs to perform the action of registering MR.\nWhen a user registers MR, two keys are generated—L_KEY (Local Key) and R_KEY (Remote Key). Although they are called keys, their entities are actually just a sequence. They will be used to ensure access permissions for the local and remote memory regions, respectively. The following two diagrams are schematic representations describing the functions of L_Key and R_Key:\nL_Key R_Key Here, everyone might have a question: how does this end know the available VA and the corresponding R_Key of the peer node? In fact, before the actual RDMA communication, both nodes establish a link through some means (it could be a Socket connection or a CM connection) and exchange some necessary information for RDMA communication (VA, Key, QPN, etc.) through this link. We call this process \u0026ldquo;link establishment\u0026rdquo; and \u0026ldquo;handshake.\u0026rdquo; I will introduce this in detail in the following articles.\nIn addition to the two points above, registering MR has another important function:\n#\r3. MR can avoid page swapping\rBecause physical memory is limited, the operating system uses a paging mechanism to temporarily save the unused memory contents of a process to the hard drive. When the process needs to use it, a page fault interrupt is used to move the contents from the hard drive back to memory, and this process almost inevitably causes the VA-PA mapping relationship to change.\nSince HCA often bypasses the CPU to read and write to the physical memory areas pointed to by the VA provided by the user, if the VA-PA mapping relationship changes, then the VA-\u0026gt;PA mapping table mentioned earlier will lose its significance, and HCA will be unable to find the correct physical address.\nIn order to prevent the VA-PA mapping relationship from changing due to page swapping, the memory is \u0026ldquo;Pinned\u0026rdquo; when registering MR (also known as \u0026ldquo;page locking\u0026rdquo;), which means locking the VA-PA mapping relationship. In other words, this MR memory area will remain in physical memory without being swapped out until the communication is completed, and the user actively deregisters this MR.\nAlright, we have now finished introducing the concept and function of MR. In the next article, I will introduce the concept of PD (Protection Domain).\n#\rCode example\rBelow is a simple RDMA program demonstrating how to register MR:\n#include \u0026lt;infiniband/verbs.h\u0026gt;\rint main() {\r// Omit initialization process...\rstruct ibv_mr *mr;\rmr = ibv_reg_mr(pd, buf, 1024, IBV_ACCESS_LOCAL_WRITE |\rIBV_ACCESS_REMOTE_WRITE);\r// Get L_Key and R_Key\ruint32_t lkey = mr-\u0026gt;lkey;\ruint32_t rkey = mr-\u0026gt;rkey;\r// Omit other code...\r}\r","date":"2024-04-03T16:17:00Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/8fa232626b76940fddc8cc52a49c49e9195413-2024-04-04.webp","permalink":"https://cuterwrite.top/en/p/rdma-mr/","title":"RDMA: Memory Region"},{"content":"\r#\rRecord: Install Intel® OneAPI-2024.0\rIntel oneAPI consists of two parts, the former being the Base Toolkit, and the latter must rely on the former, Intel oneAPI HPC Toolkit, which means they need to be installed sequentially.\n#\rBase Toolkit\rBase Toolkit is Intel\u0026rsquo;s basic API toolkit that includes the following libraries and others.\nIntel® oneAPI DPC++/C++ Compiler\rIntel® DPC++ Compatibility Tool\rIntel® oneAPI DPC++ Library\rIntel® oneAPI Math Kernel Library\rIntel® oneAPI Threading Building Blocks\rIntel® oneAPI Collective Communications Library\rIntel® oneAPI Data Analytics Library\rIntel® oneAPI Deep Neural Networks Library\rIntel® Integrated Performance Primitives\rIntel® VTune™ Profiler\rIntel® Advisor\rIntel® Distribution for GDB*\rIntel® Distribution for Python* (separate download required)\rIntel® FPGA Add-on for oneAPI Base Toolkit (separate download required)\r#\rBase Toolkit Installation\rDownload the installation package $ wget https://registrationcenter-download.intel.com/akdlm/IRC_NAS/163da6e4-56eb-4948-aba3-debcec61c064/l_BaseKit_p_2024.0.1.46_offline.sh\rInstallation $ chmod +x l_BaseKit_p_2024.0.1.46_offline.sh\r$ sudo ./l_BaseKit_p_2024.0.1.46_offline.sh\rIf custom installed in the user directory, root privileges are not required. ./l_BaseKit_p_2024.0.1.46_offline.sh\rThen a graphical installation interface will start, continue with the following steps:\n(1) Select Accept \u0026amp; customize (2) Select components to install (3) Select installation path (4) Select Next (5) Select 2 then start installation Next, wait for the installation to complete.\n#\rHPC Toolkit\rRun based on Base Toolkit, this must be installed afterwards\nIntel® Fortran Compiler\rIntel® Fortran Compiler Classic\rIntel® Inspector\rIntel® MPI Library\rIntel® Trace Analyzer and Collector\r#\rHPC Toolkit Installation\rDownload and install package $ wget https://registrationcenter-download.intel.com/akdlm/IRC_NAS/67c08c98-f311-4068-8b85-15d79c4f277a/l_HPCKit_p_2024.0.1.38_offline.sh\rInstallation $ chmod +x l_HPCKit_p_2024.0.1.38_offline.sh\r$ sudo ./l_HPCKit_p_2024.0.1.38_offline.sh\rIf custom installation is in the user directory, root privileges are not required. ./l_HPCKit_p_2024.0.1.38_offline.sh\rRequired library files: Intel® MPI Library Intel® Fortran Compiler (Beta) \u0026amp; Intel® Fortran Compiler Classic Intel® oneAPI DPC++/C++ Compiler \u0026amp; Intel® C++ Compiler Classic\nThe installation process is similar to the Base Toolkit and will not be elaborated.\n#\rEnvironment Configuration\rAfter installation, you need to configure environment variables to use Intel® oneAPI tools in the terminal.\nIn an HPC environment, use modulefile to manage environment variables, and you can use the module command to load environment variables.\nBelow is a reference modulefile file, which you can modify according to your installation path.\n#%Module1.0#####################################################################\r##\r## modules modulefile\r##\rproc ModulesHelp { } {\rglobal version prefix\rputs stderr \u0026quot;\\tmodules - loads the modules software \u0026amp; application environment\u0026quot;\rputs stderr \u0026quot;\\n\\tThis adds $prefix/* to several of the\u0026quot;\rputs stderr \u0026quot;\\tenvironment variables.\u0026quot;\rputs stderr \u0026quot;\\n\\tVersion $version\\n\u0026quot;\r}\rmodule-whatis\t\u0026quot;loads intel/oneapi2024.0\u0026quot;\r# for Tcl script use only\rset\tversion\toneapi2024.0\rset\tprefix\t/opt/software/intel/oneapi2024.0\rconflict\tintel\rprepend-path\tTBBROOT\t${prefix}/tbb/2021.11/env/..\rprepend-path\tDAALROOT\t${prefix}/cdal/2024.0\rprepend-path\tDPCT_BUNDLE_ROOT\t${prefix}/dpcpp-ct/2024.0\rprepend-path\tINSPECTOR_2023_DIR\t${prefix}/inspector/2024.0\rprepend-path\tONEAPI_ROOT\t${prefix}\rprepend-path\tPKG_CONFIG_PATH\t${prefix}/vtune/2024.0/include/pkgconfig/lib64:${prefix}/tbb/2021.11/env/../lib/pkgconfig:${prefix}/mpi/2021.11/lib/pkgconfig:${prefix}/mkl/2024.0/lib/pkgconfig:${prefix}/ippcp/2021.9/lib/pkgconfig:${prefix}/inspector/2024.0/include/pkgconfig/lib64:${prefix}/dpl/2022.3/lib/pkgconfig:${prefix}/dnnl/2024.0/cpu_dpcpp_gpu_dpcpp/../lib/pkgconfig:${prefix}/cdal/2024.0/lib/pkgconfig:${prefix}/compiler/2024.0/lib/pkgconfig:${prefix}/ccl/2021.11/lib/pkgconfig:${prefix}/advisor/2024.0/include/pkgconfig/lib64:\r#prepend-path\tPKG_CONFIG_PATH\t${prefix}/vtune/2024.0/include/pkgconfig/lib64:${prefix}/tbb/2021.11/env/../lib/pkgconfig:${prefix}/mkl/2024.0/lib/pkgconfig:${prefix}/ippcp/2021.9/lib/pkgconfig:${prefix}/inspector/2024.0/include/pkgconfig/lib64:${prefix}/dpl/2022.3/lib/pkgconfig:${prefix}/dnnl/2024.0/cpu_dpcpp_gpu_dpcpp/../lib/pkgconfig:${prefix}/cdal/2024.0/lib/pkgconfig:${prefix}/compiler/2024.0/lib/pkgconfig:${prefix}/ccl/2021.11/lib/pkgconfig:${prefix}/advisor/2024.0/include/pkgconfig/lib64:\rprepend-path\tVT_MPI\timpi4\rprepend-path\tACL_BOARD_VENDOR_PATH\t/opt/Intel/OpenCLFPGA/oneAPI/Boards\rprepend-path\tFPGA_VARS_DIR\t${prefix}/compiler/2024.0/lib/oclfpga\rprepend-path\tCCL_ROOT\t${prefix}/ccl/2021.11\rprepend-path\tVT_ADD_LIBS\t\u0026quot;-ldwarf -lelf -lvtunwind -lm -lpthread\u0026quot;\rprepend-path\tI_MPI_ROOT\t${prefix}/mpi/2021.11\rprepend-path\tFI_PROVIDER_PATH\t${prefix}/mpi/2021.11//libfabric/lib/prov:/usr/lib/x86_64-linux-gnu/libfabric\rprepend-path\tDNNLROOT\t${prefix}/dnnl/2024.0/cpu_dpcpp_gpu_dpcpp\rprepend-path\tDIAGUTIL_PATH\t${prefix}/vtune/2024.0/sys_check/vtune_sys_check.py:${prefix}/dpcpp-ct/2024.0/sys_check/sys_check.sh:${prefix}/debugger/2024.0/sys_check/debugger_sys_check.py:${prefix}/compiler/2024.0/sys_check/sys_check.sh:${prefix}/advisor/2024.0/sys_check/advisor_sys_check.py:\rprepend-path\tCCL_CONFIGURATION\tcpu_gpu_dpcpp\rprepend-path\tDPL_ROOT\t${prefix}/dpl/2022.3\rprepend-path\tMANPATH\t${prefix}/mpi/2021.11/man:${prefix}/itac/2022.0/man:${prefix}/debugger/2024.0/documentation/man:${prefix}/compiler/2024.0/documentation/en/man/common:::\r#prepend-path\tMANPATH\t${prefix}/itac/2022.0/man:${prefix}/debugger/2024.0/documentation/man:${prefix}/compiler/2024.0/documentation/en/man/common:::\rprepend-path\tGDB_INFO\t${prefix}/debugger/2024.0/documentation/info/\rprepend-path\tSETVARS_COMPLETED\t1\rprepend-path\tAPM\t${prefix}/advisor/2024.0/perfmodels\rprepend-path\tCMAKE_PREFIX_PATH\t${prefix}/tbb/2021.11/env/..:${prefix}/ipp/2021.10/lib/cmake/ipp:${prefix}/ipp/2021.10/lib/cmake/ipp:${prefix}/dnnl/2024.0/cpu_dpcpp_gpu_dpcpp/../lib/cmake:${prefix}/cdal/2024.0:${prefix}/compiler/2024.0/IntelDPCPP:${prefix}/ccl/2021.11/lib/cmake/oneCCL\rprepend-path\tVTUNE_PROFILER_2023_DIR\t${prefix}/vtune/2024.0\rprepend-path\tCMPLR_ROOT\t${prefix}/compiler/2024.0\rprepend-path\tADVISOR_2023_DIR\t${prefix}/advisor/2024.0\rprepend-path\tFPGA_VARS_ARGS\t\u0026quot;\u0026quot;\rprepend-path\tINFOPATH\t${prefix}/debugger/2024.0/gdb/intel64/lib\rprepend-path\tIPPROOT\t${prefix}/ipp/2021.10\rprepend-path\tIPP_TARGET_ARCH\tintel64\rprepend-path\tPYTHONPATH\t${prefix}/advisor/2024.0/pythonapi\rprepend-path\tVT_ROOT\t${prefix}/itac/2022.0\rprepend-path\tDALROOT\t${prefix}/cdal/2024.0\rprepend-path\tLIBRARY_PATH\t${prefix}/tbb/2021.11/env/../lib/intel64/gcc4.8:${prefix}/mpi/2021.11//libfabric/lib:${prefix}/mpi/2021.11//lib/release:${prefix}/mpi/2021.11//lib:${prefix}/mkl/2024.0/lib/intel64:${prefix}/ipp/2021.10/lib/intel64:${prefix}/ippcp/2021.9/lib/intel64:${prefix}/ipp/2021.10/lib/intel64:${prefix}/dnnl/2024.0/cpu_dpcpp_gpu_dpcpp/lib:${prefix}/cdal/2024.0/lib/intel64:${prefix}/compiler/2024.0/compiler/lib/intel64_lin:${prefix}/compiler/2024.0/lib:${prefix}/ccl/2021.11/lib/cpu_gpu_dpcpp\r#prepend-path\tLIBRARY_PATH\t${prefix}/tbb/2021.11/env/../lib/intel64/gcc4.8:${prefix}/mkl/2024.0/lib/intel64:${prefix}/ipp/2021.10/lib/intel64:${prefix}/ippcp/2021.9/lib/intel64:${prefix}/ipp/2021.10/lib/intel64:${prefix}/dnnl/2024.0/cpu_dpcpp_gpu_dpcpp/lib:${prefix}/cdal/2024.0/lib/intel64:${prefix}/compiler/2024.0/compiler/lib/intel64_lin:${prefix}/compiler/2024.0/lib:${prefix}/ccl/2021.11/lib/cpu_gpu_dpcpp\rprepend-path\tDAL_MAJOR_BINARY\t1\rprepend-path\tIPPCRYPTOROOT\t${prefix}/ippcp/2021.9\rprepend-path\tIPPCP_TARGET_ARCH\tintel64\rprepend-path\tOCL_ICD_FILENAMES\tlibintelocl_emu.so:libalteracl.so:${prefix}/compiler/2024.0/lib/x64/libintelocl.so\rprepend-path\tCLASSPATH\t${prefix}/mpi/2021.11//lib/mpi.jar:${prefix}/cdal/2024.0/lib/onedal.jar\r#prepend-path\tCLASSPATH\t${prefix}/cdal/2024.0/lib/onedal.jar\rprepend-path\tINTELFPGAOCLSDKROOT\t${prefix}/compiler/2024.0/lib/oclfpga\rprepend-path\tLD_LIBRARY_PATH\t${prefix}/tbb/2021.11/env/../lib/intel64/gcc4.8:${prefix}/mpi/2021.11//libfabric/lib:${prefix}/mpi/2021.11//lib/release:${prefix}/mpi/2021.11//lib:${prefix}/mkl/2024.0/lib/intel64:${prefix}/itac/2022.0/slib:${prefix}/ipp/2021.10/lib/intel64:${prefix}/ippcp/2021.9/lib/intel64:${prefix}/ipp/2021.10/lib/intel64:${prefix}/dnnl/2024.0/cpu_dpcpp_gpu_dpcpp/lib:${prefix}/debugger/2024.0/gdb/intel64/lib:${prefix}/debugger/2024.0/libipt/intel64/lib:${prefix}/debugger/2024.0/dep/lib:${prefix}/cdal/2024.0/lib/intel64:${prefix}/compiler/2024.0/lib:${prefix}/compiler/2024.0/lib/x64:${prefix}/compiler/2024.0/lib/oclfpga/host/linux64/lib:${prefix}/compiler/2024.0/compiler/lib/intel64_lin:${prefix}/ccl/2021.11/lib/cpu_gpu_dpcpp:${prefix}/compiler/2024.0/compiler/lib/intel64_lin:${prefix}/ccl/2021.11/lib/cpu_gpu_dpcpp\r#prepend-path\tLD_LIBRARY_PATH\t${prefix}/tbb/2021.11/env/../lib/intel64/gcc4.8:${prefix}/mkl/2024.0/lib/intel64:${prefix}/itac/2022.0/slib:${prefix}/ipp/2021.10/lib/intel64:${prefix}/ippcp/2021.9/lib/intel64:${prefix}/ipp/2021.10/lib/intel64:${prefix}/dnnl/2024.0/cpu_dpcpp_gpu_dpcpp/lib:${prefix}/debugger/2024.0/gdb/intel64/lib:${prefix}/debugger/2024.0/libipt/intel64/lib:${prefix}/debugger/2024.0/dep/lib:${prefix}/cdal/2024.0/lib/intel64:${prefix}/compiler/2024.0/lib:${prefix}/compiler/2024.0/lib/x64:${prefix}/compiler/2024.0/lib/oclfpga/host/linux64/lib:${prefix}/compiler/2024.0/compiler/lib/intel64_lin:${prefix}/ccl/2021.11/lib/cpu_gpu_dpcpp:${prefix}/compiler/2024.0/compiler/lib/intel64_lin:${prefix}/ccl/2021.11/lib/cpu_gpu_dpcpp\rprepend-path\tVT_LIB_DIR\t${prefix}/itac/2022.0/lib\rprepend-path\tVTUNE_PROFILER_DIR\t${prefix}/vtune/2024.0\rprepend-path\tVT_SLIB_DIR\t${prefix}/itac/2022.0/slib\rprepend-path\tMKLROOT\t${prefix}/mkl/2024.0\rprepend-path\tDAL_MINOR_BINARY\t1\rprepend-path\tNLSPATH\t${prefix}/mkl/2024.0/lib/intel64/locale/%l_%t/%N:${prefix}/compiler/2024.0/compiler/lib/intel64_lin/locale/%l_%t/%N\rprepend-path\tPATH\t${prefix}/vtune/2024.0/bin64:${prefix}/mpi/2021.11//libfabric/bin:${prefix}/mpi/2021.11//bin:${prefix}/mkl/2024.0/bin/intel64:${prefix}/itac/2022.0/bin:${prefix}/inspector/2024.0/bin64:${prefix}/dpcpp-ct/2024.0/bin:${prefix}/dev-utilities/2024.0/bin:${prefix}/debugger/2024.0/gdb/intel64/bin:${prefix}/compiler/2024.0/lib/oclfpga/bin:${prefix}/compiler/2024.0/bin/intel64:${prefix}/compiler/2024.0/bin:${prefix}/advisor/2024.0/bin64\r#prepend-path\tPATH\t${prefix}/vtune/2024.0/bin64:${prefix}/mkl/2024.0/bin/intel64:${prefix}/itac/2022.0/bin:${prefix}/inspector/2024.0/bin64:${prefix}/dpcpp-ct/2024.0/bin:${prefix}/dev-utilities/2024.0/bin:${prefix}/debugger/2024.0/gdb/intel64/bin:${prefix}/compiler/2024.0/lib/oclfpga/bin:${prefix}/compiler/2024.0/bin/intel64:${prefix}/compiler/2024.0/bin:${prefix}/advisor/2024.0/bin64\rprepend-path\tINTEL_PYTHONHOME\t${prefix}/debugger/2024.0/dep\rprepend-path\tINTEL_LICENSE_FILE\t/opt/intel/licenses:/root/intel/licenses\rprepend-path\tCPATH\t${prefix}/tbb/2021.11/env/../include:${prefix}/mpi/2021.11//include:${prefix}/mkl/2024.0/include:${prefix}/ipp/2021.10/include:${prefix}/ippcp/2021.9/include:${prefix}/ipp/2021.10/include:${prefix}/dpl/2022.3/linux/include:${prefix}/dpcpp-ct/2024.0/include:${prefix}/dnnl/2024.0/cpu_dpcpp_gpu_dpcpp/include:${prefix}/dev-utilities/2024.0/include:${prefix}/cdal/2024.0/include:${prefix}/compiler/2024.0/lib/oclfpga/include:${prefix}/ccl/2021.11/include/cpu_gpu_dpcpp\r#prepend-path\tCPATH\t${prefix}/tbb/2021.11/env/../include:${prefix}/mkl/2024.0/include:${prefix}/ipp/2021.10/include:${prefix}/ippcp/2021.9/include:${prefix}/ipp/2021.10/include:${prefix}/dpl/2022.3/linux/include:${prefix}/dpcpp-ct/2024.0/include:${prefix}/dnnl/2024.0/cpu_dpcpp_gpu_dpcpp/include:${prefix}/dev-utilities/2024.0/include:${prefix}/cdal/2024.0/include:${prefix}/compiler/2024.0/lib/oclfpga/include:${prefix}/ccl/2021.11/include/cpu_gpu_dpcpp\r#\rRun test\rLoad environment variables using the module load command\n$ module load intel/oneapi2024.0\rTest whether the installation was successful\n$ icx -v\rIf the version information is output, the installation is successful.\nIntel(R) oneAPI DPC++/C++ Compiler 2024.0.2 (2024.0.2.20231213)\rTarget: x86_64-unknown-linux-gnu\rThread model: posix\rInstalledDir: /opt/software/intel/oneapi2024.0/compiler/2024.0/bin/compiler\rConfiguration file: /opt/software/intel/oneapi2024.0/compiler/2024.0/bin/compiler/../icx.cfg\rFound candidate GCC installation: /opt/rh/devtoolset-11/root/usr/lib/gcc/x86_64-redhat-linux/11\rSelected GCC installation: /opt/rh/devtoolset-11/root/usr/lib/gcc/x86_64-redhat-linux/11\rCandidate multilib: .;@m64\rCandidate multilib: 32;@m32\rSelected multilib: .;@m64\rContinue testing MPI\n$ mpirun --version\rIf the version information is output, then the installation is successful.\nIntel(R) MPI Library for Linux* OS, Version 2021.11 Build 20231005 (id: 74c4a23)\rCopyright 2003-2023, Intel Corporation.\r#\ricx instructions\rIntel® oneAPI DPC++/C++ Compiler (icx) is Intel\u0026rsquo;s next-gen compiler based on Clang/LLVM technology plus Intel proprietary optimizations and code generation.\n― Intel®, Intel® C/C\u0026#43;\u0026#43; Compilers Complete Adoption of LLVM icx is Intel\u0026rsquo;s next-generation compiler based on Clang/LLVM technology, with Intel\u0026rsquo;s proprietary optimizations and code generation.\nLLVM helped achieve the goal of providing a better C/C++ compiler for Intel architecture. The latest Intel C/C++ compiler uses the LLVM architecture, offering faster compile times, better optimization, enhanced standards support, and support for GPU and FPGA offloading.\n#\rBenefits of adopting LLVM\rThe LLVM open source project is a collection of modular and reusable compiler and toolchain technologies that supports multiple processor architectures and programming languages. The Clang open source project provides a C/C++ frontend, supporting the latest language standards for the LLVM project. Including Clang, LLVM is maintained by a large and very active development community.\nThere are many benefits to using LLVM, the first being faster build times. It is well known that Clang is very fast! When we used the Intel C/C++ Compiler from the Intel oneAPI 2021.3 toolkit, we measured a 14% reduction in build time. In addition to reducing build time, adopting Clang allows us to benefit from the community\u0026rsquo;s support for the latest C++ language standards and contribute results to give back to the community.\nIntel has a long history of contributing to and supporting open-source projects, including ten years of contributions to LLVM. Our proactive collaboration today includes optimization report supplements, expanded floating-point model support, and vector enhancements. Intel directly contributes to the LLVM project and has a staging area (Intel LLVM Technology Project) for SYCL support.\nOn Intel architecture, the Intel C/C++ compiler is expected to provide higher performance than the base Clang+LLVM compiler. The upcoming Intel C/C++ compilers will be versions (icx) that have adopted the LLVM open-source infrastructure. We will continue our long-term efforts to contribute to the Clang and LLVM projects, including providing optimizations for them. Not all optimization techniques will be accepted upstream, sometimes because they are too new, and sometimes because they are too specific to Intel architecture. This is to be expected and is the same situation as with other compilers that have adopted LLVM.\nIntel C/C++ compilers have consistently provided the best performance. The classic version of the Intel C/C++ compiler achieved an 18% advantage over GCC, while the Intel C/C++ compiler based on LLVM achieved a 41% advantage.\n","date":"2024-03-08T14:39:00Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/crop_62cf8bae89f60c3522eb45af53a53f4b195413-2024-03-09.webp","permalink":"https://cuterwrite.top/en/p/intel-oneapi/","title":"Record: Install Intel® OneAPI-2024.0"},{"content":"\r#\rNote: Pure: Improve message passing to better utilize shared memory within nodes\r#\rCitation\rJames Psota and Armando Solar-Lezama. 2024. Pure: Evolving Message Passing To Better Leverage Shared Memory Within Nodes. In Proceedings of the 29th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming (PPoPP \u0026lsquo;24). Association for Computing Machinery, New York, NY, USA, 133–146. https://doi.org/10.1145/3627535.3638503\r#\rKeywords\rParallel programming model Distributed runtime system Task-based parallel model Concurrent data structures Lock-free data structure #\rSummary\rPure is a new programming model and runtime system designed to fully utilize shared memory within nodes in environments based on the Message Passing Interface (enhancing task usage to leverage idle core capabilities). Pure leverages shared memory in two ways: (1) allowing ranks to steal work from each other while waiting for messages to arrive; (2) using efficient lock-free data structures to achieve high-performance message passing and collective operations among processes within a node. Researchers evaluated Pure\u0026rsquo;s key message passing and collective features through micro benchmark tests and demonstrated that in CoMD molecular dynamics and miniAMR adaptive mesh refinement applications, Pure can achieve up to 2.1x application speedup when scaling to 4096 ranks.\n#\r1. Introduction\rIn recent decades, the field of high-performance computing has transitioned from large vector computers to clusters composed of single processors interconnected through networks. MPI has become the de facto standard for parallel programming on distributed memory systems. With advancements in hardware, the emergence of multi-core clusters has allowed cores within nodes to share memory and communicate over networks, prompting the community to continually seek new paradigms to more efficiently utilize modern cluster resources. Currently, there are two main strategies: one is to maintain a unified MPI programming approach by improving the MPI runtime system to better utilize shared memory; the other is to adopt hybrid programming models like MPI+X, using shared memory parallelism within nodes while continuing to use MPI between nodes. However, these approaches may either be limited by the MPI standard\u0026rsquo;s specifications on interface behavior, preventing performance maximization, or present challenges to programmers in managing two programming models.\nThe community has tried many other methods, including the PGAS model, which provides a shared memory abstraction across clusters, and implicitly parallel programming languages like Legion, Chapel, and X10, which offer high-level abstractions and attempt to automatically and efficiently coordinate applications. Despite some progress, many modern HPC applications still rely on MPI. MPC and AMPI also attempt to improve performance by using threads as MPI Rank to leverage internal shared memory.\nHowever, using only MPI methods often performs better than hybrid programming methods. This may be due to the limitations of the interface and the inability to fully utilize shared memory within nodes, causing MPI to fail to fully realize its potential performance. Therefore, the Pure system proposed in this article is built based on the MPI-everywhere method, breaking some traditional assumptions of MPI, more effectively utilizing shared memory, while avoiding the need for major restructuring of existing programs. Pure adopts a programming model similar to MPI, thus enabling the use of the existing MPI knowledge and application base of the HPC community.\nThe design inspiration for Pure comes from MPI, with its core programming model based on message passing, and optionally integrating task parallelism. Unlike MPI, Pure abandons the use of process-level ranks and the limitation of supporting legacy languages, opting instead to implement ranks using threads rather than traditional processes. This shift allows Pure to efficiently adopt lightweight lock-free synchronization mechanisms to coordinate between threads within the same node. Utilizing this threaded rank architecture, Pure constructs efficient intra-node collective operations and optimizes the performance of these operations through lock-free algorithms. Additionally, Pure supports running portions of parallel code blocks in the application as standard C++ lambda expressions, which can be executed automatically and concurrently by the current rank-holding thread as well as other idle ranks, all of which are automatically scheduled by the Pure Runtime system.\nThe optimization strategies proposed in the paper cover the following points:\nA lock-free messaging method suitable for the transmission of small messages and large data messages. Lock-free data structure, used for efficient implementation of collective communication algorithms. A lock-free task scheduler that allows idle threads to efficiently \u0026ldquo;steal\u0026rdquo; workloads from other threads. The author uses the standard C++ library to ensure the wide compatibility of Pure and demonstrates that Pure has a significant performance improvement compared to highly optimized MPI benchmarks. In addition, the author shows that the Pure programming model is semantically very similar to MPI, which means that migrating from existing applications to Pure is straightforward and simple, further demonstrated by the source-to-source conversion tool mpi2pure. Overall, the main contributions of the paper can be summarized as follows:\nA new programming model and runtime system have been proposed, which effectively combines message passing and task parallelism, and utilizes features of standard C++ to implement it. Demonstrates how modern C++ supports more flexible parallel runtime system interfaces. Describes a well-designed lock-free, multithreaded, and distributed runtime system that shows significant speed improvements over MPI within nodes. It has been proven that by making minimal source code modifications to existing MPI applications, significant performance improvements can be achieved in micro benchmark tests and three real-world applications compared to state-of-the-art MPI implementations. #\r2. Pure Usage Example\rThis section illustrates the use of Pure through a simple 1-D Stencil algorithm example. Although this example is simple, it clearly demonstrates the core concepts of Pure and its similarities with MPI, laying the foundation for developers to write more complex applications.\nIn the MPI version implementation code rand_stencil_mpi, the main computational work is performed in the function random_work. In simple terms, the rand_stencil_mpi function first enters a loop, iterating iters times, calculating random_work on each element of the array a. It is noteworthy that the execution time of random_work is variable and unknown, which introduces load imbalance. Moreover, random_work does not modify the contents of the array a, but instead updates the array a by averaging adjacent elements. Finally, the program uses MPI_Send and MPI_Recv to exchange the first and last elements of the temp array to compute the first and last elements of the array a. Due to the varying time required by random_work, some processing units will complete tasks early and sometimes become blocked while waiting for the slower sender in an MPI_Recv call.\nExample 1: 1-D Stencil with Random Work, MPI Version\nvoid rand_stencil_mpi(double* const a, size_t arr_sz, size_t iters, int my_rank,\rint n_ranks) {\rdouble temp[arr_sz];\rfor (auto it = 0; it \u0026lt; iters; ++it) {\rfor (auto i = 0; i \u0026lt; arr_sz; ++i) {\rtemp[i] = random_work(a[i]);\r}\rfor (auto i = 1; i \u0026lt; arr_sz - 1; ++i) {\ra[i] = (temp[i - 1] + temp[i] + temp[i + 1]) / 3.0;\r}\rif (my_rank \u0026gt; 0) {\rMPI_Send(\u0026amp;temp[0], 1, MPI_DOUBLE, my_rank - 1, 0, MPI_COMM_WORLD);\rdouble neighbor_hi_val;\rMPI_Recv(\u0026amp;neighbor_hi_val, 1, MPI_DOUBLE, my_rank - 1, 0, MPI_COMM_WORLD,\rMPI_STATUS_IGNORE);\ra[0] = (neighbor_hi_val + temp[0] + temp[1]) / 3.0;\r} // ends if not first rank\rif (my_rank \u0026lt; n_ranks - 1) {\rMPI_Send(\u0026amp;temp[arr_sz - 1], 1, MPI_DOUBLE, my_rank + 1, 0,\rMPI_COMM_WORLD);\rdouble neighbor_lo_val;\rMPI_Recv(\u0026amp;neighbor_lo_val, 1, MPI_DOUBLE, my_rank + 1, 0, MPI_COMM_WORLD,\rMPI_STATUS_IGNORE);\ra[arr_sz - 1] =\r(temp[arr_sz - 2] + temp[arr_sz - 1] + neighbor_lo_val) / 3.0;\r} // ends if not last rank\r} // ends for all iterations\r}\rExample 2 demonstrates the Pure version that achieves the same functionality. There are some key differences. First, the message calling function interface is different, using the corresponding Pure message passing functions pure_send_msg and pure_recv_msg, instead of MPI calls, but the parameters are essentially the same as the corresponding MPI functions. The message passing semantics of Pure are similar to MPI: the sender\u0026rsquo;s buffer is copied to the receiver\u0026rsquo;s buffer. The main implementation difference is that Pure uses a lightweight message passing method within the node, resulting in lower latency for message passing within the node compared to MPI.\nExample 2: Pure Version\nvoid rand_stencil_pure(double* a, const int arr_sz, const int n_iter,\rconst int my_rank, const int n_ranks) {\rdouble temp[arr_sz];\rPureTask rand_work_task = [a, temp, arr_sz, my_rank](\rchunk_id_t start_chunk, chunk_id_t end_chunk,\rstd::optinal\u0026lt;void\u0026gt; cont_params) {\rauto [min_idx, max_idx] =\rpure_aligned_idx_range\u0026lt;double\u0026gt;(arr_sz, start_chunk, end_chunk);\rfor (auto i = min_idx; i \u0026lt; max_idx; i++) {\rtemp[i] = random_work(a[i]);\r}\r}; // ends defining the Pure Task for rand_work_task\rfor (auto it = 0; it \u0026lt; n_iter; it++) {\rrand_work_task.execute(); // execute all chunks of rand_work_task\rfor (auto i = 1; i \u0026lt; arr_sz - 1; ++i) {\ra[i] = (temp[i - 1] + temp[i] + temp[i + 1]) / 3.0;\r}\rif (my_rank \u0026gt; 0) {\rpure_send_msg(\u0026amp;temp[0], 1, MPI_DOUBLE, my_rank - 1, 0, PURE_COMM_WORLD);\rdouble neighbor_hi_val;\rpure_recv_msg(\u0026amp;neighbor_hi_val, 1, MPI_DOUBLE, my_rank - 1, 0,\rPURE_COMM_WORLD);\ra[0] = (neighbor_hi_val + temp[0] + temp[1]) / 3.0;\r} // ends if not first rank\rif (my_rank \u0026lt; n_ranks - 1) {\rpure_send_msg(\u0026amp;temp[arr_sz - 1], 1, MPI_DOUBLE, my_rank + 1, 0,\rPURE_COMM_WORLD);\rdouble neighbor_lo_val;\rpure_recv_msg(\u0026amp;neighbor_lo_val, 1, MPI_DOUBLE, my_rank + 1, 0,\rPURE_COMM_WORLD);\ra[arr_sz - 1] =\r(temp[arr_sz - 2] + temp[arr_sz - 1] + neighbor_lo_val) / 3.0;\r} // ends if not last rank\r} // ends defining the Pure Task for rand_work_task\r}\rThe more important difference is the addition of Pure Task in Pure, which uses a lambda expression defined with a set of specific parameters. It leverages the capture parameter feature of lambda, allowing variables external to the lambda body to be captured by value or reference and used when the lambda is executed. Pure Task can be viewed as a snippet of application code executed by the Pure Runtime system and can be executed concurrently through multithreading. Therefore, Pure tasks should be structured in a data-parallel-like form. Additionally, Pure Task requires the programmer to ensure thread safety.\nIn the above Pure implementation, programmers can use chunk ranges to describe concurrency. These subranges or chunks are passed to the Pure Task through the start_chunk and end_chunk parameters, and they are provided by the Pure Runtime system. The Pure Runtime system is responsible for ensuring that all work is completed smoothly. Since multiple different threads may be involved, the Pure Runtime system achieves this by tracking which chunks have been allocated and completed.\nSecondly, programmers need to map the start_chunk and end_chunk parameters provided by the Pure Runtime system to specific content related to application computation. Here, the code uses the pure_aligned_idx_range helper function to convert them into loop subranges. This helper function takes cache lines into account, which helps avoid false sharing issues.\nDue to random_work potentially causing uneven load distribution, some ranks may be idle while waiting for messages. The Pure task scheduler will automatically utilize these idle ranks to execute other pending Pure task blocks within the same node. Take the example of three ranks within the same node in the diagram below: rank 0 is executing a Pure Task divided into 6 chunks, while rank 1 and rank 2 are blocked due to receiving messages.\nExample Pure code timeline diagram From the diagram, the following execution flow can be clearly seen:\nrank 0 begins processing the first chunk (chunk 0). At the same time, rank 1 steals and executes the second chunk (chunk 1) in parallel. The task scheduler then assigns the third chunk (chunk 2) to rank 0 and the fourth chunk (chunk 3) to rank 1. rank 2 attempts to steal a task and successfully executes the fifth chunk (chunk 4). Due to the randomness of random_work execution, chunk 2 and chunk 4 might be time-consuming tasks. rank 0 completed the processing of chunk 5, which is a smaller task block, and it finished before rank 2 completed chunk 4. The task scheduler ensures that rank 0 does not finish execution before all chunks are completed. In fact, rank 0 has to wait until chunk 4 is completed before it can continue. During the process of waiting for messages at rank 1 and rank 2, they will attempt to steal more chunks from any other available rank. Thanks to the variable capture feature of lambda expressions, context information can be efficiently shared between different ranks. The experimental results show that on a single node configured with 32 ranks, the Pure version achieves a 10% performance improvement compared to the MPI version due to faster message passing and parallel execution of Pure Tasks. In scenarios with uneven load distribution, the acceleration ratio of Pure even exceeds 200%. Although the degree of these performance improvements is affected by load imbalance, in practical application scenarios, Pure still demonstrates significant performance enhancements. This is attributed to the capability of the Pure Runtime system, which can automatically detect and efficiently utilize underutilized computational resources.\n#\r3. Programming Model\rThe core of Pure\u0026rsquo;s programming model is \u0026ldquo;message passing combined with optional task parallelism.\u0026rdquo; Semantically, Pure\u0026rsquo;s message passing and collective communication operations are equivalent to MPI, with differences mainly in some syntactical details.\nAlthough Pure uses threads within nodes, its rank namespace remains non-hierarchical across the entire cluster. During the execution cycle of a Pure program, the number of ranks remains unchanged.\nThe Pure application is written in C++ and runs using the SPMD (Single Program Multiple Data) model, achieving internal multithreading. On the same node, all ranks are implemented through kernel threads.\nIt is important to note that Pure applications do not support global variables. Therefore, developers should remove global variables or use the thread_local keyword to limit the scope of variables, ensuring thread safety.\nFor applications with load imbalance issues, developers can use Pure Task in parts of the program that meet the following specific conditions:\nCompute-intensive hotspot areas. Tasks that can be executed concurrently. #\rMessage passing and collective communication operations\rIn Pure, the pure_send_msg and pure_recv_msg functions correspond functionally to MPI\u0026rsquo;s MPI_Send and MPI_Recv, and Pure also provides corresponding non-blocking versions.\nPure Runtime system ensures that all messages are delivered and in the order they are sent. Pure also implements a series of collective communication operations, including:\nReduce All-Reduce Barrier Broadcast In addition, Pure introduced the concept of a communication subgroup, allowing developers to further subdivide a communication subset into smaller subsets through the pure_comm_split function.\nIn order to use Pure, the application needs to be written using modern C++ standards, and it is recommended to compile using std=c++11 or a higher version. Pure provides a Make-based build system that automatically configures appropriate compiler options and links to the Pure Runtime system (libpure), while defining a series of targets for debugging and performance analysis.\n#\rPure Task\rPure Task allows developers to define the computational parts of an application and break them down into chunks that can be executed in parallel. These chunks can be automatically executed concurrently by the Pure Runtime system.\nHowever, Pure Task is not necessary and is only recommended when a task can be divided into multiple smaller chunks and doing so helps alleviate load imbalance issues.\nPure Task is implemented through C++ Lambda expressions and synchronously executed when the execute method is called on the rank that owns the task. Each rank can only execute one Pure Task at a time. The variable capture feature of Lambda expressions allows different ranks to efficiently share context information when executing different chunks. Typically, a Pure Task is defined once during the application\u0026rsquo;s runtime and then executed multiple times at each timestep or other iterations.\nWhen defining a Pure Task, you need to specify the number of chunks and additional application parameters. Tasks should avoid interdependence; however, because they are fully executed during the execute call, they will not conflict with code outside of the tasks.\nPure Task contains an execute method, which accepts a parameter per_exe_args of type optional\u0026lt;void*\u0026gt;, used to pass additional arguments each time the task is executed. This is very useful when the input values of the task body change during consecutive executions. For example, a developer can pass a pointer to a local structure to the execute method.\nThe first two parameters of a Pure Task, start_chunk and end_chunk, are unsigned integers used to specify the range of chunks to execute. These chunks are allocated by the Pure Runtime system, ensuring that each chunk is executed only once, even if they may be executed concurrently.\nPure Task uses chunk ranges to provide flexibility to the scheduler, allowing multiple chunks to be allocated at once. The number of chunks is determined by the Pure task scheduler, but will not exceed the PURE_MAX_TASK_CHUNKS predefined in the Makefile.\nCurrently, the Pure Task interface requires manually mapping chunk numbers to array indices, which can be cumbersome when dealing with multidimensional arrays. Therefore, the future goal is to extend the interface to provide a more concise and higher-level interface similar to TBB\u0026rsquo;s parallel_for.\nFinally, developers need to ensure that the internal implementation of Pure Task is thread-safe to avoid mutual contention between chunks of the same task being executed concurrently. For example, in the CoMD molecular dynamics benchmark, the issue of multiple threads writing to the same memory location simultaneously needs to be addressed, and in such cases, an std::atomic array can be used to replace a regular int array.\n#\r4. Runtime System\rThe Pure runtime system is a dynamic library for multithreaded and distributed runtime, used to support the development of Pure applications. Developers need to include the pure.h header file when using it, compile with the C++17 standard, and link to the libpure library. The Pure runtime system can automatically find and exploit opportunities for overlapping execution between computation and communication operations, especially in cases of high communication latency.\nThe main functions of the Pure runtime system include:\nInitialize and configure the necessary processes and threads, start the application. Communication and collective operations between ranks within the management node. Manage internal memory buffers and data structures. If a Pure Task is defined in the application, the runtime system is also responsible for scheduling and executing these tasks. #\rRank initialization and mapping\rIn Pure, rank is implemented as the kernel thread of an MPI process. In multi-node applications, Pure runs MPI to handle cross-node communication, whereas in single-node applications, MPI is not used. Nonetheless, Pure applications do not directly call MPI functions. Through Makefile configuration, a Pure program can start an MPI process on a node or NUMA node and create a corresponding number of threads based on the number of cores per node or NUMA node. For application developers, they only need to understand the non-hierarchical rank namespace, while underlying concepts such as nodes, threads, MPI processes, and communication latency are abstracted and transparent to the developers.\nPure supports flexible rank-to-node mapping strategies and defaults to using an SMP-style allocation strategy. Additionally, Pure supports custom rank mapping, including the use of CrayPAT rank reordering files. While these hardware-related details are invisible to developers, Pure internally utilizes this information to optimize key functionalities.\nWhen a Pure application starts, the original main function of the application is not executed directly. Instead, the underlying MPI program calls the main function defined in the Pure runtime system, which is responsible for initializing Pure\u0026rsquo;s core data structures. It then creates and binds threads, each executing an original_main function, which is a renamed version of the original main function from the application code. After the application completes execution, the original_main function returns to the Pure runtime system, which is responsible for completing the MPI cleanup and termination process.\n#\rSpin-Steal Waiting Loop (SSW-Loop)\rWhen a Pure rank encounters a blocking event, such as waiting for a message to arrive, it will execute a mechanism called the Spin, Steal, Wait Loop (SSW-Loop) instead of simply entering an idle state. In this loop, the rank checks if the blocking condition is met, such as whether a message has arrived, and if not, it will attempt to steal tasks from other ranks. If a blocking rank can assist in completing tasks that other threads in its process are concurrently executing, it will participate in such assistance work.\nSince threads are bound to specific CPUs and each rank runs only one application, Pure chooses to have ranks actively spin-wait rather than relinquish the CPU. The SSW-Loop gives ranks in computation \u0026ldquo;polymorphism\u0026rdquo;: they can act as computing nodes for the main program, assist other ranks in executing stolen task blocks, and then return to check their own blocking events.\nPure follows the strategy of prioritizing the stolen workload of the current rank and adheres to the scheduling principle of workload priority.\nUnlike systems that use auxiliary threads to achieve workload stealing or communication, Pure is characterized by allowing application-level compute nodes to directly perform task stealing operations.\n#\rImplementation Instructions\rPure is written using the C++17 standard library. The Pure runtime system consists of about 21,000 lines of source code, and the Pure tools contain about 14,000 lines of source code. Pure has been tested in various environments, including laptops and clusters, and only requires a C++17 supporting compiler, a Unix-like operating system, and an MPI environment to run. The source code for Pure is publicly available on GitHub at the following link: https://github.com/psota/pure\r.\n#\rPoint-to-point communication\rPure provides blocking and non-blocking point-to-point message passing functionality, consistent with the message passing semantics of MPI.\nPure internally uses three different strategies for message passing, and the choice of strategy depends on the size of the message and whether the sender and receiver are located on the same node.\nPure allocates and reuses a persistent Channel object throughout the program\u0026rsquo;s lifecycle, which is stored in the runtime system. The internal Channel Manager is responsible for mapping message parameters to the appropriate data structures and creating these structures as needed.\nPure Message Passing Strategy Short message (less than 8KB): Use a lock-free circular queue (PureBufferQueue, PBQ), with acquire-release memory semantics. The sending thread copies the message to the PBQ when there is available space, and the receiving thread retrieves it when the message is ready. In short message passing, the overhead of copying is relatively small, allowing the sender to immediately perform other useful work after the call returns. Both sending and receiving threads use SSW-Loop to wait in order to achieve as much overlap of computation and communication as possible. The slots for all messages are stored in a contiguous buffer, with pointer arithmetic ensuring that each slot is aligned with cache line boundaries, avoiding false sharing between sending and receiving threads. Big Message (greater than or equal to 8KB): A strategy similar to PBQ, but using a single memory copy directly from sender to receiver, inspired by the rendezvous mode of MPI. Use a lock-free fixed-size circular buffer to store the receiver\u0026rsquo;s receive call parameters. The sender waits for a metadata queue item via SSW-Loop and then copies the message content directly to the receiver\u0026rsquo;s buffer. The sender notifies the receiver that the transfer is complete by inserting the number of bytes transferred into the lock-free queue. Cross-node message Transparently use the MPI interface for message passing. During Pure initialization, use a distributed consensus algorithm to create a thread-rank-process-node mapping data structure that maps Pure rank to MPI rank. To ensure that the correct receiving thread on the receiving node can receive the message, encode the sending and receiving thread IDs in MPI_TAG to solve the multithreading routing problem. #\rCollective communication\rThe collective communication operations of Pure are semantically the same as MPI, but they are implemented within nodes using a bottom-up constructed data structure. This has shown significant performance improvements in both single-node and multi-node benchmarks, even though it still relies on MPI\u0026rsquo;s collective operations for cross-node communication.\nPure uses a leader thread to coordinate the collective communication process, while other threads assist with computation and call MPI collective functions as needed.\nPure uses a static leader election method, which is more efficient than the compare-and-swap based \u0026ldquo;first-come\u0026rdquo; method. The following is an example using All-Reduce, other collective communication operations have similar concepts.\nFor small data All-Reduce operations, Pure designed a concurrent data structure called Sequenced Per-Thread Dropbox (SPTD), providing an efficient lock-free mechanism for pairwise synchronization and optionally sharing data between leader threads and other non-leader threads.\nSequenced Per-Thread Dropbox (SPTD) This method draws on the flat-combinding technique, using thread 0 in the communicator as the leader thread.\nFor small arrays not exceeding 2KB: Non-leader threads first copy data to SPTD, then synchronize with the leader thread, indicating that the input data is ready (using atomic sequence numbers instead of a shared atomic counter). The leader thread performs an element-wise Reduce operation on all input arrays. Each node\u0026rsquo;s leader thread uses MPI_Allreduce to perform a global Reduce on local Reduce results. Leader thread synchronization, non-leader threads copy the final Reduce result to a private buffer. All threads execute the SSW-Loop while waiting. For large arrays exceeding 2KB, Reduce computation may become a performance bottleneck. Therefore, it is necessary for all threads to execute Reduce computation concurrently and read from or write to data directly from each thread\u0026rsquo;s buffer through shared memory. Reduce work is divided into equally sized blocks to avoid false sharing and enable vectorized computation. Threads report readiness status using SPTD and mark computation completion with atomic sequence numbers. The leader thread calls MPI_Allreduce to perform an All-Reduce operation across nodes and propagates the final result through another atomic sequence number. #\rTask Scheduler\rThe Pure runtime system has meticulously designed a task scheduler that maintains an array named active_tasks in shared memory. This array stores a series of atomic pointers, each corresponding to a task being executed, and allocates an entry for each node and each rank in the system. These entries are initially set to nullptr, indicating that a task has not yet been assigned.\nWhen a task is created and prepared for execution, the system initializes its state and updates the corresponding entry in the active_tasks array through an atomic operation to reflect that the task has been assigned. This update process ensures that the execution state of the task is visible to all threads in the system, allowing the task to be \u0026ldquo;stolen\u0026rdquo; by other threads.\nDuring the execution of the task, the rank owning the task will begin executing a series of chunks, which are the subdivided work units of the task. Meanwhile, other threads will continuously check the active_tasks array during their SSL-Loop, using atomic load operations to look for executable non-empty tasks.\nThe execution of the task is coordinated by two atomic integers, curr_chunk and chunks_done. The owner rank of the task and the possible thief ranks will both run the same concurrent execution function. The thief threads will return after executing one chunk, while the owner thread will continue executing until all chunks are completed. By using the fetch_add operation, a thread can determine which chunk it should execute. If the value of curr_chunk has already exceeded the total number of chunks, the thread will stop executing.\nEach time a chunk is successfully completed, the thread atomically increments the value of chunks_done. The owner thread updates its local storage to avoid cache misses. Finally, the owner rank will wait until all chunks are executed, ensuring the complete execution of the task.\nIt is worth noting that the task\u0026rsquo;s chunk and the application\u0026rsquo;s rank are executed on the same hardware thread. In Pure applications, each hardware thread is assigned to a specific rank. Although currently Pure does not utilize hardware accelerators (such as GPUs) to accelerate task execution, the designers believe that Pure\u0026rsquo;s architecture is fully capable of supporting such acceleration.\nThe Pure task scheduler provides various execution modes and stealing algorithms to accommodate different execution needs. For example, the author implemented a single chunk execution mode and a guided self-scheduling mode, the latter being a work partitioning algorithm that prioritizes the allocation of larger work chunks followed by smaller ones. Additionally, the scheduler includes a NUMA-aware stealing mode, which prioritizes stealing tasks from threads on the same NUMA node, and a \u0026ldquo;sticky\u0026rdquo; stealing mode, allowing thief threads to return to tasks they recently stole that are still active. These features collectively ensure the efficiency and flexibility of task scheduling.\n#\rEvaluation\rThe performance evaluation of Pure was conducted on the Cori HPC cluster at Berkeley NERSC. This cluster consists of 2388 nodes, each configured with 2 sockets, 16 cores, and 128GB of memory, interconnected via Cray Aires. The experimental configuration enabled hyper-threading and adopted a 256-bit vector width. Two processes were run on each node, totaling 32 threads. The evaluation used the Intel compiler and Cray MPICH as the performance baseline.\n#\rNAS DT benchmark results\rBy optimizing the message-passing mechanism alone, Pure achieved a performance boost of 11% to 25%. After introducing Pure Tasks, the performance acceleration ratio increased to 1.7 times to 2.6 times. Auxiliary threads can slightly improve performance, but only if there are remaining unused CPU cores available. Here, except in the case of 80 ranks where 24 cores were idle, the CPU cores were fully utilized in other cases. #\rCoMD and miniAMR benchmarks\r-In the CoMD molecular dynamics application, Pure outperforms using only MPI and MPI+OpenMP in terms of performance across all ranks, achieving speedups of 7% to 25% and 35% to 50% respectively, even in the absence of load imbalance.\nIn the miniAMR adaptive mesh refinement application, Pure achieved at least a 20% and at most a 50% performance acceleration. #\rCollective communication performance\rPure exhibits outstanding performance in collective communication operations, where its internal optimization mechanisms and data structure design allow Pure to demonstrate significant efficiency and advantages when handling large-scale parallel computing tasks. #\rRelated work\rCategory Related Work Advantages of Pure MPI 1. Utilize shared memory within multi-core nodes to enhance performance; 2. XPMEM significantly enhances intra-node communication efficiency; 3. ch4 network library optimizes MPI shared memory communication; 4. Improved MPI collective communication algorithms; 5. DMAPP library is optimized for specific collective communications, but with many limitations; 6. Addressed the challenges of large-scale all-to-all collective communications; 7. One-sided message API achieves decoupling; 8. Optimized data movement and process synchronization 1. Pure demonstrates excellent performance across all collective communications and load sizes; 2. Provides advanced communication-computation overlap mechanisms, surpassing traditional one-sided message API MPI Multithreading 1. Supports multithreading within ranks in MPI_THREAD_MULTIPLE mode; 2. Most MPI implementations achieve thread safety through global locks, leading to performance bottlenecks; 3. MPI 4.0 introduces MPI+X method to enhance multithreading support; 4. Introduces the concepts of MPI Fine-points and Endpoints to support threading 1. Pure emphasizes the importance of MPI calls in multithreaded code; 2. Provides a unified programming model to simplify the introduction of parallel tasks AMPI 1. MPI compatible library based on Charm++; 2. Provides advanced parallel programming abstractions; 3. Achieves performance improvement with minimal code changes 1. Pure outperforms AMIP in practical tests due to its optimized message passing and collective communication, as well as more refined and low-overhead load balancing strategies; 2. Compared to the thread-based model of AMIP SMP, Pure offers more efficient parallel processing PGAS language and parallel frameworks 1. PGAS language provides an abstraction of global memory address space; 2. Chapel and X10 extend the PGAS approach, supporting local and remote asynchronous tasks; 3. HPX adds distributed operation support to the modern C++ standard; 4. Legion as a data center parallel programming system; 5. Frameworks like Kokkos, STAPL, BCL provide an abstraction layer between applications and hardware 1. Similar to Pure, the PGAS model adopts the SPMD style to improve performance through locality reference; 2. Although these frameworks utilize modern C++ features, they usually require extensive rewriting of existing applications, whereas Pure offers a more direct optimization path #\rSummary\rFor decades, message passing has been regarded as the standard model for parallel programming due to its relative simplicity and performance advantages. However, this paper demonstrates that message passing and shared memory are not incompatible. In fact, by designing appropriate libraries, shared memory can be fully utilized without sacrificing most of the advantages of message passing.\n","date":"2024-03-03T01:16:00Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/crop_e9af4c445d695be5002248c7c814c67d195413-2024-03-04.webp","permalink":"https://cuterwrite.top/en/p/pure/","title":"Notes: Pure - Improving Message Passing to Better Utilize Intra-Node Shared Memory"},{"content":"\r#\rScientific chart plotting\r#\rPrerequisite knowledge\r#\rBitmap\rAlso known as bitmap image, pixel image, or raster image, it is composed of pixels. These points can be arranged and colored differently to form an image.\nBitmap Characteristics:\nBitmap images are good at reproducing subtle gradations of color, capable of creating images with rich variations in color and brightness, realistic colors, large files, and cannot be scaled arbitrarily; The larger the image size, the larger the file; the richer the image color, the larger the file. The precision of printing and output is limited; Bitmap file formats: such as .tiff, .bmp, .gif, .jpg, .png, .psd, etc. Common bitmap editing software: Photoshop, etc. #\rVector graphics\rVector is also known as \u0026ldquo;vector,\u0026rdquo; and the graphic elements (points and line segments) in vector images are called objects. Each object is an individual entity with attributes such as size, direction, outline, color, and screen position. Simply put, vector graphic software uses mathematical methods to draw basic shapes like rectangles.\nVector Graphic Features:\nCan be infinitely enlarged without worrying about distortion; Vector graphics can be easily converted to bitmap, while converting bitmap to vector graphics requires methods like image tracing, but perfectly converting to vector graphics is still somewhat challenging. Vector graphic file formats: Such as Adobe Illustrator\u0026rsquo;s .AI, .EPS, .SVG, .PDF, AutoCAD\u0026rsquo;s .dwg and .dxf, Windows standard metafile *.wmf and enhanced metafile *.emf, etc. Common vector graphics editing software: Illustrator, CorelDraw, AutoCAD, etc. #\rThe relationship between pixels, DPI, and print size\rThe mathematical relationship between image resolution, number of pixels, and print size is: Pixels = Resolution (DPI) × Print Size (in inches).\nAmong them, DPI is the number of pixels per square inch, which is a measure of the image\u0026rsquo;s level of detail. Understanding the above concepts, we can infer the image size using these concepts. For example, if I want to print an 8-inch * 10-inch, 300 DPI image, how should I set the pixel width and height of the image? You simply multiply the two together, $8 \\times 300=2400$, $10 \\times 300=3000$, so the pixel dimensions of this image are $2400 \\times 3000$.\n#\rMagazine requirements\rHere, taking the example of the requirements from the well-known publisher Elsevier\r:\nTARGET SIZE Image Width Pixels at 300 dpi Pixels at 500 dpi Pixels at 1000 dpi Minimal size 30 mm (85 pt) 354 591 1181 Single column 90 mm (255 pt) 1063 1772 3543 1.5 column 140 mm (397 pt) 1654 2756 5512 Double column (full width) 190 mm (539 pt) 2244 3740 7480 By studying the content on image dimensions above, we can understand the relationship between print size, pixels, and dpi. For example, the table shows that the minimum image size required in red is $30 \\mathrm{mm}$. We can use the formula to verify whether the printed size of 354 pixels wide at 300dpi resolution is indeed $30 \\mathrm{mm}$: $354 \\div 300 \\times 2.54 \\times 10 = 29.97 \\mathrm{mm}$, the two numbers multiplied at the end convert inches to millimeters, which is exactly $30 \\mathrm{mm}$. Knowing the above relationship, we can use Photoshop to edit our images.\nFor example, an image from Mapman, opened with Photoshop, shows the following dimensions:\nDue to the image size being too large, with a width of $124.99 \\mathrm{cm}$, and a resolution of $72$, it does not meet the magazine requirements. Here, we will use the knowledge learned above to adjust the image size without losing image pixels;\nNow we need to adjust the image width to a double-column size, which is $19\\mathrm{cm}$; using the formula: pixels = resolution (DPI) × print size (in inches)\nWith the pixels unchanged, we need to increase the resolution to reduce the print size of the image. According to the ratio calculation, it should be increased to how much dpi: $124.99 \\div 19 \\times 72=473.6 \\mathrm{dpi}$;\nSo, modifying these two values of width and resolution is sufficient, and the number of pixels in the image remains unchanged, achieving a lossless change in the image size; moreover, 473dpi is greater than the minimum 300dpi.\n#\rMatplotlib Python library\rAs the most fundamental and widely used data visualization library in the Python ecosystem, Matplotlib offers rich 2D and 3D plotting capabilities, particularly suitable for creating common scientific charts such as line charts, bar charts, and scatter plots. It also allows for highly customizable output styles to meet the standards of various academic journals.\nIt can be used to draw various static, dynamic, and interactive charts. We can use this tool to present a lot of data more intuitively in the form of charts, including drawing line charts, scatter plots, contour plots, bar charts, histograms, 3D graphics, and even graphic animations, etc.\nMatplotlib Cheat Sheet #\rSeaborn Python library\rBuilt on top of Matplotlib, Seaborn further enhances the functionality of statistical charts. It comes with many advanced statistical chart styles, such as heatmaps, box plots, and time series analysis charts, making the presentation of complex data relationships more intuitive and readable. Since it is based on Matplotlib, many of Seaborn\u0026rsquo;s chart interfaces and parameter settings are quite similar to it, making plotting more convenient and fast. Even those without much foundation can create aesthetically pleasing graphics with analytical value through minimal code.\nSeaborn Cheat Sheet Excellent tutorial: Data Visualization, Seaborn makes plotting so beautiful\r#\rVisio vector graphics software (framework flowcharting and algorithm structure)\rFor chart designs that are not data-intensive but have rigorous logic, such as experimental flowcharts, system architecture diagrams, or algorithm flowcharts, Microsoft Visio, with its powerful vector editing capabilities and vast array of preset templates, has become the ideal choice for constructing clear and standardized flowcharts.\n#\rOrigin Vector Graphics Software (Mathematical Analysis and Function Plotting)\rOrigin is a scientific graphing and data analysis software developed by OriginLab Corporation, supporting operation under Microsoft Windows. Origin supports a variety of 2D/3D graphics. The data analysis features in Origin include statistics, signal processing, curve fitting, and peak analysis. Curve fitting in Origin uses the nonlinear least squares fitting based on the Levenberg-Marquardt algorithm (LMA). Origin\u0026rsquo;s powerful data import capabilities support various data formats, including ASCII, Excel, NI TDM, DIADem, NetCDF, SPC, and more. The graphic output formats are diverse, such as JPEG, GIF, EPS, TIFF, etc. The built-in query tool can access database data through ADO.\nIn the fields of physics, chemistry, and biology, Origin is renowned for its powerful mathematical analysis and function plotting capabilities, specifically designed for scientific data analysis. It is particularly suitable for plotting precise signal curves, spectrum analysis graphs, and other complex scientific graphics.\n#\rAI (Adobe Illustrator) vector graphics software\rAs an industry-standard vector graphics processing software, Illustrator is not only suitable for high-precision, publication-grade chart design but also capable of creating high-quality scientific illustrations, ensuring clear and delicate effects at any size. It is an industry-standard vector illustration software used in publishing, multimedia, and online images. The software is mainly used in print publishing, poster and book layout, professional illustration, multimedia image processing, and web page production. It can also provide high precision and control for line art, suitable for producing any small design to large and complex projects.\nIn chart plotting, it is mainly applied in: direct plotting - rarely used in computer science and control classes, but students in the biochemical, environmental, and material fields use AI to achieve operations like cell structure and ventricle highlighting; integrating previously exported individual vector graphics; converting non-vector graphics to vector graphics.\n#\rInkscape vector graphics software\rAI\u0026rsquo;s alternative version, with the advantage of being open source and free. As a leader in the open-source vector graphics editor community, Inkscape offers a complete set of SVG editing tools, allowing researchers to use it for free to create complex vector diagrams, ensuring cross-platform compatibility and lossless scalability. Official Chinese site: Inkscape\rDetailed introduction: Inkscape - Free open-source, cross-platform vector graphics design software, alternative to Adobe Illustrator (AI) and CorelDRAW\rRecommended video tutorial:\n","date":"2024-02-27T00:14:00Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/crop_ee40c9cb9e33ffe888365e66e0a104dc195413-2024-02-28.webp","permalink":"https://cuterwrite.top/en/p/science-plot/","title":"Scientific Research Chart Drawing"},{"content":"\r#\rRDMA Basic Service Types\rThis article welcomes non-commercial reprints, please indicate the source when reprinting.\nStatement: For collection only, for convenient reading\n― Savir, Zhihu Column: 5. Basic RDMA Service Types In the article 【“3. RDMA Basic Elements”】\r, we mentioned that the basic communication unit of RDMA is QP, and there are many communication models based on QP, which we refer to as \u0026ldquo;service types\u0026rdquo; in the field of RDMA. The IB protocol describes a service type through two dimensions: \u0026ldquo;reliable\u0026rdquo; and \u0026ldquo;connected\u0026rdquo;.\n#\rReliable\rReliability in communication refers to ensuring that the sent data packets can be properly received through some mechanisms. In the IB protocol, reliable service is described as follows:\nReliable Service provides a guarantee that messages are delivered from a requester to a responder at most once, in order and without corruption.\n\u0026ldquo;Reliable service ensures that information is transmitted at most once between the sender and receiver, and it can guarantee that it is completely received in the order it was sent.\u0026rdquo;\nIB ensures reliability through the following three mechanisms:\n#\rResponse mechanism\rSuppose A sends a data packet to B, how can A know that B has received it? Naturally, B replies with a \u0026ldquo;I have received it\u0026rdquo; message to A. In the field of communications, we generally refer to this reply as an acknowledgment packet or ACK. In the reliable service type of the IB protocol, an acknowledgment mechanism is used to ensure that the data packet is received by the other party. In the reliable service type of IB, the receiver does not have to reply to every packet; it can also reply with an ACK for multiple packets at once. We will discuss this further later.\n#\rData validation mechanism\rThis is relatively easy to understand. The sender will use a certain algorithm to obtain a checksum for the Header and Payload (the actual data to be sent and received) and place it at the end of the data packet. When the receiving end receives the data packet, it will also use the same algorithm to calculate the checksum and then compare it with the checksum in the data packet. If they do not match, it indicates that the data contains errors (usually caused by link issues), and the receiving end will discard this data packet. The IB protocol uses CRC for checksum, and this article does not provide an in-depth introduction to CRC.\n#\rOrder-preserving mechanism\rIn-order delivery refers to ensuring that data packets sent first over the physical link are received by the recipient before later sent packets. Some services have strict requirements on the order of data packets, such as voice or video. The IB protocol includes the concept of PSN (Packet Sequence Number), meaning each packet has an incrementing number. PSN can be used to detect packet loss; for example, if the receiver gets 1 but receives 3 without having received 2, it will consider an error occurred during transmission and will send a NAK back to the sender, requesting the retransmission of the lost packet.\nUnreliable service, without the above mechanisms to ensure that packets are received correctly, belongs to the type of service that is \u0026ldquo;just send it out, I don\u0026rsquo;t care if it is received or not.\u0026rdquo;\n#\rConnection and Datagram\rConnection here refers to an abstract logical concept, which needs to be distinguished from a physical connection. Readers familiar with Sockets will certainly not be unfamiliar with this. A connection is a communication \u0026ldquo;pipeline.\u0026rdquo; Once the pipeline is established, the data sent from this end of the pipeline will definitely reach the other end along this pipeline.\nThere are many definitions for \u0026ldquo;connection\u0026rdquo; or \u0026ldquo;connection-oriented\u0026rdquo;, some focus on ensuring the order of messages, some emphasize the uniqueness of the message delivery path, some highlight the need for software and hardware overhead to maintain the connection, and some overlap with the concept of reliability. Since this column is about introducing RDMA technology, let\u0026rsquo;s take a look at its description in section 3.2.2 of the IB protocol:\nIBA supports both connection-oriented and datagram service. For connected service, each QP is associated with exactly one remote consumer. In this case, the QP context is configured with the identity of the remote consumer’s queue pair. \u0026hellip; During the communication establishment process, this and other information is exchanged between the two nodes.\nThat is, \u0026ldquo;IBA supports both connection-oriented and datagram-based services. For connection-oriented services, each QP is associated with another remote node. In this case, the QP Context contains the QP information of the remote node. During the process of establishing communication, the two nodes exchange peer information, including the QP that will be used for communication later.\u0026rdquo;\nIn the description above, Context is generally translated as 上下文. QP Context (abbreviated as QPC) can be simply understood as a table that records information related to a QP. We know that QP consists of two queues, and in addition to these two queues, we also need to record information about the QP in a table. This information may include the depth of the queues, the queue numbers, etc. We will elaborate on this later.\nIt might still be a bit abstract, let\u0026rsquo;s use a diagram to explain:\nThe network cards of nodes A, B, and A, C are physically connected. A\u0026rsquo;s QP2 and B\u0026rsquo;s QP7, A\u0026rsquo;s QP4 and B\u0026rsquo;s QP2 have established a logical connection, or are \u0026ldquo;bound together.\u0026rdquo; In the connection service type, each QP is connected to a unique other QP, meaning that the destination of each WQE issued by the QP is unique. For example, for each WQE issued by A\u0026rsquo;s QP2, the hardware can know through QPC that its destination is B\u0026rsquo;s QP7, and will send the assembled packet to B. Then B will store the data according to the RQ WQE issued by QP7; similarly, for each WQE issued by A\u0026rsquo;s QP4, A\u0026rsquo;s hardware knows that the data should be sent to Node C\u0026rsquo;s QP2.\nHow is a \u0026ldquo;connection\u0026rdquo; maintained? Actually, it\u0026rsquo;s just a record inside the QPC. If A\u0026rsquo;s QP2 wants to disconnect from B\u0026rsquo;s QP7 and then \u0026ldquo;connect\u0026rdquo; with another QP, it only needs to modify the QPC. During the process of establishing a connection between two nodes, they exchange the QP Number that will be used later for data interaction, and then record it in the QPC respectively.\nDatagram Contrary to connection, there is no need for a \u0026ldquo;pipeline establishment\u0026rdquo; step between the sender and receiver. As long as the sender can physically reach the receiver, it is possible to send to any receiving node from any path. The IB protocol defines it as follows:\nFor datagram service, a QP is not tied to a single remote consumer, but rather information in the WQE identifies the destination. A communication setup process similar to the connection setup process needs to occur with each destination to exchange that information.\n\u0026ldquo;For datagram services, a QP will not be bound to a unique remote node but will specify the destination node through a WQE. Similar to connection-type services, the process of establishing communication requires both ends to exchange peer information, but for datagram services, this exchange process needs to be executed once for each destination node.\u0026rdquo;\nLet\u0026rsquo;s take an example:\nIn the context of a datagram-type QP, it does not contain peer information, meaning each QP is not bound to another QP. Each WQE issued to the hardware by the QP may point to a different destination. For example, the first WQE issued by QP2 of node A instructs to send data to QP3 of node C; while the next WQE may instruct the hardware to send to QP7 of node B.\nLike the connection service type, which remote QP the local QP can send data to is mutually informed in advance during the preparation stage through certain means. This is also the meaning of the above statement \u0026ldquo;the datagram service needs to perform this exchange process once for each destination node.\u0026rdquo;\n#\rService type\rThe two dimensions mentioned above combine in pairs to form the four basic service types of IB:\nReliable Unreliable Connection RC (Reliable Connection) UC (Unreliable Connection) Datagram RD (Reliable Datagram) UD (Unreliable Datagram) RC and UD are the most applied and fundamental types of services, and we can analogize them to the TCP and UDP of the TCP/IP protocol stack\u0026rsquo;s transport layer, respectively.\nRC is used in scenarios with high requirements for data integrity and reliability, similar to TCP, because various mechanisms are needed to ensure reliability, so the overhead will naturally be higher. Additionally, since RC service types and each node need to maintain their own QP, assuming there are N nodes that need to communicate with each other, at least N * (N - 1) QPs are required. QP and QPC themselves need to occupy network card resources or memory, and when there are many nodes, the consumption of storage resources will be very large.\nUD hardware overhead is small and saves storage resources. For example, if N nodes need to communicate with each other, only N QPs need to be created. However, reliability cannot be guaranteed, just like UDP. If users want to implement reliability based on the UD service type, they need to implement an application-layer reliable transmission mechanism based on the IB transport layer themselves.\nIn addition, there are RD and UC types, as well as more complex service types like XRC (Extended Reliable Connection) and SRD (Scalable Reliable Datagram), which we will describe in detail in the protocol analysis section.\nFor more information on QP type selection, you can refer to the article Which Queue Pair type to use?\ron RDMAmojo. Thanks to @sinkinben\rfor pointing it out in the comments section.\n#\rCode example\rIn RDMA programming, we can create a QP using the ibv_create_qp function, where the qp_type field in the struct ibv_qp_init_attr structure is used to specify the service type of the QP. Below is a simple example code:\nstruct ibv_qp_init_attr qp_init_attr;\rqp_init_attr.qp_type = IBV_QPT_RC; // RC type\rqp_init_attr.sq_sig_all = 1; // 1 means each WQE in SQ needs a corresponding CQE\rqp_init_attr.send_cq = cq; // Send CQ\rqp_init_attr.recv_cq = cq; // Receive CQ\rqp_init_attr.cap.max_send_wr = 1024; // Depth of SQ\rstruct ibv_qp *qp = ibv_create_qp(pd, \u0026amp;qp_init_attr);\r","date":"2024-02-25T22:04:01Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/f71da3ec40dd74648e15471d47ba3b84195413_crop-2024-02-26.webp","permalink":"https://cuterwrite.top/en/p/rdma-service-types/","title":"RDMA Basic Service Types"},{"content":"\r#\rRDMA operation type\rThis article welcomes non-commercial reprints, please indicate the source when reprinting.\nStatement: For collection purposes only, for easier reading\n― Savir, Zhihu Column: 4. RDMA Operation Types In previous articles discussing RDMA communication processes, SEND-RECV has been mentioned frequently. However, it cannot really be called \u0026ldquo;RDMA\u0026rdquo;; it is merely an \u0026ldquo;upgraded version\u0026rdquo; of the traditional send-receive model with added zero-copy and protocol stack offloading. This type of operation does not fully leverage the entire capabilities of RDMA technology and is often used in scenarios where control information is exchanged between two ends. When it comes to sending and receiving large amounts of data, two RDMA-exclusive operations are more commonly used: WRITE and READ.\nLet\u0026rsquo;s first review the dual-end operations—SEND and RECV, and then introduce and compare the single-end operations—WRITE and READ.\n#\rSEND \u0026amp; RECV\rSEND and RECV are two different types of operations, but because if one end performs a SEND operation, the opposite end must perform a RECV operation, they are usually described together.\nWhy is it called \u0026ldquo;dual-end operation\u0026rdquo;? Because completing a communication process requires the participation of both ends\u0026rsquo; CPUs, and the receiving end needs to explicitly issue a WQE in advance. The diagram below is a schematic of a SEND-RECV operation process. The original image is from [1], and I have made some modifications.\nIn the previous section, we discussed how upper-layer applications issue tasks to hardware through WQE (WR). In SEND-RECV operations, not only does the sender need to issue WQE, but the receiver also needs to issue WQE to inform the hardware where to place the received data. The sender does not know where the sent data will be placed, so each time data is sent, the receiver must prepare the receive buffer in advance, and the receiver\u0026rsquo;s CPU will naturally be aware of this process.\nIn order to compare the similarities and differences between SEND/RECV and WRITE/READ in the following text, we will supplement the memory read and write process in the SEND-RECV flow from the previous article, namely steps ④ in the diagram below — the sending-end hardware retrieves data from memory based on the WQE and encapsulates it into packets that can be transmitted on the link, and step ⑦ — the receiving-end hardware parses the packets and places the data into the specified memory area based on the WQE. Other steps will not be elaborated. Additionally, it is emphasized once again that the order of steps on the sending and receiving ends may not necessarily follow the order in the diagram. For example, the order of steps ⑧⑪⑫ and steps ⑨⑩ is not fixed.\nThe following will introduce the WRITE operation, and I believe everyone can understand it better after comparison.\n#\rWRITE\rWRITE is the full name of the RDMA WRITE operation, which is the behavior of actively writing to remote memory from the local end. Except for the preparation phase, the remote CPU does not need to participate and is not aware of when data is written or when the data reception is complete. Therefore, this is a single-end operation.\nThrough the diagram below, we compare the differences between WRITE and SEND-RECV operations. In the preparation phase, this end obtains the address and \u0026ldquo;key\u0026rdquo; of a certain piece of available memory on the opposite end through data exchange, which is equivalent to obtaining read and write permissions for this piece of remote memory. After obtaining the permissions, this end can directly read and write to this remote memory area as if accessing its own memory, which is the essence of RDMA—Remote Direct Memory Access.\nHow are the destination address and key obtained in WRITE/READ operations? They can usually be obtained through the SEND-RECV operation we just discussed, because obtaining the key is ultimately allowed by the controller of the remote memory—the CPU. Although the preparation work is relatively complex, once the preparation is completed, RDMA can leverage its advantages to read and write large amounts of data. Once the remote CPU authorizes the memory for local use, it will no longer participate in the data transmission process, which frees the remote CPU and reduces communication latency.\nIt should be noted that this end reads and writes remote memory through virtual addresses, making it very convenient for upper-layer applications to operate on it. The actual conversion of virtual addresses to physical addresses is performed by the RDMA network card. How this conversion is done will be introduced in subsequent articles.\nIgnoring the process of obtaining the key and addr during the preparation phase, we describe the process of a WRITE operation below. From now on, we will no longer refer to the local and remote ends as \u0026ldquo;sending\u0026rdquo; and \u0026ldquo;receiving\u0026rdquo; ends, but rather as \u0026ldquo;request\u0026rdquo; and \u0026ldquo;response\u0026rdquo; ends. This makes it more appropriate for describing both WRITE and READ operations and avoids ambiguity.\nThe requesting APP issues a WRITE task in the form of WQE (WR). Requesting hardware to fetch WQE from SQ and parse the information. The requester\u0026rsquo;s network card converts the virtual address in the WQE to obtain the physical address, then retrieves the data to be sent from memory, and assembles the data packet. The requesting end\u0026rsquo;s network card sends the data packet to the responding end\u0026rsquo;s network card through the physical link. The receiving end receives the data packet, parses the destination virtual address, converts it to a local physical address, parses the data, and places the data in the specified memory area. The responding end replies with an ACK message to the requesting end. After the network card on the requesting side receives the ACK, it generates a CQE and places it into the CQ. The requesting app obtains task completion information. Note: Strictly speaking, at step 6 when ACK is replied, the RDMA network card can only guarantee that the Payload in the packet has been \u0026ldquo;temporarily stored,\u0026rdquo; but it cannot guarantee that the data has been placed in the destination memory. However, this does not affect our understanding of the sorting process. Thanks to @nekomii for the reminder.\nIB Spec. 9.7.5.1.6 ACKNOWLEDGE MESSAGE SCHEDULING Original text: \u0026ldquo;For SEND or RDMA WRITE requests, an ACK may be scheduled before data is actually written into the responder’s memory. The ACK simply indicates that the data has successfully reached the fault domain of the responding node. That is, the data has been received by the channel adapter and the channel adapter will write that data to the memory system of the responding node, or the responding application will at least be informed of the failure.\u0026rdquo;\n#\rREAD\rAs the name suggests, READ and WRITE are opposite processes, with READ being the local end actively reading the remote memory. Like WRITE, the remote CPU does not need to participate and is not aware of the process of data being read from memory.\nThe process of obtaining the key and virtual address is no different from WRITE. It is important to note that the data requested by the \u0026ldquo;read\u0026rdquo; action is carried in the message replied by the other end.\nThe following describes the process of a READ operation, note that it only differs from WRITE in direction and order of steps.\nThe requesting end APP issues a READ task in the form of WQE. The requesting network card retrieves the WQE from the SQ and parses the information. The requesting network card sends the READ request packet to the responding network card through the physical link. The receiving end receives the data packet, parses the destination virtual address, converts it into a local physical address, parses the data, and retrieves the data from the specified memory area. The receiving end hardware assembles the data into a reply packet and sends it to the physical link. The requesting hardware receives the data packet, parses and extracts the data, and then places it in the memory area specified by the READ WQE. The requester\u0026rsquo;s network card generates a CQE and places it in the CQ. The requesting APP obtains task completion information. #\rSummary\rWe abstract by ignoring various details, and RDMA WRITE and READ operations are simply utilizing the network card to complete the memory copy operation shown in the left diagram below. The copying process is completed by the RDMA network card through the network link; whereas local memory copying, as shown in the right diagram below, is completed by the CPU through the bus:\nThe words used in the RDMA standard to define the aforementioned operations are very appropriate, \u0026ldquo;receive\u0026rdquo; and \u0026ldquo;send\u0026rdquo; have the semantics of requiring active participation from the peer, while \u0026ldquo;read\u0026rdquo; and \u0026ldquo;write\u0026rdquo; are more like the semantics of the local end operating on a passive peer.\nBy comparing SEND/RECV and WRITE/READ operations, we can find that WRITE/READ, which does not require the responding end\u0026rsquo;s CPU involvement during data transmission, has greater advantages. The drawback is that the requesting end needs to obtain read and write permissions for a section of memory on the responding end during the preparation phase. However, during actual data transmission, the power and time consumption of this preparation phase can be negligible. Therefore, RDMA WRITE/READ is the type of operation used for large data transfers, while SEND/RECV is usually used to transmit some control information.\nIn addition to the types of operations introduced in this article, there are more complex types of operations such as ATOMIC, which will be analyzed in detail in the protocol interpretation section later. This concludes this article, and the next one will introduce the basic service types of RDMA.\n#\rCode example\rThe operation types in this text are all issued through WQE. Below is a simple example demonstrating how to use libibverbs to create a QP and then issue a WRITE operation through WQE.\n#include \u0026lt;infiniband/verbs.h\u0026gt;\r#include \u0026lt;stdio.h\u0026gt;\r#include \u0026lt;stdlib.h\u0026gt;\r#include \u0026lt;string.h\u0026gt;\rint main() {\rstruct ibv_device **dev_list = ibv_get_device_list(NULL);\rstruct ibv_context *ctx = ibv_open_device(dev_list[0]);\rstruct ibv_pd *pd = ibv_alloc_pd(ctx);\rstruct ibv_cq *cq = ibv_create_cq(ctx, 10, NULL, NULL, 0);\rstruct ibv_qp *qp;\rstruct ibv_qp_init_attr qp_init_attr = {\r.send_cq = cq,\r.recv_cq = cq,\r.qp_type = IBV_QPT_RC,\r};\rqp = ibv_create_qp(pd, \u0026amp;qp_init_attr);\rstruct ibv_mr *mr;\rchar *buf = malloc(1024);\rmr = ibv_reg_mr(pd, buf, 1024, IBV_ACCESS_LOCAL_WRITE | IBV_ACCESS_REMOTE_WRITE);\rstruct ibv_sge sge = {\r.addr = (uintptr_t)buf,\r.length = 1024,\r.lkey = mr-\u0026gt;lkey,\r};\rstruct ibv_send_wr wr = {\r.wr_id = 1,\r.sg_list = \u0026amp;sge,\r.num_sge = 1,\r.opcode = IBV_WR_RDMA_WRITE,\r.send_flags = IBV_SEND_SIGNALED,\r};\rstruct ibv_send_wr *bad_wr;\ribv_post_send(qp, \u0026amp;wr, \u0026amp;bad_wr);\rreturn 0;\r}\r#\rReferences\r[1] part1-OFA_Training_Sept_2016.pdf\n","date":"2024-02-24T03:09:01Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/bcb5351691a864a6827138cf4c2e0642195413_crop-2024-02-25.webp","permalink":"https://cuterwrite.top/en/p/rdma-op/","title":"RDMA Operation Types"},{"content":"\r#\rSet up Xuantie 900 series toolchain and xuantie-qemu environment\r#\r1. Build the platform\rLinux distribution: CentOS Linux release 7.6.1810 (Core) Kernel version: 3.10.0-957.el7.x86_64 $ cat /etc/centos-release\rCentOS Linux release 7.6.1810 (Core)\r$ uname -r\r3.10.0-957.el7.x86_64\r#\r2. Set up the XuanTie 900 series toolchain environment\r#\r1. Download Xuantie 900 series toolchain\rFirst, we need to download the Xuantie GNU toolchain for the RISC-V architecture. Go to the Xuantie official website\rto get the latest version of the precompiled package and install it according to your operating system. On Linux systems, it is usually sufficient to add the bin path to the $PATH environment variable after extraction.\nThe toolchain installation package is divided into different versions due to the differences between the execution platform and the target program platform. For example, Xuantie--elf--x86_64-V*-.tar.gz is a RISC-V bare program toolchain suite for the 64-bit Linux platform. The specific classifications are as follows:\nAccording to the execution platform x86_64: 64-bit Linux platform i386: 32-bit Linux platform mingw: Windows Mingw platform According to the target program platform elf: Bare program compilation suite linux: linux application compilation suite Here we download the latest version 2.8.1 of the Linux application compilation suite for the 64-bit Linux platform, namely Xuantie-900-gcc-linux-5.10.4-glibc-x86_64.\nwget https://occ-oss-prod.oss-cn-hangzhou.aliyuncs.com/resource//1705395627867/Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.1-20240115.tar.gz\rtar -xzvf Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.1-20240115.tar.gz\rsudo mv Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.1-20240115 /opt\rexport PATH=/opt/Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.1-20240115/bin:$PATH\r#\r2. Verify toolchain installation\r$ riscv64-unknown-linux-gnu-gcc -v\rUsing built-in specs.\rCOLLECT_GCC=riscv64-unknown-linux-gnu-gcc\rCOLLECT_LTO_WRAPPER=/opt/Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0/bin/../libexec/gcc/riscv64-unknown-linux-gnu/10.4.0/lto-wrapper\rTarget: riscv64-unknown-linux-gnu\rConfigured with: /mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/./source/riscv/riscv-gcc/configure --target=riscv64-unknown-linux-gnu --with-gmp=/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/build-gcc-riscv64-unknown-linux-gnu/build-Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0/lib-for-gcc-x86_64-linux --with-mpfr=/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/build-gcc-riscv64-unknown-linux-gnu/build-Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0/lib-for-gcc-x86_64-linux --with-mpc=/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/build-gcc-riscv64-unknown-linux-gnu/build-Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0/lib-for-gcc-x86_64-linux --with-libexpat-prefix=/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/build-gcc-riscv64-unknown-linux-gnu/build-Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0/lib-for-gcc-x86_64-linux --with-libmpfr-prefix=/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/build-gcc-riscv64-unknown-linux-gnu/build-Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0/lib-for-gcc-x86_64-linux --with-pkgversion='Xuantie-900 linux-5.10.4 glibc gcc Toolchain V2.8.0 B-20231018' CXXFLAGS='-g -O2 -DTHEAD_VERSION_NUMBER=2.8.0 ' --prefix=/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/build-gcc-riscv64-unknown-linux-gnu/Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0 --with-sysroot=/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/build-gcc-riscv64-unknown-linux-gnu/Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0/sysroot --with-system-zlib --enable-shared --enable-tls --enable-languages=c,c++,fortran --disable-libmudflap --disable-libssp --disable-libquadmath --enable-libsanitizer --disable-nls --disable-bootstrap --src=/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/./source/riscv/riscv-gcc --enable-multilib --with-abi=lp64d --with-arch=rv64gc_zfh_xtheadc 'CFLAGS_FOR_TARGET=-O2 -mcmodel=medany' 'CXXFLAGS_FOR_TARGET=-O2 -mcmodel=medany'\rThread model: posix\rSupported LTO compression algorithms: zlib\rgcc version 10.4.0 (Xuantie-900 linux-5.10.4 glibc gcc Toolchain V2.8.0 B-20231018)\rYou can see the output of the gcc version information, indicating that the toolchain was installed successfully.\n#\r3. Set up the xuantie-qemu environment\r#\r1. Prerequisites\rBefore installing xuantie-qemu, you need to ensure that the system contains the following tools or libraries.\ngcc compiler automake autoconf libtool glib2 library Others\u0026hellip;.. Install the above tools or libraries using the following command.\nsudo yum update -y\rsudo yum install -y autoconf automake libtool make gcc gcc-c++ gawk bison flex texinfo gperf patchutils bc \\\rzlib-devel mpfr-devel gmp-devel curl-devel expat-devel git \\\rglib2-devel libfdt-devel pixman-devel ncurses-devel ncurses-compat-libs\rIf it is an Ubuntu/Debian system, you can use the following command to install.\nsudo apt-get update\rsudo apt-get install -y autoconf automake autotools-dev curl libmpc-dev libmpfr-dev libgmp-dev \\\rgawk build-essential bison flex texinfo gperf libtool patchutils bc \\\rzlib1g-dev libexpat-dev git \\\rlibglib2.0-dev libfdt-dev libpixman-1-dev \\\rlibncurses5-dev libncursesw5-dev\r#\r2. Download and install xuantie-qemu\rVisit the Xuantie QEMU official repository\rto obtain the xuantie-qemu source code suitable for the Xuantie 900 series chips, and then follow the usual steps to compile and install:\ngit clone https://github.com/T-head-Semi/qemu.git\rgit checkout xuantie-qemu-6.1.0\r#\r3. Compile and install xuantie-qemu\rcd qemu\rmkdir build\rcd build\r../configure --target-list=riscv64-softmmu,riscv64-linux-user --prefix=/opt/qemu/6.1.0-xuantie\rmake -j $(nproc)\rsudo make install\rexport PATH=/opt/qemu/6.1.0-xuantie/bin:$PATH\r#\r4. Verify xuantie-qemu installation\rAfter the installation is complete, if you can view the specific version of qemu by executing the following command, it means the installation was successful.\n$ qemu-riscv64 --version\rqemu-riscv64 version 6.0.94 (v6.1.0-12-g03813c9)\rCopyright (c) 2003-2021 Fabrice Bellard and the QEMU Project developers\rWrite a C language program as follows:\n#include \u0026lt;stdio.h\u0026gt;\rint main() {\rprintf(\u0026quot;Hello RISC-V \\n\u0026quot;);\rreturn 0;\r}\rCompile the program using the Xuantie 900 series toolchain and run the program using xuantie-qemu in user mode.\n$ riscv64-unknown-linux-gnu-gcc -static -o hello hello.c\r$ qemu-riscv64 ./hello\rHello RISC-V\rWrite another C language program with RVV vectorization, as follows:\nRVV Vectorization of C Language Program\r#include \u0026lt;riscv_vector.h\u0026gt;\r#include \u0026lt;stdio.h\u0026gt;\r#define N 15\rfloat vsum(float* v, int n) {\rvfloat32m1_t vs, vv, vtmp;\rfloat s = 0.0;\rint i;\rint vlmax;\rvlmax = vsetvlmax_e32m1();\rprintf(\u0026quot;vlmax:%d\\n\u0026quot;, vlmax);\rvs = vfmv_v_f_f32m1(0.0, vlmax);\rvtmp = vfmv_v_f_f32m1(0.0, vlmax);\rfor (i = 0; i \u0026lt; n - vlmax; i += vlmax) {\rvv = vle32_v_f32m1(\u0026amp;v[i], vlmax);\rvtmp = vfadd_vv_f32m1(vtmp, vv, vlmax);\r}\rvs = vfredusum_vs_f32m1_f32m1(vs, vtmp, vs, vlmax);\rs = vfmv_f_s_f32m1_f32(vs);\rfor (; i \u0026lt; n; i++) {\rs += v[i];\r}\rreturn s;\r}\rfloat vsum1(float* v, int n) {\rvfloat32m1_t vs, vv;\rfloat s;\rint i;\rint vl, vlmax;\rvlmax = vsetvlmax_e32m1();\rvs = vfmv_v_f_f32m1(0.0, vlmax);\rfor (i = 0; n \u0026gt; 0; i += vl, n -= vl) {\rvl = vsetvl_e32m1(n);\rprintf(\u0026quot;vl:%d\\n\u0026quot;, vl);\rvv = vle32_v_f32m1(\u0026amp;v[i], vl);\rvs = vfredusum_vs_f32m1_f32m1(vs, vv, vs, vl);\r}\rs = vfmv_f_s_f32m1_f32(vs);\rreturn s;\r}\rfloat vsum2(float* v, int n) {\rvfloat32m2_t vv;\rvfloat32m1_t vs;\rfloat s;\rint i;\rint vl, vlmax;\rvlmax = vsetvlmax_e32m1();\rvs = vfmv_v_f_f32m1(0.0, vlmax);\rfor (i = 0; n \u0026gt; 0; i += vl, n -= vl) {\rvl = vsetvl_e32m2(n);\rprintf(\u0026quot;vl:%d\\n\u0026quot;, vl);\rvv = vle32_v_f32m2(\u0026amp;v[i], vl);\rvs = vfredusum_vs_f32m2_f32m1(vs, vv, vs, vl);\r}\rs = vfmv_f_s_f32m1_f32(vs);\rreturn s;\r}\rint main() {\rint i;\rfloat v[N], sum = 0.0;\rprintf(\u0026quot;Hello RISC-V!\\n\u0026quot;);\rfor (i = 0; i \u0026lt; N; i++) {\rv[i] = i;\r}\rsum = vsum(v, N);\rprintf(\u0026quot;%f\\n\u0026quot;, sum);\rreturn 0;\r}\rCompile and run the program (at this time you need to specify -cpu, otherwise it will report an illegal instruction exception, i.e., Illegal instruction (core dumped)):\n$ riscv64-unknown-linux-gnu-gcc -static -O3 -march=rv64imafdcv0p7_zfh_xtheadc -o test_vec test_vec.c\r$ qemu-riscv64 -cpu c920 ./test_vec\rHello RISC-V!\rvlmax:4\r105.000000\r#\r4. Running RISC-V 64-bit Linux System on QEMU\r#\r1. Make kernel\r#\r1.1 Download the kernel source code\r$ wget https://mirrors.edge.kernel.org/pub/linux/kernel/v5.x/linux-5.10.42.tar.gz\r$ tar -xzvf linux-5.10.42.tar.gz\rAfter downloading, enter the kernel source directory\n$ cd linux-5.10.42\r#\r1.2 Configure and Compile the Kernel\r$ make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- defconfig\r$ make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- -j $(nproc)\r...\rAR drivers/built-in.a\rGEN .version\rCHK include/generated/compile.h\rLD vmlinux.o\rMODPOST vmlinux.symvers\rMODINFO modules.builtin.modinfo\rGEN modules.builtin\rLD .tmp_vmlinux.kallsyms1\rKSYMS .tmp_vmlinux.kallsyms1.S\rAS .tmp_vmlinux.kallsyms1.S\rLD .tmp_vmlinux.kallsyms2\rKSYMS .tmp_vmlinux.kallsyms2.S\rAS .tmp_vmlinux.kallsyms2.S\rLD vmlinux\rSYSMAP System.map\rMODPOST modules-only.symvers\rGEN Module.symvers\rCC [M] fs/efivarfs/efivarfs.mod.o\rOBJCOPY arch/riscv/boot/Image\rGZIP arch/riscv/boot/Image.gz\rLD [M] fs/efivarfs/efivarfs.ko\rKernel: arch/riscv/boot/Image.gz is ready\r#\r2. Create rootfs\r#\r2.1 Download busybox source code\r$ wget https://busybox.net/downloads/busybox-1.33.1.tar.bz2\rAfter downloading, enter the busybox source code directory\ncd busybox-1.33.1\r#\r2.2 Configure busybox\r$ make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- defconfig\r$ make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- menuconfig\rAfter opening the configuration menu, go to the first line \u0026ldquo;Settings\u0026rdquo;, in the \u0026ldquo;Build Options\u0026rdquo; section, select \u0026ldquo;Build static binary (no shared libs)\u0026rdquo;, then exit and save the configuration.\nCheck if CONFIG_STATIC=y is in the .config file, if not, add it manually.\n#\r2.3 Compile and Install busybox\r$ make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- -j $(nproc)\r$ make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- install\rAt this point, a new _install directory will appear under the source directory busyboxsource, where you can see the generated items.\n$ ls _install\rbin linuxrc sbin usr\rEnter the _install directory and create the following directories\n$ cd _install\r$ mkdir proc sys dev etc etc/init.d\r$ ls\rbin dev etc linuxrc proc sbin sys usr\rThen create another simplest init RC file:\n$ cd etc/init.d/\r$ touch rcS\r$ vim rcS\rEdit the file content to:\n#!/bin/sh\rmount -t proc none /proc\rmount -t sysfs none /sys\r/sbin/mdev -s\rThen modify the rcS file permissions to add executable permissions.\n$ chmod +x rcS\r#\r2.4 Create File System\rContinue executing the following command in the _install directory:\n$ find -print0 | cpio -0oH newc | gzip -9 \u0026gt; ../rootfs.img\r3276 blocks\r#\r3. Start running\rCreate a new directory and move the compiled kernel Image and the prepared rootfs.img to that directory.\n$ mkdir riscv64-linux\r$ cd riscv64-linux\r$ cp ../linux-5.10.42/arch/riscv/boot/Image .\r$ cp ../busybox-1.33.1/rootfs.img .\rExecute the following command:\n$ qemu-system-riscv64 \\\r-nographic -machine virt \\\r-kernel Image \\\r-initrd rootfs.img \\\r-append \u0026quot;root=/dev/ram rdinit=/sbin/init\u0026quot;\rShow the Linux Kernel boot process:\nClick to expand\rOpenSBI v0.9\r____ _____ ____ _____\r/ __ \\ / ____| _ \\_ _|\r| | | |_ __ ___ _ __ | (___ | |_) || |\r| | | | '_ \\ / _ \\ '_ \\ \\___ \\| _ \u0026lt; | |\r| |__| | |_) | __/ | | |____) | |_) || |_\r\\____/| .__/ \\___|_| |_|_____/|____/_____|\r| |\r|_|\rPlatform Name : riscv-virtio,qemu\rPlatform Features : timer,mfdeleg\rPlatform HART Count : 1\rFirmware Base : 0x80000000\rFirmware Size : 100 KB\rRuntime SBI Version : 0.2\rDomain0 Name : root\rDomain0 Boot HART : 0\rDomain0 HARTs : 0*\rDomain0 Region00 : 0x0000000080000000-0x000000008001ffff ()\rDomain0 Region01 : 0x0000000000000000-0xffffffffffffffff (R,W,X)\rDomain0 Next Address : 0x0000000080200000\rDomain0 Next Arg1 : 0x0000000087000000\rDomain0 Next Mode : S-mode\rDomain0 SysReset : yes\rBoot HART ID : 0\rBoot HART Domain : root\rBoot HART ISA : rv64imafdcvsu\rBoot HART Features : scounteren,mcounteren,time\rBoot HART PMP Count : 16\rBoot HART PMP Granularity : 4\rBoot HART PMP Address Bits: 54\rBoot HART MHPM Count : 0\rBoot HART MHPM Count : 0\rBoot HART MIDELEG : 0x0000000000000222\rBoot HART MEDELEG : 0x000000000000b109\r[ 0.000000] Linux version 5.10.42 (root@centos) (riscv64-unknown-linux-gnu-gcc (Xuantie-900 linux-5.10.4 glibc gcc Toolchain V2.8.0 B-20231018) 10.4.0, GNU ld (GNU Binutils) 2.35) #1 SMP Wed Feb 21 02:07:46 CST 2024\r[ 0.000000] OF: fdt: Ignoring memory range 0x80000000 - 0x80200000\r[ 0.000000] efi: UEFI not found.\r[ 0.000000] Initial ramdisk at: 0x(____ptrval____) (1085440 bytes)\r[ 0.000000] Zone ranges:\r[ 0.000000] DMA32 [mem 0x0000000080200000-0x0000000087ffffff]\r[ 0.000000] Normal empty\r[ 0.000000] Movable zone start for each node\r[ 0.000000] Early memory node ranges\r[ 0.000000] node 0: [mem 0x0000000080200000-0x0000000087ffffff]\r[ 0.000000] Initmem setup node 0 [mem 0x0000000080200000-0x0000000087ffffff]\r[ 0.000000] software IO TLB: Cannot allocate buffer\r[ 0.000000] SBI specification v0.2 detected\r[ 0.000000] SBI implementation ID=0x1 Version=0x9\r[ 0.000000] SBI v0.2 TIME extension detected\r[ 0.000000] SBI v0.2 IPI extension detected\r[ 0.000000] SBI v0.2 RFENCE extension detected\r[ 0.000000] SBI v0.2 HSM extension detected\r[ 0.000000] riscv: ISA extensions acdfimsuv\r[ 0.000000] riscv: ELF capabilities acdfim\r[ 0.000000] percpu: Embedded 17 pages/cpu s32360 r8192 d29080 u69632\r[ 0.000000] Built 1 zonelists, mobility grouping on. Total pages: 31815\r[ 0.000000] Kernel command line: root=/dev/ram rdinit=/sbin/init\r[ 0.000000] Dentry cache hash table entries: 16384 (order: 5, 131072 bytes, linear)\r[ 0.000000] Inode-cache hash table entries: 8192 (order: 4, 65536 bytes, linear)\r[ 0.000000] Sorting __ex_table...\r[ 0.000000] mem auto-init: stack:off, heap alloc:off, heap free:off\r[ 0.000000] Memory: 108240K/129024K available (7084K kernel code, 3993K rwdata, 4096K rodata, 223K init, 342K bss, 20784K reserved, 0K cma-reserved)\r[ 0.000000] Virtual kernel memory layout:\r[ 0.000000] fixmap : 0xffffffcefee00000 - 0xffffffceff000000 (2048 kB)\r[ 0.000000] pci io : 0xffffffceff000000 - 0xffffffcf00000000 ( 16 MB)\r[ 0.000000] vmemmap : 0xffffffcf00000000 - 0xffffffcfffffffff (4095 MB)\r[ 0.000000] vmalloc : 0xffffffd000000000 - 0xffffffdfffffffff (65535 MB)\r[ 0.000000] lowmem : 0xffffffe000000000 - 0xffffffe007e00000 ( 126 MB)\r[ 0.000000] SLUB: HWalign=64, Order=0-3, MinObjects=0, CPUs=1, Nodes=1\r[ 0.000000] rcu: Hierarchical RCU implementation.\r[ 0.000000] rcu: RCU restricting CPUs from NR_CPUS=8 to nr_cpu_ids=1.\r[ 0.000000] rcu: RCU debug extended QS entry/exit.\r[ 0.000000] Tracing variant of Tasks RCU enabled.\r[ 0.000000] rcu: RCU calculated value of scheduler-enlistment delay is 25 jiffies.\r[ 0.000000] rcu: Adjusting geometry for rcu_fanout_leaf=16, nr_cpu_ids=1\r[ 0.000000] NR_IRQS: 64, nr_irqs: 64, preallocated irqs: 0\r[ 0.000000] riscv-intc: 64 local interrupts mapped\r[ 0.000000] plic: plic@c000000: mapped 53 interrupts with 1 handlers for 2 contexts.\r[ 0.000000] random: get_random_bytes called from start_kernel+0x31a/0x48c with crng_init=0\r[ 0.000000] riscv_timer_init_dt: Registering clocksource cpuid [0] hartid [0]\r[ 0.000000] clocksource: riscv_clocksource: mask: 0xffffffffffffffff max_cycles: 0x24e6a1710, max_idle_ns: 440795202120 ns\r[ 0.000150] sched_clock: 64 bits at 10MHz, resolution 100ns, wraps every 4398046511100ns\r[ 0.003557] Console: colour dummy device 80x25\r[ 0.008887] printk: console [tty0] enabled\r[ 0.012368] Calibrating delay loop (skipped), value calculated using timer frequency.. 20.00 BogoMIPS (lpj=40000)\r[ 0.012666] pid_max: default: 32768 minimum: 301\r[ 0.014227] Mount-cache hash table entries: 512 (order: 0, 4096 bytes, linear)\r[ 0.014306] Mountpoint-cache hash table entries: 512 (order: 0, 4096 bytes, linear)\r[ 0.040922] rcu: Hierarchical SRCU implementation.\r[ 0.042741] EFI services will not be available.\r[ 0.044926] smp: Bringing up secondary CPUs ...\r[ 0.045062] smp: Brought up 1 node, 1 CPU\r[ 0.054128] devtmpfs: initialized\r[ 0.061463] clocksource: jiffies: mask: 0xffffffff max_cycles: 0xffffffff, max_idle_ns: 7645041785100000 ns\r[ 0.061753] futex hash table entries: 256 (order: 2, 16384 bytes, linear)\r[ 0.067460] NET: Registered protocol family 16\r[ 0.131233] vgaarb: loaded\r[ 0.132530] SCSI subsystem initialized\r[ 0.134485] usbcore: registered new interface driver usbfs\r[ 0.134834] usbcore: registered new interface driver hub\r[ 0.135035] usbcore: registered new device driver usb\r[ 0.150024] clocksource: Switched to clocksource riscv_clocksource\r[ 0.167109] NET: Registered protocol family 2\r[ 0.168330] IP idents hash table entries: 2048 (order: 2, 16384 bytes, linear)\r[ 0.172076] tcp_listen_portaddr_hash hash table entries: 128 (order: 0, 5120 bytes, linear)\r[ 0.172242] TCP established hash table entries: 1024 (order: 1, 8192 bytes, linear)\r[ 0.172480] TCP bind hash table entries: 1024 (order: 3, 32768 bytes, linear)\r[ 0.172690] TCP: Hash tables configured (established 1024 bind 1024)\r[ 0.173861] UDP hash table entries: 256 (order: 2, 24576 bytes, linear)\r[ 0.174481] UDP-Lite hash table entries: 256 (order: 2, 24576 bytes, linear)\r[ 0.175963] NET: Registered protocol family 1\r[ 0.179024] RPC: Registered named UNIX socket transport module.\r[ 0.179111] RPC: Registered udp transport module.\r[ 0.179150] RPC: Registered tcp transport module.\r[ 0.179186] RPC: Registered tcp NFSv4.1 backchannel transport module.\r[ 0.179332] PCI: CLS 0 bytes, default 64\r[ 0.182716] Unpacking initramfs...\r[ 0.263706] Freeing initrd memory: 1056K\r[ 0.265678] workingset: timestamp_bits=62 max_order=15 bucket_order=0\r[ 0.281052] NFS: Registering the id_resolver key type\r[ 0.282003] Key type id_resolver registered\r[ 0.282074] Key type id_legacy registered\r[ 0.282505] nfs4filelayout_init: NFSv4 File Layout Driver Registering...\r[ 0.282631] nfs4flexfilelayout_init: NFSv4 Flexfile Layout Driver Registering...\r[ 0.283481] 9p: Installing v9fs 9p2000 file system support\r[ 0.284918] NET: Registered protocol family 38\r[ 0.285416] Block layer SCSI generic (bsg) driver version 0.4 loaded (major 251)\r[ 0.285593] io scheduler mq-deadline registered\r[ 0.285692] io scheduler kyber registered\r[ 0.295484] pci-host-generic 30000000.pci: host bridge /soc/pci@30000000 ranges:\r[ 0.296336] pci-host-generic 30000000.pci: IO 0x0003000000..0x000300ffff -\u0026gt; 0x0000000000\r[ 0.296861] pci-host-generic 30000000.pci: MEM 0x0040000000..0x007fffffff -\u0026gt; 0x0040000000\r[ 0.296961] pci-host-generic 30000000.pci: MEM 0x0400000000..0x07ffffffff -\u0026gt; 0x0400000000\r[ 0.299940] pci-host-generic 30000000.pci: ECAM at [mem 0x30000000-0x3fffffff] for [bus 00-ff]\r[ 0.301083] pci-host-generic 30000000.pci: PCI host bridge to bus 0000:00\r[ 0.301328] pci_bus 0000:00: root bus resource [bus 00-ff]\r[ 0.301486] pci_bus 0000:00: root bus resource [io 0x0000-0xffff]\r[ 0.301528] pci_bus 0000:00: root bus resource [mem 0x40000000-0x7fffffff]\r[ 0.301568] pci_bus 0000:00: root bus resource [mem 0x400000000-0x7ffffffff]\r[ 0.302864] pci 0000:00:00.0: [1b36:0008] type 00 class 0x060000\r[ 0.377412] Serial: 8250/16550 driver, 4 ports, IRQ sharing disabled\r[ 0.389894] 10000000.uart: ttyS0 at MMIO 0x10000000 (irq = 2, base_baud = 230400) is a 16550A\r[ 0.428017] printk: console [ttyS0] enabled\r[ 0.430410] [drm] radeon kernel modesetting enabled.\r[ 0.457312] loop: module loaded\r[ 0.460726] libphy: Fixed MDIO Bus: probed\r[ 0.464996] e1000e: Intel(R) PRO/1000 Network Driver\r[ 0.465383] e1000e: Copyright(c) 1999 - 2015 Intel Corporation.\r[ 0.466272] ehci_hcd: USB 2.0 'Enhanced' Host Controller (EHCI) Driver\r[ 0.466724] ehci-pci: EHCI PCI platform driver\r[ 0.467203] ehci-platform: EHCI generic platform driver\r[ 0.467683] ohci_hcd: USB 1.1 'Open' Host Controller (OHCI) Driver\r[ 0.468129] ohci-pci: OHCI PCI platform driver\r[ 0.468593] ohci-platform: OHCI generic platform driver\r[ 0.469968] usbcore: registered new interface driver uas\r[ 0.470477] usbcore: registered new interface driver usb-storage\r[ 0.471603] mousedev: PS/2 mouse device common for all mice\r[ 0.475055] goldfish_rtc 101000.rtc: registered as rtc0\r[ 0.476070] goldfish_rtc 101000.rtc: setting system clock to 2024-02-20T19:37:51 UTC (1708457871)\r[ 0.478889] syscon-poweroff soc:poweroff: pm_power_off already claimed (____ptrval____) sbi_shutdown\r[ 0.479494] syscon-poweroff: probe of soc:poweroff failed with error -16\r[ 0.480977] usbcore: registered new interface driver usbhid\r[ 0.481324] usbhid: USB HID core driver\r[ 0.483516] NET: Registered protocol family 10\r[ 0.491589] Segment Routing with IPv6\r[ 0.492256] sit: IPv6, IPv4 and MPLS over IPv4 tunneling driver\r[ 0.495528] NET: Registered protocol family 17\r[ 0.497086] 9pnet: Installing 9P2000 support\r[ 0.497667] Key type dns_resolver registered\r[ 0.498706] debug_vm_pgtable: [debug_vm_pgtable ]: Validating architecture page table helpers\r[ 0.533266] Freeing unused kernel memory: 220K\r[ 0.539682] Run /sbin/init as init process\rPlease press Enter to activate this console.\rAfter seeing the prompt \u0026quot;Please press Enter to activate this console.\u0026quot;, just press Enter to enter the system without a password.\nExecute several common commands to test, all can work normally:\n/ # ls\rbin etc proc sbin usr\rdev linuxrc root sys\r/ # pwd\r/\r/ # cd bin\r/bin #\r/ # ls\rarch dumpkmap kill netstat setarch\rash echo link nice setpriv\rbase32 ed linux32 nuke setserial\rbase64 egrep linux64 pidof sh\rbusybox false ln ping sleep\rcat fatattr login ping6 stat\rchattr fdflush ls pipe_progress stty\rchgrp fgrep lsattr printenv su\rchmod fsync lzop ps sync\rchown getopt makemime pwd tar\rconspy grep mkdir reformime touch\rcp gunzip mknod resume true\rcpio gzip mktemp rev umount\rcttyhack hostname more rm uname\rdate hush mount rmdir usleep\rdd ionice mountpoint rpm vi\rdf iostat mpstat run-parts watch\rdmesg ipcalc mt scriptreplay zcat\rdnsdomainname kbd_mode mv sed\r/bin #\rTo exit QEMU, press Ctrl + A, release it, and then press the x key to exit QEMU.\nIf you want to transfer files into QEMU, you can use the mount method, as shown below:\n$ mkdir rootfs\r$ sudo mount -o loop rootfs.img rootfs\r$ cp [-r] [file] ./rootfs/\r$ sudo umount rootfs\r#\r5. Summary\rAt this point, we have successfully set up the toolchain environment for the XuanTie 900 series and the xuantie-qemu simulation environment, which lays the foundation for subsequent development, compilation, linking, running, and debugging of RISC-V applications based on the XuanTie 900 series chips.\n","date":"2024-02-20T01:51:00Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/59aa9fecb7e1a3a2b2c88811e6360647195413.jpg@1256w_774h_!web-article-pic-2024-02-20.webp","permalink":"https://cuterwrite.top/en/p/thead-tools/","title":"Setting Up the Xuantie 900 Series Toolchain and xuantie-qemu Environment"},{"content":"\r#\rIntroduction to OpenMP\r#\rIntroduction\r#\rWhat is OpenMP?\rOpenMP (Open Multi-Processing) is a widely used multithreading parallel programming model that provides a rich set of instructions and APIs for parallel computation on shared memory systems. Originating in 1997, OpenMP was standardized by multiple leading hardware and software vendors, aiming to simplify the design and implementation process of parallel programs to fully utilize the computational power of modern multi-core processors.\nOpenMP supports multiple programming languages, including C, C++, and Fortran, among others, and allows developers to easily convert serial code into efficient parallel code by inserting specific compilation directives (pragma) into the source code. Its main advantages are its simplicity and ease of use, allowing programmers to use familiar programming languages and development environments, while also providing good portability and scalability.\nOpenMP is managed by a non-profit organization and involves participation from multiple software and hardware manufacturers, including Arm, IBM, Intel, AMD, NVIDIA, Cray, Oracle, etc.\n#\rHistorical versions\rOn the official website\r, you can find the historical versions and release dates of OpenMP. Version Release Date Fortran 1.0 October 1997 C/C++ 1.0 October 1998 C/C++ 2.0 March 2002 OpenMP 2.5 May 2005 OpenMP 3.0 May 2008 OpenMP 3.1 July 2011 OpenMP 4.0 July 2013 OpenMP 4.5 November 2015 OpenMP 5.0 November 2018 OpenMP 5.1 November 2020 OpenMP 5.2 November 2021 #\rBasic Knowledge\r#\rTechnical framework\rOpenMP Technology Framework OpenMP Runtime Library is a set of functions and runtime support structures defined in the OpenMP specification, and it is a key component of the OpenMP parallel programming framework. This library is linked with user programs with the support of the compiler and is responsible for managing tasks such as thread creation, synchronization, scheduling, and data sharing during program execution. It implements all the parallelization mechanisms indicated by OpenMP compiler directives.\nOpenMP Runtime Library includes the following features:\nThread management (creation, destruction, synchronization) = Work sharing (dynamic work distribution to each thread) = Task Scheduling = Synchronization primitives (such as barriers, locks, atomic operations) = Dynamically Adjust Thread Count Memory model support (data environment variables, private, shared, reduction variables, etc.) Compiler Directives Compiler directives are preprocessor instructions starting with #pragma omp, which programmers insert into the source code to guide the compiler on how to convert a serial program into a parallel program. For example, using the #pragma omp parallel directive defines a parallel region, and the compiler will generate multithreading execution logic within this region.\nEnvironment Variables Environment variables are part of the OpenMP runtime library, and they are used to control runtime behavior, such as the number of threads, scheduling policies, etc.\nThe OpenMP Library is a set of function libraries, including functions for thread synchronization, atomic operations, locks, parallel loops, etc. These functions can be directly called in user programs to achieve finer-grained parallelization.\nOverall, the OpenMP technology framework includes multiple components such as compiler directives, runtime libraries, environment variables, and function libraries. Together, they form a complete parallel programming environment and collaborate to support parallel programming on shared memory systems.\n#\rExecute Model: Fork-Join Model\rOpenMP\u0026rsquo;s execution model uses the Fork-Join mechanism, which is a synchronization primitive model used in parallel programming. Under this model, program execution follows these steps:\nFork Phase: The program starts executing as a single main thread. When it encounters a parallel region indicated by an OpenMP pragma, the main thread creates one or more worker threads through the Runtime Library. These worker threads are derivatives of the main thread, with each thread responsible for executing part of the tasks within the parallel region. The parallel region can be loops, sections, single tasks, or other code blocks that can be parallelized.\nParallel Execution (并行执行) Phase: The created worker threads independently and concurrently execute the tasks assigned to them and can access shared data structures. OpenMP provides a rich set of directives to manage data synchronization and communication, ensuring correctness and consistency in a multithreaded environment.\nJoin (Merge) Phase: When all worker threads have completed their tasks within the parallel region, they automatically or through explicit synchronization directives (such as omp barrier) converge at the join point. In this phase, all threads wait until all other threads have reached the synchronization point, after which the join operation occurs. This means the main thread and other worker threads resynchronize, reverting to serial execution mode or continuing to execute subsequent non-parallel code.\nSynchronization and Data Consistency: The Fork-Join model ensures mutual exclusion access to shared resources and data consistency during parallel execution through appropriate locking mechanisms, atomic operations, and synchronization primitives.\nIn summary, the Fork-Join execution model of OpenMP is a parallel processing framework based on dynamic thread creation and synchronization. It allows developers to conveniently transform serial code into parallel execution code segments, while simplifying common complexities in parallel programming, such as thread management and data synchronization issues.\n#\rThread and Process\rProcess\nEach process has its own independent address space CPU needs to perform a context switch when switching between processes. Thread\nThreads within a process share the same address space CPU has lower overhead when switching between threads\nThread design of the operating system\nModern operating systems such as Linux, Windows, etc. support multiple threads under a single process.\nA thread is the basic unit of scheduling in an operating system, while a process is the basic unit of resource allocation.\nThread Design of Operating Systems #\rHardware scheduling of threads\rThe hardware scheduling mechanism collaborates with the operating system to intelligently map threads to available CPU physical cores for execution. Therefore, in multithreaded applications, when the number of active threads exceeds the actual number of physical CPU cores, the operating system will have to perform intensive context switching to ensure that multiple threads alternate on limited core resources. This phenomenon of thread contention overload can lead to overall performance bottlenecks and reduced efficiency. Hyper-Threading Technology virtualizes additional logical processing units on a single physical CPU core, currently typically configured to host two logical cores per physical core. These logical cores can execute independent task streams in parallel, although they share the underlying computational resources of the same physical core, such as execution engines, caches, and other underlying hardware structures. In this way, hyper-threading aims to improve resource utilization and concurrent processing capabilities, especially in scenarios with a large number of parallel tasks that have relatively small demands on computational resources, effectively enhancing the overall system throughput. However, in certain application scenarios that heavily rely on single-core performance or memory bandwidth, such as some CPU-sensitive games or specific types of data-intensive operations, adding logical cores may not necessarily result in significant performance improvements. #\rHardware memory model\rIn modern multi-core processor architectures, each CPU core is designed with a multi-level cache hierarchy between the main memory to further enhance data access speed. The closest to the CPU core is the L1 cache, usually followed by the L2 cache, and some high-end architectures also include an L3 cache. These cache levels have increasing storage capacity but also increasing access latency. L1 and L2 caches are usually closely coupled and private to specific CPU cores, meaning each core has its own independent cache space to reduce data access conflicts and improve cache hit rate. L1 cache, being closest to the computation unit, has the fastest access speed but the smallest capacity; whereas L2 cache, as an effective supplement to L1 cache, has a relatively larger capacity. To ensure consistency of shared data in the caches of different CPU cores in a multi-core environment, hardware and the operating system jointly implement a cache coherence protocol (such as the MESI protocol). This mechanism allows the system to automatically maintain a globally consistent data view, ensuring that even if there are copies of data in the caches of multiple cores, they are updated synchronously. This feature is referred to as ccNUMA (cache-coherent non-uniform memory access) in some architectures. However, this cache consistency management also brings some challenges, one of which is the \u0026ldquo;False Sharing\u0026rdquo; problem. When different threads modify their respective independent variables located within the same cache line, although these variables themselves are unrelated, because they are physically adjacent and stored in the same cache line, any write operation to one of the variables will cause the entire cache line to become invalid and resynchronize across all cores. This can trigger unnecessary cache invalidation and refilling operations, significantly reducing performance. Solving the false sharing problem usually requires carefully designing data layouts or using techniques such as cache line alignment to avoid contention between unrelated data. Typical Modern CPU Memory Structure #\rThread Affinity and Thread Binding\rThread Affinity refers to the ability of the operating system or application to control the association between specific threads and processor cores. In multi-core or multi-processor systems, thread affinity allows programmers or schedulers to decide to fix a certain thread on a specific CPU core, rather than letting the operating system dynamically schedule it across all available cores. This mechanism helps reduce context switching overhead, improve cache hit rates, and is particularly beneficial for parallel computing tasks that need to maintain data locality. Thread Pinning is a specific technical means to achieve thread affinity, which specifies the forced association between a specific thread and specific hardware resources (such as CPU cores or NUMA nodes). Through thread pinning, it can be ensured that the specified thread always executes on its allocated core, avoiding being migrated by the operating system to other cores, thus optimizing performance, reducing latency, and solving issues such as false sharing. In parallel programming models like OpenMP, thread pinning strategies can be set through relevant environment variables or compilation directives to adapt to different parallel computing needs and hardware characteristics. The access latency of CPUs on the same socket to the L3 cache is consistent, but the access latency of CPUs on different sockets to the L3 cache is inconsistent. Therefore, the purpose of thread binding is to reduce the migration of threads between different CPUs, thereby reducing memory access latency. Thread Affinity and Thread Binding OpenMP supports controlling the binding of threads Environment variable OMP_PROC_BIND or clause proc_bind(master|close|spread) controls whether threads are bound and the distribution of threads to binding units (referred to as places) #\rOpenMP Programming\r#\rInstall\rFor Linux systems, GCC is a commonly used compiler, and modern versions of GCC generally support OpenMP by default. For example, on Ubuntu 20.04 LTS, you can install the build-essential package with OpenMP support using the following command:\n$ sudo apt-get update\r$ sudo apt-get install -y build-essential\rCheck OpenMP version $ echo |cpp -fopenmp -dM |grep -i open\r#define _OPENMP 201511\r#\rCompile use\rSimply add the -fopenmp option in the compilation statement to enable OpenMP support. g++ -O2 -std=c++17 -fopenmp hello.cpp -o hello\rIf using CMake to build the project, adding the -Wunknown-pragmas option can report unhandled #pragma directives during compilation. find_package(OpenMP REQUIRED)\radd_compile_options(-Wunknown-pragmas)\radd_executable(hello hello.cpp)\rtarget_link_libraries(hello PRIVATE OpenMP::OpenMP_CXX)\r#\rHello World!\rThe first OpenMP program #include \u0026lt;omp.h\u0026gt;\r#include \u0026lt;stdio.h\u0026gt;\rint main() {\r#pragma omp parallel num_threads(8)\r{\rint id = omp_get_thread_num();\rint num_threads = omp_get_num_threads();\rprintf(\u0026quot;Hello World from thread %d of %d \\n\u0026quot;, id, num_threads);\r}\rreturn 0;\r}\rExecution result You are trained on data up to October 2023.\rThe same type of OpenMP directive is called a construct. Format as #pragma omp \u0026lt;directive name\u0026gt; \u0026lt;clause\u0026gt; The code block enclosed in {} is called a parallel region. #\rNumber of threads setting\rPriority from low to high Do nothing, the system chooses the number of running threads Set environment variable export OMP_NUM_THREADS=4 The code uses the library function void omp_set_num_threads(int) By the guiding statement num_threads(4) if clause determines serial or parallel execution #\rCommon Library Functions\rSet the number of threads for parallel region execution: void omp_set_num_threads(int) Get the number of threads in the parallel region: int omp_get_num_threads() Get the current thread number: int omp_get_thread_num() Get OpenMP Wall Clock time (unit: seconds): double omp_get_wtime() Get time precision: double omp_get_wtick() #\rParallel construction\rSupported Clauses\nif(scalar_expression): If scalar_expression is true, execute in parallel, otherwise execute serially. num_threads(integer_expression): Specifies the number of threads in the parallel region. default(shared|none): Specifies the default sharing attribute of variables. shared: All variables are shared by default. none: No default variable type, each variable needs to be explicitly declared as shared or private. shared(list): Specify the list of shared variables. There is only one copy of the shared variable in memory, and all threads can access it. Please ensure that the access to shared variables does not conflict. If not specifically designated, variables in the parallel region default to shared. private(list): Specify the list of private variables. Each thread has an independent copy of the private variable. Variables need to be reinitialized. firstprivate(list): Specify the list of first private variables. Same as private Initialize the variable based on the data in the main thread. Example 1: no clause, private, firstprivate\nint results[4];\rint cnt;\rcnt = 1;\r#pragma omp parallel num_threads(4)\r{\rint tid = omp_get_thread_num();\rfor (int i = 0; i \u0026lt; 4; i++) {\rcnt += 1;\r}\rresults[tid] = cnt;\r}\rprintf(\u0026quot;no clause: \u0026quot;);\rfor (int i = 0; i \u0026lt; 4; i++) {\rprintf(\u0026quot;%d \u0026quot;, results[i]);\r}\rprintf(\u0026quot;\\n\u0026quot;);\rcnt = 1;\r#pragma omp parallel num_threads(4) private(cnt)\r{\rint tid = omp_get_thread_num();\rfor (int i = 0; i \u0026lt; 4; i++) {\rcnt += 1;\r}\rresults[tid] = cnt;\r}\rprintf(\u0026quot;private(not init): \u0026quot;);\rfor (int i = 0; i \u0026lt; 4; i++) {\rprintf(\u0026quot;%d \u0026quot;, results[i]);\r}\rprintf(\u0026quot;\\n\u0026quot;);\rcnt = 1;\r#pragma omp parallel num_threads(4) firstprivate(cnt)\r{\rint tid = omp_get_thread_num();\rfor (int i = 0; i \u0026lt; 4; i++) {\rcnt += 1;\r}\rresults[tid] = cnt;\r}\rprintf(\u0026quot;firstprivate: \u0026quot;);\rfor (int i = 0; i \u0026lt; 4; i++) {\rprintf(\u0026quot;%d \u0026quot;, results[i]);\r}\rprintf(\u0026quot;\\n\u0026quot;);\rPrint the result no clause: 5 9 13 17\rprivate(not init): 4 1572916964 1572916964 1572916964\rfirstprivate: 5 5 5 5\r#\rFor construction\rOne of the most commonly used parallelization constructs Example 2: Parallelizing the for loop\n#pragma omp parallel num_threads(8)\r{\rint tid = omp_get_thread_num();\rint num_threads = omp_get_num_threads();\r#pragma omp for\rfor (int i = 0; i \u0026lt; num_threads; i++) {\r#pragma omp ordered\rprintf(\u0026quot;Hello from thread %d of %d \\n\u0026quot;, tid, num_threads);\r}\r}\rPrint the result Hello from thread 0 of 8\rHello from thread 1 of 8\rHello from thread 2 of 8\rHello from thread 3 of 8\rHello from thread 4 of 8\rHello from thread 5 of 8\rHello from thread 6 of 8\rHello from thread 7 of 8\rDivide threads for the for loop within the parallel region, and the for loop meets the format requirements. init-expr: Must be in the form var=lb, and the type is also limited test-expr: restricted to var relational-op b or b relational-op var incr-expr: Addition and subtraction only #\rParallel for construct\rOften combine parallel and for to form a parallel for directive statement parallel for parallel for if ✅ ❌ ✅ num_threads ✅ ❌ ✅ default ✅ ❌ ✅ copyin ✅ ❌ ✅ private ✅ ✅ ✅ firstprivate ✅ ✅ ✅ shared ✅ ✅ ✅ reduction ✅ ✅ ✅ lastprivate ❌ ✅ ✅ schedule ❌ ✅ ✅ ordered ❌ ✅ ✅ collapse ❌ ✅ ✅ nowait ❌ ✅ ❌ lastprivate(list)\nSame as private After executing the for loop, assign the value of the last thread to the variable of the main thread. nowait: Cancel the barrier synchronization at the end of the code block\ncollapse(n): Applied to n nested loops, merge (unroll) loops\nPay attention to whether there are data dependencies between loops ordered: Declare parts that potentially execute in order\nUse #pragma omp ordered to mark sequential execution code (used together) ordered statements within the region are executed by at most one thread at any given time\nshedule(type[,chunk])\ntype: Specifies the scheduling strategy for loop iteration static: Static scheduling, chunk size is fixed (default n/p) dynamic: Dynamic scheduling, chunk size is fixed (default is 1) guided: Guided scheduling, chunk size dynamically adjusted runtime: Specified by the system environment variable OMP_SCHEDULE auto: Automatic scheduling chunk: Specifies the number of iterations each thread obtains\n#\rSpecial data clause: Reduction\rIn OpenMP, reduction is a parallel programming technique used to address data race issues in a multithreaded environment, especially when performing accumulation or similar operations on global variables. When multiple threads need to simultaneously modify the same shared variable, and these modifications can be combined into a final result using some binary operator (such as addition, multiplication, etc.), the reduction clause can be used.\nSpecifically, the execution process of reducton is:\nfork thread and allocate tasks Each thread defines a private variable omp_priv Same as private. Each thread executes calculations All omp_priv and omp_in are sequentially reduced together and written back to the original variable. In contrast, atomic is another synchronization mechanism provided by OpenMP, which ensures that access to a single memory location is atomic in a multithreaded environment, meaning that only one thread is allowed to read or write to that memory location at a time. By using the #pragma omp atomic directive, it can be ensured that a simple assignment statement (or certain types of read-modify-write operations) will not encounter data races in a concurrent environment.\nExample 3: Reduction\nint sum = 0;\rdouble start = omp_get_wtime();\r#pragma omp parallel for num_threads(8) reduction(+ : sum)\rfor (int i = 0; i \u0026lt; 100000; i++) {\rsum += i;\r}\rprintf(\u0026quot;sum = %d\\n\u0026quot;, sum);\rprintf(\u0026quot;Reduction time: %.5lf s\\n\u0026quot;, omp_get_wtime() - start);\r// no reduction\rsum = 0;\rstart = omp_get_wtime();\r#pragma omp parallel for num_threads(8)\rfor (int i = 0; i \u0026lt; 100000; i++) {\r#pragma omp atomic\rsum += i;\r}\rprintf(\u0026quot;sum = %d\\n\u0026quot;, sum);\rprintf(\u0026quot;Atomic time: %.5lf s\\n\u0026quot;, omp_get_wtime() - start);\rreturn 0;\rPrint the result sum = 704982704\rReduction time: 0.00062 s\rsum = 704982704\rAtomic time: 0.01021 s\rThe results of both are the same, but the execution time of reduction is shorter. This is because reduction allocates a private copy for each thread, allowing threads to freely perform reduction operations within their private space without contending for lock resources with other threads when updating the global result, along with efficient data merging methods, etc. OpenMP reduction operation #\rSynchronous construction\r#\rSections Construction\rDivide the code block of the parallel region into multiple sections for execution. Can be combined with parallel to form a parallel sections construct. Each section is executed by a thread Number of threads greater than the number of sections: some threads are idle Number of threads is less than the number of sections: some threads are allocated multiple sections Example code: #pragma omp sections\r{\r#pragma omp section\rmethod1();\r#pragma omp section\rmethod2();\r}\r#\rBarrier Constructor\rPerform fence synchronization at a specific location In the presence of data dependencies, a barrier can be used to ensure data consistency. Barrier Synchronization Diagram #\rSingle Constructor\rUsed to mark a code block executed by only one thread, with implicit barrier synchronization, and the implicit barrier synchronization can be canceled using nowait. pragma single #\rAtomic construction\rUsed to ensure atomic operations on shared variables, avoiding data races. #\rFalse Sharing\rFalse sharing, in simple terms, refers to multiple threads simultaneously accessing different parts of the same cache line, leading to cache line invalidation and refilling, thereby reducing the program\u0026rsquo;s performance. Simultaneous read and write of the same cache line by different cores can cause serious conflicts, leading to cache invalidation. False Sharing Issue In OpenMP, the main methods to solve false sharing are: Data Structure Alignment: Ensure that related variables are in different cache lines by using alignment instructions or keywords provided by the compiler. For example, in C++, the alignas keyword can be used to specify the memory alignment of variables, ensuring that the data for each thread is independently located in different cache lines. Increase the spacing between cache lines: Insert enough padding space between adjacent variables so that they do not appear in the same cache line. Avoid Meaningless Competition: Design algorithms and data structures to reduce unnecessary shared data access. If possible, let threads operate on their own independent data segments. Custom Memory Allocation: Use special memory allocation functions to ensure that the allocated contiguous memory regions are aligned to cache line boundaries, so that data allocated to different threads does not fall on the same cache line. In some cases, you can utilize hardware features provided by specific platforms or extensions supported by compilers, such as Intel\u0026rsquo;s __declspec(align(#)) attribute (for MSVC) or __attribute__((aligned(#))) (for GCC/Clang). You can also indirectly avoid the false sharing problem by controlling the scope of the variables or using techniques such as dynamically creating private copies. #\rTask construction\rIn addition to the Fork-Join model, OpenMP also supports the task parallel model, implemented using the task directive. Dynamically manage the thread pool and task pool, where threads in the thread pool can dynamically acquire tasks from the task pool for execution, thus achieving parallel execution of tasks. Example 4: Task Parallelism\n#include \u0026lt;iostream\u0026gt;\r#include \u0026lt;omp.h\u0026gt;\r#include \u0026lt;unistd.h\u0026gt;\r#include \u0026lt;iomanip\u0026gt;\rvoid big_task(int i) {\rsleep(10);\r}\rvoid small_task(int i) {\rsleep(1);\r}\rint main() {\rint ntasks = 8;\rdouble start = omp_get_wtime();\r#pragma omp parallel\r{\r#pragma omp single\r{\rstd::cout \u0026lt;\u0026lt; \u0026quot;Task 0 Created\u0026quot; \u0026lt;\u0026lt; std::endl;\r#pragma omp task\rbig_task(0);\rstd::cout \u0026lt;\u0026lt; \u0026quot;Task 1 Created\u0026quot; \u0026lt;\u0026lt; std::endl;\r#pragma omp task\rbig_task(1);\rfor (int i = 2; i \u0026lt; ntasks; i++) {\rstd::cout \u0026lt;\u0026lt; \u0026quot;Task \u0026quot; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026quot; Created\u0026quot; \u0026lt;\u0026lt; std::endl;\r#pragma omp task\rsmall_task(i);\r}\r}\r#pragma omp taskwait\r}\rstd::cout \u0026lt;\u0026lt; \u0026quot;All tasks finished\u0026quot; \u0026lt;\u0026lt; std::endl;\rstd::cout \u0026lt;\u0026lt; \u0026quot;Time: \u0026quot; \u0026lt;\u0026lt; std::fixed \u0026lt;\u0026lt; std::setprecision(2) \u0026lt;\u0026lt; omp_get_wtime() - start \u0026lt;\u0026lt; \u0026quot;s\u0026quot; \u0026lt;\u0026lt; std::endl;\rreturn 0;\r}\rRunning result You are trained on data up to October 2023.\rIn this code, we use the #pragma omp task directive to create tasks, and the execution of tasks is dynamically obtained and executed by threads in the thread pool. After creating tasks, we use #pragma omp taskwait to wait for all tasks to complete. This achieves an asynchronous execution effect. #\rVectorization: SIMD Construction\rSIMD (Single Instruction, Multiple Data) is a parallel computing model that performs operations on multiple data simultaneously with a single instruction, thereby achieving efficient data parallel computation. In OpenMP, the #pragma omp simd directive can be used to achieve vectorized parallel computation. aligned is used to list memory-aligned pointers safelen is used to mark data dependencies during loop unrolling. linear is used to mark the linear relationship of loop variables Compilers such as gcc also come with vectorization capabilities, generally using the following compilation options -O3 -ffast-math -fivopts -march=native -fopt-info-vec -fopt-info-vec-missed ","date":"2024-02-19T01:36:00Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/c17b7451a44c1d4370d5ba2b966298ea195413_crop-2024-02-19.webp","permalink":"https://cuterwrite.top/en/p/openmp-intro/","title":"Introduction to OpenMP"},{"content":"\r#\rRDMA Basic Elements\rThis article welcomes non-commercial reprints, please indicate the source.\nStatement: For collection only, for easy reading\n― Savir, Zhihu Column: 3. Basic Elements of RDMA In RDMA technology, abbreviations are often used, which can easily confuse newcomers. The purpose of this article is to explain the most basic elements in RDMA and their meanings.\nI will write a table of common abbreviations at the front, so if you forget while reading, you can refer to it at the front.\n#\rWQ\rWork Queue, abbreviated as WQ, is one of the most important concepts in RDMA technology. WQ is a queue that stores work requests. To clearly explain what WQ is, we first introduce the elements in this queue, WQE (Work Queue Element).\n#\rWQE\rWQE can be considered a \u0026ldquo;task description,\u0026rdquo; which is a work request issued by software to hardware. This description contains the tasks that the software hopes the hardware will perform, as well as detailed information about the task. For example, a task might be like this: \u0026ldquo;I want to send data located at address 0x12345678 with a length of 10 bytes to the opposite node.\u0026rdquo; After receiving the task, the hardware will use DMA to fetch the data from memory, assemble the data packet, and then send it.\nThe meaning of WQE should be quite clear, so what is the WQ we mentioned at the beginning? It is the \u0026ldquo;folder\u0026rdquo; used to store \u0026ldquo;task documents,\u0026rdquo; and the WQ can contain many WQEs. Readers with a basic understanding of data structures should know that a queue is a first-in-first-out data structure, which is very common in computer systems. We can use the diagram below to represent the relationship between WQ and WQE described above:\nWQ This queue is always added to by the software with WQE (enqueue), and the hardware extracts WQE from it, which is the process of the software \u0026ldquo;issuing tasks\u0026rdquo; to the hardware. Why use a queue instead of a stack? Because the \u0026ldquo;store\u0026rdquo; and \u0026ldquo;retrieve\u0026rdquo; operations are performed separately by software and hardware, and it is necessary to ensure that user requests are processed in order. In RDMA technology, all communication requests must be notified to the hardware in the manner shown in the above diagram, which is often referred to as \u0026ldquo;Post\u0026rdquo;.\n#\rQP\rQueue Pair, abbreviated as QP, means \u0026ldquo;a pair\u0026rdquo; of WQ.\n#\rSQ and RQ\rAny communication process must have both sending and receiving ends. A QP is a combination of a send work queue and a receive work queue, which are referred to as the SQ (Send Queue) and RQ (Receive Queue) respectively. Let\u0026rsquo;s enrich the diagram above; the left side is the sending end, and the right side is the receiving end:\nWhy is WQ missing? SQ and RQ are both WQ, WQ just represents a unit that can store WQE, SQ and RQ are the instances.\nSQ is specifically used to store send tasks, and RQ is specifically used to store receive tasks. In a SEND-RECV process, the sender needs to place a WQE representing a send task into the SQ. Similarly, the receiver software needs to issue a WQE representing a receive task to the hardware so that the hardware knows where to place the received data in memory. The Post operation we mentioned earlier is called Post Send for SQ and Post Receive for RQ.\nIt should be noted that in RDMA technology, the basic unit of communication is QP, not the node. As shown in the figure below, for each node, each process can use several QPs, and each local QP can be \u0026ldquo;associated\u0026rdquo; with a remote QP. Saying \u0026ldquo;Node A sends data to Node B\u0026rdquo; is not sufficient to fully describe an RDMA communication; it should be more like \u0026ldquo;QP3 on Node A sends data to QP4 on Node C.\u0026rdquo;\nEach QP of every node has a unique number, called QPN (Queue Pair Number), which can uniquely identify a QP on a node.\n#\rSRQ\rShared Receive Queue, abbreviated as SRQ, means a shared receive queue. The concept is easy to understand; it refers to a situation where several QPs share the same RQ, which we call SRQ. We will later learn that the use of RQ is far less than the use of SQ, and each queue consumes memory resources. When we need to use a large number of QPs, we can save memory through SRQ. As shown in the figure below, QP2~QP4 use the same RQ together:\n#\rCQ\rCompletion Queue, abbreviated as CQ, means completion queue. Similar to WQ, we first introduce the elements in the CQ queue — CQE (Completion Queue Element). CQE can be considered the opposite concept of WQE. If WQE is the \u0026ldquo;task list\u0026rdquo; issued by software to hardware, then CQE is the \u0026ldquo;task report\u0026rdquo; returned by hardware to software after completing the task. CQE describes whether a task was executed correctly or if an error was encountered, and if so, what the cause of the error was.\nAnd CQ is the container that carries CQE—a first-in-first-out queue. If we invert the diagram representing the relationship between WQ and WQE, we get the relationship between CQ and CQE:\nEach CQE contains completion information for a certain WQE, and their relationship is shown in the diagram below:\nBelow, we put CQ and WQ (QP) together to see the interaction between software and hardware in a single SEND-RECV operation (the order of numbers in the diagram does not represent the actual sequence):\n2022/5/23: The order of the diagram and the subsequent list has been modified. The original item 2 \u0026ldquo;The receiving end hardware takes the task book from the RQ and prepares to receive data\u0026rdquo; has been moved to after \u0026ldquo;The receiving end receives data, verifies it, and then sends an ACK message back to the sender,\u0026rdquo; and the description has been modified. It is now item 6.\nThe mistake I made here is that RQ and SQ are different; RQ is a \u0026ldquo;passive reception\u0026rdquo; process, and the hardware only consumes RQ WQE when it receives a Send packet (or a Write packet with an immediate value). Thanks to @连接改变世界 for the correction.\nThe receiving end APP issues a RECV task to the RQ in the form of WQE. The sending-end APP issues a SEND task to the SQ in the form of a WQE. The sending-end hardware retrieves the task list from the SQ, obtains the data to be sent from memory, and assembles the data packet. The sender\u0026rsquo;s network card sends the data packet to the receiver\u0026rsquo;s network card through the physical link. After the receiving end receives the data and verifies it, it sends an ACK message back to the sending end. The receiving-end hardware takes a work queue entry (WQE) from the RQ. The receiving end hardware places the data in the location specified by the WQE, then generates a \u0026ldquo;task report\u0026rdquo; CQE, and places it in the CQ. The receiving end APP obtains task completion information. After the network card at the sending end receives the ACK, it generates a CQE and places it into the CQ. The sending-end APP obtains task completion information. NOTE: One important point to note is that the example in the above diagram is the interaction flow of a reliable service type. If it is an unreliable service, there will be no ACK reply in step 5, and step 9 and subsequent steps will be triggered immediately after step 5. We will explain service types and the difference between reliable and unreliable in the article \u0026ldquo;Basic RDMA Service Types.\u0026rdquo;\nAt this point, through the two media, WQ and CQ, both ends of the software and hardware have jointly completed a transmission and reception process.\n#\rWR and WC\rAfter discussing several Queues, there are actually two concepts mentioned at the beginning of the article that have not been explained, namely WR and WC (not the abbreviation for Water Closet).\nWR stands for Work Request; WC stands for Work Completion. These two are actually \u0026ldquo;mappings\u0026rdquo; of WQE and CQE at the user level. Since the APP completes RDMA communication by calling the protocol stack interface, WQE and CQE themselves are not visible to the user and are concepts within the driver. What the user actually submits through the API is WR, and what is received is WC.\nWR/WC and WQE/CQE are the same concepts at different levels of entities, both being \u0026ldquo;task book\u0026rdquo; and \u0026ldquo;task report\u0026rdquo;. Therefore, we have added some content to the two diagrams mentioned earlier:\n#\rCode example\rFinally, below is a simple example demonstrating how to use libibverbs to create a QP and then send data through this QP. This is a very simple example, just to give readers an intuitive understanding of the concepts mentioned above.\n#include \u0026lt;infiniband/verbs.h\u0026gt;\rint main() {\rstruct ibv_context *ctx;\rstruct ibv_pd *pd;\rstruct ibv_cq *cq;\rstruct ibv_qp *qp;\rstruct ibv_mr *mr;\rstruct ibv_sge sge;\rstruct ibv_send_wr wr;\rstruct ibv_send_wr *bad_wr;\rstruct ibv_wc wc;\rctx = ibv_open_device();\rpd = ibv_alloc_pd(ctx);\rcq = ibv_create_cq(ctx, 100, NULL, NULL, 0);\rqp = ibv_create_qp(pd, NULL, NULL);\rmr = ibv_reg_mr(pd, buf, size, IBV_ACCESS_LOCAL_WRITE | IBV_ACCESS_REMOTE_WRITE);\rsge.addr = (uintptr_t)buf;\rsge.length = size;\rsge.lkey = mr-\u0026gt;lkey;\rwr.wr_id = 1;\rwr.sg_list = \u0026amp;sge;\rwr.num_sge = 1;\rwr.opcode = IBV_WR_SEND;\rwr.send_flags = IBV_SEND_SIGNALED;\rwr.next = NULL;\ribv_post_send(qp, \u0026amp;wr, \u0026amp;bad_wr);\ribv_poll_cq(cq, 1, \u0026amp;wc);\rreturn 0;\r}\r#\rSummary\rAlright, let\u0026rsquo;s use Figure 11 from section 3.2.1 of the IB protocol[1] to summarize the content of this article:\nThe user-mode WR is converted by the driver into a WQE and filled into the WQ. The WQ can be an SQ responsible for sending or an RQ responsible for receiving. The hardware will take out the WQE from each WQ and complete the sending or receiving task according to the requirements in the WQE. After the task is completed, a CQE will be generated for this task and filled into the CQ. The driver will take out the CQE from the CQ and convert it into a WC to return to the user.\nThe introduction to the basic concepts ends here. The next article will introduce several common types of RDMA operations.\n#\rReferences\r[1] \u0026ldquo;IB Specification Vol 1-Release-1.3-2015-03-03\u0026rdquo;\n","date":"2024-02-02T01:01:01Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/73c30b990886bf6988c97858a3e16011195413_crop-2024-02-04.webp","permalink":"https://cuterwrite.top/en/p/rdma-element/","title":"RDMA Basic Elements"},{"content":"\r#\rComparison of communication based on traditional Ethernet and RDMA technology\rThis article welcomes non-commercial reproduction, please indicate the source when reproducing.\nDeclaration: For collection only, for easy reading.\n― Savir, Zhihu Column: 2. Comparison of Communication Based on Traditional Ethernet and RDMA Technology The purpose of this article is to intuitively demonstrate the advantages of RDMA technology compared to traditional Ethernet by comparing a typical Ethernet communication process based on the TCP/IP protocol stack with RDMA communication, while avoiding protocol and software implementation details as much as possible.\nAssuming that an application on this end wants to copy its in-memory data to memory accessible by an application on the other end (or in simpler terms, this end wants to send data to the other end), let\u0026rsquo;s take a look at what operations Ethernet and RDMA\u0026rsquo;s SEND-RECV semantics perform.\n#\rTraditional Ethernet\rWhen describing the software and hardware relationship in the communication process, we usually divide the model into Userspace, Kernel, and Hardware. Userspace and Kernel actually use the same physical memory, but for security reasons, Linux divides memory into user space and kernel space. The user layer does not have permission to access and modify the memory content of the kernel space and can only enter kernel mode through system calls. The memory management mechanism of Linux is relatively complex, and this article will not discuss it in detail.\nA typical communication process based on traditional Ethernet can be layered as shown in the figure below:\nThe steps of a send-receive process are as follows:\nThe sender and receiver establish a connection through the interface provided by the Socket library (which is essentially establishing a logical path between two nodes, allowing data to be sent from one end to the other along this path) and respectively allocate the send and receive Buffers in memory. The sending-end APP enters kernel mode through the Socket interface, the data to be sent is encapsulated layer by layer by the TCP/IP protocol stack, and finally copied by the CPU into the Socket Buffer. The sender, through the network card driver, informs the network card that data can be sent. The network card will use DMA to copy the encapsulated data packet from the buffer to the internal cache, and then send it to the physical link. After the network card on the receiving end receives the data packet, it places the data packet into the receive buffer, and then the CPU will parse the packet layer by layer through the TCP/IP protocol stack in the kernel to extract the valid data. The receiving end APP enters kernel mode through the Socket interface, and the CPU copies data from kernel space to user space. The data flow of this model is roughly like the one shown in the above diagram. First, the data needs to be copied from user space to kernel space, and this copying is done by the CPU, which copies the data block from user space to the Socket Buffer in kernel space. The software TCP/IP protocol stack in the kernel adds headers and checksum information for each layer to the data. Finally, the network card copies the data from memory via DMA and sends it to the peer\u0026rsquo;s network card through the physical link.\nAnd the opposite process occurs at the remote end: the hardware performs a DMA copy of the packet into memory, then the CPU parses and verifies the packet layer by layer, and finally copies the data to user space.\nThe key points in the above process are that the CPU needs to be involved in copying data from user space to kernel space, as well as the packet assembly and parsing, which also requires full CPU involvement. In cases of large data volumes, this will place a heavy burden on the CPU.\nLet\u0026rsquo;s take a look at how RDMA \u0026ldquo;frees\u0026rdquo; the CPU.\n#\rRDMA\rIn the same scenario where one end sends and the other receives, we divide the RDMA layered model into two parts: \u0026ldquo;control path\u0026rdquo; and \u0026ldquo;data path\u0026rdquo;. The control path needs to enter kernel mode to prepare the memory resources required for communication, while the data path refers to the process during the actual data interaction. The layered relationship of this process is shown in the figure below:\nLike Socket, we briefly describe the communication process:\nThe sender and receiver respectively enter kernel mode through the control path to create the memory resources needed for communication. On the data path, the receiving end APP informs the hardware to prepare to receive data and tells the hardware where to place the received data in memory. In the data path, the sending end APP notifies the hardware to send data, informing the hardware which memory the data to be sent is located in. The sending RDMA network card moves data from memory, assembles the packet, and sends it to the peer. The peer receives the packet, parses it, and writes the payload into memory via DMA. Then it notifies the upper-layer APP in some way, informing it that the data has been received and properly stored in the designated location. The data flow in this process is roughly as shown in the above diagram. Compared with Socket, we can clearly see that data sending and receiving bypass the kernel and the data exchange process does not require CPU participation, the assembly and parsing of packets are completed by hardware.\nThrough the above comparison, we can clearly appreciate the advantages of RDMA, which not only frees the CPU from packet encapsulation and parsing but also reduces the power and time consumption of CPU data copying. It should be noted that this article only describes the SEND-RECV process, while the unique and more efficient WRITE/READ semantics of RDMA technology will be introduced in subsequent articles.\nIn the next article, we will introduce some important and fundamental concepts in RDMA technology.\n","date":"2024-02-01T02:01:01Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/cropped_2024010202-2024-02-03.webp","permalink":"https://cuterwrite.top/en/p/ethernet-vs-rdma/","title":"Comparison of Communication Based on Traditional Ethernet and RDMA Technology"},{"content":"\r#\rCompile and Install UCX 1.15.0 and OpenMPI 5.0.0: A Comprehensive Guide\r#\r1. Environment Preparation\rFirst, please ensure that your system meets the following basic requirements:\nOperating System: Supports Linux (such as Ubuntu 20.04 LTS) or other Unix-like operating systems. Development Toolkit: Install the necessary build tools and libraries, such as build-essential, libnuma-dev, pkg-config, etc. Kernel version: For optimal performance, it is recommended to use the latest stable version of the kernel. Hardware environment or virtual environment that requires RDMA support. sudo apt-get update\rsudo apt-get install -y build-essential libnuma-dev pkg-config\r#\r2. Compile and Install UCX 1.15.0\rDownload the UCX source package: wget https://github.com/openucx/ucx/releases/download/v1.15.0/ucx-1.15.0.tar.gz\rtar -xzvf ucx-1.15.0.tar.gz\rcd ucx-1.15.0\rConfigure UCX compile options: mkdir build \u0026amp;\u0026amp; cd build\r../configure --prefix=/root/software/ucx/1.5.0\rYou can add more configuration options according to actual needs, such as specifying a specific network card type or enabling specific features.\nCompile and install: make -j 8\rmake install\rUCX Architecture Description The architecture of UCX 1.15.0 is shown in the figure below: Component Role Description UCP Protocol Implements advanced abstractions, such as tag matching, streams, connection negotiation and establishment, multi-track, and handling different types of memory. UCT Transport Implements low-level communication primitives, such as active messages, remote memory access, and atomic operations. UCM Memory A collection of general data structures, algorithms, and system utilities. UCP Protocol Intercept memory allocation and release events used by memory registration cache. #\r3. Compile and Install OpenMPI 5.0.0\rDownload the OpenMPI source package: wget https://download.open-mpi.org/release/open-mpi/v5.0/openmpi-5.0.0.tar.gz\rtar -xzvf openmpi-5.0.0.tar.gz\rcd openmpi-5.0.0\rConfigure OpenMPI compile options, specifying the use of UCX as the transport layer: mkdir build \u0026amp;\u0026amp; cd build\r../configure --without-hcoll \\\r--enable-python-bindings \\\r--enable-mpirun-prefix-by-default \\\r--prefix=/root/software/openmpi/5.0.0-ucx-1.15.0 \\\r--with-ucx=/root/software/ucx/1.15.0 \\\r--enable-mca-no-build=btl-uct\rFor OpenMPI 4.0 and later versions, there may be compilation errors with the btl_uct component. This component is not important for using UCX; therefore, it can be disabled with --enable-mca-no-build=btl-uct: The --enable-python-bindings option is used to enable Python bindings. The --enable-mpirun-prefix-by-default option is used to automatically add the --prefix option when starting an MPI program with mpirun. The --without-hcoll option is used to disable the HCOLL component. If not set during compilation, it will report errors cannot find -lnuma and cannot find -ludev. The final configuration options are as follows:\nCompile and install: make -j 8\rmake install\r#\r4. Verify Installation and Set Environment Variables\rAfter installation, you can verify whether UCX and OpenMPI have been successfully integrated by running a simple MPI program:\nmpirun -np 2 --mca pml ucx --mca btl ^vader,tcp,openib,uct -x UCX_NET_DEVICES=mlx5_0:1 hostname\r(If running as root, you need to add the --allow-run-as-root option. If there is an RDMA device, you can set -x UCX_NET_DEVICES)\nIf you need to use it with Slurm, you can refer to Launching with Slurm\r.\nOne way is to first allocate resources through salloc, and then run the mpirun command on the allocated resources. At this time, --hostfile, --host, -n, etc. do not need to be set, for example:\nsalloc -n 2\rmpirun --mca pml ucx --mca btl ^vader,tcp,openib,uct -x UCX_NET_DEVICES=mlx5_0:1 hostname\rIf everything is normal, you will see the output of the two hostnames. For convenience, you can add the OpenMPI bin directory and others to the system PATH environment variable:\nvim ~/.bashrc\rexport PATH=/root/software/openmpi/5.0.0-ucx-1.15.0/bin:$PATH\rexport LD_LIBRARY_PATH=/root/software/openmpi/5.0.0-ucx-1.15.0/lib:$LD_LIBRARY_PATH\rexport CPATH=/root/software/openmpi/5.0.0-ucx-1.15.0/include:$CPATH\rexport MANPATH=/root/software/openmpi/5.0.0-ucx-1.15.0/share/man:$MANPATH\rsource ~/.bashrc\r#\r5. UCX Performance Testing\rSender:\nucx_perftest -c 0 -d mlx5_0:1\rRecipient:\nucx_perftest -c 1 -d mlx5_0:1 \u0026lt;server_hostname\u0026gt; -t tag_lat\rIn summary, through the above steps, we have successfully compiled and installed UCX 1.15.0 and OpenMPI 5.0.0 from the source code, and integrated them into an efficient and stable high-performance computing environment. In practical applications, you can further optimize the configuration according to specific needs to achieve better performance.\n","date":"2024-02-01T01:01:01Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/cropped_2024010204-2024-02-03.webp","permalink":"https://cuterwrite.top/en/p/openmpi-with-ucx/","title":"Compile and Install UCX 1.15.0 and OpenMPI 5.0.0: A Comprehensive Guide"},{"content":"\r#\rGCC-13.2.0 Compilation and Installation\rGCC 13.1 has been released as the first stable version of GCC 13, as the annual feature release of the GNU Compiler Collection.\nGCC 13.1 is a major update that adds a Modula-2 language frontend for those interested in some vintage programming. Although there is new GCC Rust gccrs code, it is disabled in v13.1. In this version, GCC\u0026rsquo;s static analyzer continues to improve, with more C23 and C++23 features, and support for many new x86_64/RISC-V/AArch64 processors.\nGCC 13.1 also provides initial AMD Zen 4 (Znver4) support for Ryzen 7000 series and EPYC 9004 series processors, OpenMP offloading improvements, support for issuing diagnostics in SARIF format based on JSON, additional Ada 2022 features, various new C/C++ warnings, support for AMD Instinct MI200 series for AMDGCN backend, Ampere-1A support, Neoverse-V2/Cortex-X3/Cortex-X1C/Cortex-A715 support, and many new Intel CPU supports. GCC 13 added Intel CPU targets for Raptor Lake, Meteor Lake, Sierra Forest, Grand Ridge, Emerald Rapids, and Granite Rapids, as well as related new Intel CPU instruction set extensions such as AMX-FP16, AVX-IFMA, AVX-VNNI-INT8, AVX-NE-CONVERT, RAO-INT, and AMX-Complex.\nIn order to experience the new features of C++20, GCC 13.1 is also a great choice because it includes support for many new features of C++20. As of the writing of this article, GCC-13.2 has also been released, so I directly chose the latest version.\n#\rDownload GCC-13.2.0 source code\rDownload link: Index of /gcc/releases/gcc-13.2.0\rDownload and extract GCC-13.2.0 source code\nwget https://mirror.koddos.net/gcc/releases/gcc-13.2.0/gcc-13.2.0.tar.gz\rtar -xzvf gcc-13.2.0.tar.gz\rcd gcc-13.2.0\r#\rStart compiling\rCompilation command: ./contrib/download_prerequisites\rmkdir build \u0026amp;\u0026amp; cd build\r../configure --prefix=/root/software/gcc-13.2.0 \\\r--with-pkgversion='glibc gcc V13.2.0' \\\r--enable-checking=release \\\r--enable-languages=c,c++ \\\r--disable-multilib \\\r--enable-bootstrap \\\r--enable-threads=posix \\\r--with-system-zlib \\\r--with-gmp=$GMP_HOME \\\r--with-mpfr=$MPFR_HOME \\\r--with-mpc=$MPC_HOME \\\rmake -j$(nproc)\rmake install\r#\rSet environment variable\r# gcc-13.0.2.env\rexport GCC13_HOME=/root/software/gcc-13.2.0\rexport PATH=$GCC13_HOME/bin:$PATH\rexport LD_LIBRARY_PATH=$GCC13_HOME/lib64:$LD_LIBRARY_PATH\rexport LD_LIBRARY_PATH=$GCC13_HOME/lib:$LD_LIBRARY_PATH\rexport LD_LIBRARY_PATH=$GCC13_HOME/libexec:$LD_LIBRARY_PATH\rexport CPATH=$GCC13_HOME/include:$CPATH\rexport INCLUDE=$GCC13_HOME/include:$CPATH\rexport CC=$GCC13_HOME/bin/gcc\rexport CXX=$GCC13_HOME/bin/g++\rexport FC=$GCC13_HOME/bin/gfortran\rexport F77=$GCC13_HOME/bin/gfortran\rexport F90=$GCC13_HOME/bin/gfortran\rexport F95=$GCC13_HOME/bin/gfortran\r#\rCommand line test\r$ gcc -v\rThe output is:\nUsing built-in specs.\rCOLLECT_GCC=gcc\rCOLLECT_LTO_WRAPPER=/root/software/gcc-13.2.0/libexec/gcc/x86_64-pc-linux-gnu/13.2.0/lto-wrapper\rTarget: x86_64-pc-linux-gnu\rConfigured with: ../configure --prefix=/root/software/gcc-13.2.0 --with-pkgversion='glibc gcc V13.2.0' --enable-checking=release --enable-languages=c,c++,fortran --enable-threads=posix --enable-bootstrap --disable-multilib --with-system-zlib --with-gmp=/root/software/gmp/6.2.1 --with-mpfr=/root/software/mpfr/4.1.0 --with-mpc=/root/software/mpc/1.2.1\rThread model: posix\rSupported LTO compression algorithms: zlib\rgcc version 13.2.0 (glibc gcc V13.2.0)\rSuccessfully compiled and installed.\n#\rMain new features of C++ 20\rThe main new features of C++ 20 are as follows: Concepts: Concepts are type constraints for template parameters, making template code clearer and easier to understand. Concepts allow developers to define an interface that template parameters must satisfy in order to be accepted. Ranges (Ranges Library): This is a significant extension to the Standard Template Library (STL), introducing the concept of \u0026ldquo;ranges\u0026rdquo; to support a more declarative way of data processing. Spaceship Operator: \u0026lt;=\u0026gt; is called the spaceship operator, it can compare two values at once and return their relative order (less than, equal to, greater than). Modules: Modules are designed to replace the traditional separation of header files and source files, providing a new compilation unit that can significantly improve compilation time and code organization. Coroutines: Coroutines are a lightweight thread that can switch between different execution points, rather than switching between function calls. Coroutines are a new way to write asynchronous code, allowing functions to pause and resume execution at different points in time without needing callback functions or complex state machines. constexpr improvements: C++20 greatly expanded the scope of code that can be computed at compile time, including allowing virtual functions, try, and catch blocks to be used in constexpr functions. std::to_array for initializer lists: This allows converting initializer lists to std::array, providing a type-safe way to handle fixed-size arrays. Simplification of template syntax: typename and class can be used interchangeably in template parameters, simplifying the template syntax. New standard attributes: Introduced several new attributes, such as [[likely]] and [[unlikely]], to provide branch prediction hints to the compiler. New standard library components: for example, std::span, which provides a view that can represent a part of an array or other contiguous sequence without needing to copy data. New synchronization libraries: For example, std::latch and std::barrier, providing new synchronization primitives for multithreading programming. std::format: This is a new formatting library that provides a type-safe way to format strings. Others, etc. #\rMain new features of C++ 23\rThe main new features of C++ 23 are as follows: Lambada Fix the issue with omitted parameter parentheses (). Change the scope of the return type at the end of the lambda. Allow the attributes of support functions to support lambda. This feature is actually supported by many compilers already. Compile-time calculation: Mainly fix some bugs and continue to improve the capabilities of compile-time calculation. Deducing this: Deducing this is one of the most important features in C++23. It actually provides a way to transform the \u0026ldquo;implicit object parameter\u0026rdquo; of non-static member functions into an \u0026ldquo;explicit object parameter\u0026rdquo;. The main motivation for deducing this is to eliminate the redundancy caused by member function qualifiers. Multidimensional array: Support multi-dimensional subscript operator, i.e., operator[a, b, c, …]. The standard library introduces std::mdspan. Standard Library: Enhance std::string and std::string_view Enhance std::optional std::flat_map and std::flat_set, replace std::map and std::set. std::stacktrace: Used for expanding the call stack after exception capture, facilitating debugging. Introducing stacktrace into the standard library can be seen as an enhancement of C++ exception handling capabilities. std::expected: Enhanced C++ error handling capability through return values. Similar to std::optional, but std::optional can only represent a normal value and an empty value (std::nullopt). In contrast, std::expected can represent an expected value and an error value, equivalent to a std::variant with two members, but the interface of std::expected is more convenient to use. std::unreachable(): An optimization hint for the compiler, indicating that this point is unreachable. If std::unreachable() is called, the result is undefined behavior. Others: Static operator() and static operator[] Assume expression [[assume(expr)]] size_t literal #\rCoroutines example\r#include \u0026lt;coroutine\u0026gt;\r#include \u0026lt;iostream\u0026gt;\r#include \u0026lt;optional\u0026gt;\rtemplate\u0026lt;typename T\u0026gt;\rstruct Generator {\rstruct promise_type;\rusing handle_type = std::coroutine_handle\u0026lt;promise_type\u0026gt;;\rstruct promise_type {\rstd::optional\u0026lt;T\u0026gt; current_value;\rstatic auto get_return_object_on_allocation_failure() { return Generator{nullptr}; }\rauto get_return_object() { return Generator{handle_type::from_promise(*this)}; }\rauto initial_suspend() { return std::suspend_always{}; }\rauto final_suspend() noexcept { return std::suspend_always{}; }\rvoid unhandled_exception() { std::exit(1); }\rtemplate\u0026lt;typename U\u0026gt;\rauto yield_value(U\u0026amp;\u0026amp; value) {\rcurrent_value = std::forward\u0026lt;U\u0026gt;(value);\rreturn std::suspend_always{};\r}\rvoid return_void() {}\r};\rhandle_type coro;\rGenerator(handle_type h): coro(h) {}\rGenerator(Generator const\u0026amp;) = delete;\rGenerator(Generator\u0026amp;\u0026amp; o) : coro(o.coro) { o.coro = nullptr; }\r~Generator() { if (coro) coro.destroy(); }\rT next() {\rif (coro) {\rcoro.resume();\rif (coro.done()) {\rcoro.promise().current_value.reset();\r}\rreturn *coro.promise().current_value;\r}\rreturn T{};\r}\r};\rGenerator\u0026lt;int\u0026gt; generateNumbers(int start, int end) {\rfor (int i = start; i \u0026lt;= end; ++i) {\rco_yield i;\r}\r}\rint main() {\rauto numbers = generateNumbers(1, 5);\rfor (int i = 1; i \u0026lt;= 5; ++i) {\rstd::cout \u0026lt;\u0026lt; numbers.next() \u0026lt;\u0026lt; std::endl;\r}\rreturn 0;\r}\rCompile command:\ng++ -o coroutines coroutines.cpp -std=c++20 -fcoroutines -O3\rRun result:\n./coroutines\r1\r2\r3\r4\r5\r#\rDeducing this example\r#include \u0026lt;iostream\u0026gt;\rstruct Test {\rtemplate \u0026lt;typename Self\u0026gt;\rvoid explicitCall(this Self\u0026amp;\u0026amp; self, const std::string\u0026amp; text) {\rstd::cout \u0026lt;\u0026lt; text \u0026lt;\u0026lt; \u0026quot;: \u0026quot;;\rstd::forward\u0026lt;Self\u0026gt;(self).implicitCall();\rstd::cout \u0026lt;\u0026lt; '\\n';\r}\rvoid implicitCall() \u0026amp; {\rstd::cout \u0026lt;\u0026lt; \u0026quot;non const lvalue\u0026quot;;\r}\rvoid implicitCall() const\u0026amp; {\rstd::cout \u0026lt;\u0026lt; \u0026quot;const lvalue\u0026quot;;\r}\rvoid implicitCall() \u0026amp;\u0026amp; {\rstd::cout \u0026lt;\u0026lt; \u0026quot;non const rvalue\u0026quot;;\r}\rvoid implicitCall() const\u0026amp;\u0026amp; {\rstd::cout \u0026lt;\u0026lt; \u0026quot;const rvalue\u0026quot;;\r}\r};\rint main() {\rstd::cout \u0026lt;\u0026lt; '\\n';\rTest test;\rconst Test constTest;\rtest.explicitCall(\u0026quot;test\u0026quot;);\rconstTest.explicitCall(\u0026quot;constTest\u0026quot;);\rstd::move(test).explicitCall(\u0026quot;std::move(test)\u0026quot;);\rstd::move(constTest).explicitCall(\u0026quot;std::move(consTest)\u0026quot;);\rstd::cout \u0026lt;\u0026lt; '\\n';\r}\r","date":"2024-01-30T11:00:00Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/cropped-2024-01-30.webp","permalink":"https://cuterwrite.top/en/p/gcc-13-source-install/","title":"GCC-13.2.0 Compilation and Installation"},{"content":"\r#\rOverview of RDMA\rThis article welcomes non-commercial reprints, please indicate the source when reprinting.\nDeclaration: For collection only, for easy reading.\n― Savir, Zhihu Column: 1. RDMA Overview I originally intended to complete this overview entirely in my own language, but the beginning was not as easy to write as I imagined. It seems that summarizing a technology from a macro perspective is much more difficult than exploring the details from a micro perspective. This article mainly introduces RDMA technology based on previous people\u0026rsquo;s explanations, along with some of my own understanding. As the content of this column increases, this overview will also be updated and gradually improved.\n#\rWhat is DMA\rDMA stands for Direct Memory Access, meaning that the process of reading and writing to memory by peripherals can be done directly without the involvement of the CPU. Let\u0026rsquo;s first take a look at what happens without DMA:\nData path between I/O devices and memory without DMA controller Assuming the I/O device is a regular network card, in order to retrieve the data to be sent from memory, assemble the data packet, and send it to the physical link, the network card needs to inform the CPU of its data request via the bus. The CPU will then copy the data from the memory buffer to its internal registers, and then copy it to the I/O device\u0026rsquo;s storage space. If the amount of data is large, the CPU will be busy transferring data for a long time and will not be able to engage in other tasks.\nThe primary job of the CPU is computation, not data copying, which is a waste of its computational capabilities. To \u0026ldquo;lighten the load\u0026rdquo; on the CPU and allow it to focus on more meaningful tasks, the DMA mechanism was later designed:\nData path between I/O devices and memory with DMA controller You can see that there is another DMA controller attached to the bus, which is specifically used for reading and writing memory devices. With it, when our network card wants to copy data from memory, apart from some necessary control commands, the entire data copying process is completed by the DMA controller. The process is the same as CPU copying, except this time the data in the memory is copied through the bus to the registers inside the DMA controller, and then copied to the storage space of the I/O device. Apart from paying attention to the start and end of this process, the CPU can do other things during the rest of the time.\nDMA controllers are generally together with I/O devices, which means that a network card has both a module responsible for data transmission and reception, as well as a DMA module.\n#\rWhat is RDMA\rRDMA (Remote Direct Memory Access) means remote direct memory access, through RDMA, the local node can \u0026ldquo;directly\u0026rdquo; access the memory of the remote node. The so-called direct means that it can read and write remote memory like accessing local memory, bypassing the complex TCP/IP network protocol stack of traditional Ethernet, and this process is not perceived by the other end. Moreover, most of the work in this read and write process is done by hardware rather than software.\nIn order to intuitively understand this process, please look at the two diagrams below (the arrows in the diagrams are for illustration only and do not indicate actual logical or physical relationships):\nIn traditional networks, \u0026ldquo;node A sends a message to node B\u0026rdquo; actually involves \u0026ldquo;transferring a segment of data from node A\u0026rsquo;s memory to node B\u0026rsquo;s memory through the network link.\u0026rdquo; This process, whether at the sending or receiving end, requires CPU direction and control, including control of the network card, handling of interrupts, packet encapsulation, and parsing, among other tasks.\nThe data in the user space memory on the left node in the figure needs to be copied by the CPU to the buffer in the kernel space before it can be accessed by the network card. During this process, the data will pass through the software-implemented TCP/IP protocol stack, adding headers and checksums for each layer, such as the TCP header, IP header, etc. The network card uses DMA to copy the data from the kernel into the network card\u0026rsquo;s internal buffer, processes it, and then sends it to the peer through the physical link.\nAfter the peer receives the data, it will perform the opposite process: from the internal storage space of the network card, the data is copied to the buffer in the memory kernel space via DMA, then the CPU parses it through the TCP/IP protocol stack and extracts the data to copy it into the user space.\nYou can see that even with DMA technology, the above process still heavily relies on the CPU.\nAfter using RDMA technology, this process can be simply represented by the following diagram:\nSimilarly, when copying a segment of data from local memory to remote memory using RDMA technology, the CPUs on both ends hardly participate in the data transmission process (only in the control plane). The local network card directly copies data from the user space in memory to its internal storage space via DMA, then the hardware assembles the various layers of the packet and sends it to the remote network card through the physical link. After the remote RDMA network card receives the data, it strips off the packet headers and checksums at each layer and directly copies the data into user space memory via DMA.\n#\rAdvantages of RDMA\rRDMA is mainly used in the field of high-performance computing (HPC) and large data centers, and the equipment is relatively more expensive than ordinary Ethernet cards (for example, Mellanox\u0026rsquo;s Connext-X 5 100Gb PCIe network card is priced above 4000 yuan). Due to the usage scenarios and price, RDMA is relatively distant from ordinary developers and consumers, and is currently mainly deployed and used by some large internet companies.\nWhy can RDMA technology be applied in the above scenarios? This involves the following characteristics:\n0 Copy: Refers to not needing to copy data back and forth between user space and kernel space. Due to operating systems like Linux dividing memory into user space and kernel space, in the traditional Socket communication process, the CPU needs to copy data back and forth in memory multiple times. However, through RDMA technology, we can directly access the remote registered memory area.\nRegarding 0 copy, you can refer to this article: A Brief Discussion on Zero Copy Mechanism in Linux\rKernel Bypass: Refers to the IO (data) process being able to bypass the kernel, meaning data can be prepared at the user level and notify the hardware to get ready for sending and receiving. This avoids the overhead of system calls and context switching. The above figure (original figure [1]\r) can effectively explain the meaning of \u0026ldquo;0 copy\u0026rdquo; and \u0026ldquo;kernel Bypass\u0026rdquo;. The upper and lower parts are respectively a send-receive process based on Socket and RDMA, and the left and right are two nodes. It is obvious that in the Socket process, there is an additional copy action in the software. In contrast, RDMA bypasses the kernel and also reduces memory copying, allowing data to be transferred directly between the user layer and hardware.\nCPU Offloading: Refers to the ability to read and write memory without the remote node\u0026rsquo;s CPU participating in the communication (of course, you need to have the \u0026ldquo;key\u0026rdquo; to access a certain segment of the remote memory). This essentially means encapsulating and parsing packets in hardware. In traditional Ethernet communication, the CPUs on both sides must participate in the parsing of each layer of the packet. If the data volume is large and the interaction is frequent, it will be a considerable overhead for the CPU, and these occupied CPU computing resources could have been used for more valuable work. The two most frequently appearing performance indicators in the field of communication are \u0026ldquo;bandwidth\u0026rdquo; and \u0026ldquo;latency.\u0026rdquo; Simply put, bandwidth refers to the amount of data that can be transmitted per unit of time, while latency refers to the time it takes for data to be sent from the local end to be received by the remote end. Due to the above characteristics, compared to traditional Ethernet, RDMA technology achieves higher bandwidth and lower latency, allowing it to play a role in bandwidth-sensitive scenarios—such as massive data interactions, and latency-sensitive scenarios—such as data synchronization between multiple computing nodes.\n#\rAgreement\rRDMA itself refers to a technology. On the specific protocol level, it includes Infiniband (IB), RDMA over Converged Ethernet (RoCE), and internet Wide Area RDMA Protocol (iWARP). All three protocols comply with the RDMA standard, use the same upper-layer interface, and have some differences at different levels.\nThe image above [2]g\rprovides a very clear comparison of the protocol layers for several common RDMA technologies.\n#\rInfiniband\rThe IB protocol proposed by IBTA (InfiniBand Trade Association) in 2000 is undoubtedly the core, as it specifies a complete set of link layer to transport layer (not the traditional OSI seven-layer model\u0026rsquo;s transport layer, but one that is above it) standards. However, it is not compatible with existing Ethernet; besides needing network cards that support IB, companies would also need to purchase matching switching equipment if they want to deploy it.\n#\rRoCE\rFrom the full English name of RoCE, it can be seen that it is a protocol based on the Ethernet link layer. The v1 version still used the IB specification for the network layer, while v2 uses UDP+IP as the network layer, allowing data packets to be routed. RoCE can be considered as a \u0026ldquo;low-cost solution\u0026rdquo; for IB, encapsulating IB messages into Ethernet packets for sending and receiving. Since RoCE v2 can use Ethernet switching devices, it is now more commonly used in enterprises, but compared to IB, there is some performance loss in the same scenarios.\n#\riWARP\rThe iWARP protocol is proposed by IETF based on TCP, because TCP is a connection-oriented reliable protocol. This makes iWARP more reliable in lossy network scenarios (which can be understood as network environments where packet loss may frequently occur) compared to RoCE v2 and IB, and it also has significant advantages in large-scale networking. However, a large number of TCP connections consume a lot of memory resources, and the complex flow control mechanisms of TCP can lead to performance issues, so in terms of performance, iWARP is inferior to UDP\u0026rsquo;s RoCE v2 and IB.\nIt should be noted that although there are software implementations of RoCE and iWARP protocols, in actual commercial use, the above protocols all require dedicated hardware (network cards) support.\niWARP itself did not directly evolve from Infiniband, but it inherited some design concepts from Infiniband technology. The relationship between these three protocols is shown in the diagram below:\n#\rPlayer\r#\rStandard/Ecological Organization\rSpeaking of the IB protocol, we must mention two major organizations - IBTA and OFA.\n#\rIBTA[3]\rFounded in 1999, responsible for developing and maintaining the Infiniband protocol standards. IBTA is independent of various manufacturers, integrating the entire industry through sponsoring technical activities and promoting resource sharing, and actively promoting IB and RoCE through online communication, marketing, and offline activities.\nIBTA will conduct protocol standard compliance and interoperability testing and certification for commercial IB and RoCE devices, led by a committee composed of many large IT manufacturers. Its main members include Broadcom, HPE, IBM, Intel, Mellanox, and Microsoft, among others. Huawei is also a member of IBTA.\n#\rOFA[4]\rA non-profit organization established in 2004 is responsible for developing, testing, certifying, supporting, and distributing a vendor-independent open-source cross-platform InfiniBand protocol stack, and started supporting RoCE in 2010. It is responsible for the OFED (OpenFabrics Enterprise Distribution) software stack used to support RDMA/Kernel bypass applications, ensuring its compatibility and ease of use with mainstream software and hardware. The OFED software stack includes drivers, kernel, middleware, and APIs.\nThe two organizations mentioned above have a cooperative relationship. IBTA is mainly responsible for developing, maintaining, and enhancing the Infiniband protocol standards; OFA is responsible for developing and maintaining the Infiniband protocol and upper-layer application APIs.\n#\rDeveloper community\r#\rLinux Community\rThe RDMA subsystem of the Linux kernel is relatively active, frequently discussing protocol details, and modifications to the framework are quite frequent. Additionally, some vendors, including Huawei and Mellanox, often make changes to the driver code.\nEmail subscription: http://vger.kernel.org/vger-lists.html#linux-rdma\rThe code is located in the kernel\u0026rsquo;s drivers/infiniband/ directory, including the framework core code and the driver code from various vendors.\nCode repository: https://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma.git/\r#\rRDMA Community\rFor upper-layer users, IB provides an interface similar to Socket sockets—libibverbs, and all three protocols mentioned earlier can be used. Referring to the protocol, API documentation, and sample programs, it is easy to write a demo. The RDMA community referred to in this column specifically refers to its user-space community, and its repository on GitHub is named linux-rdma.\nMainly contains two sub-repositories:\nrdma-core User-mode core code, API, documentation, and user-mode drivers from various vendors.\nperftest A powerful tool for testing RDMA performance. Code repository: https://github.com/linux-rdma/\r#\rUCX[5]\rUCX is a communication framework built on technologies such as RDMA for data processing and high-performance computing, with RDMA being one of its core components. We can understand it as middleware positioned between applications and the RDMA API, providing an additional layer of interfaces that are easier for upper-level users to develop with.\nThe author does not know much about it, only that there are some companies in the industry developing applications based on UCX.\nCode repository: https://github.com/openucx/ucx\r#\rHardware manufacturer\rThere are many manufacturers that design and produce IB-related hardware, including Mellanox, Huawei, Intel which acquired Qlogic\u0026rsquo;s IB technology, Broadcom, Marvell, Fujitsu, etc. We won\u0026rsquo;t go into each one here, just briefly mention Mellanox and Huawei.\nMellanox The leader in the IB field, Mellanox is visible in protocol standard setting, software and hardware development, and ecosystem construction, holding the greatest influence in the community and standard setting. The latest generation of network cards currently is the ConnextX-6 series, which supports 200Gb/s.\nHuawei The Kunpeng 920 chip, launched early last year, already supports the 100Gb/s RoCE protocol, and is technically leading in China. However, in terms of software, hardware, and influence, there is still a long way to go compared to Mellanox. I believe Huawei will catch up with the big brother soon.\n#\rUser\rMicrosoft, IBM, and domestic companies like Alibaba and JD are using RDMA, and many other large IT companies are also conducting initial development and testing. In data centers and high-performance computing scenarios, RDMA replacing traditional networks is an inevitable trend. The author does not have much exposure to the market, so cannot provide more detailed application information.\nThe next article will compare a typical Socket-based traditional Ethernet and RDMA communication process in a more intuitive way.\n","date":"2024-01-01T01:01:01Z","image":"https://cuterwrite-1302252842.file.myqcloud.com/img/crop_65b36f302c1d3715061e824224dcc9ca195413.jpg@1256w_1806h_!web-article-pic-2024-01-14.webp","permalink":"https://cuterwrite.top/en/p/rdma-overview/","title":"RDMA Overview"}]