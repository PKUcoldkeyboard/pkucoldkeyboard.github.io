<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Artificial Intelligence and Data Science on Cuterwrite's Blog</title><link>https://cuterwrite.top/en/categories/ai/</link><description>Recent content in Artificial Intelligence and Data Science on Cuterwrite's Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>cuterwrite</copyright><lastBuildDate>Fri, 05 Jul 2024 22:46:00 +0000</lastBuildDate><atom:link href="https://cuterwrite.top/en/categories/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>Introduction to LLM Ecosystem: From Model Fine-tuning to Application Implementation</title><link>https://cuterwrite.top/en/p/llm-ecosystem/</link><pubDate>Fri, 05 Jul 2024 22:46:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/llm-ecosystem/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_119269220_p0_master1200.webp" alt="Featured image of post Introduction to LLM Ecosystem: From Model Fine-tuning to Application Implementation" />&lt;h1 id="llm-ecosystem-introduction-from-model-fine-tuning-to-application-implementation">LLM Ecosystem Introduction: From Model Fine-Tuning to Application Implementation&lt;/h1>
&lt;h2 id="model-fine-tuning">Model fine-tuning&lt;/h2>
&lt;p>Pre-trained LLMs typically possess broad knowledge, but fine-tuning is essential for them to excel in specific tasks. Here are some commonly used LLM fine-tuning tools:&lt;/p>
&lt;h3 id="axolotl">Axolotl&lt;/h3>
&lt;a href="https://github.com/OpenAccess-AI-Collective/axolotl" target="_blank" class="card-github fetch-waiting no-styling"
repo="OpenAccess-AI-Collective/axolotl" id="repo-UdTcm3Mx02YJe1xi-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-UdTcm3Mx02YJe1xi-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">OpenAccess-AI-Collective&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">axolotl&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-UdTcm3Mx02YJe1xi-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-UdTcm3Mx02YJe1xi-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-UdTcm3Mx02YJe1xi-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-UdTcm3Mx02YJe1xi-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-UdTcm3Mx02YJe1xi-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-UdTcm3Mx02YJe1xi-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/OpenAccess-AI-Collective\/axolotl', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-UdTcm3Mx02YJe1xi-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-UdTcm3Mx02YJe1xi-language').innerText = data.language;
document.getElementById('repo-UdTcm3Mx02YJe1xi-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-UdTcm3Mx02YJe1xi-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-UdTcm3Mx02YJe1xi-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-UdTcm3Mx02YJe1xi-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-UdTcm3Mx02YJe1xi-license').classList.add = "no-license"
};
document.getElementById('repo-UdTcm3Mx02YJe1xi-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for OpenAccess-AI-Collective\/axolotl.")
}).catch(err => {
const c = document.getElementById('repo-UdTcm3Mx02YJe1xi-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for OpenAccess-AI-Collective\/axolotl.")
})
&lt;/script>
&lt;p>Axolotl is a tool designed to simplify the fine-tuning of various AI models, supporting multiple configurations and architectures.&lt;/p>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Train various Huggingface models, such as llama, pythia, falcon, mpt&lt;/li>
&lt;li>Supports fullfinetune, lora, qlora, relora, and gptq&lt;/li>
&lt;li>Customize configuration using simple yaml files or CLI rewrite functions&lt;/li>
&lt;li>Load different dataset formats, use custom formats, or built-in tokenized datasets&lt;/li>
&lt;li>Integrated with xformer, flash attention, rope scaling, and multi-packing&lt;/li>
&lt;li>Can work with a single GPU or multiple GPUs through FSDP or Deepspeed.&lt;/li>
&lt;li>Easily run locally or in the cloud using Docker&lt;/li>
&lt;li>Record the results and optional checkpoints to wandb or mlflow&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Quick Start:&lt;/strong>
Requirements: Python &amp;gt;=3.10 and Pytorch &amp;gt;=2.1.1&lt;/p>
&lt;pre>&lt;code class="language-bash">git clone https://github.com/OpenAccess-AI-Collective/axolotl
cd axolotl
pip3 install packaging ninja
pip3 install -e '.[flash-attn,deepspeed]'
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>Usage:&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash"># preprocess datasets - optional but recommended
CUDA_VISIBLE_DEVICES=&amp;quot;&amp;quot; python -m axolotl.cli.preprocess examples/openllama-3b/lora.yml
# finetune lora
accelerate launch -m axolotl.cli.train examples/openllama-3b/lora.yml
# inference
accelerate launch -m axolotl.cli.inference examples/openllama-3b/lora.yml \
--lora_model_dir=&amp;quot;./outputs/lora-out&amp;quot;
# gradio
accelerate launch -m axolotl.cli.inference examples/openllama-3b/lora.yml \
--lora_model_dir=&amp;quot;./outputs/lora-out&amp;quot; --gradio
# remote yaml files - the yaml config can be hosted on a public URL
# Note: the yaml config must directly link to the **raw** yaml
accelerate launch -m axolotl.cli.train https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/examples/openllama-3b/lora.yml
&lt;/code>&lt;/pre>
&lt;p>For more detailed information, please visit the &lt;a class="link" href="https://github.com/OpenAccess-AI-Collective/axolotl" target="_blank" rel="noopener" >Axolotl
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
project homepage.&lt;/p>
&lt;h3 id="llama-factory">Llama-Factory&lt;/h3>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-07-07_llama-factory-logo.webp"
alt="Llama Factory Logo" width="80%" loading="lazy">
&lt;/figure>
&lt;a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" class="card-github fetch-waiting no-styling"
repo="hiyouga/LLaMA-Factory" id="repo-ZsKkcmosREw8nSOY-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-ZsKkcmosREw8nSOY-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">hiyouga&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">LLaMA-Factory&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-ZsKkcmosREw8nSOY-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-ZsKkcmosREw8nSOY-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-ZsKkcmosREw8nSOY-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-ZsKkcmosREw8nSOY-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-ZsKkcmosREw8nSOY-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-ZsKkcmosREw8nSOY-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/hiyouga\/LLaMA-Factory', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-ZsKkcmosREw8nSOY-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-ZsKkcmosREw8nSOY-language').innerText = data.language;
document.getElementById('repo-ZsKkcmosREw8nSOY-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-ZsKkcmosREw8nSOY-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-ZsKkcmosREw8nSOY-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-ZsKkcmosREw8nSOY-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-ZsKkcmosREw8nSOY-license').classList.add = "no-license"
};
document.getElementById('repo-ZsKkcmosREw8nSOY-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for hiyouga\/LLaMA-Factory.")
}).catch(err => {
const c = document.getElementById('repo-ZsKkcmosREw8nSOY-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for hiyouga\/LLaMA-Factory.")
})
&lt;/script>
&lt;p>Llama-Factory is launched by Meta and is a framework focused on fine-tuning Llama models. It is built on top of the PyTorch ecosystem and provides efficient training and evaluation tools.&lt;/p>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Multiple models&lt;/strong>: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Yi, Gemma, Baichuan, ChatGLM, Phi, etc.&lt;/li>
&lt;li>&lt;strong>Integration Methods&lt;/strong>: (Incremental) Pre-training, (Multimodal) Instruction Supervised Fine-tuning, Reward Model Training, PPO Training, DPO Training, KTO Training, ORPO Training, etc.&lt;/li>
&lt;li>&lt;strong>Multiple Precisions&lt;/strong>: 16-bit full parameter fine-tuning, frozen fine-tuning, LoRA fine-tuning, and 2/3/4/5/6/8-bit QLoRA fine-tuning based on AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.&lt;/li>
&lt;li>&lt;strong>Advanced Algorithms&lt;/strong>: GaLore, BAdam, DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ, PiSSA, and Agent fine-tuning.&lt;/li>
&lt;li>&lt;strong>Practical Tips&lt;/strong>: FlashAttention-2, Unsloth, RoPE scaling, NEFTune, and rsLoRA.&lt;/li>
&lt;li>&lt;strong>Experiment Monitoring&lt;/strong>: LlamaBoard, TensorBoard, Wandb, MLflow, etc.&lt;/li>
&lt;li>&lt;strong>Rapid Reasoning&lt;/strong>: OpenAI-style API, browser interface, and command-line interface based on vLLM.&lt;/li>
&lt;/ul>
&lt;link href="https://web.sdk.qcloud.com/player/tcplayer/release/v4.2.1/tcplayer.min.css" rel="stylesheet">
&lt;script src="https://web.sdk.qcloud.com/player/tcplayer/release/v4.2.1/libs/hls.min.0.13.2m.js">&lt;/script>
&lt;script src="https://web.sdk.qcloud.com/player/tcplayer/release/v4.2.1/tcplayer.v4.2.1.min.js">&lt;/script>
&lt;style>
.tcplayer {
position: absolute;
width: 100%;
height: 100%;
left: 0;
top: 0;
border: 0;
}
&lt;/style>
&lt;div class="video-wrapper">
&lt;video
id="player-container-id"
preload="auto"
width="100%"
height="100%"
playsinline
webkit-playsinline>
&lt;/video>
&lt;/div>
&lt;script>
var tcplayer = TCPlayer("player-container-id", {
reportable: false,
poster: "",
});
tcplayer.src('https:\/\/cuterwrite-1302252842.file.myqcloud.com\/img\/2024-07-07_309488092-ec36a9dd-37f4-4f72-81bd-d76c6d0a6594_output.mp4');
&lt;/script>
&lt;p>&lt;strong>Performance Metrics&lt;/strong>&lt;/p>
&lt;p>Compared to the &lt;a class="link" href="https://github.com/THUDM/ChatGLM2-6B/tree/main/ptuning" target="_blank" rel="noopener" >P-Tuning
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
fine-tuning by ChatGLM official, the LoRA fine-tuning by LLaMA Factory provides a &lt;strong>3.7 times&lt;/strong> speedup and achieves higher Rouge scores in advertising copy generation tasks. Combined with 4-bit quantization technology, LLaMA Factory&amp;rsquo;s QLoRA fine-tuning further reduces GPU memory consumption.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-07-07_benchmark.webp"
alt="Performance" width="auto" loading="lazy">
&lt;/figure>
&lt;details>
&lt;summary>Variable Definition&lt;/summary>
&lt;ul>
&lt;li>&lt;strong>Training Speed&lt;/strong>: Number of samples processed per second during the training phase. (Batch size=4, truncation length=1024)&lt;/li>
&lt;li>&lt;strong>Rouge Score&lt;/strong>: Rouge-2 score on the validation set of the &lt;a class="link" href="https://aclanthology.org/D19-1321.pdf" target="_blank" rel="noopener" >advertising copy generation
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
task. (Batch size=4, truncation length=1024)&lt;/li>
&lt;li>&lt;strong>GPU Memory&lt;/strong>: Peak GPU memory for 4-bit quantization training. (Batch size=1, truncation length=1024)&lt;/li>
&lt;li>We use &lt;code>pre_seq_len=128&lt;/code> in the P-Tuning of ChatGLM, and &lt;code>lora_rank=32&lt;/code> in the LoRA fine-tuning of LLaMA Factory.&lt;/li>
&lt;/ul>
&lt;/details>
&lt;p>&lt;strong>Quick Start&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash">git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e &amp;quot;.[torch,metrics]&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Optional additional dependencies: torch, torch-npu, metrics, deepspeed, bitsandbytes, hqq, eetq, gptq, awq, aqlm, vllm, galore, badam, qwen, modelscope, quality&lt;/p>
&lt;div class="notice notice-tip" >
&lt;div class="notice-title">&lt;svg t="1705945832245" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="19409" width="200" height="200">&lt;path d="M512 74.666667C270.933333 74.666667 74.666667 270.933333 74.666667 512S270.933333 949.333333 512 949.333333 949.333333 753.066667 949.333333 512 753.066667 74.666667 512 74.666667z m238.933333 349.866666l-2.133333 2.133334-277.333333 277.333333c-10.666667 10.666667-29.866667 12.8-42.666667 2.133333L426.666667 704l-149.333334-149.333333c-12.8-12.8-12.8-32 0-44.8 10.666667-10.666667 29.866667-12.8 42.666667-2.133334l2.133333 2.133334 125.866667 125.866666 253.866667-253.866666c10.666667-10.666667 29.866667-12.8 42.666666-2.133334l2.133334 2.133334c12.8 12.8 12.8 32 4.266666 42.666666z" fill="#ffffff" p-id="19410">&lt;/path>&lt;/svg>&lt;/div>&lt;p>When encountering package conflicts, you can use pip install &amp;ndash;no-deps -e . to resolve them.&lt;/p>&lt;/div>
&lt;details>
&lt;summary>Windows User Guide&lt;/summary>
&lt;p>If you want to enable Quantized LoRA (QLoRA) on the Windows platform, you need to install the precompiled &lt;code>bitsandbytes&lt;/code> library, which supports CUDA 11.1 to 12.2. Please choose the appropriate &lt;a class="link" href="https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels" target="_blank" rel="noopener" >release version
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
according to your CUDA version.&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl
&lt;/code>&lt;/pre>
&lt;p>If you want to enable FlashAttention-2 on the Windows platform, you need to install the precompiled &lt;code>flash-attn&lt;/code> library, which supports CUDA 12.1 to 12.2. Please download and install the corresponding version according to your needs from &lt;a class="link" href="https://github.com/bdashore3/flash-attention/releases" target="_blank" rel="noopener" >flash-attention
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;/details>
&lt;details>
&lt;summary>Ascend NPU User Guide&lt;/summary>
&lt;p>When installing LLaMA Factory on Ascend NPU devices, you need to specify additional dependencies and use the command &lt;code>pip install -e &amp;quot;.[torch-npu,metrics]&amp;quot;&lt;/code> to install them. Additionally, you need to install the &lt;strong>&lt;a class="link" href="https://www.hiascend.com/developer/download/community/result?module=cann" target="_blank" rel="noopener" >Ascend CANN Toolkit and Kernels
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/strong>. Please refer to the &lt;a class="link" href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/80RC2alpha002/quickstart/quickstart/quickstart_18_0004.html" target="_blank" rel="noopener" >installation tutorial
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
or use the following command:&lt;/p>
&lt;pre>&lt;code class="language-bash"># Please replace URL with the URL corresponding to the CANN version and device model
# Install CANN Toolkit
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-toolkit_8.0.RC1.alpha001_linux-&amp;quot;$(uname -i)&amp;quot;.run
bash Ascend-cann-toolkit_8.0.RC1.alpha001_linux-&amp;quot;$(uname -i)&amp;quot;.run --install
# Install CANN Kernels
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run
bash Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run --install
# Set environment variables
source /usr/local/Ascend/ascend-toolkit/set_env.sh
&lt;/code>&lt;/pre>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Dependencies&lt;/th>
&lt;th>Minimum&lt;/th>
&lt;th>Recommended&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>CANN&lt;/td>
&lt;td>8.0.RC1&lt;/td>
&lt;td>8.0.RC1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>torch&lt;/td>
&lt;td>2.1.0&lt;/td>
&lt;td>2.1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>torch-npu&lt;/td>
&lt;td>2.1.0&lt;/td>
&lt;td>2.1.0.post3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>deepspeed&lt;/td>
&lt;td>0.13.2&lt;/td>
&lt;td>0.13.2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Please use &lt;code>ASCEND_RT_VISIBLE_DEVICES&lt;/code> instead of &lt;code>CUDA_VISIBLE_DEVICES&lt;/code> to specify the computation device.&lt;/p>
&lt;p>If you encounter a situation where reasoning cannot proceed normally, try setting &lt;code>do_sample: false&lt;/code>.&lt;/p>
&lt;p>Download pre-built Docker image: &lt;a class="link" href="http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html" target="_blank" rel="noopener" >32GB
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
| &lt;a class="link" href="http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html" target="_blank" rel="noopener" >64GB
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;/details>
&lt;p>The following three commands perform LoRA fine-tuning, inference, and merging on the Llama3-8B-Instruct model.&lt;/p>
&lt;pre>&lt;code class="language-bash">llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml
&lt;/code>&lt;/pre>
&lt;p>For more detailed information, please visit the &lt;a class="link" href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" rel="noopener" >Llama-Factory
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
project homepage.&lt;/p>
&lt;h3 id="firefly">Firefly&lt;/h3>
&lt;a href="https://github.com/yangjianxin1/Firefly" target="_blank" class="card-github fetch-waiting no-styling"
repo="yangjianxin1/Firefly" id="repo-4dfRhutiXGvC7QXe-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-4dfRhutiXGvC7QXe-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">yangjianxin1&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">Firefly&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-4dfRhutiXGvC7QXe-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-4dfRhutiXGvC7QXe-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-4dfRhutiXGvC7QXe-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-4dfRhutiXGvC7QXe-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-4dfRhutiXGvC7QXe-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-4dfRhutiXGvC7QXe-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/yangjianxin1\/Firefly', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-4dfRhutiXGvC7QXe-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-4dfRhutiXGvC7QXe-language').innerText = data.language;
document.getElementById('repo-4dfRhutiXGvC7QXe-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-4dfRhutiXGvC7QXe-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-4dfRhutiXGvC7QXe-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-4dfRhutiXGvC7QXe-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-4dfRhutiXGvC7QXe-license').classList.add = "no-license"
};
document.getElementById('repo-4dfRhutiXGvC7QXe-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for yangjianxin1\/Firefly.")
}).catch(err => {
const c = document.getElementById('repo-4dfRhutiXGvC7QXe-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for yangjianxin1\/Firefly.")
})
&lt;/script>
&lt;p>&lt;strong>Firefly&lt;/strong> is an open-source large model training project that supports pre-training, instruction fine-tuning, and DPO for mainstream large models, including but not limited to Qwen2, Yi-1.5, Llama3, Gemma, Qwen1.5, MiniCPM, Llama, InternLM, Baichuan, ChatGLM, Yi, Deepseek, Qwen, Orion, Ziya, Xverse, Mistral, Mixtral-8x7B, Zephyr, Vicuna, Bloom, etc.
This project supports &lt;strong>full parameter training, LoRA, QLoRA efficient training&lt;/strong>, and supports &lt;strong>pre-training, SFT, DPO&lt;/strong>. If your training resources are limited, we strongly recommend using QLoRA for instruction fine-tuning, as we have validated the effectiveness of this method on the Open LLM Leaderboard and achieved very good results.&lt;/p>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>📗 Supports pre-training, instruction fine-tuning, DPO, full parameter training, LoRA, QLoRA efficient training. Train different models through configuration files, allowing beginners to quickly get started with model training.&lt;/li>
&lt;li>📗 Supports using &lt;a class="link" href="https://github.com/yangjianxin1/unsloth" target="_blank" rel="noopener" >Unsloth
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
to accelerate training and save video memory.&lt;/li>
&lt;li>📗 Supports most mainstream open-source large models, such as Llama3, Gemma, MiniCPM, Llama, InternLM, Baichuan, ChatGLM, Yi, Deepseek, Qwen, Orion, Ziya, Xverse, Mistral, Mixtral-8x7B, Zephyr, Vicuna, Bloom, aligning with the templates of each official chat model during training.&lt;/li>
&lt;li>📗 Organize and open-source instruction fine-tuning datasets: firefly-train-1.1M, moss-003-sft-data, ultrachat, WizardLM_evol_instruct_V2_143k, school_math_0.25M.&lt;/li>
&lt;li>📗 Open source &lt;a class="link" href="https://huggingface.co/YeungNLP" target="_blank" rel="noopener" >Firefly series instruction fine-tuning model weights
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/li>
&lt;li>📗 Validated the effectiveness of the QLoRA training process on the Open LLM Leaderboard.&lt;/li>
&lt;/ul>
&lt;p>The README of the project contains detailed usage instructions, including how to install, how to train, how to fine-tune, and how to evaluate, etc. Please visit the &lt;a class="link" href="https://github.com/yangjianxin1/Firefly" target="_blank" rel="noopener" >Firefly
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
project homepage.&lt;/p>
&lt;h3 id="xtuner">XTuner&lt;/h3>
&lt;a href="https://github.com/InternLM/xtuner" target="_blank" class="card-github fetch-waiting no-styling"
repo="InternLM/xtuner" id="repo-blyC9Y0GfybqdmGh-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-blyC9Y0GfybqdmGh-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">InternLM&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">xtuner&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-blyC9Y0GfybqdmGh-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-blyC9Y0GfybqdmGh-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-blyC9Y0GfybqdmGh-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-blyC9Y0GfybqdmGh-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-blyC9Y0GfybqdmGh-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-blyC9Y0GfybqdmGh-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/InternLM\/xtuner', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-blyC9Y0GfybqdmGh-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-blyC9Y0GfybqdmGh-language').innerText = data.language;
document.getElementById('repo-blyC9Y0GfybqdmGh-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-blyC9Y0GfybqdmGh-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-blyC9Y0GfybqdmGh-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-blyC9Y0GfybqdmGh-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-blyC9Y0GfybqdmGh-license').classList.add = "no-license"
};
document.getElementById('repo-blyC9Y0GfybqdmGh-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for InternLM\/xtuner.")
}).catch(err => {
const c = document.getElementById('repo-blyC9Y0GfybqdmGh-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for InternLM\/xtuner.")
})
&lt;/script>
&lt;p>XTuner is an efficient, flexible, and versatile lightweight large model fine-tuning tool library.&lt;/p>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Efficient&lt;/strong>
&lt;ul>
&lt;li>Supports pre-training and lightweight fine-tuning of large language models (LLM) and multimodal image-text models (VLM). XTuner supports fine-tuning a 7B model with 8GB of video memory and also supports multi-node cross-device fine-tuning of larger scale models (70B+).&lt;/li>
&lt;li>Automatically distribute high-performance operators (such as FlashAttention, Triton kernels, etc.) to accelerate training throughput.&lt;/li>
&lt;li>Compatible with &lt;a class="link" href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener" >DeepSpeed
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
🚀, easily apply various ZeRO training optimization strategies.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Flexible&lt;/strong>
&lt;ul>
&lt;li>Supports multiple large language models, including but not limited to &lt;a class="link" href="https://huggingface.co/internlm" target="_blank" rel="noopener" >InternLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, &lt;a class="link" href="https://huggingface.co/mistralai" target="_blank" rel="noopener" >Mixtral-8x7B
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, &lt;a class="link" href="https://huggingface.co/meta-llama" target="_blank" rel="noopener" >Llama 2
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, &lt;a class="link" href="https://huggingface.co/THUDM" target="_blank" rel="noopener" >ChatGLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, &lt;a class="link" href="https://huggingface.co/Qwen" target="_blank" rel="noopener" >Qwen
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, &lt;a class="link" href="https://huggingface.co/baichuan-inc" target="_blank" rel="noopener" >Baichuan
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Supports pre-training and fine-tuning of the multimodal text-image model LLaVA. The model &lt;a class="link" href="https://huggingface.co/xtuner/llava-internlm2-20b" target="_blank" rel="noopener" >LLaVA-InternLM2-20B
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
trained with XTuner performs excellently.
&lt;ul>
&lt;li>Carefully designed data pipeline, compatible with any data format, both open-source data and custom data can be quickly adopted.&lt;/li>
&lt;li>Supports multiple fine-tuning algorithms such as &lt;a class="link" href="http://arxiv.org/abs/2305.14314" target="_blank" rel="noopener" >QLoRA
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, &lt;a class="link" href="http://arxiv.org/abs/2106.09685" target="_blank" rel="noopener" >LoRA
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, and full parameter fine-tuning, allowing users to make the optimal choice based on specific needs.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Omnipotent&lt;/strong>
&lt;ul>
&lt;li>Supports incremental pre-training, instruction fine-tuning, and Agent fine-tuning.&lt;/li>
&lt;li>Predefined numerous open-source dialogue templates, supporting conversation with open-source or trained models.&lt;/li>
&lt;li>The trained model can be seamlessly integrated with the deployment toolkit &lt;a class="link" href="https://github.com/InternLM/lmdeploy" target="_blank" rel="noopener" >LMDeploy
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, the large-scale evaluation toolkit &lt;a class="link" href="https://github.com/open-compass/opencompass" target="_blank" rel="noopener" >OpenCompass
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, and &lt;a class="link" href="https://github.com/open-compass/VLMEvalKit" target="_blank" rel="noopener" >VLMEvalKit
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Quick Start:&lt;/strong>
&lt;details>
&lt;summary>Installation&lt;/summary>
&lt;ul>
&lt;li>It is recommended to use conda to first build a Python-3.10 virtual environment&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash"> conda create --name xtuner-env python=3.10 -y
conda activate xtuner-env
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Install XTuner via pip:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-shell"> pip install -U xtuner
&lt;/code>&lt;/pre>
&lt;p>Can also integrate DeepSpeed installation:&lt;/p>
&lt;pre>&lt;code class="language-shell"> pip install -U 'xtuner[deepspeed]'
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Install XTuner from source:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-shell"> git clone https://github.com/InternLM/xtuner.git
cd xtuner
pip install -e '.[all]'
&lt;/code>&lt;/pre>
&lt;/details>
&lt;/p>
&lt;details>
&lt;summary>Fine-tuning&lt;/summary>
&lt;p>XTuner supports fine-tuning large language models. For dataset preprocessing guidelines, please refer to the &lt;a class="link" href="./docs/zh_cn/user_guides/dataset_prepare.md" >documentation
&lt;/a>
.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Step 0&lt;/strong>, prepare the configuration file. XTuner provides multiple out-of-the-box configuration files, which users can view with the following command:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-shell"> xtuner list-cfg
&lt;/code>&lt;/pre>
&lt;p>Or, if the provided configuration file does not meet the usage requirements, please export the provided configuration file and make the corresponding changes:&lt;/p>
&lt;pre>&lt;code class="language-shell"> xtuner copy-cfg ${CONFIG_NAME} ${SAVE_PATH}
vi ${SAVE_PATH}/${CONFIG_NAME}_copy.py
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;strong>Step 1&lt;/strong>, start fine-tuning.&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-shell"> xtuner train ${CONFIG_NAME_OR_PATH}
&lt;/code>&lt;/pre>
&lt;p>For example, we can use the QLoRA algorithm to fine-tune InternLM2.5-Chat-7B on the oasst1 dataset:&lt;/p>
&lt;pre>&lt;code class="language-shell"># Single card
xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2
# Multi-card
(DIST) NPROC_PER_NODE=${GPU_NUM} xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2
(SLURM) srun ${SRUN_ARGS} xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --launcher slurm --deepspeed deepspeed_zero2
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>
&lt;p>&lt;code>--deepspeed&lt;/code> indicates using &lt;a class="link" href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener" >DeepSpeed
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
🚀 to optimize the training process. XTuner has multiple built-in strategies, including ZeRO-1, ZeRO-2, ZeRO-3, etc. If the user wishes to disable this feature, please remove this parameter directly.&lt;/p>
&lt;ul>
&lt;li>For more examples, please refer to the &lt;a class="link" href="./docs/zh_cn/user_guides/finetune.md" >documentation
&lt;/a>
.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Step 2&lt;/strong>, convert the saved PTH model (if using DeepSpeed, it will be a folder) to a HuggingFace model:&lt;/p>
&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-shell"> xtuner convert pth_to_hf ${CONFIG_NAME_OR_PATH} ${PTH} ${SAVE_PATH}
&lt;/code>&lt;/pre>
&lt;/details>
&lt;p>For more detailed information, please visit the &lt;a class="link" href="https://github.com/InternLM/xtuner" target="_blank" rel="noopener" >XTuner
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
project homepage.&lt;/p>
&lt;h2 id="model-quantization">Model quantization&lt;/h2>
&lt;p>LLM is usually large in volume and requires high computational resources. Model quantization techniques can compress model size, improve operational efficiency, and make it easier to deploy:&lt;/p>
&lt;h3 id="autogptq">AutoGPTQ&lt;/h3>
&lt;a href="https://github.com/PanQiWei/AutoGPTQ" target="_blank" class="card-github fetch-waiting no-styling"
repo="PanQiWei/AutoGPTQ" id="repo-MH4mPbsVKHrSyWY5-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-MH4mPbsVKHrSyWY5-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">PanQiWei&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">AutoGPTQ&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-MH4mPbsVKHrSyWY5-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-MH4mPbsVKHrSyWY5-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-MH4mPbsVKHrSyWY5-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-MH4mPbsVKHrSyWY5-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-MH4mPbsVKHrSyWY5-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-MH4mPbsVKHrSyWY5-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/PanQiWei\/AutoGPTQ', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-MH4mPbsVKHrSyWY5-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-MH4mPbsVKHrSyWY5-language').innerText = data.language;
document.getElementById('repo-MH4mPbsVKHrSyWY5-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-MH4mPbsVKHrSyWY5-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-MH4mPbsVKHrSyWY5-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-MH4mPbsVKHrSyWY5-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-MH4mPbsVKHrSyWY5-license').classList.add = "no-license"
};
document.getElementById('repo-MH4mPbsVKHrSyWY5-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for PanQiWei\/AutoGPTQ.")
}).catch(err => {
const c = document.getElementById('repo-MH4mPbsVKHrSyWY5-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for PanQiWei\/AutoGPTQ.")
})
&lt;/script>
&lt;p>AutoGPTQ is a large language model quantization toolkit based on the GPTQ algorithm, simple to use and with a user-friendly interface.&lt;/p>
&lt;p>&lt;strong>Quick Installation&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>For CUDA 11.7:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>For CUDA 11.8:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>For RoCm 5.4.2:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/rocm542/
&lt;/code>&lt;/pre>
&lt;p>For more detailed information, please visit the project homepage of &lt;a class="link" href="https://github.com/AutoGPTQ/AutoGPTQ/" target="_blank" rel="noopener" >AutoGPTQ
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;h3 id="autoawq">AutoAWQ&lt;/h3>
&lt;a href="https://github.com/casper-hansen/AutoAWQ" target="_blank" class="card-github fetch-waiting no-styling"
repo="casper-hansen/AutoAWQ" id="repo-jKfBONPNGZ8INGWo-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-jKfBONPNGZ8INGWo-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">casper-hansen&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">AutoAWQ&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-jKfBONPNGZ8INGWo-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-jKfBONPNGZ8INGWo-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-jKfBONPNGZ8INGWo-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-jKfBONPNGZ8INGWo-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-jKfBONPNGZ8INGWo-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-jKfBONPNGZ8INGWo-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/casper-hansen\/AutoAWQ', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-jKfBONPNGZ8INGWo-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-jKfBONPNGZ8INGWo-language').innerText = data.language;
document.getElementById('repo-jKfBONPNGZ8INGWo-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-jKfBONPNGZ8INGWo-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-jKfBONPNGZ8INGWo-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-jKfBONPNGZ8INGWo-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-jKfBONPNGZ8INGWo-license').classList.add = "no-license"
};
document.getElementById('repo-jKfBONPNGZ8INGWo-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for casper-hansen\/AutoAWQ.")
}).catch(err => {
const c = document.getElementById('repo-jKfBONPNGZ8INGWo-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for casper-hansen\/AutoAWQ.")
})
&lt;/script>
&lt;p>AutoAWQ is another automated model quantization tool that supports multiple quantization precisions and offers flexible configuration options, allowing adjustments based on different hardware platforms and performance requirements.&lt;/p>
&lt;p>AutoAWQ is an easy-to-use 4-bit quantization model package. Compared to FP16, AutoAWQ can increase model speed by 3 times and reduce memory requirements by 3 times. AutoAWQ implements the activation-aware weight quantization (AWQ) algorithm for quantizing LLMs. AutoAWQ was created and improved based on the original work &lt;a class="link" href="https://github.com/mit-han-lab/llm-awq" target="_blank" rel="noopener" >AWQ
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
from MIT.&lt;/p>
&lt;p>&lt;strong>Installation Method:&lt;/strong>&lt;/p>
&lt;p>Before installing, ensure that CUDA &amp;gt;= 12.1 is installed (Note: The following is just the quickest installation method)&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install autoawq
&lt;/code>&lt;/pre>
&lt;p>For more details and examples, please visit the project homepage of &lt;a class="link" href="https://github.com/casper-hansen/AutoAWQ" target="_blank" rel="noopener" >AutoAWQ
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;h3 id="neural-compressor">Neural Compressor&lt;/h3>
&lt;a href="https://github.com/intel/neural-compressor" target="_blank" class="card-github fetch-waiting no-styling"
repo="intel/neural-compressor" id="repo-XT9XaFaDMz62zN5j-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-XT9XaFaDMz62zN5j-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">intel&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">neural-compressor&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-XT9XaFaDMz62zN5j-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-XT9XaFaDMz62zN5j-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-XT9XaFaDMz62zN5j-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-XT9XaFaDMz62zN5j-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-XT9XaFaDMz62zN5j-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-XT9XaFaDMz62zN5j-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/intel\/neural-compressor', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-XT9XaFaDMz62zN5j-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-XT9XaFaDMz62zN5j-language').innerText = data.language;
document.getElementById('repo-XT9XaFaDMz62zN5j-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-XT9XaFaDMz62zN5j-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-XT9XaFaDMz62zN5j-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-XT9XaFaDMz62zN5j-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-XT9XaFaDMz62zN5j-license').classList.add = "no-license"
};
document.getElementById('repo-XT9XaFaDMz62zN5j-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for intel\/neural-compressor.")
}).catch(err => {
const c = document.getElementById('repo-XT9XaFaDMz62zN5j-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for intel\/neural-compressor.")
})
&lt;/script>
&lt;p>Neural Compressor is a model compression toolkit developed by Intel, supporting popular model compression techniques on all major deep learning frameworks (TensorFlow, PyTorch, ONNX Runtime, and MXNet).&lt;/p>
&lt;p>&lt;strong>Installation Method:&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install &amp;quot;neural-compressor&amp;gt;=2.3&amp;quot; &amp;quot;transformers&amp;gt;=4.34.0&amp;quot; torch torchvision
&lt;/code>&lt;/pre>
&lt;p>For more detailed information and examples, please visit the project homepage of &lt;a class="link" href="https://github.com/intel/neural-compressor" target="_blank" rel="noopener" >Neural Compressor
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;h2 id="model-deployment">Model deployment&lt;/h2>
&lt;p>Deploying a trained LLM to a production environment is crucial. Here are some commonly used LLM deployment tools:&lt;/p>
&lt;h3 id="vllm">vLLM&lt;/h3>
&lt;a href="https://github.com/vllm-project/vllm" target="_blank" class="card-github fetch-waiting no-styling"
repo="vllm-project/vllm" id="repo-YeNaImuen8wZmyeJ-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-YeNaImuen8wZmyeJ-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">vllm-project&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">vllm&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-YeNaImuen8wZmyeJ-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-YeNaImuen8wZmyeJ-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-YeNaImuen8wZmyeJ-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-YeNaImuen8wZmyeJ-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-YeNaImuen8wZmyeJ-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-YeNaImuen8wZmyeJ-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/vllm-project\/vllm', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-YeNaImuen8wZmyeJ-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-YeNaImuen8wZmyeJ-language').innerText = data.language;
document.getElementById('repo-YeNaImuen8wZmyeJ-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-YeNaImuen8wZmyeJ-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-YeNaImuen8wZmyeJ-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-YeNaImuen8wZmyeJ-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-YeNaImuen8wZmyeJ-license').classList.add = "no-license"
};
document.getElementById('repo-YeNaImuen8wZmyeJ-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for vllm-project\/vllm.")
}).catch(err => {
const c = document.getElementById('repo-YeNaImuen8wZmyeJ-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for vllm-project\/vllm.")
})
&lt;/script>
&lt;p>vLLM is a fast and easy-to-use LLM inference service library.&lt;/p>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Fast&lt;/strong>
&lt;ul>
&lt;li>SOTA service throughput&lt;/li>
&lt;li>Efficiently manage attention key-value memory using PagedAttention&lt;/li>
&lt;li>Continuously batch process received requests&lt;/li>
&lt;li>Use CUDA/HIP graphs for acceleration&lt;/li>
&lt;li>Quantization: Supports GPTQ, AWQ, SqueezeLLM, FP8 KV cache&lt;/li>
&lt;li>Optimized CUDA kernel&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Flexible&lt;/strong>
&lt;ul>
&lt;li>Seamless integration with popular Hugging Face models&lt;/li>
&lt;li>Provide high-throughput services using various decoding algorithms (including parallel sampling, beam search, etc.)&lt;/li>
&lt;li>Provide tensor parallel support for distributed inference&lt;/li>
&lt;li>Stream output&lt;/li>
&lt;li>Compatible with OpenAI&amp;rsquo;s application programming interface server&lt;/li>
&lt;li>Supports NVIDIA GPU, AMD GPU, Intel CPU, and GPU&lt;/li>
&lt;li>(Experimental) Support prefix caching&lt;/li>
&lt;li>(Experimental) Support for multiple languages&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Seamless Support&lt;/strong>
&lt;ul>
&lt;li>Transformer-based models, such as Llama&lt;/li>
&lt;li>MoE-based model, such as Mixtral&lt;/li>
&lt;li>Multimodal models, such as LLaVA&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Quick Installation:&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install vllm
&lt;/code>&lt;/pre>
&lt;p>For more detailed information, please refer to the &lt;a class="link" href="https://vllm.readthedocs.io/en/latest/" target="_blank" rel="noopener" >vLLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
official documentation.&lt;/p>
&lt;h3 id="sgl">SGL&lt;/h3>
&lt;a href="https://github.com/sgl-project/sglang" target="_blank" class="card-github fetch-waiting no-styling"
repo="sgl-project/sglang" id="repo-ZEqQg1tlmNbWItOM-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-ZEqQg1tlmNbWItOM-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">sgl-project&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">sglang&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-ZEqQg1tlmNbWItOM-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-ZEqQg1tlmNbWItOM-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-ZEqQg1tlmNbWItOM-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-ZEqQg1tlmNbWItOM-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-ZEqQg1tlmNbWItOM-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-ZEqQg1tlmNbWItOM-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/sgl-project\/sglang', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-ZEqQg1tlmNbWItOM-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-ZEqQg1tlmNbWItOM-language').innerText = data.language;
document.getElementById('repo-ZEqQg1tlmNbWItOM-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-ZEqQg1tlmNbWItOM-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-ZEqQg1tlmNbWItOM-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-ZEqQg1tlmNbWItOM-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-ZEqQg1tlmNbWItOM-license').classList.add = "no-license"
};
document.getElementById('repo-ZEqQg1tlmNbWItOM-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for sgl-project\/sglang.")
}).catch(err => {
const c = document.getElementById('repo-ZEqQg1tlmNbWItOM-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for sgl-project\/sglang.")
})
&lt;/script>
&lt;p>SGLang is a structured generation language designed specifically for large language models (LLMs). By co-designing the front-end language and the runtime system, it makes your interactions with LLMs faster and more controllable.&lt;/p>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Flexible front-end language&lt;/strong>: Easily write LLM applications through chainable generation calls, advanced prompts, control flow, multiple modes, concurrency, and external interaction.&lt;/li>
&lt;li>&lt;strong>High-performance backend runtime&lt;/strong>: Features RadixAttention capability, which can accelerate complex LLM programs by reusing KV cache across multiple calls. It can also function as a standalone inference engine, implementing all common techniques (such as continuous batching and tensor parallelism).&lt;/li>
&lt;/ul>
&lt;p>For more detailed information, please visit the &lt;a class="link" href="https://github.com/sgl-project/" target="_blank" rel="noopener" >SGL
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
project homepage.&lt;/p>
&lt;h3 id="skypilot">SkyPilot&lt;/h3>
&lt;a href="https://github.com/skypilot-org/skypilot" target="_blank" class="card-github fetch-waiting no-styling"
repo="skypilot-org/skypilot" id="repo-QR2qWaUaNByWti3T-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-QR2qWaUaNByWti3T-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">skypilot-org&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">skypilot&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-QR2qWaUaNByWti3T-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-QR2qWaUaNByWti3T-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-QR2qWaUaNByWti3T-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-QR2qWaUaNByWti3T-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-QR2qWaUaNByWti3T-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-QR2qWaUaNByWti3T-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/skypilot-org\/skypilot', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-QR2qWaUaNByWti3T-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-QR2qWaUaNByWti3T-language').innerText = data.language;
document.getElementById('repo-QR2qWaUaNByWti3T-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-QR2qWaUaNByWti3T-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-QR2qWaUaNByWti3T-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-QR2qWaUaNByWti3T-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-QR2qWaUaNByWti3T-license').classList.add = "no-license"
};
document.getElementById('repo-QR2qWaUaNByWti3T-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for skypilot-org\/skypilot.")
}).catch(err => {
const c = document.getElementById('repo-QR2qWaUaNByWti3T-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for skypilot-org\/skypilot.")
})
&lt;/script>
&lt;p>SkyPilot is a flexible cloud LLM deployment tool launched by UC Berkeley RISELab, supporting multiple cloud platforms and hardware accelerators. It can automatically select the optimal deployment plan and provide cost optimization features.&lt;/p>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Multi-cloud support:&lt;/strong> Supports various cloud platforms such as AWS, GCP, Azure, allowing users to choose the appropriate deployment environment.&lt;/li>
&lt;li>&lt;strong>Easy to expand&lt;/strong>: Queue and run multiple jobs, automatic management&lt;/li>
&lt;li>&lt;strong>Easy Access to Object Storage&lt;/strong>: Easily access object storage (S3, GCS, R2)&lt;/li>
&lt;/ul>
&lt;p>For more detailed information, please visit the &lt;a class="link" href="https://github.com/skypilot-org/skypilot" target="_blank" rel="noopener" >SkyPilot
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
project homepage.&lt;/p>
&lt;h3 id="tensorrt-llm">TensorRT-LLM&lt;/h3>
&lt;a href="https://github.com/NVIDIA/TensorRT-LLM" target="_blank" class="card-github fetch-waiting no-styling"
repo="NVIDIA/TensorRT-LLM" id="repo-No6vJMxC2IpWPFpw-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-No6vJMxC2IpWPFpw-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">NVIDIA&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">TensorRT-LLM&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-No6vJMxC2IpWPFpw-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-No6vJMxC2IpWPFpw-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-No6vJMxC2IpWPFpw-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-No6vJMxC2IpWPFpw-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-No6vJMxC2IpWPFpw-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-No6vJMxC2IpWPFpw-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/NVIDIA\/TensorRT-LLM', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-No6vJMxC2IpWPFpw-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-No6vJMxC2IpWPFpw-language').innerText = data.language;
document.getElementById('repo-No6vJMxC2IpWPFpw-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-No6vJMxC2IpWPFpw-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-No6vJMxC2IpWPFpw-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-No6vJMxC2IpWPFpw-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-No6vJMxC2IpWPFpw-license').classList.add = "no-license"
};
document.getElementById('repo-No6vJMxC2IpWPFpw-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for NVIDIA\/TensorRT-LLM.")
}).catch(err => {
const c = document.getElementById('repo-No6vJMxC2IpWPFpw-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for NVIDIA\/TensorRT-LLM.")
})
&lt;/script>
&lt;p>TensorRT-LLM is a high-performance LLM inference engine launched by NVIDIA, capable of fully utilizing GPU accelerated computation and optimized for the Transformer model architecture, significantly improving inference speed.&lt;/p>
&lt;p>TensorRT-LLM provides users with an easy-to-use Python API for defining large language models (LLMs) and building TensorRT engines, which incorporate state-of-the-art optimization techniques for efficient inference execution on NVIDIA® graphics processors. TensorRT-LLM also includes components for creating Python and C++ runtimes that execute these TensorRT engines.&lt;/p>
&lt;p>For more details, please visit the &lt;a class="link" href="https://github.com/NVIDIA/TensorRT-LLM" target="_blank" rel="noopener" >TensorRT-LLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
project homepage.&lt;/p>
&lt;h3 id="openvino">OpenVino&lt;/h3>
&lt;a href="https://github.com/openvinotoolkit/openvino" target="_blank" class="card-github fetch-waiting no-styling"
repo="openvinotoolkit/openvino" id="repo-1kzcQNfgVSdbRxmI-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-1kzcQNfgVSdbRxmI-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">openvinotoolkit&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">openvino&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-1kzcQNfgVSdbRxmI-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-1kzcQNfgVSdbRxmI-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-1kzcQNfgVSdbRxmI-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-1kzcQNfgVSdbRxmI-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-1kzcQNfgVSdbRxmI-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-1kzcQNfgVSdbRxmI-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/openvinotoolkit\/openvino', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-1kzcQNfgVSdbRxmI-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-1kzcQNfgVSdbRxmI-language').innerText = data.language;
document.getElementById('repo-1kzcQNfgVSdbRxmI-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-1kzcQNfgVSdbRxmI-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-1kzcQNfgVSdbRxmI-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-1kzcQNfgVSdbRxmI-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-1kzcQNfgVSdbRxmI-license').classList.add = "no-license"
};
document.getElementById('repo-1kzcQNfgVSdbRxmI-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for openvinotoolkit\/openvino.")
}).catch(err => {
const c = document.getElementById('repo-1kzcQNfgVSdbRxmI-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for openvinotoolkit\/openvino.")
})
&lt;/script>
&lt;p>OpenVINO™ is an open-source toolkit for optimizing and deploying artificial intelligence inference.&lt;/p>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Inference Optimization&lt;/strong>: Enhance the performance of deep learning in computer vision, automatic speech recognition, generative AI, natural language processing using large and small language models, and many other common tasks.&lt;/li>
&lt;li>&lt;strong>Flexible Model Support&lt;/strong>: Models trained with popular frameworks such as TensorFlow, PyTorch, ONNX, Keras, and PaddlePaddle. Convert and deploy models without the need for the original framework.&lt;/li>
&lt;li>&lt;strong>Broad Platform Compatibility&lt;/strong>: Reduce resource requirements and efficiently deploy across a range of platforms from edge to cloud. OpenVINO™ supports inference on CPUs (x86, ARM), GPUs (integrated and discrete GPUs supporting OpenCL), and AI accelerators (Intel NPU).&lt;/li>
&lt;li>&lt;strong>Community and Ecosystem&lt;/strong>: Join an active community contributing to improving deep learning performance in various fields.&lt;/li>
&lt;/ul>
&lt;p>For more information, please visit the project homepage of &lt;a class="link" href="https://github.com/openvinotoolkit/openvino" target="_blank" rel="noopener" >OpenVino
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;h3 id="tgi">TGI&lt;/h3>
&lt;a href="https://github.com/huggingface/text-generation-inference" target="_blank" class="card-github fetch-waiting no-styling"
repo="huggingface/text-generation-inference" id="repo-FeqR64BaFZwsX1pw-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-FeqR64BaFZwsX1pw-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">huggingface&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">text-generation-inference&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-FeqR64BaFZwsX1pw-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-FeqR64BaFZwsX1pw-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-FeqR64BaFZwsX1pw-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-FeqR64BaFZwsX1pw-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-FeqR64BaFZwsX1pw-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-FeqR64BaFZwsX1pw-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/huggingface\/text-generation-inference', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-FeqR64BaFZwsX1pw-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-FeqR64BaFZwsX1pw-language').innerText = data.language;
document.getElementById('repo-FeqR64BaFZwsX1pw-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-FeqR64BaFZwsX1pw-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-FeqR64BaFZwsX1pw-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-FeqR64BaFZwsX1pw-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-FeqR64BaFZwsX1pw-license').classList.add = "no-license"
};
document.getElementById('repo-FeqR64BaFZwsX1pw-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for huggingface\/text-generation-inference.")
}).catch(err => {
const c = document.getElementById('repo-FeqR64BaFZwsX1pw-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for huggingface\/text-generation-inference.")
})
&lt;/script>
&lt;p>Text Generation Inference (TGI) is a toolkit for deploying and serving large language models (LLMs). TGI enables high-performance text generation for the most popular open-source LLMs, including Llama, Falcon, StarCoder, BLOOM, GPT-NeoX, and others.&lt;/p>
&lt;p>TGI has implemented many features, and detailed information can be found on the project homepage of &lt;a class="link" href="https://github.com/huggingface/text-generation-inference" target="_blank" rel="noopener" >TGI
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;h2 id="local-run">Local run&lt;/h2>
&lt;p>Thanks to model compression and optimization techniques, we can also run LLM on personal devices:&lt;/p>
&lt;h3 id="mlx">MLX&lt;/h3>
&lt;a href="https://github.com/ml-explore/mlx" target="_blank" class="card-github fetch-waiting no-styling"
repo="ml-explore/mlx" id="repo-GeuXiF6p0nVfTbDE-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-GeuXiF6p0nVfTbDE-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">ml-explore&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">mlx&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-GeuXiF6p0nVfTbDE-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-GeuXiF6p0nVfTbDE-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-GeuXiF6p0nVfTbDE-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-GeuXiF6p0nVfTbDE-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-GeuXiF6p0nVfTbDE-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-GeuXiF6p0nVfTbDE-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/ml-explore\/mlx', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-GeuXiF6p0nVfTbDE-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-GeuXiF6p0nVfTbDE-language').innerText = data.language;
document.getElementById('repo-GeuXiF6p0nVfTbDE-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-GeuXiF6p0nVfTbDE-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-GeuXiF6p0nVfTbDE-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-GeuXiF6p0nVfTbDE-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-GeuXiF6p0nVfTbDE-license').classList.add = "no-license"
};
document.getElementById('repo-GeuXiF6p0nVfTbDE-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for ml-explore\/mlx.")
}).catch(err => {
const c = document.getElementById('repo-GeuXiF6p0nVfTbDE-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for ml-explore\/mlx.")
})
&lt;/script>
&lt;p>MLX is a framework specifically designed to support running LLM on Apple devices, fully utilizing Metal to accelerate computation, and providing easy-to-use APIs to facilitate developers in integrating LLM into iOS applications.&lt;/p>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Similar Application Programming Interface&lt;/strong>: MLX&amp;rsquo;s Python API is very similar to NumPy. MLX also has fully functional C++, C, and Swift APIs, which are very similar to the Python API. MLX has higher-level packages like &lt;code>mlx.nn&lt;/code> and &lt;code>mlx.optimizers&lt;/code>, whose APIs are very close to PyTorch, simplifying the construction of more complex models.&lt;/li>
&lt;li>&lt;strong>Composable Function Transformations&lt;/strong>: MLX supports composable function transformations for automatic differentiation, automatic vectorization, and computation graph optimization.&lt;/li>
&lt;li>&lt;strong>Lazy Evaluation&lt;/strong>: In MLX, computations only materialize arrays when needed.&lt;/li>
&lt;li>&lt;strong>Dynamic Graph Construction&lt;/strong>: In MLX, the computational graph is dynamically constructed. Changing the shape of function parameters does not slow down the compilation speed, and debugging is simple and intuitive.&lt;/li>
&lt;li>&lt;strong>Multi-device&lt;/strong>: Operations can run on any supported device (currently CPU and GPU).&lt;/li>
&lt;li>&lt;strong>Unified Memory&lt;/strong>: The unified memory model is a significant difference between MLX and other frameworks. Arrays in MLX reside in shared memory. Operations on MLX arrays can be performed on any supported device type without the need to transfer data.&lt;/li>
&lt;/ul>
&lt;p>MLX is designed by machine learning researchers for machine learning researchers. The framework aims to be user-friendly while still efficiently training and deploying models. The design concept of the framework itself is also very simple. Our goal is to enable researchers to easily expand and improve MLX, thus quickly exploring new ideas. For more details, please visit the project homepage of &lt;a class="link" href="https://github.com/ml-explore/mlx" target="_blank" rel="noopener" >MLX
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;h3 id="llamacpp">Llama.cpp&lt;/h3>
&lt;p>Llama.cpp is a Llama model inference engine implemented in C++, capable of running efficiently on CPUs, and supports multiple operating systems and hardware platforms, allowing developers to run LLM on resource-constrained devices.&lt;/p>
&lt;a href="https://github.com/ggerganov/llama.cpp" target="_blank" class="card-github fetch-waiting no-styling"
repo="ggerganov/llama.cpp" id="repo-4hEG605uOtrb8BVw-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-4hEG605uOtrb8BVw-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">ggerganov&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">llama.cpp&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-4hEG605uOtrb8BVw-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-4hEG605uOtrb8BVw-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-4hEG605uOtrb8BVw-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-4hEG605uOtrb8BVw-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-4hEG605uOtrb8BVw-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-4hEG605uOtrb8BVw-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/ggerganov\/llama.cpp', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-4hEG605uOtrb8BVw-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-4hEG605uOtrb8BVw-language').innerText = data.language;
document.getElementById('repo-4hEG605uOtrb8BVw-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-4hEG605uOtrb8BVw-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-4hEG605uOtrb8BVw-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-4hEG605uOtrb8BVw-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-4hEG605uOtrb8BVw-license').classList.add = "no-license"
};
document.getElementById('repo-4hEG605uOtrb8BVw-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for ggerganov\/llama.cpp.")
}).catch(err => {
const c = document.getElementById('repo-4hEG605uOtrb8BVw-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for ggerganov\/llama.cpp.")
})
&lt;/script>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>CPU Inference:&lt;/strong> Optimized for CPU platforms, allowing LLM to run on devices without a GPU.&lt;/li>
&lt;li>&lt;strong>Cross-platform support:&lt;/strong> Supports multiple operating systems such as Linux, macOS, Windows, making it convenient for users to use on different platforms.&lt;/li>
&lt;li>&lt;strong>Lightweight Deployment:&lt;/strong> The compiled binary files are small, making it convenient for users to deploy and use.&lt;/li>
&lt;/ul>
&lt;p>For more detailed information, please visit the project homepage of &lt;a class="link" href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener" >Llama.cpp
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;h3 id="ollama">Ollama&lt;/h3>
&lt;a href="https://github.com/ollama/ollama" target="_blank" class="card-github fetch-waiting no-styling"
repo="ollama/ollama" id="repo-DLY4LGxWVkiheO0L-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-DLY4LGxWVkiheO0L-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">ollama&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">ollama&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-DLY4LGxWVkiheO0L-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-DLY4LGxWVkiheO0L-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-DLY4LGxWVkiheO0L-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-DLY4LGxWVkiheO0L-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-DLY4LGxWVkiheO0L-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-DLY4LGxWVkiheO0L-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/ollama\/ollama', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-DLY4LGxWVkiheO0L-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-DLY4LGxWVkiheO0L-language').innerText = data.language;
document.getElementById('repo-DLY4LGxWVkiheO0L-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-DLY4LGxWVkiheO0L-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-DLY4LGxWVkiheO0L-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-DLY4LGxWVkiheO0L-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-DLY4LGxWVkiheO0L-license').classList.add = "no-license"
};
document.getElementById('repo-DLY4LGxWVkiheO0L-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for ollama\/ollama.")
}).catch(err => {
const c = document.getElementById('repo-DLY4LGxWVkiheO0L-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for ollama\/ollama.")
})
&lt;/script>
&lt;p>In the article &lt;a class="link" href="https://cuterwrite.top/en/p/ollama/" >【Ollama: From Beginner to Advanced】
&lt;/a>
, it is introduced that Ollama is a tool for building large language model applications. It provides a simple and easy-to-use command line interface and server, allowing you to easily download, run, and manage various open-source LLMs. Unlike traditional LLMs that require complex configuration and powerful hardware, Ollama allows you to conveniently experience the powerful features of LLMs as easily as using a mobile app.&lt;/p>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Simple and Easy to Use&lt;/strong>: Ollama provides a simple and easy-to-use command line tool for users to download, run, and manage LLM.&lt;/li>
&lt;li>&lt;strong>Multiple models&lt;/strong>: Ollama supports various open-source LLMs, including Qwen2, Llama3, Mistral, etc.&lt;/li>
&lt;li>&lt;strong>Compatible with OpenAI Interface&lt;/strong>: Ollama supports the OpenAI API interface, making it easy to switch existing applications to Ollama.&lt;/li>
&lt;/ul>
&lt;p>For more details, please visit the project homepage of &lt;a class="link" href="https://github.com/ollama/ollama" target="_blank" rel="noopener" >Ollama
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;h2 id="agent-and-rag-framework">Agent and RAG framework&lt;/h2>
&lt;p>Combining LLM with external data and tools can build more powerful applications. Here are some commonly used Agent and RAG frameworks:&lt;/p>
&lt;h3 id="llamaindex">LlamaIndex&lt;/h3>
&lt;a href="https://github.com/run-llama/llama_index" target="_blank" class="card-github fetch-waiting no-styling"
repo="run-llama/llama_index" id="repo-lJa6BVaPQCUe8PsU-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-lJa6BVaPQCUe8PsU-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">run-llama&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">llama_index&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-lJa6BVaPQCUe8PsU-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-lJa6BVaPQCUe8PsU-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-lJa6BVaPQCUe8PsU-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-lJa6BVaPQCUe8PsU-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-lJa6BVaPQCUe8PsU-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-lJa6BVaPQCUe8PsU-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/run-llama\/llama_index', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-lJa6BVaPQCUe8PsU-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-lJa6BVaPQCUe8PsU-language').innerText = data.language;
document.getElementById('repo-lJa6BVaPQCUe8PsU-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-lJa6BVaPQCUe8PsU-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-lJa6BVaPQCUe8PsU-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-lJa6BVaPQCUe8PsU-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-lJa6BVaPQCUe8PsU-license').classList.add = "no-license"
};
document.getElementById('repo-lJa6BVaPQCUe8PsU-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for run-llama\/llama_index.")
}).catch(err => {
const c = document.getElementById('repo-lJa6BVaPQCUe8PsU-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for run-llama\/llama_index.")
})
&lt;/script>
&lt;p>LlamaIndex (GPT Index) is a data framework for LLM applications. Building applications with LlamaIndex typically requires using LlamaIndex core and a selected set of integrations (or plugins). There are two ways to build applications with LlamaIndex in Python:&lt;/p>
&lt;ul>
&lt;li>Launcher: &lt;code>llama-index&lt;/code> ( &lt;a class="link" href="https://pypi.org/project/llama-index/%29" target="_blank" rel="noopener" >https://pypi.org/project/llama-index/)
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
. Python starter package, includes core LlamaIndex and some integrations.&lt;/li>
&lt;li>Customization: &lt;code>llama-index-core&lt;/code> ( &lt;a class="link" href="https://pypi.org/project/llama-index-core/%29" target="_blank" rel="noopener" >https://pypi.org/project/llama-index-core/)
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
. Install the core LlamaIndex and add the necessary LlamaIndex integration packages for your application on LlamaHub. There are currently over 300 LlamaIndex integration packages that can seamlessly collaborate with the core, allowing you to build using your preferred LLM, embeddings, and vector storage databases.&lt;/li>
&lt;/ul>
&lt;p>The LlamaIndex Python library is named as such, so import statements containing &lt;code>core&lt;/code> mean that the core package is being used. Conversely, those statements without &lt;code>core&lt;/code> mean that the integration package is being used.&lt;/p>
&lt;pre>&lt;code class="language-python"># typical pattern
from llama_index.core.xxx import ClassABC # core submodule xxx
from llama_index.xxx.yyy import (
SubclassABC,
) # integration yyy for submodule xxx
# concrete example
from llama_index.core.llms import LLM
from llama_index.llms.openai import OpenAI
&lt;/code>&lt;/pre>
&lt;h3 id="crewai">CrewAI&lt;/h3>
&lt;a href="https://github.com/joaomdmoura/crewAI" target="_blank" class="card-github fetch-waiting no-styling"
repo="joaomdmoura/crewAI" id="repo-2ZgJM0JlZBfRLgZ2-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-2ZgJM0JlZBfRLgZ2-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">joaomdmoura&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">crewAI&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-2ZgJM0JlZBfRLgZ2-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-2ZgJM0JlZBfRLgZ2-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-2ZgJM0JlZBfRLgZ2-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-2ZgJM0JlZBfRLgZ2-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-2ZgJM0JlZBfRLgZ2-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-2ZgJM0JlZBfRLgZ2-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/joaomdmoura\/crewAI', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-2ZgJM0JlZBfRLgZ2-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-2ZgJM0JlZBfRLgZ2-language').innerText = data.language;
document.getElementById('repo-2ZgJM0JlZBfRLgZ2-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-2ZgJM0JlZBfRLgZ2-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-2ZgJM0JlZBfRLgZ2-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-2ZgJM0JlZBfRLgZ2-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-2ZgJM0JlZBfRLgZ2-license').classList.add = "no-license"
};
document.getElementById('repo-2ZgJM0JlZBfRLgZ2-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for joaomdmoura\/crewAI.")
}).catch(err => {
const c = document.getElementById('repo-2ZgJM0JlZBfRLgZ2-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for joaomdmoura\/crewAI.")
})
&lt;/script>
&lt;p>CrewAI is a framework for building AI Agents that can integrate LLM with other tools and APIs to accomplish more complex tasks, such as automating web operations, generating code, and more.&lt;/p>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Role-Based Agent Design&lt;/strong>: You can customize agents using specific roles, goals, and tools.&lt;/li>
&lt;li>&lt;strong>Delegation between Autonomous Agents&lt;/strong>: Agents can autonomously delegate tasks to other agents and query information from each other, thereby improving problem-solving efficiency.&lt;/li>
&lt;li>&lt;strong>Flexible task management&lt;/strong>: Customizable tools can be used to define tasks and dynamically assign tasks to agents.&lt;/li>
&lt;li>&lt;strong>Process-Driven&lt;/strong>: The system is process-centered, currently supporting sequential task execution and hierarchical processes. In the future, it will also support more complex processes, such as negotiation and autonomous processes.&lt;/li>
&lt;li>&lt;strong>Save output as file&lt;/strong>: Allows saving the output of a single task as a file for later use.&lt;/li>
&lt;li>&lt;strong>Parse output to Pydantic or Json&lt;/strong>: It is possible to parse the output of a single task into a Pydantic model or Json format for easy subsequent processing and analysis.&lt;/li>
&lt;li>&lt;strong>Support for Open Source Models&lt;/strong>: You can use OpenAI or other open source models to run your agent team. For more information on configuring agent and model connections, including how to connect to a locally running model, see &lt;a class="link" href="https://docs.crewai.com/how-to/LLM-Connections/" target="_blank" rel="noopener" >Connecting crewAI to Large Language Models
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/li>
&lt;/ul>
&lt;p>For more detailed information, please visit the project homepage of &lt;a class="link" href="https://github.com/joaomdmoura/crewAI" target="_blank" rel="noopener" >CrewAI
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;h3 id="opendevin">OpenDevin&lt;/h3>
&lt;a href="https://github.com/opendevin/opendevin" target="_blank" class="card-github fetch-waiting no-styling"
repo="opendevin/opendevin" id="repo-x18TfkWNLTLM8txJ-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-x18TfkWNLTLM8txJ-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">opendevin&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">opendevin&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-x18TfkWNLTLM8txJ-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-x18TfkWNLTLM8txJ-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-x18TfkWNLTLM8txJ-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-x18TfkWNLTLM8txJ-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-x18TfkWNLTLM8txJ-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-x18TfkWNLTLM8txJ-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/opendevin\/opendevin', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-x18TfkWNLTLM8txJ-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-x18TfkWNLTLM8txJ-language').innerText = data.language;
document.getElementById('repo-x18TfkWNLTLM8txJ-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-x18TfkWNLTLM8txJ-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-x18TfkWNLTLM8txJ-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-x18TfkWNLTLM8txJ-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-x18TfkWNLTLM8txJ-license').classList.add = "no-license"
};
document.getElementById('repo-x18TfkWNLTLM8txJ-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for opendevin\/opendevin.")
}).catch(err => {
const c = document.getElementById('repo-x18TfkWNLTLM8txJ-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for opendevin\/opendevin.")
})
&lt;/script>
&lt;p>OpenDevin is an autonomous software engineer platform powered by artificial intelligence and LLMs.&lt;/p>
&lt;p>OpenDevin agents collaborate with human developers to write code, fix bugs, and release features.&lt;/p>
&lt;p>For more information, please visit the project homepage of &lt;a class="link" href="https://github.com/opendevin/opendevin" target="_blank" rel="noopener" >OpenDevin
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;h2 id="model-evaluation">Model evaluation&lt;/h2>
&lt;p>In order to select a suitable LLM and evaluate its performance, we need to conduct model evaluation:&lt;/p>
&lt;h3 id="lmsys">LMSys&lt;/h3>
&lt;p>LMSys Org is an open research organization founded by students and faculty from the University of California, Berkeley, in collaboration with the University of California, San Diego, and Carnegie Mellon University.&lt;/p>
&lt;p>The goal is to make large models accessible to everyone by jointly developing open models, datasets, systems, and evaluation tools. Train large language models and make their applications widely available, while also developing distributed systems to accelerate their training and inference process.&lt;/p>
&lt;p>Currently, the LMSys Chatbot Area is one of the most recognized large model rankings, acknowledged by many companies and research institutions.&lt;/p>
&lt;p>Leaderboard address: &lt;a class="link" href="https://arena.lmsys.org/" target="_blank" rel="noopener" >https://arena.lmsys.org/
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h3 id="opencompass">OpenCompass&lt;/h3>
&lt;p>OpenCompass is an LLM evaluation platform that supports various models (Llama3, Mistral, InternLM2, GPT-4, LLaMa2, Qwen, GLM, Claude, etc.) on over 100 datasets.&lt;/p>
&lt;a href="https://github.com/open-compass/opencompass" target="_blank" class="card-github fetch-waiting no-styling"
repo="open-compass/opencompass" id="repo-jPPTwMsieg9SpucH-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-jPPTwMsieg9SpucH-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">open-compass&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">opencompass&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-jPPTwMsieg9SpucH-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-jPPTwMsieg9SpucH-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-jPPTwMsieg9SpucH-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-jPPTwMsieg9SpucH-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-jPPTwMsieg9SpucH-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-jPPTwMsieg9SpucH-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/open-compass\/opencompass', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-jPPTwMsieg9SpucH-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-jPPTwMsieg9SpucH-language').innerText = data.language;
document.getElementById('repo-jPPTwMsieg9SpucH-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-jPPTwMsieg9SpucH-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-jPPTwMsieg9SpucH-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-jPPTwMsieg9SpucH-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-jPPTwMsieg9SpucH-license').classList.add = "no-license"
};
document.getElementById('repo-jPPTwMsieg9SpucH-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for open-compass\/opencompass.")
}).catch(err => {
const c = document.getElementById('repo-jPPTwMsieg9SpucH-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for open-compass\/opencompass.")
})
&lt;/script>
&lt;h3 id="open-llm-leaderboard">Open LLM Leaderboard&lt;/h3>
&lt;p>Open LLM Leaderboard is a continuously updated LLM ranking list that ranks different models based on multiple evaluation metrics, making it convenient for developers to understand the latest model performance and development trends.&lt;/p>
&lt;p>Leaderboard address: &lt;a class="link" href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard" target="_blank" rel="noopener" >https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>The LLM ecosystem is thriving, covering all aspects from model training to application implementation. With continuous technological advancements, it is believed that LLM will play a more important role in more fields, bringing us a more intelligent application experience.&lt;/p></description></item><item><title>Ollama: From Beginner to Advanced</title><link>https://cuterwrite.top/en/p/ollama/</link><pubDate>Sat, 15 Jun 2024 23:10:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/ollama/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116903387_p0_master1200.webp" alt="Featured image of post Ollama: From Beginner to Advanced" />&lt;p>In recent years, large language models (LLM) have become a cornerstone of the artificial intelligence field due to their powerful text generation and understanding capabilities. Commercial LLMs are often expensive and have closed-source code, limiting the exploration space for researchers and developers. Fortunately, the open-source community offers excellent alternatives like Ollama, allowing everyone to easily experience the charm of LLMs and combine them with HPC and IDE plugins to create more powerful personal assistants.&lt;/p>
&lt;h2 id="what-is-ollama">What is Ollama?&lt;/h2>
&lt;p>Ollama is a tool for building large language model applications. It offers a simple and easy-to-use command line interface and server, allowing you to easily download, run, and manage various open-source LLMs. Unlike traditional LLMs that require complex configurations and powerful hardware, Ollama allows you to conveniently experience the powerful features of LLMs as if you were using a mobile app.&lt;/p>
&lt;h2 id="advantages-of-ollama">Advantages of Ollama&lt;/h2>
&lt;p>Ollama has the following significant advantages:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Open source and free&lt;/strong>: Ollama and its supported models are completely open source and free, allowing anyone to use, modify, and distribute them freely.&lt;/li>
&lt;li>&lt;strong>Simple and Easy to Use&lt;/strong>: No need for complex configuration and installation processes, Ollama can be started and run with just a few commands.&lt;/li>
&lt;li>&lt;strong>Rich Model&lt;/strong>: Ollama supports many popular open-source LLMs such as Llama 3, Mistral, Qwen2, and provides one-click download and switching features.&lt;/li>
&lt;li>&lt;strong>Low resource consumption&lt;/strong>: Compared to commercial LLMs, Ollama has lower hardware requirements and can run smoothly even on ordinary laptops.&lt;/li>
&lt;li>&lt;strong>Community Activity&lt;/strong>: Ollama has a large and active community where users can easily get help, share experiences, and participate in model development.&lt;/li>
&lt;/ul>
&lt;h2 id="how-to-use-ollama">How to use Ollama?&lt;/h2>
&lt;p>Using Ollama is very simple, just follow these steps:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Install Ollama&lt;/strong>: Download and install the latest version from the &lt;a class="link" href="https://ollama.com/" target="_blank" rel="noopener" >Ollama official website
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
according to your operating system.&lt;/li>
&lt;li>&lt;strong>Start Ollama&lt;/strong>: Open the terminal or command line and enter the &lt;code>ollama serve&lt;/code> command to start the Ollama server.&lt;/li>
&lt;li>&lt;strong>Download Model&lt;/strong>: Find the desired model in the &lt;a class="link" href="https://ollama.com/library" target="_blank" rel="noopener" >model library
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, then use the &lt;code>ollama pull&lt;/code> command to download it, for example, &lt;code>ollama pull llama3:70b&lt;/code>.&lt;/li>
&lt;li>&lt;strong>Run the model&lt;/strong>: Use the &lt;code>ollama run&lt;/code> command to start the model, for example &lt;code>ollama run llama3:70b&lt;/code>.&lt;/li>
&lt;li>&lt;strong>Start chatting&lt;/strong>: Enter your question or command in the terminal, and Ollama will generate a corresponding response based on the model.&lt;/li>
&lt;/ol>
&lt;h3 id="install-ollama">Install Ollama&lt;/h3>
&lt;h4 id="macos">macOS&lt;/h4>
&lt;p>&lt;a class="link" href="https://ollama.com/download/Ollama-darwin.zip" target="_blank" rel="noopener" >Download Ollama for macOS
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h4 id="windows">Windows&lt;/h4>
&lt;p>&lt;a class="link" href="https://ollama.com/download/OllamaSetup.exe" target="_blank" rel="noopener" >Download Ollama for Windows
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h4 id="linux">Linux&lt;/h4>
&lt;pre>&lt;code class="language-bash">curl -fsSL https://ollama.com/install.sh | sh
&lt;/code>&lt;/pre>
&lt;h4 id="docker">Docker&lt;/h4>
&lt;h5 id="cpu-version">CPU version&lt;/h5>
&lt;pre>&lt;code class="language-bash">docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
&lt;/code>&lt;/pre>
&lt;h5 id="gpu-version">GPU version&lt;/h5>
&lt;ol>
&lt;li>Install &lt;a class="link" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation" target="_blank" rel="noopener" >Nvidia container toolkit
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;li>Run Ollama in a Docker container&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
&lt;/code>&lt;/pre>
&lt;h3 id="launch-ollama">Launch Ollama&lt;/h3>
&lt;pre>&lt;code class="language-bash">ollama serve
&lt;/code>&lt;/pre>
&lt;p>Output the following information indicating that the Ollama server has successfully started (V100 machine):&lt;/p>
&lt;pre>&lt;code class="language-bash">$ ollama serve
### Omitted log output ###
Listening on [::]:11434 (version 0.1.42)
&lt;/code>&lt;/pre>
&lt;h3 id="download-model">Download model&lt;/h3>
&lt;pre>&lt;code class="language-bash">ollama pull qwen2:72b
&lt;/code>&lt;/pre>
&lt;h3 id="run-model">Run model&lt;/h3>
&lt;pre>&lt;code class="language-bash">ollama run qwen2:72b
&lt;/code>&lt;/pre>
&lt;p>For example, after running the following command:&lt;/p>
&lt;pre>&lt;code class="language-bash">You are trained on data up to October 2023.
&lt;/code>&lt;/pre>
&lt;h4 id="run-model-in-docker-container">Run model in Docker container&lt;/h4>
&lt;pre>&lt;code class="language-bash">docker exec -it ollama ollama run qwen2:72b
&lt;/code>&lt;/pre>
&lt;h3 id="configure-ollama">Configure Ollama&lt;/h3>
&lt;p>Ollama provides a variety of environment variables for configuration:&lt;/p>
&lt;ul>
&lt;li>&lt;code>OLLAMA_DEBUG&lt;/code>: Whether to enable debug mode, default is &lt;code>false&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_FLASH_ATTENTION&lt;/code>: Whether to flash attention, default is &lt;code>true&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_HOST&lt;/code>: Host address of the Ollama server, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_KEEP_ALIVE&lt;/code>: Time to keep the connection alive, default is &lt;code>5m&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_LLM_LIBRARY&lt;/code>: LLM library, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_LOADED_MODELS&lt;/code>: Maximum number of loaded models, default is &lt;code>1&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_QUEUE&lt;/code>: Maximum number of queues, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_VRAM&lt;/code>: Maximum virtual memory, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_MODELS&lt;/code>: Model directory, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_NOHISTORY&lt;/code>: Whether to save history, defaults to &lt;code>false&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_NOPRUNE&lt;/code>: Whether to enable pruning, default is &lt;code>false&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_NUM_PARALLEL&lt;/code>: Number of parallels, default is &lt;code>1&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_ORIGINS&lt;/code>: Allowed origins, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_RUNNERS_DIR&lt;/code>: Runner directory, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_SCHED_SPREAD&lt;/code>: Scheduling distribution, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_TMPDIR&lt;/code>: Temporary file directory, defaults to empty. Here is the optimized list in the desired format:&lt;/li>
&lt;li>&lt;code>OLLAMA_DEBUG&lt;/code>: Whether to enable debug mode, default is &lt;code>false&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_FLASH_ATTENTION&lt;/code>: Whether to flash attention, default is &lt;code>true&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_HOST&lt;/code>: Host address of the Ollama server, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_KEEP_ALIVE&lt;/code>: Duration to keep the connection alive, default is &lt;code>5m&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_LLM_LIBRARY&lt;/code>: LLM library, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_LOADED_MODELS&lt;/code>: Maximum number of loaded models, default is &lt;code>1&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_QUEUE&lt;/code>: Maximum queue number, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_VRAM&lt;/code>: Maximum virtual memory, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_MODELS&lt;/code>: Model directory, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_NOHISTORY&lt;/code>: Whether to save history, default is &lt;code>false&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_NOPRUNE&lt;/code>: Whether to enable pruning, default is &lt;code>false&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_NUM_PARALLEL&lt;/code>: Number of parallels, default is &lt;code>1&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_ORIGINS&lt;/code>: Allowed origins, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_RUNNERS_DIR&lt;/code>: Runner directory, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_SCHED_SPREAD&lt;/code>: Scheduling distribution, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_TMPDIR&lt;/code>: Temporary file directory, default is empty.&lt;/li>
&lt;/ul>
&lt;h2 id="advanced-usage-deploying-ollama-on-an-hpc-cluster">Advanced Usage: Deploying Ollama on an HPC Cluster&lt;/h2>
&lt;p>For large models or situations requiring higher performance, the powerful computing power of an HPC cluster can be used to run Ollama. By combining with Slurm for task management and using port mapping to expose the service locally, remote access and use can be conveniently achieved:&lt;/p>
&lt;ol>
&lt;li>Configure the Ollama environment on the login node: Install Ollama and download the required models.&lt;/li>
&lt;li>&lt;strong>Write a slurm script&lt;/strong>: Specify resource requirements (CPU, memory, GPU, etc.), and use the &lt;code>ollama serve&lt;/code> command to start the model service and bind it to a specific port.&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">#!/bin/bash
#SBATCH --job-name=ollama
#SBATCH -N 1
#SBATCH -p GPU
#SBATCH --gres=gpu:1
module load CUDA
ollama serve
&lt;/code>&lt;/pre>
&lt;ol start="3">
&lt;li>&lt;strong>Submit slurm task&lt;/strong>: Use the &lt;code>sbatch&lt;/code> command to submit the script, Slurm will allocate the task to compute nodes for execution.&lt;/li>
&lt;li>&lt;strong>Local Port Mapping&lt;/strong>: Use the ssh -L command to map the compute node&amp;rsquo;s port to the local machine, for example:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">ssh -t -t username@login node ip -L 11434:localhost:11434 -i login node private key ssh compute node IP -L 11434:127.0.0.1:11434
&lt;/code>&lt;/pre>
&lt;ol start="5">
&lt;li>&lt;strong>Local Access&lt;/strong>: Access http://localhost:11434 in a browser or application to use the Ollama service.&lt;/li>
&lt;/ol>
&lt;div class="notice notice-tip" >
&lt;div class="notice-title">&lt;svg t="1705945832245" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="19409" width="200" height="200">&lt;path d="M512 74.666667C270.933333 74.666667 74.666667 270.933333 74.666667 512S270.933333 949.333333 512 949.333333 949.333333 753.066667 949.333333 512 753.066667 74.666667 512 74.666667z m238.933333 349.866666l-2.133333 2.133334-277.333333 277.333333c-10.666667 10.666667-29.866667 12.8-42.666667 2.133333L426.666667 704l-149.333334-149.333333c-12.8-12.8-12.8-32 0-44.8 10.666667-10.666667 29.866667-12.8 42.666667-2.133334l2.133333 2.133334 125.866667 125.866666 253.866667-253.866666c10.666667-10.666667 29.866667-12.8 42.666666-2.133334l2.133334 2.133334c12.8 12.8 12.8 32 4.266666 42.666666z" fill="#ffffff" p-id="19410">&lt;/path>&lt;/svg>&lt;/div>&lt;p>Note: Since the compute node is not connected to the internet, you need to use &lt;code>ollama pull&lt;/code> on the login node in advance to download the required model. Additionally, you need to set &lt;code>OLLAMA_ORIGINS&lt;/code> to &lt;code>*&lt;/code> and &lt;code>OLLAMA_HOST&lt;/code> to &lt;code>0.0.0.0&lt;/code> to allow all sources to access the service.&lt;/p>&lt;/div>
&lt;h2 id="advanced-usage-local-code-completion-assistant">Advanced Usage: Local Code Completion Assistant&lt;/h2>
&lt;p>Ollama can not only be used for chatting and text creation but also for creating a powerful code completion assistant by combining code generation models and IDE plugins. For example, using the Codeqwen 7B model and the VS Code plugin &lt;a class="link" href="https://continue.dev/" target="_blank" rel="noopener" >Continue
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, efficient and convenient code completion functionality can be achieved.&lt;/p>
&lt;p>First, let me introduce Continue:
&lt;blockquote>
&lt;p>&lt;a class="link" href="https://continue.dev/" target="_blank" rel="noopener" >Continue
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
allows you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs. All of this can run entirely on your own laptop or have Ollama deployed on a server to remotely power code completion and chat experiences based on your needs.&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Continue&lt;/span>&lt;cite>&lt;/cite>&lt;/span>&lt;/blockquote>&lt;/p>
&lt;p>Before starting, you need to install the following tools:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://docs.continue.dev/quickstart" target="_blank" rel="noopener" >Continue
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
: &lt;a class="link" href="https://marketplace.visualstudio.com/items?itemName=Continue.continue" target="_blank" rel="noopener" >VS Code Version
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
or &lt;a class="link" href="https://plugins.jetbrains.com/plugin/22707-continue" target="_blank" rel="noopener" >JetBrains Version
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;li>&lt;a class="link" href="https://ollama.com/" target="_blank" rel="noopener" >Ollama
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;/ul>
&lt;p>Next, using VS Code as an example, we will introduce how to use Ollama + Continue to implement code completion functionality:&lt;/p>
&lt;h3 id="codestral-22b-model">Codestral 22B Model&lt;/h3>
&lt;p>Codestral is capable of both code auto-completion and chat functionality. However, given that it has 22 billion parameters and lacks a production license, it requires a significant amount of video memory and is limited to research and testing use, so it may not be suitable for everyday local applications.&lt;/p>
&lt;h4 id="download-and-run-the-codestral-model">Download and run the Codestral model&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run codestral
&lt;/code>&lt;/pre>
&lt;h4 id="configure-configjson">Configure config.json&lt;/h4>
&lt;ul>
&lt;li>In the VS Code sidebar, click the Continue plugin icon, then click the &amp;ldquo;gear&amp;rdquo; icon at the bottom right of the panel to open the &lt;code>config.json&lt;/code> file. Then copy the following configuration into the &lt;code>config.json&lt;/code> file:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-json">{
&amp;quot;models&amp;quot;: [
{
&amp;quot;title&amp;quot;: &amp;quot;Codestral&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;codestral&amp;quot;
}
],
&amp;quot;tabAutocompleteModel&amp;quot;: {
&amp;quot;title&amp;quot;: &amp;quot;Codestral&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;codestral&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="deepseek-coder-67b-model--llama-3-8b-model">DeepSeek Coder 6.7B model + Llama 3 8B model&lt;/h3>
&lt;p>Depending on the machine&amp;rsquo;s VRAM size, you can utilize Ollama&amp;rsquo;s ability to run multiple models simultaneously and handle multiple concurrent requests, using &lt;code>DeepSeek Coder 6.7B&lt;/code> for auto-completion, and &lt;code>Llama 3 8B&lt;/code> for chatting. If your machine cannot run both at the same time, you can try them separately to decide whether you prefer the local auto-completion or the local chat experience.&lt;/p>
&lt;h4 id="download-and-run-the-deepseek-coder-model">Download and run the DeepSeek Coder model&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run deepseek-coder:6.7b-base
&lt;/code>&lt;/pre>
&lt;h4 id="download-and-run-the-llama-3-model">Download and run the Llama 3 model&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run llama3:8b
&lt;/code>&lt;/pre>
&lt;h4 id="configure-configjson-1">Configure config.json&lt;/h4>
&lt;pre>&lt;code class="language-json">{
&amp;quot;models&amp;quot;: [
{
&amp;quot;title&amp;quot;: &amp;quot;Llama 3 8B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;llama3:8b&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
],
&amp;quot;tabAutocompleteModel&amp;quot;: {
&amp;quot;title&amp;quot;: &amp;quot;DeepSeek Coder 6.7B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;deepseek-coder:6.7b-base&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="codeqwen-7b-model--qwen2-7b-model">Codeqwen 7B model + Qwen2 7B model&lt;/h3>
&lt;p>The Codeqwen 7B model is a model specifically designed for code completion, while the Qwen2 7B model is a general-purpose chat model. These two models can be well combined to achieve both code completion and chat functions.&lt;/p>
&lt;h4 id="download-and-run-the-codeqwen-model">Download and run the Codeqwen model&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run codeqwen
&lt;/code>&lt;/pre>
&lt;h4 id="download-and-run-the-qwen2-model">Download and run the Qwen2 model&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run qwen2:7b
&lt;/code>&lt;/pre>
&lt;h4 id="configure-configjson-2">Configure config.json&lt;/h4>
&lt;pre>&lt;code class="language-json">{
&amp;quot;models&amp;quot;: [
{
&amp;quot;title&amp;quot;: &amp;quot;Codeqwen 7B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;codeqwen&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
],
&amp;quot;tabAutocompleteModel&amp;quot;: {
&amp;quot;title&amp;quot;: &amp;quot;Qwen2 7B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;qwen2:7b&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="optimize-chat-using-rag-vector-retrieval">Optimize chat using RAG vector retrieval&lt;/h3>
&lt;p>Continue has a built-in &lt;a class="link" href="https://docs.continue.dev/customization/context-providers#codebase-retrieval" target="_blank" rel="noopener" >@codebase
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
context provider that can automatically retrieve the most relevant code snippets from the codebase. If you have set up a chat model (such as Codestral, Llama 3), then with the help of Ollama and &lt;a class="link" href="https://blog.lancedb.com/lancedb-x-continue/" target="_blank" rel="noopener" >LanceDB
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&amp;rsquo;s vectorization technology, you can achieve more efficient code retrieval and chat experience.&lt;/p>
&lt;p>Here, we use the &lt;code>nomic-embed-text&lt;/code> model as the vector retrieval model:&lt;/p>
&lt;h4 id="download-and-run-the-nomic-embed-text-model">Download and run the Nomic Embed Text model&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run nomic-embed-text
&lt;/code>&lt;/pre>
&lt;h4 id="configure-configjson-3">Configure config.json&lt;/h4>
&lt;ul>
&lt;li>Add the following content to the file:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-json">{
&amp;quot;embeddingsProvider&amp;quot;: {
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;nomic-embed-text&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="code-completion-effect">Code completion effect&lt;/h3>
&lt;ul>
&lt;li>&lt;code>Ctrl + I&lt;/code>: Generate code snippet based on instructions.&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_1-2024-06-17.webp"
alt="codeqwen_1-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_2-2024-06-17.webp"
alt="codeqwen_2-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_3-2024-06-17.webp"
alt="codeqwen_3-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>Cursor hover auto-complete code&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_4-2024-06-17.webp"
alt="codeqwen_4-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;h3 id="chat-with-ollama">Chat with Ollama&lt;/h3>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_5-2024-06-17.webp"
alt="codeqwen_5-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;h3 id="code-auto-comment">Code auto-comment&lt;/h3>
&lt;ul>
&lt;li>Select code to open the right-click menu&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_6-2024-06-17.webp"
alt="codeqwen_6-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_7-2024-06-17.webp"
alt="codeqwen_7-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Ollama has opened the door to the world of open-source LLM, allowing everyone to easily experience the powerful features of LLM and customize applications according to their own needs. Whether for research, development, or daily use, Ollama can provide you with a platform to explore the limitless possibilities of LLM. With the continuous development of Ollama, it is believed that it will bring us more surprises and promote the application and development of LLM technology in various fields.&lt;/p></description></item></channel></rss>