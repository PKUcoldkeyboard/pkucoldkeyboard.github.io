<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>High Performance Computing on Cuterwrite's Blog</title><link>https://cuterwrite.top/en/categories/hpc/</link><description>Recent content in High Performance Computing on Cuterwrite's Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>cuterwrite</copyright><lastBuildDate>Tue, 13 Aug 2024 22:44:00 +0000</lastBuildDate><atom:link href="https://cuterwrite.top/en/categories/hpc/index.xml" rel="self" type="application/rss+xml"/><item><title>Arm Matrix Acceleration: Scalable Matrix Extension SME</title><link>https://cuterwrite.top/en/p/arm-sme-for-performance/</link><pubDate>Tue, 13 Aug 2024 22:44:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/arm-sme-for-performance/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-14_117622407_p0_master1200.webp" alt="Featured image of post Arm Matrix Acceleration: Scalable Matrix Extension SME" />&lt;h1 id="arm-matrix-acceleration-scalable-matrix-extension-sme">
&lt;a href="#arm-matrix-acceleration-scalable-matrix-extension-sme" class="header-anchor">#&lt;/a>
Arm Matrix Acceleration: Scalable Matrix Extension SME
&lt;/h1>
&lt;h2 id="1-sme-introduction">
&lt;a href="#1-sme-introduction" class="header-anchor">#&lt;/a>
1. SME Introduction
&lt;/h2>
&lt;p>Scalable Matrix Extension SME is built on the basis of Scalable Vector Extensions (SVE and SVE2) and adds the capability to efficiently handle matrices. The main features include:&lt;/p>
&lt;ul>
&lt;li>Calculate the SVE vector&amp;rsquo;s outer product&lt;/li>
&lt;li>Matrix tile storage&lt;/li>
&lt;li>Loading, storing, inserting, and extracting tile vectors (including dynamic transposition)&lt;/li>
&lt;li>Streaming SVE mode&lt;/li>
&lt;/ul>
&lt;p>The table below summarizes the main features of SME, SVE, and SVE2:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">SME&lt;/th>
&lt;th style="text-align: left">SVE&lt;/th>
&lt;th style="text-align: left">SVE2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">Streaming SVE Mode&lt;/td>
&lt;td style="text-align: left">NEON DSP++&lt;/td>
&lt;td style="text-align: left">Scalable Vector&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Dynamic Matrix Transpose&lt;/td>
&lt;td style="text-align: left">Multi-Precision Arithmetic&lt;/td>
&lt;td style="text-align: left">Per-Lane Predication&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Vector Cross Product&lt;/td>
&lt;td style="text-align: left">Match Detection and Histogram&lt;/td>
&lt;td style="text-align: left">Gather-load and Scatter-store&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Load, store, insert, and extract matrix vectors&lt;/td>
&lt;td style="text-align: left">Non-temporal scatter/gather&lt;/td>
&lt;td style="text-align: left">Predict vectorization&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">&lt;/td>
&lt;td style="text-align: left">Bitwise Permute&lt;/td>
&lt;td style="text-align: left">ML Extension (FP16 + DOT)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">&lt;/td>
&lt;td style="text-align: left">AE, SHA3, SM4, Crypto&lt;/td>
&lt;td style="text-align: left">V8.6 BF16, FP and Int8 support&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>SME has defined the following new features:&lt;/p>
&lt;ul>
&lt;li>New architecture state, can be used to store two-dimensional matrix tile&lt;/li>
&lt;li>Streaming SVE mode, supports SVE2 instructions where the execution vector length matches the tile length.&lt;/li>
&lt;li>New instruction to accumulate (or decrement) the outer product of two vectors into a matrix tile.&lt;/li>
&lt;li>New load, store, and move instructions: Vectors can be written to a row or column of a matrix tile, or a row or column of a matrix tile can be read into a vector.&lt;/li>
&lt;/ul>
&lt;p>Similar to SVE2, SME is also an extension that supports scalable vector length, enabling vector length agnosticism (VLA), per-lane predication, predication-driven loop control, and management functions.&lt;/p>
&lt;h2 id="2-streaming-sve-mode">
&lt;a href="#2-streaming-sve-mode" class="header-anchor">#&lt;/a>
2. Streaming SVE mode
&lt;/h2>
&lt;p>SME introduced the Streaming SVE mode, which implements a subset of the SVE2 instruction set and adds new SME-specific instructions.&lt;/p>
&lt;p>Streaming SVE mode supports high-throughput streaming data processing for large datasets, and streaming data usually has simple loop control flow and limited conditionality.&lt;/p>
&lt;p>In Non-streaming SVE mode, the complete SVE2 instruction set is supported, typically handling complex data structures and complex judgments.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-14_3443.webp"
alt="Streaming SVE Mode and Non-streaming SVE Mode" width="80%" loading="lazy">&lt;figcaption>
&lt;h4>Streaming SVE Mode and Non-streaming SVE Mode&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>Most SME instructions are only available in Streaming SVE mode. The streaming vector length (SVL) in Streaming SVE mode may differ from the non-streaming vector length (NSVL).&lt;/p>
&lt;p>The expectation is: SVL should be longer than or equal to NSVL, that is, SVL &amp;gt;= NSVL. For example, the length of NSVL can be 128-bit, while the length of SVL can be 512-bit.&lt;/p>
&lt;p>The SVL of SME can be 128-bit, 256-bit, 512-bit, 1024-bit, or 2048-bit. SVL needs to be a power of 2, and NSVL needs to be a multiple of 128.&lt;/p>
&lt;p>Similar to SVE2, the software can control the &lt;code>SMCR_ELx.LEN&lt;/code> register bit to set the effective SVL length that EL1, EL2, EL3 want to use (it can be set shorter than the SVL supported by the hardware).&lt;/p>
&lt;p>For more information on the Streaming SVE mode, refer to section B1.4.6 of the Arm Architecture Reference Manual (A-profile architecture).&lt;/p>
&lt;h2 id="3-switch-between-non-streaming-and-streaming-sve-modes">
&lt;a href="#3-switch-between-non-streaming-and-streaming-sve-modes" class="header-anchor">#&lt;/a>
3. Switch between Non-streaming and Streaming SVE modes
&lt;/h2>
&lt;p>If the CPU hardware implementation supports both Streaming SVE mode of SME and Non-streaming SVE mode of SVE2, applications can dynamically switch between these two operation modes based on their needs.&lt;/p>
&lt;p>Provide an independent operating mode for SME, allowing CPU hardware implementations to offer different vector lengths for the same application. For example, a CPU hardware implementation can choose to support a longer Streaming SVE mode vector length and optimize the hardware for stream operations suitable for high throughput.&lt;/p>
&lt;p>Applications can easily switch dynamically between Streaming SVE mode and Non-streaming SVE mode. The &lt;code>PSTATE.{SM, ZA}&lt;/code> bits introduced by SME can enable and disable Streaming SVE mode and SME ZA storage:&lt;/p>
&lt;ul>
&lt;li>SM: Enable and disable Streaming SVE mode&lt;/li>
&lt;li>ZA: Enable and disable ZA storage access&lt;/li>
&lt;/ul>
&lt;p>You can use the &lt;code>MSR/MRS&lt;/code> instructions to operate the Streaming Vector Control Register (SVCR) to set and read the &lt;code>PSTATE.{SM, ZA}&lt;/code> bits, with specific operations as follows:&lt;/p>
&lt;ul>
&lt;li>&lt;code>MSR SVCRSM, #&amp;lt;imm&amp;gt; MSR SVCRSM, #&lt;/code>&lt;/li>
&lt;li>&lt;code>MSR SVCRZA, #&amp;lt;imm&amp;gt;&lt;/code>&lt;/li>
&lt;li>&lt;code>MSR SVCRSMZA, #&amp;lt;imm&amp;gt;&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>The SMSTART instruction is an alias for the &lt;code>MSR&lt;/code> instruction that sets &lt;code>PSTATE.SM&lt;/code> and &lt;code>PSTATE.ZA&lt;/code>.&lt;/p>
&lt;ul>
&lt;li>&lt;code>SMSTART&lt;/code>: Simultaneously enable Streaming SVE mode and ZA storage access&lt;/li>
&lt;li>&lt;code>SMSTART SM&lt;/code>: Enable Streaming SVE mode&lt;/li>
&lt;li>&lt;code>SMSTART ZA&lt;/code>: Enable ZA storage access&lt;/li>
&lt;/ul>
&lt;p>The SMSTOP instruction is an alias for the &lt;code>MSR&lt;/code> instruction that clears &lt;code>PSTATE.SM&lt;/code> and &lt;code>PSTATE.ZA&lt;/code>.&lt;/p>
&lt;ul>
&lt;li>&lt;code>SMSTOP&lt;/code>: Simultaneously disable Streaming SVE mode and ZA storage access&lt;/li>
&lt;li>&lt;code>SMSTOP SM&lt;/code>: Disable Streaming SVE mode&lt;/li>
&lt;li>&lt;code>SMSTOP ZA&lt;/code>: Disable ZA storage access&lt;/li>
&lt;/ul>
&lt;p>The diagram below shows how the application switches between Streaming SVE mode and Non-streaming SVE mode:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_Scalable_Matrix_p1.webp"
alt="Application switching Streaming SVE mode and Non-streaming SVE mode" width="50%" loading="lazy">&lt;figcaption>
&lt;h4>Application switching Streaming SVE mode and Non-streaming SVE mode&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>For more information on switching between Streaming SVE mode and Non-Streaming SVE mode using SMSTART and SMSTOP, please refer to sections C6.2.327 and C6.2.328 of the Arm Architecture Reference Manual on A-profile architecture.&lt;/p>
&lt;h2 id="4-sme-architecture-status">
&lt;a href="#4-sme-architecture-status" class="header-anchor">#&lt;/a>
4. SME Architecture Status
&lt;/h2>
&lt;p>Similar to SVE2, in Streaming SVE mode, it has &lt;code>Z0-Z31&lt;/code> vector registers and &lt;code>P0-P15&lt;/code> predicate registers.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_4130_ARM2799_3_Scalable_Matrix_p1.webp"
alt="Streaming mode registers" width="70%" loading="lazy">
&lt;/figure>
&lt;p>The lowest numbered SVE vector register &lt;code>Zn&lt;/code> also holds fixed-length &lt;code>Vn&lt;/code>, &lt;code>Qn&lt;/code>, &lt;code>Dn&lt;/code>, &lt;code>Sn&lt;/code>, &lt;code>Hn&lt;/code>, and &lt;code>Bn&lt;/code> registers.&lt;/p>
&lt;p>When entering Streaming SVE mode (&lt;code>PSTATE.SM&lt;/code> changes from 0 to 1) or exiting Streaming SVE mode (&lt;code>PSTATE.SM&lt;/code> changes from 1 to 0), all these registers will be zeroed.&lt;/p>
&lt;p>Most non-streaming SVE2 instructions can be used in Streaming SVE mode, but &lt;strong>may use different vector lengths&lt;/strong> (streaming mode uses VSL length, non-streaming mode uses NVSL length). The &lt;code>RDSVL&lt;/code> instruction can be used to read the current effective vector length VL.&lt;/p>
&lt;pre>&lt;code class="language-armasm">// Read multiple of Streaming SVE vector register size to Xd
RDSVL &amp;lt;Xd&amp;gt;, #&amp;lt;imm&amp;gt;
&lt;/code>&lt;/pre>
&lt;blockquote class="alert-blockquote alert-note">
&lt;p class="alert-heading">
&lt;svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16">
&lt;path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z">&lt;/path>
&lt;/svg>
&lt;span>Note&lt;/span>
&lt;/p>
&lt;p>Because SME supports Vector Length Agnostic (VLA), in Streaming SVE mode, software rarely needs to explicitly read the SVL vector length. In Non-streaming SVE mode, the RDSVL instruction is usually used to determine the value of SVL.&lt;/p>
&lt;/blockquote>
&lt;h2 id="5-za-array">
&lt;a href="#5-za-array" class="header-anchor">#&lt;/a>
5. ZA array
&lt;/h2>
&lt;p>The newly introduced ZA (Z Array, ZA Storage) in SME is a two-dimensional (2D) square array with a size of SVL x SVL. It is called Z Array because the length of its rows and columns is consistent with the Zn registers in Streaming SVE mode.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_4314_ARM2799_4_Scalable_Matrix_p1.webp"
alt="ZA array" width="50%" loading="lazy">&lt;figcaption>
&lt;h4>ZA array&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>For example: If the vector length in Streaming SVE mode is 256-bit, i.e., the length of the Zn register is 256-bit, then the size of ZA is 256/8 bytes x 256/8 bytes.&lt;/p>
&lt;p>The ZA array can be accessed in the following way:&lt;/p>
&lt;ul>
&lt;li>ZA array vector access&lt;/li>
&lt;li>ZA tiles&lt;/li>
&lt;li>ZA tile slices&lt;/li>
&lt;/ul>
&lt;h3 id="51-za-array-vector-access">
&lt;a href="#51-za-array-vector-access" class="header-anchor">#&lt;/a>
5.1 ZA array vector access
&lt;/h3>
&lt;p>A row of the ZA array can be accessed as a vector of SVL length, and this vector can contain elements with data type lengths of 8-bit, 16-bit, 32-bit, 64-bit, or 128-bit, such as 32-bit fp32 floating-point numbers.&lt;/p>
&lt;pre>&lt;code class="language-c">ZA.B[N], ZA.H[N], ZA.S[N], ZA.D[N], ZA.Q[N]
&lt;/code>&lt;/pre>
&lt;p>Among them, &lt;code>B, H, S, D, Q&lt;/code> represent 8-bit, 16-bit, 32-bit, 64-bit, 128-bit, respectively.&lt;/p>
&lt;p>The number of ZA array vectors is the same as the number of bytes in SVL. For example, if SLV is 256-bit, then the number of ZA array vectors is 32, and the range of N is from 0 to 31.&lt;/p>
&lt;p>To support context switching, SME introduces new &lt;code>LDR&lt;/code> and &lt;code>STR&lt;/code> instructions for loading and storing a ZA array vector from memory.&lt;/p>
&lt;pre>&lt;code class="language-armasm">LDR ZA[&amp;lt;Wv&amp;gt;, &amp;lt;imm&amp;gt;], [&amp;lt;Xn|SP&amp;gt;{, #&amp;lt;imm&amp;gt;, MUL VL}]
STR ZA[&amp;lt;Wv&amp;gt;, &amp;lt;imm&amp;gt;], [&amp;lt;Xn|SP&amp;gt;{, #&amp;lt;imm&amp;gt;, MUL VL}]
&lt;/code>&lt;/pre>
&lt;h3 id="52-za-tiles">
&lt;a href="#52-za-tiles" class="header-anchor">#&lt;/a>
5.2 ZA tiles
&lt;/h3>
&lt;p>ZA tile is a square two-dimensional submatrix within ZA. The width of a ZA tile is always SVL, which is the same as the width of the ZA array.&lt;/p>
&lt;p>How many usable ZA tiles ZA can be divided into is determined by the size of the data type of the elements:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Element Data Type Size&lt;/th>
&lt;th style="text-align: left">Tile Quantity&lt;/th>
&lt;th style="text-align: left">Tile Name&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">8-bit&lt;/td>
&lt;td style="text-align: left">1&lt;/td>
&lt;td style="text-align: left">ZA0.B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">16-bit&lt;/td>
&lt;td style="text-align: left">2&lt;/td>
&lt;td style="text-align: left">ZA0.H-ZA1.H&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">32-bit&lt;/td>
&lt;td style="text-align: left">4&lt;/td>
&lt;td style="text-align: left">ZA0.S-ZA3.S&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">64-bit&lt;/td>
&lt;td style="text-align: left">8&lt;/td>
&lt;td style="text-align: left">ZA0.D-ZA7.D&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">128-bit&lt;/td>
&lt;td style="text-align: left">16&lt;/td>
&lt;td style="text-align: left">ZA0.Q-ZA15.Q&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>When the element data type is 8-bit, ZA can only be accessed as a ZA tile (ZA0.B).&lt;/li>
&lt;li>When the element data type is 16-bit, ZA can be accessed as 2 ZA tiles (ZA0.H and ZA1.H).&lt;/li>
&lt;li>When the element data type is 32-bit, ZA can be accessed as 4 ZA tiles (ZA0.S to ZA3.S).&lt;/li>
&lt;li>When the element data type is 64-bit, ZA can be accessed as 8 ZA tiles (ZA0.D to ZA7.D).&lt;/li>
&lt;li>When the element data type is 128-bit, ZA can be accessed as 16 ZA tiles (ZA0.Q to ZA15.Q).&lt;/li>
&lt;/ul>
&lt;p>For example, if SVL is 256-bit and the element data type size is 8-bit, then ZA can be considered as ZA0.B, or it can be seen as 32 vectors (32 rows, each row size is 32 x 8-bit, i.e., 32 elements per row).&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_ZA0B.webp"
alt="ZA0.B" width="50%" loading="lazy">
&lt;/figure>
&lt;p>If SVL is 256-bit and the element data type size is 16-bit, then ZA can be considered as 2 ZA tiles (ZA0.H and ZA1.H), with each tile considered as 16 vectors (16 rows, each row size is 16 x 16-bit, i.e., 16 elements per row).&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_ZA0H_ZA1H.webp"
alt="ZA0.H and ZA1.H" width="40%" loading="lazy">
&lt;/figure>
&lt;p>The advantage of doing this is to fully utilize ZA storage. In practical applications, for example, when the SVL is 256-bit, the element data type size is 32-bit, and the size of ZA is 256-bit x 256-bit, &lt;strong>to perform an outer product operation on vectors in two Z registers&lt;/strong>, the outer product result is a 2D array of 8 x 8 floating-point numbers. This outer product only requires 1/4 of the storage space of ZA. By dividing ZA into 4 ZA tiles, ZA storage can be fully utilized.&lt;/p>
&lt;h3 id="53-za-tile-slices">
&lt;a href="#53-za-tile-slices" class="header-anchor">#&lt;/a>
5.3 ZA tile slices
&lt;/h3>
&lt;p>A ZA tile can be accessed as a whole or in the form of individual ZA tile slices.&lt;/p>
&lt;p>When accessed as a whole, instructions can be accessed using the name of the tile:&lt;/p>
&lt;pre>&lt;code class="language-text">ZA0.B, ZA0.H-ZA1.H, ZA0.S-ZA3.S, ZA0.D-ZA7.D or ZA0.Q-ZA15.Q
&lt;/code>&lt;/pre>
&lt;p>A ZA tile slice is a one-dimensional array composed of &lt;strong>continuous elements in the horizontal or vertical direction of its ZA tile&lt;/strong>, that is, a row or a column in the ZA tile.&lt;/p>
&lt;p>Accessing a vector of a ZA tile is reading and writing a ZA tile slice:&lt;/p>
&lt;ul>
&lt;li>Horizontal or vertical ZA tile slice access is indicated by the &lt;code>H&lt;/code> or &lt;code>V&lt;/code> suffix following the ZA tile name.&lt;/li>
&lt;li>A specific ZA tile slice is represented by an index, indicated by the slice index &lt;code>[N]&lt;/code> following the ZA tile name.&lt;/li>
&lt;/ul>
&lt;p>For example, if the SVL is 128 bits and the element data type size is 8-bit, then its horizontal and vertical ZA tile slice can be represented as shown in the figure below:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_6724_ARM2799_7_Scalable_Matrix_p1.webp"
alt="ZA tile slices" width="50%" loading="lazy">
&lt;/figure>
&lt;p>For example, if the SVL is 128 bits and the element data type size is 16-bit, then its horizontal and vertical ZA tile slice can be represented as shown in the figure below:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_6888_ARM2799_8_Scalable_Matrix_p1.webp"
alt="ZA tile slices" width="50%" loading="lazy">
&lt;/figure>
&lt;p>In order to improve the efficiency of hardware access to ZA tile and ZA tile slices, the ZA tile slices of a ZA tile are interleaved.&lt;/p>
&lt;p>The image below shows an example of this interleaved arrangement. In this example, SVL is 256 bits, and the element data type size is 16 bits. This means that ZA can be viewed as two ZA tiles (ZA0H and ZA1H) and has interleaved horizontal tile slices:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_4885_SME_interleave.webp"
alt="ZA tile slices" width="auto" loading="lazy">
&lt;/figure>
&lt;p>The figure below shows a mixed view of the horizontal and vertical ZA tile slice sizes for different element data types:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_7673_SME_V_H.webp"
alt="ZA tile slices" width="auto" loading="lazy">
&lt;/figure>
&lt;p>The left columns show the different processing methods for each row of the ZA memory.&lt;/p>
&lt;p>Set SIZE as the size of vector elements, where SIZE is 1, 2, 4, 8, 16, representing data types B, H, S, D, or Q, respectively.&lt;/p>
&lt;p>Set NUM_OF_ELEMENTS as the number of elements in the vector, i.e., bytes_of(SVL)/SIZE.&lt;/p>
&lt;p>Horizontal tile slice, &lt;code>ZAnH.&amp;lt;B|H|S|D|Q&amp;gt;[m]&lt;/code> accesses a vector that contains a whole row (m x SIZE + n) in ZA storage. The vector contains elements of data type B, H, S, D, or Q.&lt;/p>
&lt;p>Vertical tile slice, &lt;code>ZAnV.&amp;lt;B|H|S|D|Q&amp;gt;[m] &lt;/code> accesses a vector that contains the entire column (m x SIZE) in ZA storage. This vector contains elements of data type B, H, S, D, or Q.&lt;/p>
&lt;p>&lt;code>ZAnV.[m]&lt;/code> accesses a vector containing column (m x SIZE) and row elements (i x SIZE + n), where i ranges from 0 to NUM_OF_ELEMENTS-1. This vector contains elements of data types B, H, S, D, or Q.&lt;/p>
&lt;p>Be careful with overlapping when applying mixed element data type sizes and horizontal and vertical tile slices.&lt;/p>
&lt;p>For more information on ZA Array, ZA array vectors, tile, and tile slices, refer to sections B1.4.8 to B1.4.12 of the Arm Architecture Reference Manual for the A-profile architecture.&lt;/p>
&lt;h2 id="6-instructions-supported-in-steaming-sve-mode">
&lt;a href="#6-instructions-supported-in-steaming-sve-mode" class="header-anchor">#&lt;/a>
6. Instructions supported in Steaming SVE mode
&lt;/h2>
&lt;p>Some instructions have limitations in Streaming SVE mode:&lt;/p>
&lt;ul>
&lt;li>Some SVE/SVE2 instructions become illegal to execute
&lt;ul>
&lt;li>Gather-load and Scatter-store instructions&lt;/li>
&lt;li>Use the SVE2 instruction of the First Fault register&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Most NEON instructions become UNDEFINED&lt;/li>
&lt;/ul>
&lt;p>For more information about instructions affected by the Streaming SVE mode, please refer to the document &amp;ldquo;Arm Architecture Reference Manual.&amp;rdquo;&lt;/p>
&lt;p>SME has added several new instructions, including:&lt;/p>
&lt;ul>
&lt;li>Matrix outer product and accumulate or subtract instructions, including FMOPA, UMOPA, and BFMOPA.
&lt;ul>
&lt;li>SVE2 vector registers (Z0-Z31) serve as the row and column inputs for outer product operations.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>ZA storage stores the output results of the two-dimensional matrix tile.&lt;/li>
&lt;li>Instructions for performing addition operations with the SVE2 Z vector and the rows or columns of ZA&lt;/li>
&lt;li>Instruction for clearing ZA tiles&lt;/li>
&lt;li>Added some instructions that can be used in both Streaming and Non-streaming modes.&lt;/li>
&lt;/ul>
&lt;h2 id="7-sme-directive">
&lt;a href="#7-sme-directive" class="header-anchor">#&lt;/a>
7. SME Directive
&lt;/h2>
&lt;p>The main SME commands for operating ZA storage include:&lt;/p>
&lt;ul>
&lt;li>Calculate the cross product of two vectors, and then accumulate or decrement, and place the result into an instruction of a ZA tile.&lt;/li>
&lt;li>Instructions to store or load SVE vectors (Z registers) into or from rows or columns of the ZA tile&lt;/li>
&lt;li>In the horizontal or vertical direction, an SVE vector and ZA tile addition instruction&lt;/li>
&lt;li>An instruction to add a multiple of the vector length in Streaming SVE mode to a scalar register&lt;/li>
&lt;/ul>
&lt;h3 id="71-outer-product-and-accumulate-or-subtract-instructions">
&lt;a href="#71-outer-product-and-accumulate-or-subtract-instructions" class="header-anchor">#&lt;/a>
7.1 Outer Product and Accumulate or Subtract Instructions
&lt;/h3>
&lt;p>In order to help understand outer product and accumulate or subtract instructions, let&amp;rsquo;s see how to use the outer product operation to perform matrix multiplication.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_2313_Picture1_png-1280x960.webp"
alt="Outer product" width="auto" loading="lazy">
&lt;/figure>
&lt;p>Calculating the outer product of two vectors a and b will yield a result matrix C containing the outer product:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_1665_Picture2_png-1280x960.webp"
alt="Outer product" width="auto" loading="lazy">
&lt;/figure>
&lt;p>Now consider the matrix multiplication operation of two matrices a and b:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_8117_Picture3_png-1280x960.webp"
alt="Matrix multiplication" width="auto" loading="lazy">
&lt;/figure>
&lt;p>This matrix multiplication can be achieved by calculating two outer product operations and accumulating the two resulting matrices (which is the commonly used handwritten calculation method), as shown in the diagram below:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_3731_Picture4_png-1280x960.webp"
alt="Matrix multiplication with outer product" width="auto" loading="lazy">
&lt;/figure>
&lt;p>SME introduced efficient outer product and accumulate or subtract instructions for the following data types:&lt;/p>
&lt;ul>
&lt;li>8-bit, 16-bit integers&lt;/li>
&lt;li>FP16, BF16, FP32, and FP64 floating point numbers&lt;/li>
&lt;/ul>
&lt;p>These instructions calculate the outer product of two vectors in two Z vector registers (Zn and Zm), accumulate or subtract the resulting array with the existing data in a ZA tile (ZAda), and store the result in the same ZA tile (ZAda). Each source vector is independently predicated by the corresponding control predicate registers (Pn and Pm).&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Output Array&lt;/th>
&lt;th style="text-align: left">Input Vector&lt;/th>
&lt;th style="text-align: left">Description&lt;/th>
&lt;th style="text-align: left">Example&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">INT32&lt;/td>
&lt;td style="text-align: left">INT8, INT8&lt;/td>
&lt;td style="text-align: left">Store the sum of the outer products of four INT8s into each INT32 element&lt;/td>
&lt;td style="text-align: left">SMOPA or SMOPS or UMOPA or UMOPS: Signed or unsigned integer outer product sum, and accumulate or subtract. For example: &lt;code>UMOPS &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.B, &amp;lt;Zm&amp;gt;.B&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">INT32&lt;/td>
&lt;td style="text-align: left">INT16, INT16&lt;/td>
&lt;td style="text-align: left">Store the sum of the outer product of two INT16 in each INT32 element&lt;/td>
&lt;td style="text-align: left">SMOPA or SMOPS or UMOPA or UMOPS: Signed or unsigned integer outer product sum, and accumulate or subtract. For example: &lt;code>UMOPS &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.H, &amp;lt;Zm&amp;gt;.H&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">INT64&lt;/td>
&lt;td style="text-align: left">INT16, INT16&lt;/td>
&lt;td style="text-align: left">If FEAT_SME_I16I64 is implemented, the sum of the outer products of four INT16s is stored in each INT64 element&lt;/td>
&lt;td style="text-align: left">SMOPA or SMOPS or UMOPA or UMOPS: signed or unsigned integer outer product sum, and accumulate or subtract. For example: &lt;code>UMOPS &amp;lt;ZAda&amp;gt;.D, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.H, &amp;lt;Zm&amp;gt;.H&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">FP32&lt;/td>
&lt;td style="text-align: left">BF16, BF16&lt;/td>
&lt;td style="text-align: left">Store the sum of two BF16 outer products into each FP32 element&lt;/td>
&lt;td style="text-align: left">BFMOPA or BFMOPS: BFloat16 outer product sum, with accumulation or subtraction. For example: &lt;code>BFMOPS &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.H, &amp;lt;Zm&amp;gt;.H&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">FP32&lt;/td>
&lt;td style="text-align: left">FP16, FP16&lt;/td>
&lt;td style="text-align: left">Store the sum of two FP16 outer products into each FP32 element&lt;/td>
&lt;td style="text-align: left">FMOPA or FMOPS: Half-precision floating-point outer product sum, and accumulate or subtract. For example: &lt;code>FMOPS &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.H, &amp;lt;Zm&amp;gt;.H&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">FP32&lt;/td>
&lt;td style="text-align: left">FP32, FP32&lt;/td>
&lt;td style="text-align: left">Simple FP32 outer product&lt;/td>
&lt;td style="text-align: left">FMOPA or FMOPS: Floating-point outer product and accumulate or subtract. For example: &lt;code>FMOPS &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.S, &amp;lt;Zm&amp;gt;.S&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">FP64&lt;/td>
&lt;td style="text-align: left">FP64, FP64&lt;/td>
&lt;td style="text-align: left">If FEAT_SME_F64F64 is implemented, perform a simple FP64 outer product&lt;/td>
&lt;td style="text-align: left">FMOPA or FMOPS: Floating point outer product and accumulate or subtract. For example: &lt;code>FMOPS &amp;lt;ZAda&amp;gt;.D, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.D, &amp;lt;Zm&amp;gt;.D&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="711-fp32-fp64-outer-product-and-accumulate-or-subtract-instructions">
&lt;a href="#711-fp32-fp64-outer-product-and-accumulate-or-subtract-instructions" class="header-anchor">#&lt;/a>
7.1.1 FP32, FP64 outer product and accumulate or subtract instructions
&lt;/h4>
&lt;p>Instructions where the input vectors and output arrays have the same data type (FP32, FP64) are relatively simple.&lt;/p>
&lt;p>The following example demonstrates FP32 type outer product with accumulation or subtraction instructions.&lt;/p>
&lt;pre>&lt;code class="language-armasm">FMOPA &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.S, &amp;lt;Zm&amp;gt;.S
FMOPS &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.S, &amp;lt;Zm&amp;gt;.S
&lt;/code>&lt;/pre>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_3613751670-667e5f923c64.webp"
alt="FMOPA and FMOPS" width="auto" loading="lazy">
&lt;/figure>
&lt;p>In this example, assuming the SVL vector length is 128, &lt;code>Zn.S&lt;/code> and &lt;code>Zm.S&lt;/code> contain vectors composed of 4 FP32 numbers, this instruction calculates the outer product of &lt;code>Zn.S&lt;/code> and &lt;code>Zm.S&lt;/code>, the result of the outer product is the gray matrix in the figure, then accumulates or subtracts this outer product result with the existing values in the ZA tile &lt;code>ZAda.S&lt;/code>, and stores the result in the same ZA tile.&lt;/p>
&lt;h4 id="712-fp16-bf16-int16-int8-i16i64-type-outer-product-and-accumulate-or-subtract-instructions">
&lt;a href="#712-fp16-bf16-int16-int8-i16i64-type-outer-product-and-accumulate-or-subtract-instructions" class="header-anchor">#&lt;/a>
7.1.2 FP16, BF16, INT16, INT8, I16I64 type outer product and accumulate or subtract instructions
&lt;/h4>
&lt;p>Because these instructions will expand the data type of the calculation results, these operations are not as straightforward as the previous FP32 and FP64 type instructions.&lt;/p>
&lt;ul>
&lt;li>BF16 instruction calculates the outer product of two BF16s, expands the result type to FP32, and then destructively adds or subtracts the result with the target tile.&lt;/li>
&lt;li>INT8 instructions compute the sum of the outer product of four INT8s, expanding the result type to INT32, and then perform destructive addition or subtraction of the result with the target tile.&lt;/li>
&lt;li>INT16 instruction calculates the outer product sum of two INT16s, expands the result type to INT32, and then performs a destructive add or subtract with the target tile.&lt;/li>
&lt;li>FP16 instructions calculate the sum of the outer product of two FP16s, expand the result type to FP32, and then perform destructive addition or subtraction of the result with the target tile.&lt;/li>
&lt;li>If FEAT_SME_I16I64 is implemented, the I16I64 instruction calculates the sum of the outer products of four INT16s, expands the result type to INT64, and then destructively adds or subtracts the result with the target tile.&lt;/li>
&lt;/ul>
&lt;p>The following example demonstrates the operation of the INT8 UMOPA instruction with an SVL vector length of 128:&lt;/p>
&lt;pre>&lt;code class="language-armasm">UMOPA &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.B, &amp;lt;Zm&amp;gt;.B
&lt;/code>&lt;/pre>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_1030_Picture6_png-1280x960.webp"
alt="INT8 UMOPA" width="auto" loading="lazy">
&lt;/figure>
&lt;p>Each input register (&lt;code>Zn.B&lt;/code>, &lt;code>Zm.B&lt;/code>) is treated as a matrix containing 4x4 elements, which can be seen as blocks composed of 4 consecutive elements (as marked by the red lines in the diagram) that have been transposed.&lt;/p>
&lt;p>In this example, because the SVL vector length is 128-bit:&lt;/p>
&lt;ul>
&lt;li>The first source vector &lt;code>Zn.B&lt;/code> contains a 4x4 submatrix of unsigned 8-bit integers.&lt;/li>
&lt;li>The second source vector &lt;code>Zm.B&lt;/code>, contains a 4x4 submatrix of unsigned 8-bit integers.&lt;/li>
&lt;li>UMOPA instruction calculates the sum of the 4x4 expanded 32-bit integer outer product, then destructively accumulates the integers in the target tile (ZAda).&lt;/li>
&lt;/ul>
&lt;p>More generally, the UMOPA instruction multiplies submatrices from the first source vector with submatrices from the second source vector. Each source vector contains a submatrix of unsigned 8-bit integers of size (SVL/32) x 4. The resulting (SVL/32) x (SVL/32) expanded 32-bit integer outer product is then destructively added to a 32-bit integer target tile.&lt;/p>
&lt;p>The following example demonstrates the operation of a BF16 BFMOPA with an SVL of 128-bit:&lt;/p>
&lt;pre>&lt;code class="language-armasm">BFMOPA &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.H, &amp;lt;Zm&amp;gt;.H
&lt;/code>&lt;/pre>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_6545_Picture7_png-1280x960.webp"
alt="BF16 BFMOPA" width="auto" loading="lazy">
&lt;/figure>
&lt;p>In this example, because the SVL vector length is 128-bit:&lt;/p>
&lt;ul>
&lt;li>The first source vector &lt;code>Zn.H&lt;/code>, contains a 4x2 submatrix of BF16 integers, which is expanded into single-precision floating-point numbers.&lt;/li>
&lt;li>The second source vector &lt;code>Zm.H&lt;/code>, contains a 2x4 submatrix of a BF16 integer, which is expanded into a single-precision floating-point number.&lt;/li>
&lt;li>BMOPA instruction calculates the sum of a 4x4 single-precision outer product, and then destructively accumulates it with the single-precision floating-point numbers in the target tile (ZAda).&lt;/li>
&lt;/ul>
&lt;p>More generally speaking, the BFMOPA instruction expands the type of the (SVL/32) x2 BF16 submatrix stored in the first source vector to single precision, expands the type of the 2x (SVL/32) BF16 submatrix stored in the second source vector to single precision, and multiplies these two submatrices. Then, the resulting (SVL/32) x (SVL/32) single-precision outer product is destructively added to a single-precision target tile.&lt;/p>
&lt;p>The following table shows the number of MACs (Multiply-Accumulate) for the corresponding data type performed by an outer product and accumulate or subtract instruction for several data types and SVL lengths:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">&lt;/th>
&lt;th style="text-align: left">128-bit&lt;/th>
&lt;th style="text-align: left">256-bit&lt;/th>
&lt;th style="text-align: left">512-bit&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">FP32&lt;/td>
&lt;td style="text-align: left">16&lt;/td>
&lt;td style="text-align: left">64&lt;/td>
&lt;td style="text-align: left">256&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">FP64&lt;/td>
&lt;td style="text-align: left">4&lt;/td>
&lt;td style="text-align: left">16&lt;/td>
&lt;td style="text-align: left">64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">INT8&lt;/td>
&lt;td style="text-align: left">64&lt;/td>
&lt;td style="text-align: left">256&lt;/td>
&lt;td style="text-align: left">1024&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">INT16&lt;/td>
&lt;td style="text-align: left">32&lt;/td>
&lt;td style="text-align: left">128&lt;/td>
&lt;td style="text-align: left">512&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">BF16&lt;/td>
&lt;td style="text-align: left">32&lt;/td>
&lt;td style="text-align: left">128&lt;/td>
&lt;td style="text-align: left">512&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">FP16&lt;/td>
&lt;td style="text-align: left">32&lt;/td>
&lt;td style="text-align: left">128&lt;/td>
&lt;td style="text-align: left">512&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="72-sme-instructions-with-predication">
&lt;a href="#72-sme-instructions-with-predication" class="header-anchor">#&lt;/a>
7.2 SME Instructions with Predication
&lt;/h3>
&lt;p>Each source vector can be independently predicated by its corresponding control predicate register:&lt;/p>
&lt;ul>
&lt;li>Outer product and accumulate or subtract instructions use Pn/M and Pn/M (without /Z form): Inactive source elements are treated as having a value of 0.&lt;/li>
&lt;li>Slice move command uses Pg/M: The Inactive elements in the target slice remain unchanged.&lt;/li>
&lt;li>Tile slice load instruction uses Pg/Z: Inactive elements in the target tile slice are set to 0.&lt;/li>
&lt;li>Tile slice store instruction uses Pg: Inactive elements that will not be written to memory.&lt;/li>
&lt;/ul>
&lt;p>Predication makes it easier to handle cases where the dimensions of the matrix are not a multiple of SVL.&lt;/p>
&lt;p>For example, the instructions in the image below:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_2656_Picture12_png-600x0.webp"
alt="SME prediction" width="auto" loading="lazy">
&lt;/figure>
&lt;p>The input vector &lt;code>Z0&lt;/code> is predicated by &lt;code>P0&lt;/code>, &lt;code>Z1&lt;/code> is predicated by &lt;code>P1&lt;/code>.&lt;/p>
&lt;p>In this example:&lt;/p>
&lt;ul>
&lt;li>SVL vector length is 512-bit.&lt;/li>
&lt;li>The Z register contains a vector of 16 FP32 numbers.&lt;/li>
&lt;li>The last two elements in &lt;code>P0&lt;/code> are inactive.&lt;/li>
&lt;li>The last element in &lt;code>P1&lt;/code> is inactive.&lt;/li>
&lt;/ul>
&lt;p>This instruction updates (16-2) x (16-1) FP32 elements in &lt;code>ZA0.S&lt;/code>, because &lt;code>Pn/M&lt;/code> is used, the remaining elements in &lt;code>ZA0.S&lt;/code> remain unchanged.&lt;/p>
&lt;p>The figure below shows more examples of predicated outer products with accumulation or subtraction. The underlined text in the figure indicates the parts of the calculation affected by inactive predicate elements.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_2072_Picture14_png-1280x960.webp"
alt="SME prediction FMOPA" width="auto" loading="lazy">
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_3513_Picture16_png-1280x960.webp"
alt="SME prediction UMOPA" width="auto" loading="lazy">
&lt;/figure>
&lt;h3 id="73-za-tile-and-addition-operation-with-a-z-vector">
&lt;a href="#73-za-tile-and-addition-operation-with-a-z-vector" class="header-anchor">#&lt;/a>
7.3 ZA tile and addition operation with a Z vector
&lt;/h3>
&lt;p>SME includes instructions to add a vector to the rows or columns of a ZA tile, and these instructions also support predication.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Instruction&lt;/th>
&lt;th style="text-align: left">Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">ADDHA&lt;/td>
&lt;td style="text-align: left">Add the source vector to each horizontal slice of the ZA tile&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">ADDVA&lt;/td>
&lt;td style="text-align: left">Add the source vector to each vertical slice of the ZA tile&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>For example:&lt;/p>
&lt;pre>&lt;code class="language-armasm">ADDHA ZA0.S, P0/M, P1/M, Z1.S
&lt;/code>&lt;/pre>
&lt;p>Will perform the following actions:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_ARM2799_9_Scalable_Matrix_p2_png-1200x0.webp"
alt="SME ADDHA" width="auto" loading="lazy">
&lt;/figure>
&lt;p>This ADDHA instruction adds each element of the source vector Z1 to the corresponding active element of each horizontal slice of the ZA0.S tile.&lt;/p>
&lt;p>Elements in a Tile are predicated by a pair of governing predicates. An element in a horizontal slice can be considered active under the following conditions:&lt;/p>
&lt;ul>
&lt;li>It is TRUE for the element corresponding to the second governing predicate, and&lt;/li>
&lt;li>It corresponds to TRUE at the row number of the first governing predicate&amp;rsquo;s horizontal slice, and the inactive elements in the target tile remain unchanged.&lt;/li>
&lt;/ul>
&lt;h3 id="74-tile-load-store-move-instructions">
&lt;a href="#74-tile-load-store-move-instructions" class="header-anchor">#&lt;/a>
7.4 Tile load, store, move instructions
&lt;/h3>
&lt;p>SME tile load, store, move instructions can:&lt;/p>
&lt;ul>
&lt;li>Read data from memory and place it into a row or column of the ZA tile&lt;/li>
&lt;li>Write the row or column of the ZA tile into memory&lt;/li>
&lt;li>Move the row of the ZA tile to the SVE Z vector register&lt;/li>
&lt;li>Move the SVE Z vector register to a ZA tile row or column&lt;/li>
&lt;/ul>
&lt;h4 id="741-tile-slice-load-and-store-instructions">
&lt;a href="#741-tile-slice-load-and-store-instructions" class="header-anchor">#&lt;/a>
7.4.1 Tile slice load and store instructions
&lt;/h4>
&lt;p>The LD1B, LD1H, LD1S, LD1D, and LD1Q instructions load consecutive memory values into a ZA tile slice with 8-bit, 16-bit, 32-bit, 64-bit, or 128-bit elements, respectively.&lt;/p>
&lt;p>The ST1B, ST1H, ST1S, ST1D, and ST1Q instructions store a ZA tile slice containing 8-bit, 16-bit, 32-bit, 64-bit, or 128-bit elements, respectively, into contiguous memory.&lt;/p>
&lt;p>These instructions also support predication, for example:&lt;/p>
&lt;pre>&lt;code class="language-armasm">LD1B ZA0H.B[W0, #imm], P0/Z, [X1, X2]
&lt;/code>&lt;/pre>
&lt;p>This LD1B instruction performs a predicated continuous byte read, reading data from memory at address (X1+X2) into the horizontal tile slice in ZA0 at row number (W0+imm). Inactive elements in the target tile slice are set to 0.&lt;/p>
&lt;pre>&lt;code class="language-armasm">ST1H ZA1V.H[W0, #imm], P2, [X1, X2, LSL #1]
&lt;/code>&lt;/pre>
&lt;p>This ST1H instruction executes a predicated continuous halfword store operation, storing the vertical tile slice in ZA1 with the column number (W0+imm) to the memory address (X1+X2*2), and elements that are inactive in the tile slice are not written to memory.&lt;/p>
&lt;h4 id="742-tile-slice-move-instruction">
&lt;a href="#742-tile-slice-move-instruction" class="header-anchor">#&lt;/a>
7.4.2 Tile slice move instruction
&lt;/h4>
&lt;p>The MOV instruction (alias for the MOVA instruction) moves the value of a Z vector register to a ZA tile slice, or moves the value from a ZA tile slice to a Z vector register. This instruction operates on a single horizontal or vertical tile slice of a ZA tile with a specified element size. The row number/column number of the slice is specified by the slice&amp;rsquo;s retrieval register plus an immediate offset. Inactive elements in the target slice remain unchanged.&lt;/p>
&lt;p>For example:&lt;/p>
&lt;pre>&lt;code class="language-armasm">MOV ZA0H.B[W0, #imm], P0/M, Z0.B
&lt;/code>&lt;/pre>
&lt;p>Or&lt;/p>
&lt;pre>&lt;code class="language-armasm">MOVA ZA0H.B[W0, #imm], P0/M, Z0.B
&lt;/code>&lt;/pre>
&lt;p>This instruction moves the values in vector register &lt;code>Z0.B&lt;/code> to the horizontal ZA tile slice &lt;code>ZA0H.B[W0,#imm]&lt;/code>, using &lt;code>P0&lt;/code> as the predication register. Inactive elements in the target tile slice remain unchanged.&lt;/p>
&lt;h3 id="75-za-array-vector-loadstore-instructions">
&lt;a href="#75-za-array-vector-loadstore-instructions" class="header-anchor">#&lt;/a>
7.5 ZA array vector load/store instructions
&lt;/h3>
&lt;p>SME LDR instruction reads data from memory into a ZA array vector, SME STR instruction stores the values from a ZA array vector into memory.
These instructions do not have predication functionality. They are primarily for saving/restoring ZA storage during software context switching. SME LDR/STR instructions can also be used in Non-streaming SVE mode when PSTATE.ZA is enabled.
For example, the ZA array vector in the following STR instruction is specified by a vector selection register Wv (scalar register W) plus an optional immediate number (Wv+Imm). The address for accessing memory is: a scalar register as the base, plus the same optional immediate offset multiplied by the current vector length in bytes.&lt;/p>
&lt;pre>&lt;code class="language-armasm">STR ZA[&amp;lt;Wv&amp;gt;, &amp;lt;imm&amp;gt;], [&amp;lt;Xn|SP&amp;gt;{, #&amp;lt;imm&amp;gt;, MUL VL}]
&lt;/code>&lt;/pre>
&lt;h3 id="76-za-tile-clear-instruction">
&lt;a href="#76-za-tile-clear-instruction" class="header-anchor">#&lt;/a>
7.6 ZA tile clear instruction
&lt;/h3>
&lt;p>SME ZERO instruction can clear a group of 64-bit ZA tile:&lt;/p>
&lt;pre>&lt;code class="language-armasm">ZERO { &amp;lt;mask&amp;gt;}
&lt;/code>&lt;/pre>
&lt;p>The ZERO instruction can zero out up to 8 ZA tiles named &lt;code>ZA0.D&lt;/code> to &lt;code>ZA8.D&lt;/code>. The tiles to be zeroed are specified by the mask in the instruction, while the remaining tiles remain unchanged.&lt;/p>
&lt;p>This instruction can also be used in Non-streaming SVE mode when &lt;code>PSTATE.ZA&lt;/code> is enabled.&lt;/p>
&lt;p>If you want to clear the entire ZA array, you can use an instruction alias, &lt;code>ZERO {ZA}&lt;/code>.&lt;/p>
&lt;h3 id="77-new-sve2-instructions">
&lt;a href="#77-new-sve2-instructions" class="header-anchor">#&lt;/a>
7.7 New SVE2 Instructions
&lt;/h3>
&lt;p>The SME architecture extension has added some new SVE2 instructions, which can also be used in PE that implements SVE2 when in Non-streaming SVE mode. These instructions include:&lt;/p>
&lt;ul>
&lt;li>Select a predicate register or an all-false Predicate select instruction&lt;/li>
&lt;li>Reverse 64-bit double word element instruction&lt;/li>
&lt;li>Signed/Unsigned clamp to smaller/larger value vector instructions&lt;/li>
&lt;/ul>
&lt;p>The following introduces the Predicate select instruction.&lt;/p>
&lt;h4 id="771-psel-instruction">
&lt;a href="#771-psel-instruction" class="header-anchor">#&lt;/a>
7.7.1 PSEL Instruction
&lt;/h4>
&lt;p>PSEL instruction selects a predicate register or all-false to the target predicate register, as follows:&lt;/p>
&lt;pre>&lt;code class="language-armasm">PSEL &amp;lt;Pd&amp;gt;, &amp;lt;Pn&amp;gt;, &amp;lt;Pm&amp;gt;.&amp;lt;T&amp;gt;[&amp;lt;Wv&amp;gt;, &amp;lt;imm&amp;gt;]
&lt;/code>&lt;/pre>
&lt;p>If the element specified in the second source predicate register (Pm) is True, this instruction places the content of the first source predicate register (Pn) into the destination predicate register (Pd), otherwise, it sets the value of the destination predicate register to all false.
For example, the following instruction, assuming the value of W12 is 0:&lt;/p>
&lt;pre>&lt;code class="language-armasm">PSEL P0, P1, P2.B[W12, #0]
&lt;/code>&lt;/pre>
&lt;p>The [0]th element of the second source predicate register [W12+0] is False, so the target register P0 is set to all 0 (all-false), as shown in the figure below:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_4401_Picture10_png-1280x960.webp"
alt="SME PSEL" width="auto" loading="lazy">
&lt;/figure>
&lt;p>Now look at the following instruction, still assuming the value of W12 is 0, but this time the immediate offset is 1:&lt;/p>
&lt;pre>&lt;code class="language-armasm">PSEL P0, P1, P2.B[W12, #1]
&lt;/code>&lt;/pre>
&lt;p>The [1] element of the second source predicate register [W12+1] is True, therefore select the value of the first source predicate register to the destination register P0, as shown in the diagram below:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_0116_Picture11_png-1280x960.webp"
alt="SME PSEL" width="auto" loading="lazy">
&lt;/figure>
&lt;h2 id="references">
&lt;a href="#references" class="header-anchor">#&lt;/a>
References
&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://community.arm.com/arm-community-blogs/b/architectures-and-processors-blog/posts/arm-scalable-matrix-extension-introduction" target="_blank" rel="noopener" >Arm Scalable Matrix Extension (SME) Introduction
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;li>&lt;a class="link" href="https://community.arm.com/arm-community-blogs/b/architectures-and-processors-blog/posts/arm-scalable-matrix-extension-introduction-p2" target="_blank" rel="noopener" >Arm Scalable Matrix Extension (SME) Instructions
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;/ul></description></item><item><title>Arm Performance Optimization: Scalable Vector Extension SVE</title><link>https://cuterwrite.top/en/p/arm-sve-for-performance/</link><pubDate>Sun, 11 Aug 2024 02:13:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/arm-sve-for-performance/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_117622464_p0_master1200.webp" alt="Featured image of post Arm Performance Optimization: Scalable Vector Extension SVE" />&lt;h1 id="arm-performance-optimization-scalable-vector-extension-sve">
&lt;a href="#arm-performance-optimization-scalable-vector-extension-sve" class="header-anchor">#&lt;/a>
ARM Performance Optimization: Scalable Vector Extension SVE
&lt;/h1>
&lt;h2 id="1-sve-introduction">
&lt;a href="#1-sve-introduction" class="header-anchor">#&lt;/a>
1. SVE Introduction
&lt;/h2>
&lt;p>After the Neon architecture extension with a fixed 128-bit vector length instruction set, Arm designed the Scalable Vector Extension (SVE) as the next-generation SIMD extension for AArch64. SVE introduces the scalable concept, allowing flexible vector length implementations and providing a range of possible values in CPU implementations. The vector length can vary from a minimum of 128 bits to a maximum of 2048 bits, in increments of 128 bits. &lt;strong>The SVE design ensures that the same application can run on different SVE-supporting implementations without recompiling the code&lt;/strong>. SVE enhances the architecture&amp;rsquo;s applicability to high-performance computing (HPC) and machine learning (ML) applications, which require very large amounts of data processing. SVE2 is a superset of SVE and Neon. SVE2 allows the use of more functional domains in data-level parallelism. SVE2 inherits the concepts, vector registers, and operation principles of SVE. SVE and SVE2 define 32 scalable vector registers. Chip partners can choose an appropriate vector length design implementation, with hardware varying between 128 bits and 2048 bits (in increments of 128 bits). The advantage of SVE and SVE2 is that only one vector instruction set uses scalable variables.&lt;/p>
&lt;p>The SVE design philosophy allows developers to write and build software once, and then run the same binary on different AArch64 hardware with various SVE vector length implementations. The portability of the binary means developers do not need to know the vector length implementation of their system. This eliminates the need to rebuild the binary, making the software easier to port. In addition to scalable vectors, SVE and SVE2 also include:&lt;/p>
&lt;ul>
&lt;li>per-lane predication&lt;/li>
&lt;li>Gather Load/Scatter Store&lt;/li>
&lt;li>Speculative Vectorization&lt;/li>
&lt;/ul>
&lt;p>These features help vectorize and optimize loops when dealing with large datasets.&lt;/p>
&lt;p>The main difference between SVE2 and SVE lies in the functional coverage of the instruction set. SVE is specifically designed for HPC and ML applications. SVE2 extends the SVE instruction set to enable accelerated data processing in areas beyond HPC and ML. The SVE2 instruction set can also accelerate common algorithms used in the following applications:&lt;/p>
&lt;ul>
&lt;li>Computer Vision&lt;/li>
&lt;li>Multimedia&lt;/li>
&lt;li>LTE Basic Processing&lt;/li>
&lt;li>Genomics&lt;/li>
&lt;li>In-memory database&lt;/li>
&lt;li>Web Service&lt;/li>
&lt;li>General software&lt;/li>
&lt;/ul>
&lt;p>SVE and SVE2 both support collecting and processing large amounts of data. SVE and SVE2 are not extensions of the Neon instruction set. Instead, SVE and SVE2 are redesigned to offer better data parallelism than Neon. However, the hardware logic of SVE and SVE2 covers the implementation of Neon hardware. When a microarchitecture supports SVE or SVE2, it also supports Neon. To use SVE and SVE2, the software running on that microarchitecture must first support Neon.&lt;/p>
&lt;h2 id="2-sve-architecture-basics">
&lt;a href="#2-sve-architecture-basics" class="header-anchor">#&lt;/a>
2. SVE Architecture Basics
&lt;/h2>
&lt;p>This section introduces the basic architectural features shared by SVE and SVE2. Like SVE, SVE2 is also based on scalable vectors. In addition to the existing register file provided by Neon, SVE and SVE2 add the following registers:&lt;/p>
&lt;ul>
&lt;li>32 scalable vector registers, &lt;code>Z0-Z31&lt;/code>&lt;/li>
&lt;li>16 scalable Predicate registers, &lt;code>P0-P15&lt;/code>
&lt;ul>
&lt;li>1 First Fault Predicate register, &lt;code>FFR&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Scalable Vector System Control Register, &lt;code>ZCR_ELx&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="21-scalable-vector-registers">
&lt;a href="#21-scalable-vector-registers" class="header-anchor">#&lt;/a>
2.1 Scalable Vector Registers
&lt;/h3>
&lt;p>Scalable vector registers &lt;code>Z0-Z31&lt;/code> can be implemented in microarchitecture as 128-2048 bits. The lowest 128 bits are shared with Neon&amp;rsquo;s fixed 128-bit vectors &lt;code>V0-V31&lt;/code>.&lt;/p>
&lt;p>The image below shows scalable vector registers &lt;code>Z0-Z31&lt;/code>:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_Z-register.webp"
alt="Z Registers-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Scalable Vector Registers Z0-Z31&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>Scalable Vector:&lt;/p>
&lt;ul>
&lt;li>Can accommodate 64, 32, 16, and 8-bit elements&lt;/li>
&lt;li>Supports integer, double precision, single precision, and half precision floating-point elements&lt;/li>
&lt;li>The vector length can be configured for each exception level (EL)&lt;/li>
&lt;/ul>
&lt;h3 id="22-scalable-predicate-register">
&lt;a href="#22-scalable-predicate-register" class="header-anchor">#&lt;/a>
2.2 Scalable Predicate Register
&lt;/h3>
&lt;p>In order to control which active elements participate in operations, Predicate registers (abbreviated as P registers) are used as masks in many SVE instructions, which also provides flexibility for vector operations. The figure below shows the scalable Predicate registers &lt;code>P0-P15&lt;/code>:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_Predicate-register.webp"
alt="P Register-2024-08-12" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Scalable Predicate Registers P0-P15&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>The P register is typically used as a bitmask for data manipulation:&lt;/p>
&lt;ul>
&lt;li>Each P register is 1/8 the length of a Z register&lt;/li>
&lt;li>&lt;code>P0-P7&lt;/code> are used for loading, storing, and arithmetic operations&lt;/li>
&lt;li>&lt;code>P8-P15&lt;/code> used for loop management&lt;/li>
&lt;li>FFR is a special P register set by the first-fault vector load and store instructions, used to indicate the success of load and store operations for each element. FFR is designed to support speculative memory access, making vectorization easier and safer in many cases.&lt;/li>
&lt;/ul>
&lt;h3 id="23-scalable-vector-system-control-register">
&lt;a href="#23-scalable-vector-system-control-register" class="header-anchor">#&lt;/a>
2.3 Scalable Vector System Control Register
&lt;/h3>
&lt;p>The figure below shows the Scalable Vector System Control Register &lt;code>ZCR_ELx&lt;/code>:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_ZCR_Elx.webp"
alt="ZCR_Elx-2024-08-12" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Scalable Vector System Control Register ZCR_Elx&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>Scalable Vector System Control Register indicates SVE implementation features:&lt;/p>
&lt;ul>
&lt;li>&lt;code>ZCR_Elx.LEN&lt;/code> field is used for the vector length of the current and lower anomaly levels.&lt;/li>
&lt;li>Most bits are currently reserved for future use.&lt;/li>
&lt;/ul>
&lt;h3 id="24-sve-assembly-syntax">
&lt;a href="#24-sve-assembly-syntax" class="header-anchor">#&lt;/a>
2.4 SVE Assembly Syntax
&lt;/h3>
&lt;p>The SVE assembly syntax format consists of an opcode, destination register, P register (if the instruction supports a Predicate mask), and input operands. The following instruction example will detail this format.&lt;/p>
&lt;p>Example 1:&lt;/p>
&lt;pre>&lt;code class="language-armasm">LDFF1D {&amp;lt;Zt&amp;gt;.D}, &amp;lt;Pg&amp;gt;/Z, [&amp;lt;Xn|SP&amp;gt;, &amp;lt;Zm&amp;gt;.D, LSL #3]
&lt;/code>&lt;/pre>
&lt;p>Among them:&lt;/p>
&lt;ul>
&lt;li>&lt;code>&amp;lt;Zt&amp;gt;&lt;/code> is the Z register, &lt;code>Z0-Z31&lt;/code>&lt;/li>
&lt;li>&lt;code>&amp;lt;Zt&amp;gt;&lt;/code>.D and &lt;code>&amp;lt;Zm&amp;gt;&lt;/code>.D specify the element type of the target and operand vectors, without needing to specify the number of elements.&lt;/li>
&lt;li>&lt;code>&amp;lt;Pg&amp;gt;&lt;/code> is the P register, &lt;code>P0-P15&lt;/code>&lt;/li>
&lt;li>&lt;code>&amp;lt;Pg&amp;gt;/Z&lt;/code> is to zero the P register.&lt;/li>
&lt;li>&lt;code>&amp;lt;Zm&amp;gt;&lt;/code> specifies the offset for the Gather Load address mode.&lt;/li>
&lt;/ul>
&lt;p>Example 2:&lt;/p>
&lt;pre>&lt;code class="language-armasm">ADD &amp;lt;Zdn&amp;gt;.&amp;lt;T&amp;gt;, &amp;lt;Pg&amp;gt;/M, &amp;lt;Zdn&amp;gt;.&amp;lt;T&amp;gt;, &amp;lt;Zm&amp;gt;.&amp;lt;T&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>Among them:&lt;/p>
&lt;ul>
&lt;li>&lt;code>&amp;lt;Pg&amp;gt;/M&lt;/code> is the merge P register.&lt;/li>
&lt;li>&lt;code>&amp;lt;Zdn&amp;gt;&lt;/code> is both the destination register and one of the input operands. The instruction syntax shows &lt;code>&amp;lt;Zdn&amp;gt;&lt;/code> in both places for convenience. In the assembly encoding, for simplification, they are only encoded once.&lt;/li>
&lt;/ul>
&lt;p>Example 3:&lt;/p>
&lt;pre>&lt;code class="language-armasm">ORRS &amp;lt;Pd&amp;gt;.B, &amp;lt;Pg&amp;gt;.Z, &amp;lt;Pn&amp;gt;.B, &amp;lt;Pm&amp;gt;.B
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>S&lt;/code> is the new interpretation of the P register condition flags &lt;code>NZCV&lt;/code>.&lt;/li>
&lt;li>&lt;code>&amp;lt;Pg&amp;gt;&lt;/code> controls the P register to act as a bitmask in the example operation.&lt;/li>
&lt;/ul>
&lt;h3 id="25-sve-architecture-features">
&lt;a href="#25-sve-architecture-features" class="header-anchor">#&lt;/a>
2.5 SVE Architecture Features
&lt;/h3>
&lt;p>SVE includes the following key architectural features:&lt;/p>
&lt;ul>
&lt;li>per-lane predication&lt;/li>
&lt;/ul>
&lt;p>In order to allow flexible operations on selected elements, SVE introduces 16 P registers, &lt;code>P0-P15&lt;/code>, to indicate valid operations on vector active channels. For example:&lt;/p>
&lt;pre>&lt;code class="language-armasm">ADD Z0.D, P0/M, Z0.D, Z1.D
&lt;/code>&lt;/pre>
&lt;p>Add the active elements &lt;code>Z0&lt;/code> and &lt;code>Z1&lt;/code> and place the result in &lt;code>Z0&lt;/code>. &lt;code>P0&lt;/code> indicates which elements of the operands are active and inactive. The &lt;strong>M&lt;/strong> following &lt;code>P0&lt;/code> stands for Merging, meaning the inactive elements of &lt;code>Z0&lt;/code> will retain their initial values after the &lt;code>ADD&lt;/code> operation. If &lt;strong>Z&lt;/strong> follows &lt;code>P0&lt;/code>, the inactive elements will be zeroed, and the inactive elements of the destination register will be zeroed after the operation.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_Per-lane_Predication.webp"
alt="Per-lane_Predication-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Per-lane predication merging&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>If &lt;strong>\Z&lt;/strong> is used, the inactive elements will be zeroed, and the inactive elements of the target register will be zeroed after the operation. For example&lt;/p>
&lt;pre>&lt;code class="language-armasm">CPY Z0.B, P0/Z, #0xFF
&lt;/code>&lt;/pre>
&lt;p>Indicates that the signed integer 0xFF will be copied to the active channel of &lt;code>Z0&lt;/code>, while the inactive channels will be cleared.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_Per-lane_Predicate_Zeroing.webp"
alt="Per-lane_Predicate_Zeroing-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Per-lane predication zeroing&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;blockquote class="alert-blockquote alert-note">
&lt;p class="alert-heading">
&lt;svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16">
&lt;path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z">&lt;/path>
&lt;/svg>
&lt;span>Note&lt;/span>
&lt;/p>
&lt;p>Not all instructions have the Predicate option. Additionally, not all Predicate operations have both merge and zeroing options. You must refer to the &lt;a class="link" href="https://developer.arm.com/documentation/ddi0487/latest/t" target="_blank" rel="noopener" >AArch64 SVE Supplement
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
to understand the specification details of each instruction.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>Gather Load and Scatter Store&lt;/li>
&lt;/ul>
&lt;p>The addressing modes in SVE allow vectors to be used as base addresses and offsets in Gather Load and Scatter Store instructions, which enables access to non-contiguous memory locations. For example:&lt;/p>
&lt;pre>&lt;code class="language-armasm">LD1SB Z0.S, P0/Z, [Z1.S] // Gather Load signed bytes from memory addresses generated by the 32-bit vector base address Z1 into the active 32-bit elements of Z0.
LD1SB Z0.D, P0/Z, [X0, Z1.D] // Gather Load signed bytes from memory addresses generated by the 64-bit scalar base address X0 plus the vector index in Z1.D into the active elements of Z0.
&lt;/code>&lt;/pre>
&lt;p>The following example shows the load operation &lt;code>LD1SB Z0.S, P0/Z, [Z1.S]&lt;/code>, where &lt;code>P0&lt;/code> contains all true elements, and &lt;code>Z1&lt;/code> contains scattered addresses. After loading, the least significant byte of each element in &lt;code>Z0.S&lt;/code> will be updated with data fetched from scattered memory locations.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_gather-load_and_scatter_store_example.webp"
alt="gather-load_and_scatter_store_example-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Gather-load and Scatter-store Example&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>Loop control and management of the P register driver&lt;/li>
&lt;/ul>
&lt;p>As a key feature of SVE, the P register not only flexibly controls individual elements of vector operations but also enables P register-driven loop control. P register-driven loop control and management make loop control efficient and flexible. This feature eliminates the overhead of extra loop heads and tails for processing partial vectors by registering active and inactive element indices in the P register. P register-driven loop control and management mean that in the subsequent loop iterations, only active elements will perform the intended operations. For example:&lt;/p>
&lt;pre>&lt;code class="language-armasm">WHILEL0 P0.S, x8, x9 // Generate a predicate in P0, starting from the lowest numbered element, true when the incremented value of the first unsigned scalar operand X8 is less than the second scalar operand X9, then false until the highest numbered element.
B.FIRST Loop_start // B.FIRST (equivalent to B.MI) or B.NFRST (equivalent to B.PL) is usually used to branch based on the test result of the above instruction, determining whether the first element of P0 is true or false as the condition to end or continue the loop.
&lt;/code>&lt;/pre>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_Predicate-driver_loop_control_and_management_example.webp"
alt="Predicate-driver_loop_control_and_management_example-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Example of loop control and management driven by P register&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>Vector partitioning for speculation in software management&lt;/li>
&lt;/ul>
&lt;p>Speculative loading can pose challenges for memory reading of traditional vectors, &lt;strong>if errors occur in certain elements during the reading process, it is difficult to reverse the load operation and track which elements failed to load&lt;/strong>. Neon does not allow speculative loading. To allow speculative loading of vectors (e.g., LDRFF), SVE introduces the first-fault vector load instruction. To allow vector access across invalid pages, SVE also introduces the FFR register. &lt;strong>When using the first-fault vector load instruction to load into an SVE vector, the FFR register updates with the success or failure result of each element&amp;rsquo;s load&lt;/strong>. When a load error occurs, FFR immediately registers the corresponding element, registers the remaining elements as 0 or false, and does not trigger an exception. Typically, the RDFFR instruction is used to read the FFR status. The RDFFR instruction ends iteration when the first element is false. If the first element is true, the RDFFR instruction continues iteration. The length of FFR is the same as the P vector. This value can be initialized using the SETFFR instruction. The following example uses LDFF1D to read data from memory, and FFR is updated accordingly:&lt;/p>
&lt;pre>&lt;code class="language-armasm">LDFF1D Z0.D, P0/Z, [Z1.D, #0] // Use the first-fault behavior to gather doublewords from the memory address generated by vector base address Z1 plus 0, loading into the active elements of Z0. Inactive elements do not read device memory or trigger a fault, and are set to zero in the destination vector. A successful load from valid memory sets the corresponding element in the FFR to true. The first-fault load sets the corresponding element and the remaining elements in the FFR to false or 0.
&lt;/code>&lt;/pre>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_Vector-partioning-for-software-managed-speculation-example.webp"
alt="Vector-partioning-for-software-managed-speculation-example-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Example of Vector Partitioning for Software-Managed Speculation&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>Extended floating point and horizontal reduction&lt;/li>
&lt;/ul>
&lt;p>In order to allow efficient reduction operations in vectors and meet different precision requirements, SVE enhances floating-point and horizontal reduction operations. These instructions may have a sequential (low to high) or tree-based (pairwise) floating-point reduction order, where the order of operations may lead to different rounding results. These operations require a trade-off between reproducibility and performance. For example:&lt;/p>
&lt;pre>&lt;code class="language-armasm">FADDA D0, P0/M, D1, Z2.D // Perform a floating-point addition strict-order reduction from the low to high elements of the source vector, accumulating the result into the SIMD&amp;amp;FP scalar register. This example instruction adds D1 to all active elements of Z2.D and stores the result into scalar register D0. Vector elements are processed in strict order from low to high, with scalar source D1 providing the initial value. Inactive elements in the source vector are ignored. FADDV performs a recursive pairwise reduction and stores the result into the scalar register.
&lt;/code>&lt;/pre>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_Extended_Floating-poing-and-horizontal-reductions-example.webp"
alt="Extended_Floating-poing-and-horizontal-reductions-example-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Extended Floating-point and Horizontal Reductions Example&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h2 id="3-new-features-of-sve2">
&lt;a href="#3-new-features-of-sve2" class="header-anchor">#&lt;/a>
3. New Features of SVE2
&lt;/h2>
&lt;p>This section introduces the features added by SVE2 to the Arm AArch64 architecture. To achieve scalable performance, SVE2 is built on SVE, allowing vectors to reach up to 2048 bits.&lt;/p>
&lt;p>In SVE2, many instructions that replicate existing instructions in Neon have been added, including:&lt;/p>
&lt;ul>
&lt;li>Converted Neon integer operations, for example, Signed Absolute Difference Accumulate (SAB) and Signed Halving Add (SHADD).&lt;/li>
&lt;li>Converted Neon extensions, narrowing and paired operations, for example, Unsigned Add Long - Bottom (UADDLB) and Unsigned Add Long - Top (UADDLT).&lt;/li>
&lt;/ul>
&lt;p>The order of element processing has changed. SVE2 processes interleaved even and odd elements, while Neon processes the low half and high half elements of narrow or wide operations. The diagram below illustrates the difference between Neon and SVE2 processing:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_transformed_neon_widen_narraow_pairwise_operations.webp"
alt="transformed_neon_widen_narraow_pairwise_operations-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Comparison of Transformed Neon Narrow or Wide Operations&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>Complex number operations, such as complex integer multiplication-accumulation with rotation (CMLA).&lt;/li>
&lt;li>Multi-precision arithmetic, used for large integer arithmetic and cryptography, for example, carry-in long addition - bottom (ADCLB), carry-in long addition - top (ADCLT) and SM4 encryption and decryption (SM4E).&lt;/li>
&lt;/ul>
&lt;p>For backward compatibility, the latest architecture requires Neon and VFP. Although SVE2 includes some features of SVE and Neon, SVE2 does not preclude the presence of Neon on the chip.&lt;/p>
&lt;p>SVE2 supports optimization for emerging applications beyond the HPC market, such as in machine learning (ML) (UDOT instructions), computer vision (TBL and TBX instructions), baseband networks (CADD and CMLA instructions), genomics (BDEP and BEXT instructions), and servers (MATCH and NMATCH instructions).&lt;/p>
&lt;p>SVE2 enhances the overall performance of general-purpose processors in handling large volumes of data, without the need for additional off-chip accelerators.&lt;/p>
&lt;h2 id="4-using-sve-programming">
&lt;a href="#4-using-sve-programming" class="header-anchor">#&lt;/a>
4. Using SVE programming
&lt;/h2>
&lt;p>This section introduces software tools and libraries that support SVE2 application development. This section also explains how to develop applications for targets that support SVE2, run the application on hardware that supports SVE2, and simulate the application on any Armv8-A hardware.&lt;/p>
&lt;h3 id="41-software-and-library-support">
&lt;a href="#41-software-and-library-support" class="header-anchor">#&lt;/a>
4.1 Software and Library Support
&lt;/h3>
&lt;p>To build SVE or SVE2 applications, you must choose a compiler that supports SVE and SVE2 features.&lt;/p>
&lt;ul>
&lt;li>GNU tools version 8.0+ supports SVE.&lt;/li>
&lt;li>&lt;a class="link" href="https://developer.arm.com/tools-and-software/server-and-hpc/compile/arm-compiler-for-linux" target="_blank" rel="noopener" >Arm Compiler for Linux
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
Version 18.0+ supports SVE, Version 20.0+ supports SVE and SVE2.&lt;/li>
&lt;li>Both GNU and Arm Compiler for Linux compilers support optimizing C/C++/Fortran code.&lt;/li>
&lt;li>LLVM (open-source Clang) version 5 and above includes support for SVE, and version 9 and above includes support for SVE2. To find out which SVE or SVE2 features are supported by each version of the LLVM tools, please refer to the &lt;a class="link" href="https://developer.arm.com/tools-and-software/open-source-software/developer-tools/llvm-toolchain/sve-support" target="_blank" rel="noopener" >LLVM toolchain SVE support page
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/li>
&lt;/ul>
&lt;p>&lt;a class="link" href="https://developer.arm.com/Tools%20and%20Software/Arm%20Performance%20Libraries" target="_blank" rel="noopener" >Arm Performance Libraries
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
are highly optimized for mathematical routines and can be linked to your applications. Arm Performance Libraries version 19.3+ supports SVE&amp;rsquo;s math library.&lt;/p>
&lt;p>Arm Compiler for Linux is part of Arm Allinea Studio, including Arm C/C++ Compiler, Arm Fortran Compiler, and Arm Performance Libraries.&lt;/p>
&lt;h3 id="42-how-to-program-using-sve2">
&lt;a href="#42-how-to-program-using-sve2" class="header-anchor">#&lt;/a>
4.2 How to Program Using SVE2
&lt;/h3>
&lt;p>There are several methods to write or generate SVE and SVE2 code. In this section, we will explore some of these methods.&lt;/p>
&lt;p>To write or generate SVE and SVE2 code, you can:&lt;/p>
&lt;ul>
&lt;li>Write SVE assembly code&lt;/li>
&lt;li>Programming with SVE intrinsics&lt;/li>
&lt;li>Automatic vectorization&lt;/li>
&lt;li>Use SVE optimization library&lt;/li>
&lt;/ul>
&lt;p>Let&amp;rsquo;s take a closer look at these four options.&lt;/p>
&lt;h4 id="421-write-sve-assembly-code">
&lt;a href="#421-write-sve-assembly-code" class="header-anchor">#&lt;/a>
4.2.1 Write SVE assembly code
&lt;/h4>
&lt;p>You can write SVE instructions as inline assembly in C/C++ code, or as a complete function in assembly source code. For example:&lt;/p>
&lt;pre>&lt;code class="language-armasm">```assembly
.globl subtract_arrays // -- Begin function
.p2align 2
.type subtract_arrays, @function
subtract_arrays: // @subtract_arrays
.cfi_startproc
// %bb.0:
orr w9, wzr, #0x400
mov x8, xzr
whilelo p0.s, xzr, x9
.LBB0_1: // =&amp;gt;This Inner Loop Header: Depth=1
ld1w { z0.s }, p0/z, [x1, x8, lsl #2]
ld1w { z1.s }, p0/z, [x2, x8, lsl #2]
sub z0.s, z0.s, z1.s
st1w { z0.s }, p0, [x0, x8, lsl #2]
incw x8
whilelo p0.s, x8, x9
b.mi .LBB0_1
// %bb.2:
ret
.Lfunc_end0:
.size subtract_arrays, .Lfunc_end0-subtract_arrays
.cfi_endproc
&lt;/code>&lt;/pre>
&lt;p>If you write functions that mix high-level language and assembly language, you must be familiar with the &lt;a class="link" href="https://developer.arm.com/documentation/ihi0036/latest/" target="_blank" rel="noopener" >Application Binary Interface (ABI)
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
standards updated for SVE. The &lt;a class="link" href="https://developer.arm.com/documentation/ihi0055/latest" target="_blank" rel="noopener" >Arm Architecture Procedure Call Standard (AAPCS)
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
specifies data types and register allocation, and is most relevant to assembly programming. AAPCS requires:&lt;/p>
&lt;ul>
&lt;li>&lt;code>Z0-Z7&lt;/code> and &lt;code>P0-P3&lt;/code> are used to pass scalable vector parameters and results.&lt;/li>
&lt;li>&lt;code>Z8-Z15&lt;/code> and &lt;code>P4-P15&lt;/code> are callee-saved.&lt;/li>
&lt;li>All other vector registers (&lt;code>Z16-Z31&lt;/code>) may be corrupted by the called function, and the calling function is responsible for backing up and restoring them when necessary.&lt;/li>
&lt;/ul>
&lt;h4 id="422-using-sve-instruction-functions-intrinsics">
&lt;a href="#422-using-sve-instruction-functions-intrinsics" class="header-anchor">#&lt;/a>
4.2.2 Using SVE Instruction Functions (Intrinsics)
&lt;/h4>
&lt;p>SVE intrinsic functions are functions supported by the compiler that can be replaced with corresponding instructions. Programmers can directly call instruction functions in high-level languages such as C and C++. The ACLE (Arm C Language Extensions) for SVE defines which SVE intrinsic functions are available, their parameters, and their functionality. A compiler that supports ACLE can replace intrinsics with mapped SVE instructions during compilation. To use ACLE intrinsics, you must include the header file &lt;code>arm_sve.h&lt;/code>, which contains a list of vector types and intrinsic functions (for SVE) that can be used in C/C++. Each data type describes the size and data type of the elements in the vector:&lt;/p>
&lt;ul>
&lt;li>&lt;code>svint8_t svuint8_t&lt;/code>&lt;/li>
&lt;li>&lt;code>svint16_t svuint16_t svfloat16_t&lt;/code>&lt;/li>
&lt;li>&lt;code>svint32_t svuint32_t svfloat32_t&lt;/code>&lt;/li>
&lt;li>&lt;code>svint64_t svuint64_t svfloat64_t&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>For example, &lt;code>svint64_t&lt;/code> represents a 64-bit signed integer vector, &lt;code>svfloat16_t&lt;/code> represents a half-precision floating-point vector.&lt;/p>
&lt;p>The following example C code has been manually optimized using SVE intrinsics:&lt;/p>
&lt;pre>&lt;code class="language-c">// intrinsic_example.c
#include &amp;lt;arm_sve.h&amp;gt;
svuint64_t uaddlb_array(svuint32_t Zs1, svuint32_t Zs2)
{
// widening add of even elements
svuint64_t result = svaddlb(Zs1, Zs2);
return result;
}
&lt;/code>&lt;/pre>
&lt;p>The source code that includes the &lt;code>arm_sve.h&lt;/code> header file can use SVE vector types, just like data types can be used for variable declarations and function parameters. To compile the code using the Arm C/C++ compiler and target the Armv8-A architecture that supports SVE, use:&lt;/p>
&lt;pre>&lt;code class="language-bash">armclang -O3 -S -march=armv8-a+sve2 -o intrinsic_example.s intrinsic_example.c
&lt;/code>&lt;/pre>
&lt;p>This command generates the following assembly code:&lt;/p>
&lt;pre>&lt;code class="language-armasm">// instrinsic_example.s
uaddlb_array: // @uaddlb_array
.cfi_startproc
// %bb.0:
uaddlb z0.d, z0.s, z1.s
ret
&lt;/code>&lt;/pre>
&lt;h4 id="423-automatic-vectorization">
&lt;a href="#423-automatic-vectorization" class="header-anchor">#&lt;/a>
4.2.3 Automatic Vectorization
&lt;/h4>
&lt;p>C/C++/Fortran compilers (for example, the native &lt;a class="link" href="https://developer.arm.com/tools-and-software/server-and-hpc/compile/arm-compiler-for-linux" target="_blank" rel="noopener" >Arm Compiler for Linux
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
for the Arm platform and the GNU compiler) support vectorization of C, C++, and Fortran loops using SVE or SVE2 instructions. To generate SVE or SVE2 code, choose the appropriate compiler options. For example, one option to enable SVE2 optimization using armclang is &lt;code>-march=armv8-a+sve2&lt;/code>. If you want to use the SVE version of the library, combine &lt;code>-march=armv8-a+sve2&lt;/code> with &lt;code>-armpl=sve&lt;/code>.&lt;/p>
&lt;h4 id="424-using-svesve2-to-optimize-libraries">
&lt;a href="#424-using-svesve2-to-optimize-libraries" class="header-anchor">#&lt;/a>
4.2.4 Using SVE/SVE2 to Optimize Libraries
&lt;/h4>
&lt;p>Use libraries highly optimized for SVE/SVE2, such as &lt;a class="link" href="https://developer.arm.com/Tools%20and%20Software/Arm%20Performance%20Libraries" target="_blank" rel="noopener" >Arm Performance Libraries
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
and Arm Compute Libraries. Arm Performance Libraries contain highly optimized implementations of mathematical functions optimized for BLAS, LAPACK, FFT, sparse linear algebra, and libamath. To be able to link any Arm Performance Libraries function, you must install Arm Allinea Studio and include armpl.h in your code. To build applications using Arm Compiler for Linux and Arm Performance Libraries, you must specify &lt;code>-armpl=&amp;lt;arg&amp;gt;&lt;/code> on the command line. If you are using GNU tools, you must include the Arm Performance Libraries installation path in the linker command line with &lt;code>-L&amp;lt;armpl_install_dir&amp;gt;/lib&lt;/code> and specify the GNU option equivalent to the Arm Compiler for Linux &lt;code>-armpl=&amp;lt;arg&amp;gt;&lt;/code> option, which is &lt;code>-larmpl_lp64&lt;/code>. For more information, please refer to the Arm Performance Libraries Getting Started Guide.&lt;/p>
&lt;h3 id="43-how-to-run-svesve2-programs">
&lt;a href="#43-how-to-run-svesve2-programs" class="header-anchor">#&lt;/a>
4.3 How to run SVE/SVE2 programs
&lt;/h3>
&lt;p>If you do not have access to SVE hardware, you can use models or simulators to run the code. You can choose from the following models and simulators:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>QEMU&lt;/strong>: Cross-compilation and native models, supporting modeling on Arm AArch64 platforms with SVE.&lt;/li>
&lt;li>&lt;strong>Fast Models&lt;/strong>: Cross-platform models that support modeling on Arm AArch64 platforms with SVE running on x86-based hosts. Architecture Envelope Model (AEM) with SVE2 support is only available to major partners.&lt;/li>
&lt;li>&lt;strong>Arm Instruction Emulator (ArmIE)&lt;/strong>: Runs directly on the Arm platform. Supports SVE and supports SVE2 from version 19.2+.&lt;/li>
&lt;/ul>
&lt;h2 id="5-acle-intrinsics">
&lt;a href="#5-acle-intrinsics" class="header-anchor">#&lt;/a>
5. ACLE Intrinsics
&lt;/h2>
&lt;h3 id="51-acle-introduction">
&lt;a href="#51-acle-introduction" class="header-anchor">#&lt;/a>
5.1 ACLE Introduction
&lt;/h3>
&lt;p>ACLE (Arm C Language Extensions) is used in C and C++ code to support Arm features through intrinsics and other characteristics.&lt;/p>
&lt;ul>
&lt;li>ACLE (ARM C Language Extensions) extends the C/C++ language with Arm-specific features.
&lt;ul>
&lt;li>Predefined macros: &lt;code>__ARM_ARCH_ISA_A64&lt;/code>, &lt;code>__ARM_BIG_ENDIAN&lt;/code>, etc.&lt;/li>
&lt;li>Internal functions: &lt;code>__clz(uint32_t x)&lt;/code>, &lt;code>__cls(uint32_t x)&lt;/code>, etc.&lt;/li>
&lt;li>Data types: SVE, NEON, and FP16 data types.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>ACLE support for SVE uses ACLE for variable-length vector (VLA) programming.
&lt;ul>
&lt;li>Almost every SVE instruction has a corresponding intrinsic function.&lt;/li>
&lt;li>Data type used to represent size-agnostic vectors used by SVE intrinsics.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Applicable scenarios for the following users:
&lt;ul>
&lt;li>Users who wish to manually adjust SVE code.&lt;/li>
&lt;li>Users who wish to adapt or manually optimize applications and libraries.&lt;/li>
&lt;li>Users who need low-level access to Arm targets.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="52-how-to-use-acle">
&lt;a href="#52-how-to-use-acle" class="header-anchor">#&lt;/a>
5.2 How to use ACLE
&lt;/h3>
&lt;ul>
&lt;li>Include header files
&lt;ul>
&lt;li>&lt;code>arm_acle.h&lt;/code>: Core ACLE&lt;/li>
&lt;li>&lt;code>arm_fp16.h&lt;/code>: Add FP16 data type.
&lt;ul>
&lt;li>The target platform must support FP16, i.e., &lt;code>march=armv8-a+fp16&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>arm_neon.h&lt;/code>: Add NEON Intrinsics and data types.
&lt;ul>
&lt;li>The target platform must support NEON, i.e., &lt;code>march=armv8-a+simd&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>arm_sve.h&lt;/code>: Add SVE Intrinsics and data types.
&lt;ul>
&lt;li>The target platform must support SVE, i.e., &lt;code>march=armv8-a+sve&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="53-sve-acle">
&lt;a href="#53-sve-acle" class="header-anchor">#&lt;/a>
5.3 SVE ACLE
&lt;/h3>
&lt;ul>
&lt;li>The first thing to do is to include the header files&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-c">#include &amp;lt;arm_sve.h&amp;gt;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>VLA data type
&lt;ul>
&lt;li>&lt;code>svfloat64_t&lt;/code>, &lt;code>svfloat16_t&lt;/code>, &lt;code>svuint32_t&lt;/code>, etc.&lt;/li>
&lt;li>Naming convention: &lt;code>sv&amp;lt;datatype&amp;gt;&amp;lt;datasize&amp;gt;_t&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Prediction
&lt;ul>
&lt;li>Merge: &lt;code>_m&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Reset: &lt;code>_z&lt;/code>&lt;/li>
&lt;li>Uncertain: &lt;code>_x&lt;/code>&lt;/li>
&lt;li>Data type of P register: &lt;code>svbool_t&lt;/code>&lt;/li>
&lt;li>Use generics for function overloading, for example, the function &lt;code>svadd&lt;/code> will automatically select the corresponding function based on the parameter type.&lt;/li>
&lt;li>Function naming convention: &lt;code>svbase[disambiguator][type0][type1]...[predication]&lt;/code>
&lt;ul>
&lt;li>base refers to basic operations, such as &lt;code>add&lt;/code>, &lt;code>mul&lt;/code>, &lt;code>sub&lt;/code>, etc.&lt;/li>
&lt;li>disambiguator is used to distinguish different variants of the same basic operation.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>typeN specifies the type of vector and P register.&lt;/li>
&lt;li>predication specifies the handling method for inactive elements.
&lt;ul>
&lt;li>For example: &lt;code>svfloat64_t svld1_f64&lt;/code>, &lt;code>svbool_t svwhilelt_b8&lt;/code>, &lt;code>svuint32_t svmla_u32_z&lt;/code>, &lt;code>svuint32_t svmla_u32_m&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="54-common-sve-intrinsics">
&lt;a href="#54-common-sve-intrinsics" class="header-anchor">#&lt;/a>
5.4 Common SVE Intrinsics
&lt;/h3>
&lt;ul>
&lt;li>Predicate
&lt;ul>
&lt;li>Predicate is a vector of type bool, used to control whether the corresponding position in the vector participates in the computation during the process.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>svbool_t pg = svwhilelt_b32(i, num)&lt;/code> generates a predicate for (i, i + 1, i + 2, &amp;hellip;, i + vl - 1) &amp;lt; num
&lt;ul>
&lt;li>&lt;code>svbool_t pg = svptrue_b32()&lt;/code> generates a predicate that is all true&lt;/li>
&lt;li>Among them, b32 corresponds to processing 32-bit data (int/float), in addition, there are also intrinsics corresponding to b8, b16, b64.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Memory data access
&lt;ul>
&lt;li>&lt;code>svld1(pg, *base)&lt;/code>: Load contiguous vector from address base.&lt;/li>
&lt;li>&lt;code>svst1(pg, *base, vec)&lt;/code>: Store the vector vec into the address base.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>svld1_gather_index(pg, *base, vec_index)&lt;/code>: Load the data corresponding to the vector index from the address base.&lt;/li>
&lt;li>&lt;code>svst1_scatter_index(pg, *base, vec_index, vec)&lt;/code>: Store data from vector vec to the positions corresponding to the vector indices.&lt;/li>
&lt;li>Basic calculation
&lt;ul>
&lt;li>&lt;code>svadd_z(pg, sv_vec1, sv_vec2)&lt;/code>&lt;/li>
&lt;li>&lt;code>svadd_m(pg, sv_vec1, sv_vec2)&lt;/code>&lt;/li>
&lt;li>&lt;code>svadd_x(pg, sv_vec1, sv_vec2)&lt;/code>&lt;/li>
&lt;li>&lt;code>svadd_x(pg, sv_vec1, x)&lt;/code>&lt;/li>
&lt;li>Among them, &lt;code>_z&lt;/code> indicates setting the position where pg is false to zero, &lt;code>_m&lt;/code> indicates retaining the original value, and &lt;code>_x&lt;/code> indicates uncertainty (any value is possible).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>The second operand can be scalar data.&lt;/li>
&lt;li>&lt;code>svmul&lt;/code>, &lt;code>svsub&lt;/code>, &lt;code>svsubr&lt;/code>, &lt;code>svdiv&lt;/code>, &lt;code>svdivr&lt;/code>: Among them, &lt;code>svsubr&lt;/code> swaps the position of the subtrahend and the minuend compared to &lt;code>svsub&lt;/code>.&lt;/li>
&lt;li>Others&lt;/li>
&lt;li>&lt;code>svdup_f64(double x)&lt;/code>: Generate a vector with all elements being x.
&lt;ul>
&lt;li>&lt;code>svcntd()&lt;/code>: Returns the vector length of 64-bit data: &lt;code>svcntb&lt;/code> corresponds to 8 bits, &lt;code>svcnth&lt;/code> corresponds to 16 bits, &lt;code>svcntw&lt;/code> corresponds to 32 bits.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="55-sve-structure-intrinsics">
&lt;a href="#55-sve-structure-intrinsics" class="header-anchor">#&lt;/a>
5.5 SVE Structure Intrinsics
&lt;/h3>
&lt;p>For corresponding structure data, SVE provides some special intrinsics, such as: &lt;code>svld3&lt;/code>, &lt;code>svget3&lt;/code>, &lt;code>svset3&lt;/code>, &lt;code>svst3&lt;/code>, etc. These intrinsics are used for processing structure data.&lt;/p>
&lt;p>For example, for the particle structure:&lt;/p>
&lt;pre>&lt;code class="language-c">typedef struct {
float x;
float y;
float z;
} Particle;
&lt;/code>&lt;/pre>
&lt;p>You can use &lt;code>svld3&lt;/code> to load all the data in the structure as a group of 3 vectors, and then use &lt;code>svget3&lt;/code> to extract a vector from the group of 3 vectors, where the value of index 0, 1, 2 corresponds to x, y, z respectively.&lt;/p>
&lt;pre>&lt;code class="language-c">Particle *ps;
float factor = 2.2;
// Initialization part omitted
for (int i = 0; i &amp;lt; num; i += svcntw()) {
svbool_t pg = svwhilelt_b32(i, num);
svfloat32x3_t sv_ps = svld3(pg, (float32_t *)&amp;amp;ps[i]);
svfloat32_t sv_ps_x = svget3(sv_ps, 0);
svfloat32_t sv_ps_y = svget3(sv_ps, 1);
// Perform calculation
sv_ps_x = svmul_x(pg, sv_ps_x, factor);
sv_ps_y = svmul_x(pg, sv_ps_y, factor);
// Save results
sv_ps = svset3(sv_ps, 0, sv_ps_x);
sv_ps = svset3(sv_ps, 1, sv_ps_y);
svst3(pg, (float32_t *)&amp;amp;ps[i], sv_ps);
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>svld3(pg, *base)&lt;/code>: Load all data in the structure as a group of 3 vectors; where base is the address of the 3-element structure array.&lt;/li>
&lt;li>&lt;code>svget3(tuple, index)&lt;/code>: Extract a vector from a group of 3 vectors; the value of index is 0, 1, or 2.&lt;/li>
&lt;li>&lt;code>svset3(tuple, index, vec)&lt;/code>: Set one vector in a group of 3 vectors; the value of index is 0, 1, or 2.&lt;/li>
&lt;li>&lt;code>svst3(pg, *base, vec)&lt;/code>: Store a group of 3 vectors into a structure; where base is the address of an array of structures with 3 elements.&lt;/li>
&lt;/ul>
&lt;h3 id="56-sve-condition-selection">
&lt;a href="#56-sve-condition-selection" class="header-anchor">#&lt;/a>
5.6 SVE Condition Selection
&lt;/h3>
&lt;p>SVE provides methods such as &lt;code>svcmplt&lt;/code>, &lt;code>svcompact&lt;/code>, &lt;code>svcntp_b32&lt;/code>, etc., which can select elements to retain in the vector based on conditions.&lt;/p>
&lt;p>For example, for non-vectorized code:&lt;/p>
&lt;pre>&lt;code class="language-c">for (int i = 0; i &amp;lt; num; i++) {
float tmp = provided[i];
if (tmp &amp;lt; mark) {
selected[count++] = tmp;
if (count &amp;gt;= maxSize) {
break;
}
}
}
&lt;/code>&lt;/pre>
&lt;p>The purpose of this code is to select elements from the provided array that are less than mark and store them in the selected array until the selected array is full.&lt;/p>
&lt;p>Rewrite with SVE Intrinsic:&lt;/p>
&lt;pre>&lt;code class="language-c">for (int i = 0; i &amp;lt; num; i += svcntw()) {
svbool_t pg = svwhilelt_b32(i, num);
svfloat32_t sv_tmp = svld1(pg, &amp;amp;provided[i]);
svbool_t pg_sel = svcmplt(pg, sv_tmp, mark);
sv_tmp = svcompact(pg_sel, sv_tmp);
svst1(pg, &amp;amp;selected[count], sv_tmp);
count += svcntp_b32(pg, pg_sel);
if (count &amp;gt;= maxSize) {
break;
}
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>svcmplt(pg, vec1, vec2)&lt;/code>: Compare the size of two vectors, returning a predicate indicating the positions in vec1 that are less than vec2.&lt;/li>
&lt;li>&lt;code>svcompact(pg, sv_tmp)&lt;/code>: Compress the vector, move the data with &lt;code>pg&lt;/code> as active to the lower positions of the vector in order, and set the remaining positions to zero.&lt;/li>
&lt;li>&lt;code>svcntp_b32(pg, pg2)&lt;/code>: Returns the number of active elements in pg2&lt;/li>
&lt;li>This code first loads the data from the provided array into sv_tmp, then uses &lt;code>svcmplt&lt;/code> to generate a predicate indicating the positions less than mark. Next, it uses &lt;code>svcompact&lt;/code> to compress sv_tmp, obtaining the data less than mark, and then stores it into the selected array using &lt;code>svst1&lt;/code>. Finally, it uses &lt;code>svcntp_b32&lt;/code> to count the number of active elements and update count.&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_compact.webp"
alt="compact-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>svcompact schematic diagram (256-bit vector)&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>Due to the compact operation, the selected array stores new data less than mark continuously from the count position, and the remaining positions are set to zero.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_svst1.webp"
alt="svst1-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>svst1 schematic diagram (256-bit vector)&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="57-sve-vectorized-loop-interleaving">
&lt;a href="#57-sve-vectorized-loop-interleaving" class="header-anchor">#&lt;/a>
5.7 SVE Vectorized Loop Interleaving
&lt;/h3>
&lt;p>The vectorized loop interleaving implemented by SVE Intrinsic can greatly reduce the number of times vectors are read compared to compiler auto vectorization.&lt;/p>
&lt;p>For example, for non-vectorized code:&lt;/p>
&lt;pre>&lt;code class="language-c">for (int j = offset; j &amp;lt; outerLen - offset; j++) {
int m2index = (j - offset) * innerLen;
int m1index = m2index + innerLen;
int m0index = m1index + innerLen;
int p1index = m0index + innerLen;
int p2index = p1index + innerLen;
for (int i = 0; i &amp;lt; innerLen; i++) {
res[m0index + i] = m2factor * field[m2index + i] +
m1factor * field[m1index + i] +
m0factor * field[m0index + i] +
p1factor * field[p1index + i] +
p2factor * field[p2index + i];
}
}
&lt;/code>&lt;/pre>
&lt;p>After the compiler automatically vectorizes the code, each iteration requires reading data from five different vectors, resulting in low efficiency.&lt;/p>
&lt;p>Rewrite with SVE Intrinsic:&lt;/p>
&lt;pre>&lt;code class="language-c">for (int i = 0; i &amp;lt; innerLen; i += svcntd()) {
svbool_t pg = svwhilelt_b32(i, innerLen);
int dataIndex = i;
svfloat64_t jm2Field = svld1(pg, &amp;amp;field[dataIndex]);
dataIndex += innerLen;
svfloat64_t jm1Field = svld1(pg, &amp;amp;field[dataIndex]);
dataIndex += innerLen;
svfloat64_t jm0Field = svld1(pg, &amp;amp;field[dataIndex]);
dataIndex += innerLen;
svfloat64_t jp1Field = svld1(pg, &amp;amp;field[dataIndex]);
for (int j = offset; j &amp;lt; outerLen - offset; j += 1) {
svfloat64_t jp2Field = svld1(pg, &amp;amp;field[(j + offset) * innerLen + i]);
svfloat64_t svRes = svmul_x(pg, jm2Field, m2factor);
svRes = svmad_x(pg, jm1Field, m1factor, svRes);
svRes = svmad_x(pg, jm0Field, m0factor, svRes);
svRes = svmad_x(pg, jp1Field, p1factor, svRes);
svRes = svmad_x(pg, jp2Field, p2factor, svRes);
svst1(pg, &amp;amp;res[j * innerLen + 1], svRes);
jm2Field = jm1Field;
jm1Field = jm0Field;
jm0Field = jp1Field;
jp1Field = jp2Field;
}
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>svmad_x(pg, vec1, vec2, vec3)&lt;/code>: Calculates vec1 * vec2 + vec3, returns a vector.&lt;/li>
&lt;li>This code only needs to read one vector per iteration, greatly reducing the number of vector reads.&lt;/li>
&lt;/ul>
&lt;h2 id="references">
&lt;a href="#references" class="header-anchor">#&lt;/a>
References
&lt;/h2>
&lt;ol>
&lt;li>&lt;a class="link" href="https://developer.arm.com/-/media/Arm%20Developer%20Community/PDF/102340_0001_02_en_introduction-to-sve2.pdf?revision=b208e56b-6569-4ae2-b6f3-cd7d5d1ecac3" target="_blank" rel="noopener" >Introduction to SVE2
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;li>&lt;a class="link" href="https://www.stonybrook.edu/commcms/ookami/support/_docs/5%20-%20Advanced%20SVE.pdf" target="_blank" rel="noopener" >SVE Deep Dive
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;li>&lt;a class="link" href="https://arm-software.github.io/acle/main/acle.html" target="_blank" rel="noopener" >Arm C Language Extensions
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;/ol></description></item><item><title>RDMA: Memory Window</title><link>https://cuterwrite.top/en/p/rdma-memory-window/</link><pubDate>Wed, 26 Jun 2024 23:55:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/rdma-memory-window/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116373724_p0_master1200.webp" alt="Featured image of post RDMA: Memory Window" />&lt;h1 id="memory-window-of-rdma">
&lt;a href="#memory-window-of-rdma" class="header-anchor">#&lt;/a>
Memory Window of RDMA
&lt;/h1>
&lt;p>&lt;strong>This article welcomes non-commercial reprints, please indicate the source when reprinting.&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>Statement: For collection only, for easier reading.&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/353590347">&lt;cite>Zhihu Column: 14. RDMA Memory Window&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>&lt;strong>This article is the 14th in the &amp;ldquo;RDMA Talk&amp;rdquo; column. Welcome to repost, please indicate the source when reposting.&lt;/strong>&lt;/p>
&lt;p>In the article &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-mr/" >【RDMA Memory Region】
&lt;/a>
, we introduced Memory Region, which is a special memory area registered by the user: on one hand, its contents will not be swapped to the hard disk, and on the other hand, the RDMA network card records its address translation relationship, allowing the hardware to find the corresponding physical address after obtaining the virtual address specified by the user in the WR.&lt;/p>
&lt;p>In this article, we will explain the concept of Memory Window, which is a more flexible memory management unit based on Memory Region. Besides the concept of MW, this article will also provide a more detailed introduction to some memory-related concepts in the RDMA field, such as L_Key/R_Key, etc. It is recommended to read this article in conjunction with &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-mr/" >【RDMA Memory Region】
&lt;/a>
for better understanding, and it is suggested that readers review it first.&lt;/p>
&lt;h2 id="what-is-memory-window">
&lt;a href="#what-is-memory-window" class="header-anchor">#&lt;/a>
What is Memory Window
&lt;/h2>
&lt;p>Memory Window, abbreviated as MW, can be translated into Chinese as 内存窗口. It is an RDMA resource requested by the user to allow a remote node to access the local memory area. Each MW is bound (referred to as bind) to an already registered MR, but compared to MR, it can provide more flexible permission control. MW can be roughly understood as a subset of MR, and many MWs can be divided from one MR, each MW can set its own permissions. The relationship between MW and MR is shown in the following diagram:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_1.webp"
alt="2024-06-28_12_1" width="30%" loading="lazy">&lt;figcaption>
&lt;h4>The relationship between MR and MW&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h2 id="memory-access-permission-control">
&lt;a href="#memory-access-permission-control" class="header-anchor">#&lt;/a>
Memory access permission control
&lt;/h2>
&lt;p>To explain why MW is designed, let&amp;rsquo;s first discuss the access control involved in both MR and MW.&lt;/p>
&lt;h3 id="mrmw-permissions-configuration">
&lt;a href="#mrmw-permissions-configuration" class="header-anchor">#&lt;/a>
MR/MW permissions configuration
&lt;/h3>
&lt;p>The permissions here refer to the local/remote node, for the read/write permissions of the local memory, they form four combinations:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">&lt;/th>
&lt;th style="text-align: left">Local End&lt;/th>
&lt;th style="text-align: left">Remote End&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">Read&lt;/td>
&lt;td style="text-align: left">Local Read&lt;/td>
&lt;td style="text-align: left">Remote Read&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Write&lt;/td>
&lt;td style="text-align: left">Local Write&lt;/td>
&lt;td style="text-align: left">Remote Write&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Apart from these four types of permissions, there are also Atomic permissions, etc., which are not within the scope of this article.&lt;/p>
&lt;p>Among the four types of permissions in the table, the lowest is Local Read, which is a permission that users must grant to MR/MW because if a piece of memory is inaccessible to local users, it loses its meaning. Additionally, there is a restriction: if an MR needs to be configured with Remote Write or the not-yet-introduced Remote Atomic permissions, it must also be configured with Local Write permissions. Under this constraint, each MR or MW can configure permissions as needed. For example, if an MR we registered needs to allow remote nodes to write data but not read, we enable the Remote Write permission and disable the Remote Read permission. In this way, when the HCA (network card) receives a WRITE request initiated by the peer for a certain address within the range of this MR, it can allow it; however, when the HCA receives a READ operation from the peer on this MR, it will reject the request and return an error message to the peer.&lt;/p>
&lt;h3 id="memory-key">
&lt;a href="#memory-key" class="header-anchor">#&lt;/a>
Memory Key
&lt;/h3>
&lt;p>The above access permission configuration cannot prevent malicious users from accessing local or remote memory. For example, if a node grants Remote Write permission to a memory region, wouldn&amp;rsquo;t any remote node (process) be able to write to this region as long as it provides the correct address information? Therefore, the IB specification designed the Memory Key, which can be simply understood as a key mechanism for accessing MR. Only with the correct key can one open the door to MR/MW.&lt;/p>
&lt;p>Key is a string of numbers, consisting of two parts: a 24-bit Index and an 8-bit Key:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_2.webp"
alt="2024-06-28_12_2" width="60%" loading="lazy">&lt;figcaption>
&lt;h4>Composition of L_Key/R_Key&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>Among them, Index is used by HCA for quick indexing to local virtual-to-physical address translation tables and other MR-related information, while Key is used to verify the legality of the entire field to prevent unauthorized users from arbitrarily passing the Index.&lt;/p>
&lt;p>Memory Key is divided into two types according to their usage, Local Key and Remote Key:&lt;/p>
&lt;h4 id="l_key">
&lt;a href="#l_key" class="header-anchor">#&lt;/a>
L_Key
&lt;/h4>
&lt;p>Local Key, associated with an MR, is used for HCA to access local memory. When a process on the local side attempts to use memory of an already registered MR, the HCA will verify the L_Key it passes. It uses the index in the L_Key to look up the address translation table, translates the virtual address into a physical address, and then accesses the memory.&lt;/p>
&lt;p>In the article &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-shared-receive-queue/" >【RDMA Shared Receive Queue】
&lt;/a>
, we described sge, which consists of a starting address, length, and key. When users fill out a WR, if they need the HCA to access the local memory, they need to describe the memory block through a linked list of sge (sgl). Here, the key in the sge is filled with L_Key, which are key1 and key3 in the diagram below, representing the L_Key of MR1 and MR2, respectively. Without L_Key, any local user process could direct the hardware to access the contents of other locally registered MRs, and the hardware would find it difficult to efficiently translate virtual addresses to physical addresses.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_3.webp"
alt="2024-06-28_12_3" width="50%" loading="lazy">&lt;figcaption>
&lt;h4>The function of L_Key&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h4 id="r_key">
&lt;a href="#r_key" class="header-anchor">#&lt;/a>
R_Key
&lt;/h4>
&lt;p>Remote Key, associated with an MR or MW, is used for a remote node to access local memory. When a remote node attempts to access local memory, on one hand, the local HCA will verify whether the R_Key is valid, and on the other hand, it will use the index in the R_Key to check the address translation table, translating the virtual address into a physical address and then accessing the memory.&lt;/p>
&lt;p>For any RDMA operation (i.e., Write/Read/Atomic), the user must carry the remote memory region&amp;rsquo;s R_Key in the WR.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_4.webp"
alt="2024-06-28_12_4" width="70%" loading="lazy">&lt;figcaption>
&lt;h4>The Function of R_Key&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>The IB specification ensures that MR can be accessed correctly and safely according to the user&amp;rsquo;s expectations through the two mechanisms mentioned above. We use a metaphor to summarize the content related to MR/MW permission control:&lt;/p>
&lt;p>A equipped their room (MR) with two keys (Memory Key), one for personal use (L_Key), and the other key (R_Key) was sent to B (can be via any communication method). B can open the door when A is not home (the local CPU does not perceive the remote node&amp;rsquo;s RDMA operations on local memory) using the key (R_Key). After opening the door, B might only be able to view the room&amp;rsquo;s arrangement through glass (A only granted remote read permission for this MR), or enter the room and find it completely dark, unable to see anything, but can place items in the room (A only granted remote write permission for this MR), and of course, it is also possible that there&amp;rsquo;s no glass and the lights are on (remote read and write permissions were granted simultaneously).&lt;/p>
&lt;h2 id="why-have-mw">
&lt;a href="#why-have-mw" class="header-anchor">#&lt;/a>
Why have MW
&lt;/h2>
&lt;p>In short, the purpose of designing MW is to control remote memory access permissions more flexibly.&lt;/p>
&lt;p>In the article &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-mr/" >【RDMA 之 Memory Region】
&lt;/a>
, we introduced the process of user registering MR, which requires transitioning from user mode to kernel mode, calling the function provided by the kernel to pin the memory (to prevent paging), and then creating a virtual-physical address mapping table and issuing it to the hardware.&lt;/p>
&lt;p>Because MR is managed by the kernel, if a user wants to modify the information of an existing MR, for example, if I want to revoke the remote write permission of a certain MR, leaving only the remote read permission; or if I want to invalidate an R_Key that was previously authorized to a remote node, the user needs to use the Reregister MR interface to make modifications. This interface is equivalent to first Deregister MR and then Register MR. &lt;strong>The above process requires transitioning to kernel mode to complete, and this process is time-consuming.&lt;/strong>&lt;/p>
&lt;p>Unlike MR, which requires permission modification through the control path, &lt;strong>MW can be dynamically bound to an already registered MR through the data path (i.e., directly issuing WR to the hardware from user space) after creation, and simultaneously set or change its access permissions. This process is much faster than re-registering MR&lt;/strong>.&lt;/p>
&lt;p>In order for a piece of memory to be capable of RDMA WRITE/READ operations by a remote node, we have two methods: registering an MR and registering an MW and then binding it to an already registered MR. Both will generate an R_Key to provide to the remote node. The first method has simpler preparation steps but is less flexible, and once registered, modifications are relatively troublesome. The second method involves additional steps of registering an MW and binding the MW to an MR compared to the first method, but it allows for convenient and quick control over remote access permissions.&lt;/p>
&lt;h2 id="the-relationship-between-mw-and-mr-permissions">
&lt;a href="#the-relationship-between-mw-and-mr-permissions" class="header-anchor">#&lt;/a>
The relationship between MW and MR permissions
&lt;/h2>
&lt;p>Perhaps some readers might think, when configuring their permissions during MR application, and when MW is bound to MR, their permissions are also configured, what is the relationship between these two permissions? The IB specification has a dedicated section on this in 10.6.7.2.2:&lt;/p>
&lt;blockquote>
&lt;p>When binding a Memory Window, a Consumer can request any combination of remote access rights for the Window. However, if the associated Region does not have local write access enabled and the Consumer requests remote write or remote atomic access for the Window, the Channel Interface must return an error either at bind time or access time.&lt;/p>
&lt;/blockquote>
&lt;p>In summary, &lt;strong>if you want to configure remote write or remote atomic operation (Atomic) permissions for MW, then the MR it is bound to must have local write permissions. In other cases, the permissions of the two do not interfere with each other&lt;/strong>: remote users using MW must follow the permission configuration of MW; remote users using MR must follow the permission configuration of MR.&lt;/p>
&lt;h2 id="user-interface">
&lt;a href="#user-interface" class="header-anchor">#&lt;/a>
User Interface
&lt;/h2>
&lt;p>As usual, when it comes to user interfaces, we classify them according to control paths and data paths:&lt;/p>
&lt;h3 id="control-path">
&lt;a href="#control-path" class="header-anchor">#&lt;/a>
Control path
&lt;/h3>
&lt;p>MW supports addition, deletion, and search, but cannot be directly modified:&lt;/p>
&lt;h4 id="create---allocate-mw">
&lt;a href="#create---allocate-mw" class="header-anchor">#&lt;/a>
Create - Allocate MW
&lt;/h4>
&lt;p>Apply for MW, mainly to create the software structure related to MW and prepare the hardware. The user needs to specify the type of MW introduced later in the text. This interface will generate a handle for the Memory Window, which the user can use to refer to this MW in the future.&lt;/p>
&lt;p>Note that at this time MW is not bound to MR and is in a state that cannot be accessed remotely.&lt;/p>
&lt;h4 id="delete---deallocate-mw">
&lt;a href="#delete---deallocate-mw" class="header-anchor">#&lt;/a>
Delete - Deallocate MW
&lt;/h4>
&lt;p>Unregister MW. It&amp;rsquo;s easy to understand, just destroy the related resources.&lt;/p>
&lt;h4 id="query---query-mw">
&lt;a href="#query---query-mw" class="header-anchor">#&lt;/a>
Query - Query MW
&lt;/h4>
&lt;p>Query MW information, including R_Key and its status, MW type, and PD, etc.&lt;/p>
&lt;p>It needs to be emphasized again that although this Verbs is described in the IB specification, the related API has not been implemented in the RDMA software stack. There are quite a few Verbs interfaces in similar situations. The RDMA software stack is based on practicality, and interfaces without user demand are generally not implemented.&lt;/p>
&lt;h3 id="data-path">
&lt;a href="#data-path" class="header-anchor">#&lt;/a>
Data path
&lt;/h3>
&lt;p>MW has a unique set of interfaces in the data path, divided into Bind and Invalidate categories:&lt;/p>
&lt;h4 id="bind">
&lt;a href="#bind" class="header-anchor">#&lt;/a>
Bind
&lt;/h4>
&lt;p>Bind(ing) means &amp;ldquo;binding,&amp;rdquo; which refers to associating an MW with a specified range of an already registered MR and configuring certain read and write permissions. The result of binding will generate an R_key, which the user can pass to a remote node for remote access. Note that an MW can be bound multiple times, and multiple MWs can be bound to a single MR. If an MR still has bound MWs, then this MR cannot be deregistered.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_5.webp"
alt="2024-06-28_12_5" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>Bind&amp;#39;s Software and Hardware Interaction&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>There are two ways to bind: one is to call the Post Send interface to issue a Bind MW WR, and the other is to call the Bind MW interface.&lt;/p>
&lt;ul>
&lt;li>Post Send Bind MW WR&lt;/li>
&lt;/ul>
&lt;p>In the previous text, we discussed that compared to MR, the biggest advantage of MW is the ability to quickly configure permissions from the data path. Post Send Bind MW WR operation refers to the user issuing a WR to the SQ through the post send interface (such as ibv_post_send()), where the operation type of this WR (such as SEND/RDMA WRITE/RDMA READ) is specified as BIND MW. Additionally, the WR carries information about the permissions and the range of the MR to be bound. Unlike other WRs, after issuing a Bind MW WR, the hardware does not send any packets but instead binds the MW to the specified MR.&lt;/p>
&lt;p>This method is only applicable to Type 2 MW introduced later.&lt;/p>
&lt;ul>
&lt;li>Bind MW&lt;/li>
&lt;/ul>
&lt;p>Although this is an independent interface, it is actually an additional layer encapsulated outside Post Send Bind MW WR. The user provides the relevant information for MW binding, including permissions and the information of the MR to be bound. The driver is responsible for assembling and issuing the WR to the hardware. After the interface succeeds, the newly generated R_Key will be returned to the user.&lt;/p>
&lt;p>This method is only applicable to Type 1 MW introduced later.&lt;/p>
&lt;p>The relationship between the above two operations is as follows:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_6.webp"
alt="2024-06-28_12_6" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>The relationship between two types of Bind operations&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h4 id="invalidation">
&lt;a href="#invalidation" class="header-anchor">#&lt;/a>
Invalidation
&lt;/h4>
&lt;p>Invalidate means invalidation, referring to the operation where a user sends a WR with an Invalidate opcode to the hardware to invalidate an R_Key.&lt;/p>
&lt;p>&lt;strong>It is important to emphasize that the object of the Invalidate operation is the R_Key, not the MW itself. The effect after Invalidate is that the remote user can no longer use this R_Key to access the corresponding MW, but the MW resource still exists, and new R_Keys can still be generated for remote use in the future.&lt;/strong>&lt;/p>
&lt;p>The Invalidate operation can only be used for Type 2 MW introduced below.&lt;/p>
&lt;p>According to the different initiators of the Invalidate operation, it can be further divided into two types:&lt;/p>
&lt;ul>
&lt;li>Local Invalidate&lt;/li>
&lt;/ul>
&lt;p>Invalid local operation. If a higher-level user wants to revoke the R_Key permissions of a certain remote user without reclaiming MW resources, they can issue a Local Invalidate operation to the SQ. After the hardware receives it, it will modify the configuration of the corresponding MR. After successful execution, if the remote user holding this R_Key attempts to perform RDMA operations on the MW, the local hardware will reject it and return an error.&lt;/p>
&lt;p>Because it is a local operation, the hardware will not send a message to the link after receiving this WR.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_7.webp"
alt="2024-06-28_12_7" width="60%" loading="lazy">&lt;figcaption>
&lt;h4>Software and Hardware Interaction of Local Invalidate Operation&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>Remote Invalidate&lt;/li>
&lt;/ul>
&lt;p>Remote invalid operation. When a remote user no longer uses an R_Key, they can proactively send a message to allow the local end to reclaim this R_Key. The remote user issues a WR with this operation code to the SQ, and once the hardware receives it, it will assemble a message and send it to the local end. After the local hardware receives the remote&amp;rsquo;s Remote Invalidate operation, it will set the corresponding R_Key to an unusable state. Just like Local Invalidate, thereafter the remote end will not be able to use this R_Key to perform RDMA operations on the corresponding MW.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_8.webp"
alt="2024-06-28_12_8" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>Remote Invalidate operation&amp;#39;s software and hardware interaction&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h2 id="type-of-mw">
&lt;a href="#type-of-mw" class="header-anchor">#&lt;/a>
Type of MW
&lt;/h2>
&lt;p>According to different implementations and application scenarios, the IB specification classifies MW:&lt;/p>
&lt;h3 id="type-1">
&lt;a href="#type-1" class="header-anchor">#&lt;/a>
Type 1
&lt;/h3>
&lt;p>Type 1 MW is associated with a PD and a QP, and it is not bound to a QP, so it will not affect the destruction of a QP under the same PD.&lt;/p>
&lt;p>The key field of the R_Key for Type 1 MW is controlled by the driver and hardware. Here, &amp;ldquo;controlled&amp;rdquo; means that the key is allocated by the driver and hardware, not by the upper-level user. This is also the reason mentioned earlier that Type 1 MW cannot perform the Invalidate operation. If a user of Type 1 MW wants to invalidate an R_Key, they can bind this MW again through the Bind MW interface. The hardware or driver will automatically allocate a new key field for the R_Key, and the original R_Key will become invalid.&lt;/p>
&lt;p>In addition, if a user temporarily wants to unbind an MW from any MR but still wants to retain the related resources instead of destroying this MW, they can achieve this by calling the Bind MW interface and setting the MW length to 0.&lt;/p>
&lt;p>The IB specification allows multiple Type 1 MWs to be bound to the same MR, and their ranges can overlap.&lt;/p>
&lt;h3 id="type-2">
&lt;a href="#type-2" class="header-anchor">#&lt;/a>
Type 2
&lt;/h3>
&lt;p>Type 2 MW grants users greater freedom, with the key field segment of the R_Key controlled by the user, allowing them to allocate it as they wish. As mentioned earlier, users perform binding through the Post Send Bind MW WR operation, and this process does not return an R_Key. Users must remember the index from the Allocate MW operation and combine it with their chosen 8-bit key to form the R_Key and send it to the peer.&lt;/p>
&lt;p>The user can invalidate an R_Key through the Invalidate operation introduced earlier. If you want to assign a new R_Key to the MW, you must first invalidate the previous R_Key through the Invalidate operation.&lt;/p>
&lt;p>Unlike Type 1, Type 2&amp;rsquo;s MW does not support 0-length binding.&lt;/p>
&lt;p>The IB specification also allows multiple Type 2s to be bound to the same MR, and the ranges can overlap.&lt;/p>
&lt;p>In addition, based on different binding relationships, Type 2 can be further divided into two implementation methods, with their differences lying solely in the binding relationship with QP.&lt;/p>
&lt;h4 id="type-2a">
&lt;a href="#type-2a" class="header-anchor">#&lt;/a>
Type 2A
&lt;/h4>
&lt;p>Associated with a QP through QPN, meaning that when remote access occurs within this MW range, in addition to the R_Key, the correct QPN must also be specified. If a QP has a bound Type 2A MW, then this QP cannot be destroyed.&lt;/p>
&lt;h4 id="type-2b">
&lt;a href="#type-2b" class="header-anchor">#&lt;/a>
Type 2B
&lt;/h4>
&lt;p>By associating a QP with QPN and PD, there is an additional PD verification compared to Type 2A. When the remote end accesses the memory of the MW through RDMA operations, besides the QPN needing to be correct, the PD specified for the local QP must also be the same as the PD bound to this MW. Additionally, unlike Type 2A, a QP can be destroyed even if there is still a Type 2B MW binding relationship.&lt;/p>
&lt;p>The introduction in the original IB specification is relatively scattered, so let&amp;rsquo;s briefly summarize the similarities and differences of several MWs:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">&lt;/th>
&lt;th style="text-align: left">Type 1&lt;/th>
&lt;th style="text-align: left">Type 2A&lt;/th>
&lt;th style="text-align: left">Type 2B&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">Correlation&lt;/td>
&lt;td style="text-align: left">PD&lt;/td>
&lt;td style="text-align: left">QP&lt;/td>
&lt;td style="text-align: left">PD + QP&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">R_Key&amp;rsquo;s key field ownership&lt;/td>
&lt;td style="text-align: left">Driver + Hardware&lt;/td>
&lt;td style="text-align: left">User&lt;/td>
&lt;td style="text-align: left">User&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Binding Method&lt;/td>
&lt;td style="text-align: left">Bind MW After binding, the previous R_Key automatically becomes invalid&lt;/td>
&lt;td style="text-align: left">Post Send Bind MWWR Before binding, the previous R_Key needs to be invalidated&lt;/td>
&lt;td style="text-align: left">Post Send Bind MWWR Before binding, the previous R_Key needs to be invalidated&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Is zero length supported&lt;/td>
&lt;td style="text-align: left">Yes&lt;/td>
&lt;td style="text-align: left">No&lt;/td>
&lt;td style="text-align: left">No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Supports Invalidate&lt;/td>
&lt;td style="text-align: left">No&lt;/td>
&lt;td style="text-align: left">Yes&lt;/td>
&lt;td style="text-align: left">Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Can the associated QP be destroyed&lt;/td>
&lt;td style="text-align: left">-&lt;/td>
&lt;td style="text-align: left">No&lt;/td>
&lt;td style="text-align: left">Yes&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>In addition, the IB specification also provides the following descriptions for the above types: HCA must implement Type 1 MW, and can optionally choose to implement either Type 2A or 2B. Type 1 and Type 2 MW can be simultaneously associated with the same MR. Since I have not encountered many applications using MW, I cannot clearly explain in which scenarios each type of MW should be used. If readers have insights on this topic, they are welcome to share and discuss.&lt;/p>
&lt;p>Alright, MW will be discussed up to here, and this concludes the introduction of common resources in RDMA technology.&lt;/p>
&lt;p>Given that devices generally supporting RDMA are quite expensive, in the next article I will introduce how to conduct some programming experiments through software-simulated devices—namely Soft-RoCE.&lt;/p>
&lt;h2 id="ib-specification-related-chapters">
&lt;a href="#ib-specification-related-chapters" class="header-anchor">#&lt;/a>
IB specification related chapters
&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>3.5.3 Memory Keys Introduction&lt;/p>
&lt;/li>
&lt;li>
&lt;p>9.4.1.1 Invalidate Operation&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.6.7 Permission Management&lt;/p>
&lt;/li>
&lt;li>
&lt;p>11.2.10.9~12 Related Verbs Introduction&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="reference-document">
&lt;a href="#reference-document" class="header-anchor">#&lt;/a>
Reference document
&lt;/h2>
&lt;p>[1] IB Specification Vol 1-Release-1.4&lt;/p>
&lt;p>[2] Linux Kernel Networking - Implementation and Theory. Chapter 13&lt;/p></description></item><item><title>RDMA: Shared Receive Queue</title><link>https://cuterwrite.top/en/p/rdma-shared-receive-queue/</link><pubDate>Wed, 26 Jun 2024 23:34:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/rdma-shared-receive-queue/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116373922_p0_master1200.webp" alt="Featured image of post RDMA: Shared Receive Queue" />&lt;h1 id="shared-receive-queue-in-rdma">
&lt;a href="#shared-receive-queue-in-rdma" class="header-anchor">#&lt;/a>
Shared Receive Queue in RDMA
&lt;/h1>
&lt;p>&lt;strong>This article welcomes non-commercial reprints, please indicate the source when reprinting.&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>Statement: For collection only, for easy reading.&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/279904125">&lt;cite>Zhihu Column: 11. RDMA Shared Receive Queue&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>We briefly introduced the concept of SRQ in &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-element/" >【3. Basic Elements of RDMA】
&lt;/a>
. This article will take you through more details about SRQ.&lt;/p>
&lt;h2 id="basic-concepts">
&lt;a href="#basic-concepts" class="header-anchor">#&lt;/a>
Basic Concepts
&lt;/h2>
&lt;h3 id="what-is-srq">
&lt;a href="#what-is-srq" class="header-anchor">#&lt;/a>
What is SRQ?
&lt;/h3>
&lt;p>The full name is Shared Receive Queue, literally translated as a shared receive queue. We know that the basic unit of RDMA communication is QP, and each QP consists of a send queue SQ and a receive queue RQ.&lt;/p>
&lt;p>SRQ is designed by the IB protocol to save resources for the receiver. We can share an RQ with all associated QPs, and this shared RQ is called an SRQ. When a QP associated with it wants to post a receive WQE, it is filled into this SRQ. Then, whenever the hardware receives data, it stores the data in the specified location based on the content of the next WQE in the SRQ.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_11_1.webp"
alt="2024-06-28_11_1" width="80%" loading="lazy">
&lt;/figure>
&lt;h3 id="why-use-srq">
&lt;a href="#why-use-srq" class="header-anchor">#&lt;/a>
Why use SRQ
&lt;/h3>
&lt;p>Under normal circumstances, the number of tasks we issue to SQ is far greater than the number of tasks issued to RQ. Why is that? Please first recall which types of operations use SQ and which use RQ.&lt;/p>
&lt;p>SEND/WRITE/READ all require the communication initiator to issue a WR to the SQ, and only the RECV operation paired with SEND requires the communication responder to issue a WR to the RQ (the Write operation with immediate value will also consume Receive WR, which we haven&amp;rsquo;t discussed yet). As we know, the SEND-RECV pair of operations is usually used for transmitting control information, while WRITE and READ are the main operations for performing large amounts of remote memory read and write operations, so naturally, the usage rate of SQ is much higher than that of RQ.&lt;/p>
&lt;p>Each queue is an entity, occupying memory and on-chip storage space of the network card. In commercial scenarios, the number of QPs can reach hundreds of thousands or even higher, which places high demands on memory capacity. Memory is bought with hard-earned money, and SRQ is a mechanism designed by the IB protocol to save user memory.&lt;/p>
&lt;p>Let&amp;rsquo;s take a look at the official explanation in the agreement for why SRQ is used (Section 10.2.9.1):&lt;/p>
&lt;blockquote>
&lt;p>Without SRQ, an RC, UC or UD Consumer must post the number of receive WRs necessary to handle incoming receives on a given QP. If the Consumer cannot predict the incoming rate on a given QP, because, for example, the connection has a bursty nature, the Consumer must either: post a sufficient number of RQ WRs to handle the highest incoming rate for each connection, or, for RC, let message flow control cause the remote sender to back off until local Consumer posts more WRs.&lt;/p>
&lt;p>• Posting sufficient WRs on each QP to hold the possible incoming rate, wastes WQEs, and the associated Data Segments, when the Receive Queue is inactive. Furthermore, the HCA doesn’t provide a way of reclaiming these WQEs for use on other connections.&lt;/p>
&lt;p>• Letting the RC message flow control cause the remote sender to back off can add unnecessary latencies, especially if the local Consumer is unaware that the RQ is starving.&lt;/p>
&lt;/blockquote>
&lt;p>In simple terms, without SRQ, because the receiver of RC/UC/UD does not know how much data the other end will send and when it will arrive, it must prepare for the worst-case scenario, preparing for the possibility of receiving a large amount of data suddenly, which means issuing a sufficient number of receive WQEs to the RQ. Additionally, the RC service type can use flow control mechanisms to exert backpressure on the sender, essentially telling the other end &amp;ldquo;I don&amp;rsquo;t have enough RQ WQEs here,&amp;rdquo; so the sender will temporarily slow down or stop sending data.&lt;/p>
&lt;p>However, as we mentioned earlier, the first method, being prepared for the worst-case scenario, often results in a large number of RQ WQEs being idle and unused, which is a significant waste of memory. Although the second method doesn&amp;rsquo;t require issuing as many RQ WQEs, flow control comes at a cost, which is the increased communication latency.&lt;/p>
&lt;p>And SRQ solves the above problem by allowing many QPs to share receive WQEs (as well as memory space for storing data). When any QP receives a message, the hardware will take a WQE from the SRQ, store the received data according to its content, and then the hardware will return the completion information of the receive task to the corresponding upper-level user through the Completion Queue.&lt;/p>
&lt;p>Let&amp;rsquo;s take a look at how much memory can be saved by using SRQ compared to using a standard RQ&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>:&lt;/p>
&lt;p>Assume that there are N pairs of QP on the node receiving data, and each QP may receive a consecutive M messages at random times (each message consumes a WQE from an RQ),&lt;/p>
&lt;ul>
&lt;li>If SRQ is not used, the user needs to issue N * M RQ WQEs in total.&lt;/li>
&lt;li>If using SRQ, the user only needs to issue K * M RQ WQEs, where K is much smaller than N.&lt;/li>
&lt;/ul>
&lt;p>This K can be configured by the user according to the business needs. If there is a large amount of concurrent reception, then set K to a larger value; otherwise, setting K to a single digit is sufficient to handle general situations.&lt;/p>
&lt;p>We have saved a total of (N - K) * M RQ WQEs, and RQ WQEs themselves are not very large, approximately a few KB in size, which doesn&amp;rsquo;t seem to take up much memory. However, as mentioned earlier, what is actually saved is the &lt;strong>memory space used to store data&lt;/strong>, which is a significant amount of memory. We will use a diagram to illustrate:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_11_2.webp"
alt="2024-06-28_11_2" width="80%" loading="lazy">
&lt;/figure>
&lt;p>In the above diagram, there are two RQ WQEs in the SRQ. Let&amp;rsquo;s take a look at the contents of an RQ WQE, which are composed of several SGEs (Scatter/Gather Elements). Each SGE consists of a memory address, length, and key. With a starting address and length, an SGE can point to a contiguous memory region, and multiple SGEs can represent multiple discrete contiguous memory blocks. We refer to multiple SGEs as an SGL (Scatter/Gather List). SGEs are ubiquitous in the IB software protocol stack (and indeed very common throughout Linux), allowing very large memory regions to be represented with minimal space. IB users use SGEs to specify send and receive areas.&lt;/p>
&lt;p>You can simply estimate the size of the memory region each sge can point to. The length is a 32-bit unsigned integer, which can represent 4GB of space. Assuming an RQ WQE can hold a maximum of 256 sge, then an RQ WQE would be a total of 1TB. Of course, in reality, it cannot be that large, this is just to intuitively inform the reader of the potential memory space an RQ WQE might occupy.&lt;/p>
&lt;h3 id="srqc">
&lt;a href="#srqc" class="header-anchor">#&lt;/a>
SRQC
&lt;/h3>
&lt;p>That is SRQ Context. Like QPC, SRQC is used to inform the hardware about attributes related to SRQ, including depth, WQE size, and other information, which will not be elaborated on in this article.&lt;/p>
&lt;h3 id="srqn">
&lt;a href="#srqn" class="header-anchor">#&lt;/a>
SRQN
&lt;/h3>
&lt;p>That is SRQ Number. Like QP, there may be multiple SRQs in each node. To identify and distinguish these SRQs, each SRQ has a serial number, called SRQN.&lt;/p>
&lt;h3 id="pd-of-srq">
&lt;a href="#pd-of-srq" class="header-anchor">#&lt;/a>
PD of SRQ
&lt;/h3>
&lt;p>In &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-protection-domain/" >【7. RDMA Protection Domain】
&lt;/a>
, we introduced the concept of Protection Domain, which is used to isolate different RDMA resources. Each SRQ must specify its own PD, which can be the same as the PD of its associated QP, or it can be different; SRQs can also use the same PD.&lt;/p>
&lt;p>If a packet is received while using SRQ, it will only be properly received if the MR and SRQ being accessed are under the same PD; otherwise, an immediate error will occur.&lt;/p>
&lt;h2 id="asynchronous-event">
&lt;a href="#asynchronous-event" class="header-anchor">#&lt;/a>
Asynchronous event
&lt;/h2>
&lt;p>In the article &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-completion-queue/" >【10. RDMA Completion Queue】
&lt;/a>
, we introduced that the IB protocol classifies error types into immediate errors, completion errors, and asynchronous errors based on the method of error reporting. Among them, asynchronous errors are similar to interrupts/events, so we sometimes refer to them as asynchronous events. Each HCA registers an event handling function specifically for handling asynchronous events. Upon receiving an asynchronous event, the driver performs the necessary processing and further reports it to the user.&lt;/p>
&lt;p>There is a special asynchronous event regarding SRQ, used to promptly notify upper-level users of the SRQ status, namely the SRQ Limit Reached event.&lt;/p>
&lt;h3 id="srq-limit">
&lt;a href="#srq-limit" class="header-anchor">#&lt;/a>
SRQ Limit
&lt;/h3>
&lt;p>SRQ can set a watermark/threshold, when the number of remaining WQEs in the queue is less than the watermark, this SRQ will report an asynchronous event. It reminds the user &amp;ldquo;The WQEs in the queue are about to run out, please issue more WQEs to prevent having no place to receive new data.&amp;rdquo; This watermark/threshold is called the SRQ Limit, and the reported event is called SRQ Limit Reached.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_11_3.webp"
alt="2024-06-28_11_3" width="30%" loading="lazy">
&lt;/figure>
&lt;p>Because the SRQ is shared by multiple QPs, if the depth is relatively small, it is very likely that the WQE inside will suddenly run out. Therefore, the protocol is designed with this mechanism to ensure that users can promptly intervene in situations where the WQE is insufficient.&lt;/p>
&lt;p>After reporting an asynchronous event, the value of SRQ Limit will be reset to 0 by the hardware (presumably to prevent continuously reporting asynchronous events to the upper layer). Of course, users can choose not to use this mechanism by simply setting the value of SRQ Limit to 0.&lt;/p>
&lt;h2 id="user-interface">
&lt;a href="#user-interface" class="header-anchor">#&lt;/a>
User interface
&lt;/h2>
&lt;h3 id="control-surface">
&lt;a href="#control-surface" class="header-anchor">#&lt;/a>
Control surface
&lt;/h3>
&lt;p>Still the old four types—&amp;ldquo;Add, Delete, Modify, Query&amp;rdquo;:&lt;/p>
&lt;ul>
&lt;li>Create SRQ&lt;/li>
&lt;/ul>
&lt;p>When creating an SRQ, similar to a QP, all software and hardware resources related to the SRQ are allocated. For example, the driver will request an SRQN, allocate space for the SRQC, and fill in the configuration. When creating an SRQ, you must also specify the depth of each SRQ (how many WQEs it can store) and the maximum number of sges per WQE.&lt;/p>
&lt;ul>
&lt;li>Destroy SRQ&lt;/li>
&lt;/ul>
&lt;p>Destroy all related software and hardware resources of SRQ.&lt;/p>
&lt;ul>
&lt;li>Modify SRQ&lt;/li>
&lt;/ul>
&lt;p>In addition to attributes such as SRQ depth, the value of SRQ Limit is also set through this interface. Because the value of the watermark is cleared every time an SRQ Limit Reached event occurs, the user needs to call Modify SRQ to reset the watermark each time.&lt;/p>
&lt;ul>
&lt;li>Query SRQ&lt;/li>
&lt;/ul>
&lt;p>It is usually used to query the configuration of the waterline.&lt;/p>
&lt;h3 id="data-surface">
&lt;a href="#data-surface" class="header-anchor">#&lt;/a>
Data surface
&lt;/h3>
&lt;h3 id="post-srq-receive">
&lt;a href="#post-srq-receive" class="header-anchor">#&lt;/a>
Post SRQ Receive
&lt;/h3>
&lt;p>Just like Post Receive, it issues a receive WQE to the SRQ, which contains information about the memory block used as the receive buffer. It is important to note that the subject is SRQ and has nothing to do with QP. Currently, the user is not concerned with which QP this SRQ is associated with.&lt;/p>
&lt;h2 id="the-difference-between-srq-and-rq">
&lt;a href="#the-difference-between-srq-and-rq" class="header-anchor">#&lt;/a>
The difference between SRQ and RQ
&lt;/h2>
&lt;p>In terms of functionality, both SRQ and RQ are used to store received task requests, but due to the shared nature of SRQ, there are some differences between it and RQ.&lt;/p>
&lt;h3 id="state-machine">
&lt;a href="#state-machine" class="header-anchor">#&lt;/a>
State machine
&lt;/h3>
&lt;p>We introduced in &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-queue-pair/" >【9. RDMA Queue Pair】
&lt;/a>
that QP has a complex state machine, and the sending and receiving capabilities of QP vary in different states. However, SRQ only has two states: non-error and error.&lt;/p>
&lt;p>Regardless of the state, users can issue WQEs to the SRQ. However, in an error state, the associated QP cannot receive data from this SRQ. Additionally, in an error state, users cannot query or modify the attributes of the SRQ.&lt;/p>
&lt;p>When a QP is in an error state, it can be returned to the RESET state through Modify QP, but for SRQ, it can only exit the error state by destroying it.&lt;/p>
&lt;h3 id="receiving-process">
&lt;a href="#receiving-process" class="header-anchor">#&lt;/a>
Receiving process
&lt;/h3>
&lt;p>For a QP, RQ and SRQ cannot be used simultaneously, one must be chosen. If a WQE is issued to the RQ of a QP that is already associated with SRQ, an immediate error will be returned.&lt;/p>
&lt;p>Let&amp;rsquo;s compare the reception processes of SRQ and RQ. The content of this section is a key point of this article, and I believe that after reading it, readers will have a more complete understanding of the SRQ mechanism.&lt;/p>
&lt;h3 id="rqs-receiving-process">
&lt;a href="#rqs-receiving-process" class="header-anchor">#&lt;/a>
RQ&amp;rsquo;s receiving process
&lt;/h3>
&lt;p>First, let&amp;rsquo;s revisit the receiving process of a regular RQ (for the complete process on the sender&amp;rsquo;s side, please read &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-op/" >【4. RDMA Operation Types】
&lt;/a>
):&lt;/p>
&lt;ol start="0">
&lt;li>
&lt;p>Create QP.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Through the Post Recv interface, the user submits receive WQE to the RQ of QP2 and QP3, respectively. The WQE contains information about which memory region to place the received data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The hardware receives the data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Hardware discovery is sent to QP3, then WQE1 is taken from QP3&amp;rsquo;s RQ, and the received data is placed in the memory area specified by WQE1.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>After the hardware completes data storage, it generates a CQE to CQ3 associated with RQ of QP3, reporting task completion information.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The user retrieves WC (CQE) from CQ3, and then takes data from the specified memory area.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The hardware receives the data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The hardware discovery is sent to QP2, then WQE1 is extracted from the RQ of QP2, and the received data is placed in the memory area specified by WQE1.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>After the hardware completes data storage, it generates a CQE for CQ2 associated with RQ of QP2, reporting task completion information.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The user retrieves WC (CQE) from CQ2, and then takes data from the specified memory area.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_11_4.webp"
alt="2024-06-28_11_4" width="90%" loading="lazy">
&lt;/figure>
&lt;h3 id="srqs-reception-process">
&lt;a href="#srqs-reception-process" class="header-anchor">#&lt;/a>
SRQ&amp;rsquo;s reception process
&lt;/h3>
&lt;p>And the SRQ receiving process has some differences:&lt;/p>
&lt;ol start="0">
&lt;li>
&lt;p>Create SRQ1, and create QP2 and QP3, both associated with SRQ1.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Through the Post SRQ Recv interface, the user issues two receive WQEs to SRQ1, containing information about which memory region to place the received data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Hardware receives data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The hardware discovery is sent to QP3, extracting the first WQE from SRQ1 (now it is WQE1), and storing the received data according to the content of the WQE.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>Each WQE in the SRQ is &amp;ldquo;ownerless&amp;rdquo;, not associated with any QP. The hardware sequentially takes out the WQE according to the queue order and places the data inside.&lt;/p>
&lt;/blockquote>
&lt;ol start="4">
&lt;li>
&lt;p>The hardware discovers that the CQ associated with QP3&amp;rsquo;s RQ is CQ3, so it generates a CQE in it.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The user retrieves the CQE from CQ3 and takes data from the specified memory area.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>Attentive readers may ask, when a user issues a WR, each WR specifies some memory regions for storing data in the future. However, an SRQ is a pool where each WQE points to several different memory regions. After the user receives a WC in the CQ corresponding to a certain QP, how do they know where the received data has been stored?&lt;/p>
&lt;p>There is actually wr_id information in the WC, informing the user of which WR (WQE) designated memory area the data is placed in. Since the WR is issued by the user, the user naturally knows its specific location.&lt;/p>
&lt;/blockquote>
&lt;ol start="6">
&lt;li>
&lt;p>Hardware received data&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The hardware discovery is sent to QP2, and the first WQE is taken from SRQ1 (now it is WQE2), and the received data is stored according to the content of the WQE.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The hardware discovers that the CQ associated with QP2&amp;rsquo;s RQ is CQ2, so a CQE is generated in it.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The user takes out the CQE from CQ2 and retrieves data from the specified memory area.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_11_5.webp"
alt="2024-06-28_11_5" width="90%" loading="lazy">
&lt;/figure>
&lt;h2 id="summary">
&lt;a href="#summary" class="header-anchor">#&lt;/a>
Summary
&lt;/h2>
&lt;p>This text first introduces the basic concept of SRQ, followed by its design purpose, related mechanisms, and user interface. Finally, it compares the SRQ receiving process with RQ. In actual business, the usage rate of SRQ is quite high, and it is hoped that readers can gain a deep understanding.&lt;/p>
&lt;p>Let&amp;rsquo;s stop here, thank you for reading. In the next article, I will introduce the Memory Window.&lt;/p>
&lt;h2 id="relevant-sections-of-the-agreement">
&lt;a href="#relevant-sections-of-the-agreement" class="header-anchor">#&lt;/a>
Relevant sections of the agreement
&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>10.2.9 The design concept of SRQ and related operations&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.2.3 PD of SRQ and QP&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.8.2 The relationship between QP associated with SRQ and QP not using SRQ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.8.5 SRQ related returns WC&lt;/p>
&lt;/li>
&lt;li>
&lt;p>11.5.2.4 Asynchronous Events&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="other-references">
&lt;a href="#other-references" class="header-anchor">#&lt;/a>
Other references
&lt;/h2>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>Linux Kernel Networking - Implementation and Theory. Chapter 13. Shared Receive Queue&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>RDMA: Completion Queue</title><link>https://cuterwrite.top/en/p/rdma-completion-queue/</link><pubDate>Wed, 26 Jun 2024 23:11:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/rdma-completion-queue/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116903369_p0_master1200.webp" alt="Featured image of post RDMA: Completion Queue" />&lt;h1 id="rdmas-completion-queue">
&lt;a href="#rdmas-completion-queue" class="header-anchor">#&lt;/a>
RDMA&amp;rsquo;s Completion Queue
&lt;/h1>
&lt;p>&lt;strong>This article welcomes non-commercial reproduction, please indicate the source.&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>Statement: For collection only, for easy reading&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/259650980">&lt;cite>Zhihu Column: 10. RDMA Completion Queue&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>We have briefly introduced CQ in previous articles, and this article will delve deeper into some of its details. Before reading this article, readers can first review this article: &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-element/" >【“3. RDMA Basic Elements”】
&lt;/a>
.&lt;/p>
&lt;h2 id="basic-concepts">
&lt;a href="#basic-concepts" class="header-anchor">#&lt;/a>
Basic Concepts
&lt;/h2>
&lt;p>Let&amp;rsquo;s first review the function of CQ. CQ stands for Completion Queue, and its function is opposite to that of WQ (SQ and RQ). The hardware uses CQE/WC in the CQ to inform the software about the completion status of a certain WQE/WR. A reminder to readers: for upper-layer users, WC is generally used, while for drivers, it is generally referred to as CQE. This article does not distinguish between the two.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_1.webp"
alt="2024-06-27_10_1" width="80%" loading="lazy">
&lt;/figure>
&lt;p>CQE can be regarded as a &amp;ldquo;report&amp;rdquo; that specifies the execution status of a certain task, including:&lt;/p>
&lt;ul>
&lt;li>Which task specified by which WQE of which QP was completed this time (QP Number and WR ID)&lt;/li>
&lt;li>What operation was performed in this task (Opcode operation type)&lt;/li>
&lt;li>This task executed successfully/failed, the reason for failure is XXX (Status and error code)&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>Whenever the hardware completes processing a WQE, a CQE is generated and placed in the CQ queue. If a CQE corresponding to a WQE is not generated, then this WQE will always be considered as not yet processed. What does this mean?&lt;/p>
&lt;ul>
&lt;li>Operations involving fetching data from memory (SEND and WRITE)&lt;/li>
&lt;/ul>
&lt;p>Before generating a CQE, the hardware may not have sent the message yet, may be in the process of sending the message, or the peer may have received the correct message. Since the memory region is allocated before sending, the upper-level software must consider this memory region still in use before receiving the corresponding CQE and cannot release all related memory resources.&lt;/p>
&lt;ul>
&lt;li>Operations involving storing data in memory (RECV and READ)&lt;/li>
&lt;/ul>
&lt;p>Before the CQE is generated, it is possible that the hardware has not started writing data, it is possible that only half of the data has been written, or it is possible that a data verification error has occurred. Therefore, before the upper-layer software receives the CQE, the contents of the memory area used to store the received data are unreliable.&lt;/p>
&lt;p>In summary, the user must obtain the CQE and confirm its content before considering the message sending and receiving task complete.&lt;/p>
&lt;h3 id="when-was-it-generated">
&lt;a href="#when-was-it-generated" class="header-anchor">#&lt;/a>
When was it generated?
&lt;/h3>
&lt;p>We will explain separately according to the service type (this article only discusses RC and UD) and the operation type, because the timing and meaning of generating CQE are different in different situations. Readers are advised to review the 4th article &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-op/" >&amp;ldquo;4. Basic RDMA Operations&amp;rdquo;
&lt;/a>
and the 5th article &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-service-types/" >&amp;ldquo;5. Basic RDMA Service Types&amp;rdquo;
&lt;/a>
.&lt;/p>
&lt;ul>
&lt;li>Reliable Service Type (RC)&lt;/li>
&lt;/ul>
&lt;p>The previous article mentioned that &lt;strong>reliability means that the sender is concerned that the message sent can be accurately received by the receiver&lt;/strong>, which is ensured through mechanisms such as ACK, checksum, and retransmission.&lt;/p>
&lt;ul>
&lt;li>SEND&lt;/li>
&lt;/ul>
&lt;p>SEND operation requires hardware to fetch data from memory, then assemble it into packets to send to the other end through a physical link. For SEND, the Client side generates a CQE indicating &lt;strong>the other end has received the data accurately&lt;/strong>, after the other end&amp;rsquo;s hardware receives and verifies the data, it will reply with an ACK packet to the sender. Only after the sender receives this ACK will a CQE be generated, thus informing the user that the task has been successfully executed. As shown in the figure, the left Client side generates the CQE for this task at the position marked by the red dot.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_2.webp"
alt="2024-06-27_10_2" width="50%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>RECV&lt;/li>
&lt;/ul>
&lt;p>The RECV operation requires the hardware to place the received data into the memory area specified in the user&amp;rsquo;s WQE. After completing the checksum and data storage actions, the hardware will generate a CQE, as shown on the right side of the above figure on the server side.&lt;/p>
&lt;ul>
&lt;li>WRITE&lt;/li>
&lt;/ul>
&lt;p>For the Client side, WRITE operation and SEND operation are the same, the hardware will fetch data from memory and wait for the peer to reply with an ACK before generating a CQE. The difference is that because WRITE is an RDMA operation, the peer CPU is not aware of it, and naturally the user is not aware of it either, so the diagram above becomes like this:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_3.webp"
alt="2024-06-27_10_3" width="50%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>READ&lt;/li>
&lt;/ul>
&lt;p>READ and RECV are somewhat similar. After the Client initiates a READ operation, the other side will reply with the data we want to read. Then, after verifying that there are no issues, the data will be placed in the specified location in the WQE. After completing the above actions, a CQE will be generated on our side. READ is also an RDMA operation, which is not perceived by the other side&amp;rsquo;s user, and naturally, no CQE is generated. In this situation, the diagram becomes like this:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_4.webp"
alt="2024-06-27_10_4" width="50%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>Unreliable Service Type (UD)&lt;/li>
&lt;/ul>
&lt;p>Because unreliable service types lack retransmission and acknowledgment mechanisms, generating a CQE indicates that the hardware &lt;strong>has already sent out the data specified by the corresponding WQE&lt;/strong>. It was previously mentioned that UD only supports SEND-RECV operations and does not support RDMA operations. Therefore, for both ends of the UD service, the timing for CQE generation is as shown in the figure below:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_5.webp"
alt="2024-06-27_10_5" width="50%" loading="lazy">
&lt;/figure>
&lt;h3 id="the-correspondence-between-wq-and-cq">
&lt;a href="#the-correspondence-between-wq-and-cq" class="header-anchor">#&lt;/a>
The correspondence between WQ and CQ
&lt;/h3>
&lt;p>&lt;strong>Each WQ must be associated with a CQ, and each CQ can be associated with multiple SQs and RQs.&lt;/strong>&lt;/p>
&lt;p>The so-called &amp;ldquo;association&amp;rdquo; here refers to the fact that all CQEs corresponding to a WQ&amp;rsquo;s WQEs will be placed by the hardware into the bound CQ. It&amp;rsquo;s important to note that the SQ and RQ belonging to the same QP can each be associated with different CQs. As shown in the diagram below, both the SQ and RQ of QP1 are associated with CQ1, while the RQ of QP2 is associated with CQ1 and the SQ is associated with CQ2.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_6.webp"
alt="2024-06-27_10_6" width="auto" loading="lazy">
&lt;/figure>
&lt;p>Because each WQ must be associated with a CQ, the user needs to create the CQ in advance before creating the QP, and then specify which CQ will be used by the SQ and RQ respectively.&lt;/p>
&lt;p>&lt;strong>The WQEs in the same WQ correspond to CQEs that are ordered&lt;/strong>&lt;/p>
&lt;p>The hardware retrieves WQEs from a certain WQ (SQ or RQ) and processes them in a &amp;ldquo;First In, First Out&amp;rdquo; FIFO order, and when placing CQEs in the CQ associated with WRs, it also follows the order in which these WQEs were placed in the WQ. Simply put, whoever is placed in the queue first is completed first. This process is shown in the diagram below:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_7.webp"
alt="2024-06-27_10_7" width="auto" loading="lazy">
&lt;/figure>
&lt;p>It should be noted that the use of SRQ and the RQ in RD service type are both non-order-preserving, which will not be discussed in this article.&lt;/p>
&lt;p>&lt;strong>The WQEs in different WQs are not ordered with respect to their corresponding CQEs.&lt;/strong>&lt;/p>
&lt;p>In the previous text, we mentioned that a CQ might be shared by multiple WQs. In this case, the order of generation for the CQEs corresponding to these WQEs cannot be guaranteed. As shown in the figure below (the WQE number indicates the order of issuance, i.e., 1 is issued first, and 6 is issued last):&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_8.webp"
alt="2024-06-27_10_8" width="auto" loading="lazy">
&lt;/figure>
&lt;p>The above description actually also includes the situation of &amp;ldquo;WQE in SQ and RQ of the same QP, their corresponding CQE is not ordered.&amp;rdquo; This is actually quite easy to understand. SQ and RQ, one is responsible for actively initiating tasks, and the other for passively receiving tasks. They can be considered as channels in two different directions and naturally should not affect each other. Suppose the user first issues a Receive WQE and then a Send WQE for the same QP. It can&amp;rsquo;t be that if the peer doesn&amp;rsquo;t send a message to the local end, the local end cannot send a message to the peer, right?&lt;/p>
&lt;p>In this case, since the order in which CQEs are generated is not related to the order in which WQEs are obtained, how do the upper-level application and driver know which WQE the received CQE is associated with? It&amp;rsquo;s actually quite simple, &lt;strong>the CQE indicates the number of the WQE it corresponds to&lt;/strong>.&lt;/p>
&lt;p>Additionally, it should be noted that even when multiple WQs share a single CQ, &amp;ldquo;WQEs in the same WQ have their corresponding CQEs ordered&amp;rdquo; is always guaranteed. This means that the CQEs corresponding to WQE 1, 3, and 4 belonging to WQ1 in the above diagram are generated in sequence, and the same applies to WQE 2, 5, and 6 belonging to WQ2.&lt;/p>
&lt;h3 id="cqc">
&lt;a href="#cqc" class="header-anchor">#&lt;/a>
CQC
&lt;/h3>
&lt;p>Just like QP, CQ is merely a queue memory space for storing CQEs. Apart from knowing the starting address, the hardware is essentially unaware of this area. Therefore, it is necessary to agree on a format with the software in advance, and then the driver will allocate memory and fill in the basic information of the CQ in this memory according to the format for the hardware to read. This memory is the CQC. The CQC contains information such as the capacity size of the CQ, the sequence number of the currently processed CQE, and so on. So by slightly modifying the QPC diagram, you can represent the relationship between CQC and CQ:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_9.webp"
alt="2024-06-27_10_9" width="auto" loading="lazy">
&lt;/figure>
&lt;h3 id="cqn">
&lt;a href="#cqn" class="header-anchor">#&lt;/a>
CQN
&lt;/h3>
&lt;p>CQ Number is the CQ&amp;rsquo;s identifier, used to distinguish different CQs. CQ does not have special reserved numbers like QP0 and QP1, which will not be further elaborated in this article.&lt;/p>
&lt;h2 id="complete-error">
&lt;a href="#complete-error" class="header-anchor">#&lt;/a>
Complete error
&lt;/h2>
&lt;p>There are three types of errors in the IB protocol: immediate error, Completion Error, and Asynchronous Errors.&lt;/p>
&lt;p>Immediate error refers to &amp;ldquo;immediately stop the current operation and return an error to the upper-level user&amp;rdquo;; completion error refers to &amp;ldquo;return the error information to the upper-level user via CQE&amp;rdquo;; whereas asynchronous error refers to &amp;ldquo;report to the upper-level user through an interrupt event.&amp;rdquo; It might still be a bit abstract, so let&amp;rsquo;s give an example to illustrate under what circumstances these two types of errors might occur:&lt;/p>
&lt;ul>
&lt;li>The user passed an illegal opcode when sending a Post Send, for example, trying to use RDMA WRITE operation during UD.&lt;/li>
&lt;/ul>
&lt;p>Result: Immediate error generated (some manufacturers may generate a completion error in this situation)&lt;/p>
&lt;p>Generally, in this situation, the driver will directly exit the post send process and return an error code to the upper-level user. Note that at this point, the WQE has not yet been issued to the hardware before returning.&lt;/p>
&lt;ul>
&lt;li>The user issued a WQE with the operation type SEND, but did not receive an ACK from the other party for a long time.&lt;/li>
&lt;/ul>
&lt;p>Result: Generation completed with error&lt;/p>
&lt;p>Because the WQE has already reached the hardware, the hardware will generate the corresponding CQE, which contains error details of the timeout unresponse.&lt;/p>
&lt;ul>
&lt;li>Multiple WQEs were issued in user mode, so the hardware generated multiple CQEs, but the software did not retrieve the CQEs from the CQ, causing the CQ to overflow.
Result: Generate asynchronous error&lt;/li>
&lt;/ul>
&lt;p>Because the software has not fetched the CQE, it naturally will not obtain information from the CQE. At this time, the IB framework will call the event handler function registered by the software to notify the user to handle the current error.&lt;/p>
&lt;p>From this, it can be seen that they are all ways for the lower layer to report errors to the upper layer users, only the timing of their occurrence is different. In the IB protocol, it is specified which method should be used to report errors in different situations. For example, in the diagram below, for modifying illegal parameters during the Modify QP process, an immediate error should be returned.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_10.webp"
alt="2024-06-27_10_10" width="auto" loading="lazy">
&lt;/figure>
&lt;p>The focus of this text is on CQ, so after introducing the error types, we will take a closer look at completion errors. Completion errors are reported by the hardware through filling error codes in the CQE. A communication process requires the participation of a requester and a responder, and the specific error causes are divided into local and remote. Let&amp;rsquo;s first take a look at the stage at which error detection is performed (the figure below is a redrawn version of Figure 118 in the IB protocol):&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_11.webp"
alt="2024-06-27_10_11" width="auto" loading="lazy">
&lt;/figure>
&lt;p>There are two error detection points for the Requester:&lt;/p>
&lt;ol>
&lt;li>Local error detection&lt;/li>
&lt;/ol>
&lt;p>Check the WQE in the SQ, if an error is detected, directly generate a CQE from the local error checking module to the CQ, and no data will be sent to the responder; if there is no error, send the data to the peer.&lt;/p>
&lt;ol start="2">
&lt;li>Remote Error Detection&lt;/li>
&lt;/ol>
&lt;p>Detect whether the response side&amp;rsquo;s ACK is abnormal. ACK/NAK is generated by the peer&amp;rsquo;s local error detection module after detection, and it contains whether there is an error on the response side and the specific type of error. Regardless of whether there is an issue with the remote error detection result, a CQE will be generated in the CQ.&lt;/p>
&lt;p>Responder&amp;rsquo;s error detection point is only one:&lt;/p>
&lt;ol>
&lt;li>Local error detection&lt;/li>
&lt;/ol>
&lt;p>In fact, what is detected is whether there is an issue with the peer message, which is also referred to as &amp;ldquo;local&amp;rdquo; error detection in the IB protocol. If an error is detected, it will be reflected in the ACK/NAK message sent back to the peer and will generate a CQE locally.&lt;/p>
&lt;p>It should be noted that the generation of ACK and remote error detection mentioned above is only applicable to connection-oriented service types. Connectionless service types, such as UD type, do not care whether the peer receives it, and the receiver will not generate an ACK. Therefore, a CQE will definitely be generated after the local error detection of the Requester, regardless of whether there is a remote error.&lt;/p>
&lt;p>Then we will briefly introduce several common completion errors:&lt;/p>
&lt;ul>
&lt;li>RC service type SQ completion error&lt;/li>
&lt;li>Local Protection Error
&lt;ul>
&lt;li>Local protection domain error. The data memory address specified in the local WQE is invalid for the MR, meaning the user is attempting to use data from an unregistered memory region.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Remote Access Error
&lt;ul>
&lt;li>Remote permission error. The local end does not have permission to read/write the specified remote memory address.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Transport Retry Counter Exceeded Error
&lt;ul>
&lt;li>Retransmission limit exceeded error. The peer has not responded with the correct ACK, causing multiple retransmissions from this end, exceeding the preset number of times.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>RC service type RQ completion error&lt;/li>
&lt;li>Local Access Error
&lt;ul>
&lt;li>Local access error. Indicates that the peer attempted to write to a memory area it does not have permission to write to.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Local Length Error
&lt;ul>
&lt;li>Local length error. The local RQ does not have enough space to receive the data sent by the peer.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>For a complete list of error types, please refer to Section 10.10.3 of the IB protocol.&lt;/p>
&lt;h2 id="user-interface">
&lt;a href="#user-interface" class="header-anchor">#&lt;/a>
User interface
&lt;/h2>
&lt;p>Like QP, we still introduce the interface provided by the IB protocol to the upper layer regarding CQ from the communication preparation phase (control plane) and the communication execution phase (data plane).&lt;/p>
&lt;h3 id="control-surface">
&lt;a href="#control-surface" class="header-anchor">#&lt;/a>
Control surface
&lt;/h3>
&lt;p>Just like QP, there are still the four types of &amp;ldquo;add, delete, modify, and query,&amp;rdquo; but perhaps because for CQ, the upper-layer users are resource users rather than managers, they can only read data from CQ and cannot write data. Therefore, the configurable parameter open to users is only the &amp;ldquo;CQ specification.&amp;rdquo;&lt;/p>
&lt;ul>
&lt;li>Create CQ&lt;/li>
&lt;/ul>
&lt;p>When creating, the user must specify the size of the CQ, i.e., how many CQEs it can store. Additionally, the user can provide a pointer to a callback function that is triggered after a CQE is generated (this will be discussed later). The kernel-mode driver will configure other related parameters and fill them into the CQC, as agreed with the hardware, to inform the hardware.&lt;/p>
&lt;ul>
&lt;li>Destroy CQ&lt;/li>
&lt;/ul>
&lt;p>Release a CQ hardware and software resource, including CQ itself and CQC, and naturally, CQN will also become invalid.&lt;/p>
&lt;ul>
&lt;li>Resize CQ&lt;/li>
&lt;/ul>
&lt;p>The name here is slightly different because CQ only allows users to modify the size of the specifications, so Resize is used instead of Modify.&lt;/p>
&lt;ul>
&lt;li>Query CQ&lt;/li>
&lt;/ul>
&lt;p>Query the current specifications of CQ, as well as the callback function pointer used for notifications.&lt;/p>
&lt;blockquote>
&lt;p>By comparing RDMA specifications and software protocol stacks, it can be found that many verbs interfaces are not implemented according to the specifications. Therefore, if readers find discrepancies between the software API and the protocol, there is no need to be puzzled, as RDMA technology itself is still evolving, and the software framework is in an active state of updates. If you are more concerned with programming implementation, please refer to the API documentation of the software protocol stack; if you are more concerned with academic research, please refer to the RDMA specifications.&lt;/p>
&lt;/blockquote>
&lt;h3 id="data-surface">
&lt;a href="#data-surface" class="header-anchor">#&lt;/a>
Data surface
&lt;/h3>
&lt;p>CQE is the medium through which hardware conveys information to software. Although the software knows under what circumstances a CQE will be generated, it does not know exactly when the hardware will place the CQE into the CQ. In the fields of communication and computing, this mode where the receiver does not know when the sender will send is called &amp;ldquo;asynchronous&amp;rdquo;. Let&amp;rsquo;s first take an example of a network card and then explain how a user can obtain a CQE (WC) through the data plane interface.&lt;/p>
&lt;p>After the network card receives a data packet, how to let the CPU know about this and process the packet, there are two common modes:&lt;/p>
&lt;ul>
&lt;li>Interrupt mode&lt;/li>
&lt;/ul>
&lt;p>When the amount of data is small, or when there are frequent sporadic data exchanges, it is suitable to use the interrupt mode—meaning the CPU is usually doing other tasks, and when the network card receives a data packet, it will report an interrupt to interrupt the current task of the CPU, and the CPU will switch to handle the data packet (such as parsing the various layers of the TCP/IP protocol stack). After processing the data, the CPU jumps back to the task before the interrupt to continue execution.&lt;/p>
&lt;p>Each interrupt requires saving the context, which means saving the current values of various registers, local variables, etc., to the stack, and then restoring the context (popping from the stack) upon return. This itself incurs overhead. If the business load is heavy and the network card is constantly receiving packets, the CPU will continuously receive interrupts, and the CPU will be busy with interrupt switching, causing other tasks to not be scheduled.&lt;/p>
&lt;ul>
&lt;li>Polling mode&lt;/li>
&lt;/ul>
&lt;p>So in addition to interrupt mode, the network card also has a polling mode, where received packets are first placed in the buffer, and the CPU periodically checks whether the network card has received data. If there is data, it takes the data from the buffer for processing; if not, it continues to handle other tasks.&lt;/p>
&lt;p>By comparing interrupt modes, we can find that although the polling mode requires the CPU to check at intervals, which brings some overhead, using polling mode when the business is busy can greatly reduce the number of context switches for interrupts, thereby reducing the CPU&amp;rsquo;s burden.&lt;/p>
&lt;p>The current network cards generally use a combination of interrupt and polling, which dynamically switches based on business load.&lt;/p>
&lt;p>In the RDMA protocol, a CQE is equivalent to a data packet received by the network card, and the RDMA hardware passes it to the CPU for processing. The RDMA framework defines two types of interfaces for the upper layer, namely poll and notify, corresponding to polling and interrupt modes.&lt;/p>
&lt;h3 id="poll-completion-queue">
&lt;a href="#poll-completion-queue" class="header-anchor">#&lt;/a>
Poll completion queue
&lt;/h3>
&lt;p>Very straightforward, poll means polling. After the user calls this interface, the CPU will periodically check if there are fresh CQEs in the CQ. If there are, it will extract this CQE (note that once extracted, the CQE is &amp;ldquo;consumed&amp;rdquo;), parse the information within, and return it to the upper-level user.&lt;/p>
&lt;h3 id="solicitud-de-notificación-de-finalización">
&lt;a href="#solicitud-de-notificaci%c3%b3n-de-finalizaci%c3%b3n" class="header-anchor">#&lt;/a>
Solicitud de notificación de finalización
&lt;/h3>
&lt;p>Literally translated, it is a request completion notification. After the user calls this interface, it is equivalent to registering an interrupt with the system. This way, when the hardware places a CQE into the CQ, it will immediately trigger an interrupt to the CPU. The CPU will then stop its current work to retrieve the CQE, process it, and return it to the user.&lt;/p>
&lt;p>Similarly, which of these two interfaces to use depends on the user&amp;rsquo;s requirements for real-time performance and the actual busyness of the business.&lt;/p>
&lt;p>Thank you for reading, that concludes the introduction to CQ. In the next article, I plan to discuss SRQ in detail.&lt;/p>
&lt;h2 id="relevant-sections-of-the-agreement">
&lt;a href="#relevant-sections-of-the-agreement" class="header-anchor">#&lt;/a>
Relevant sections of the agreement
&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>9.9 CQ Error Detection and Recovery&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.2.6 The relationship between CQ and WQ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.10 Error Types and Their Handling&lt;/p>
&lt;/li>
&lt;li>
&lt;p>11.2.8 CQ Related Control Plane Interface&lt;/p>
&lt;/li>
&lt;li>
&lt;p>11.4.2 CQ related data surface interface&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="other-references">
&lt;a href="#other-references" class="header-anchor">#&lt;/a>
Other references
&lt;/h2>
&lt;p>[1] Linux Kernel Networking - Implement and Theory. Chapter 13. Completion Queue&lt;/p></description></item><item><title>RDMA: Queue Pair</title><link>https://cuterwrite.top/en/p/rdma-queue-pair/</link><pubDate>Tue, 25 Jun 2024 02:21:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/rdma-queue-pair/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116342820_p0_master1200.webp" alt="Featured image of post RDMA: Queue Pair" />&lt;h1 id="queue-pair-of-rdma">
&lt;a href="#queue-pair-of-rdma" class="header-anchor">#&lt;/a>
Queue Pair of RDMA
&lt;/h1>
&lt;p>&lt;strong>This article welcomes non-commercial reposting, please indicate the source.&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>Statement: For collection only, for convenient reading&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/195757767">&lt;cite>Zhihu Column: 9. Basic RDMA Service Types&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;h2 id="queue-pair">
&lt;a href="#queue-pair" class="header-anchor">#&lt;/a>
Queue Pair
&lt;/h2>
&lt;p>We have previously provided a brief introduction to the concept of QP in the article &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-element/" >&amp;ldquo;3. Basic Elements of RDMA&amp;rdquo;
&lt;/a>
. This article will delve deeper into some details about QP.&lt;/p>
&lt;h2 id="review-of-basic-concepts">
&lt;a href="#review-of-basic-concepts" class="header-anchor">#&lt;/a>
Review of Basic Concepts
&lt;/h2>
&lt;p>First, let&amp;rsquo;s briefly review the basic knowledge about QP:&lt;/p>
&lt;p>According to the description in the IB protocol, QP is a virtual interface between hardware and software. QP is a queue structure that sequentially stores tasks (WQE) issued by software to hardware. The WQE contains information such as where to retrieve data, how long the data is, and to which destination it should be sent.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-26_9_1.webp"
alt="2024-06-26_9_1" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Concept of QP&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>Each QP is independent and isolated from each other through PD, so a QP can be regarded as a resource exclusively used by a certain user, and a user can also use multiple QPs simultaneously.&lt;/p>
&lt;p>QP has many types of services, including RC, UD, RD, and UC, etc. All source QPs and destination QPs must be of the same type to interact with each other.&lt;/p>
&lt;p>Although the IB protocol refers to QP as a &amp;ldquo;virtual interface,&amp;rdquo; it is tangible:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>On the hardware side, a QP is a storage space containing several WQEs. The IB network card reads the contents of the WQEs from this space and accesses the memory to store or retrieve data according to the user&amp;rsquo;s expectations. As for whether this storage space is memory space or on-chip storage space of the IB network card, the IB protocol does not impose restrictions, and each manufacturer has its own implementation.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>In software, QP is a data structure maintained by the driver of the IB network card, which contains the address pointer of the QP and some related software attributes.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="qpc">
&lt;a href="#qpc" class="header-anchor">#&lt;/a>
QPC
&lt;/h3>
&lt;p>In the article &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-service-types/" >&amp;ldquo;5. RDMA Basic Service Types&amp;rdquo;
&lt;/a>
, we mentioned that QPC stands for Queue Pair Context, which is used to store properties related to QP. The driver does store the software properties of QP, so if we can store QP properties in software, why do we still use QPC?&lt;/p>
&lt;p>This is because &lt;strong>QPC is mainly for hardware viewing and is also used to synchronize QP information between software and hardware.&lt;/strong>&lt;/p>
&lt;p>We have mentioned that the entity of a QP on hardware is merely a segment of storage space, and the hardware knows nothing beyond the starting address and size of this space, not even the service type of this QP. There is also a lot of other important information, such as a QP containing several WQEs. How does the hardware know how many there are and which one it should currently process?&lt;/p>
&lt;p>All of the above information can be structured into a data structure by the software, and memory space can be allocated for it. However, the software only sees virtual addresses, and these memory spaces are physically discrete; the hardware does not know where this data is stored. Therefore, the software needs to pre-allocate a large contiguous space through the operating system, namely QPC, to present this information to the hardware. The network card and its accompanying driver program have pre-agreed on what content is included in the QPC, how much space each content occupies, and in what order they are stored. This way, the driver and hardware can read and write the status and other information of the QP through this QPC space.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-26_9_2_QPC.webp"
alt="2024-06-26_9_2" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>The concept of QPC&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>As shown in the figure above, the hardware actually only needs to know the address 0x12350000 of the QPC, because it can parse the contents of the QPC to determine the position of the QP, the QP sequence number, the QP size, and other information. Consequently, it can locate the QP and determine which WQE to process. Different manufacturers may have some variations in implementation, but the general principle is like this.&lt;/p>
&lt;p>There are many Context concepts in the IB software stack, in addition to QPC, there are also Device Context, SRQC, CQC, EQC (Event Queue Context), etc. Their functions are similar to QPC, all used to record and synchronize the related attributes of certain resources.&lt;/p>
&lt;h3 id="qp-number">
&lt;a href="#qp-number" class="header-anchor">#&lt;/a>
QP Number
&lt;/h3>
&lt;p>Referred to as QPN, which is the number of each QP. The IB protocol specifies using $2^{24}$ bits to represent QPN, meaning each node can simultaneously use up to $2^{24}$ QPs, which is already a very large number and almost impossible to exhaust. Each node maintains its own set of QPNs independently, meaning QPs with the same number can exist on different nodes.&lt;/p>
&lt;p>The concept of QPN itself is very simple, but there are two special reserved numbers that require extra attention:&lt;/p>
&lt;h4 id="qp0">
&lt;a href="#qp0" class="header-anchor">#&lt;/a>
QP0
&lt;/h4>
&lt;p>QP with ID 0 is used for the Subnet Management Interface (SMI), which is used to manage all nodes in the subnet. To be honest, I haven&amp;rsquo;t figured out the purpose of this interface yet, so let&amp;rsquo;s put it aside for now.&lt;/p>
&lt;h4 id="qp1">
&lt;a href="#qp1" class="header-anchor">#&lt;/a>
QP1
&lt;/h4>
&lt;p>QP numbered 1 is used for the General Service Interface (GSI), which is a set of management services, the most well-known of which is CM (Communication Management). It is a method used to exchange necessary information before formally establishing a connection between the communication nodes. Its details will be elaborated in a later article.&lt;/p>
&lt;p>This is the reason why QP0 and QP1 did not appear in the diagram about QP in our previous article. All other QPs besides these two are regular QPs. When a user creates a QP, the driver or hardware will assign a QPN to this new QP, and generally, QPNs are assigned sequentially like 2, 3, 4. After a QP is destroyed, its QPN will be reclaimed and allocated to other newly created QPs at an appropriate time.&lt;/p>
&lt;h2 id="user-interface">
&lt;a href="#user-interface" class="header-anchor">#&lt;/a>
User interface
&lt;/h2>
&lt;p>We classify and introduce user interfaces from the control plane and data plane perspectives. The control plane refers to the user&amp;rsquo;s configuration of a certain resource, which is generally done before the actual data transmission; whereas the data plane naturally involves operations during the actual data transmission process.&lt;/p>
&lt;h3 id="control-surface">
&lt;a href="#control-surface" class="header-anchor">#&lt;/a>
Control surface
&lt;/h3>
&lt;p>Readers who have encountered algorithms should all understand that the nodes of a linked list involve four operations: &amp;ldquo;add, delete, modify, and search.&amp;rdquo; The nodes of a linked list are a memory area and a type of software resource.&lt;/p>
&lt;p>&amp;ldquo;Increase&amp;rdquo; means requesting a piece of memory from the operating system to store data. The system will allocate a space in memory and mark it as &amp;ldquo;in use by process XX,&amp;rdquo; and other unauthorized processes will not be able to overwrite or even read this memory space.&lt;/p>
&lt;p>&amp;ldquo;Delete&amp;rdquo; means notifying the operating system that I am no longer using this space, and it can be marked as &amp;ldquo;unused&amp;rdquo; and made available for other processes to use.&lt;/p>
&lt;p>&amp;ldquo;Modify&amp;rdquo; means to write, i.e., to change the contents of this memory area.&lt;/p>
&lt;p>&amp;ldquo;Query&amp;rdquo; means read, that is, to obtain the content of this memory area.&lt;/p>
&lt;p>QP, as one of the most important resources in RDMA technology, is no different from a linked list in its lifecycle:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Operation&lt;/th>
&lt;th style="text-align: left">Linked List Node&lt;/th>
&lt;th style="text-align: left">QP&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">Increase&lt;/td>
&lt;td style="text-align: left">struct ListNode *node = malloc(sizeof(struct ListNode *));&lt;/td>
&lt;td style="text-align: left">Create QP&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Delete&lt;/td>
&lt;td style="text-align: left">free(node);&lt;/td>
&lt;td style="text-align: left">Destroy QP&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Modify&lt;/td>
&lt;td style="text-align: left">node-&amp;gt;val = xxx;&lt;/td>
&lt;td style="text-align: left">Modify QP&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Check&lt;/td>
&lt;td style="text-align: left">xxx = node-&amp;gt;val;&lt;/td>
&lt;td style="text-align: left">Query QP&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>These four operations are actually the Verbs (RDMA&amp;rsquo;s API for upper-layer applications) that provide several interfaces to upper-layer users on the control plane:&lt;/p>
&lt;h4 id="create-qp">
&lt;a href="#create-qp" class="header-anchor">#&lt;/a>
Create QP
&lt;/h4>
&lt;p>Create a QP&amp;rsquo;s hardware and software resources, including the QP itself and the QPC. When the user creates it, they will input a series of initialization attributes, including the service type of the QP, the number of WQEs that can be stored, and other information.&lt;/p>
&lt;h4 id="destroy-qp">
&lt;a href="#destroy-qp" class="header-anchor">#&lt;/a>
Destroy QP
&lt;/h4>
&lt;p>Release all software and hardware resources of a QP, including the QP itself and the QPC. After destroying the QP, the user will no longer be able to index this QP through QPN.&lt;/p>
&lt;h4 id="modify-qp">
&lt;a href="#modify-qp" class="header-anchor">#&lt;/a>
Modify QP
&lt;/h4>
&lt;p>Modify certain attributes of a QP, such as the state of the QP, the MTU of the path, etc. This modification process includes both the modification of software data structures and the modification of the QPC.&lt;/p>
&lt;h4 id="query-qp">
&lt;a href="#query-qp" class="header-anchor">#&lt;/a>
Query QP
&lt;/h4>
&lt;p>Query the current status and some attributes of a QP. The data queried comes from the driver and the content of the QPC.&lt;/p>
&lt;p>These four operations all have corresponding Verbs interfaces, similar to &lt;code>ibv_create_qp()&lt;/code> form, which we can directly call when writing the APP. More details about the upper-level API will be introduced later.&lt;/p>
&lt;h2 id="data-surface">
&lt;a href="#data-surface" class="header-anchor">#&lt;/a>
Data surface
&lt;/h2>
&lt;p>In terms of data, a QP actually has only two interfaces to the upper layer, used to fill in send and receive requests in the QP. &lt;strong>Here, &amp;ldquo;send&amp;rdquo; and &amp;ldquo;receive&amp;rdquo; do not refer to sending and receiving data, but rather the &amp;ldquo;initiator&amp;rdquo; (Requestor) and &amp;ldquo;responder&amp;rdquo; (Responser) in a communication process.&lt;/strong>&lt;/p>
&lt;p>In behavior, the software fills a WQE (called WR at the application layer) into the QP, requesting the hardware to perform an action. Therefore, both behaviors are called &amp;ldquo;Post XXX Request,&amp;rdquo; meaning issuing an XXX request.&lt;/p>
&lt;h3 id="send-request">
&lt;a href="#send-request" class="header-anchor">#&lt;/a>
Send Request
&lt;/h3>
&lt;p>To emphasize again, Post Send itself does not mean that the operation type of this WQE is Send, but indicates that this WQE belongs to the initiator of the communication. The WQE/WR filled into the QP in this process can be a Send operation, RDMA Write operation, or RDMA Read operation, etc.&lt;/p>
&lt;p>The user needs to prepare the data buffer, destination address, and other information in advance, then call the interface to pass the WR to the driver, and the driver will fill the WQE into the QP.&lt;/p>
&lt;h3 id="post-receive-request">
&lt;a href="#post-receive-request" class="header-anchor">#&lt;/a>
Post Receive Request
&lt;/h3>
&lt;p>The usage scenarios for Post Recv are relatively fewer, generally only executed on the receiving end of the Send-Recv operation. The receiving end needs to prepare the buffer for receiving data in advance and inform the hardware of the buffer address and other information in the form of a WQE.&lt;/p>
&lt;h2 id="qp-state-machine">
&lt;a href="#qp-state-machine" class="header-anchor">#&lt;/a>
QP state machine
&lt;/h2>
&lt;p>Speaking of the state of QP, we have to bring out the following image (taken from section 10.3.1 of the IB protocol):&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-26_9_3.webp"
alt="2024-06-26_9_3" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>QP State Machine&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>The so-called state machine describes the different states of an object and the conditions that trigger transitions between states. Designing a state machine for an object can make the lifecycle of this object very clear, and in implementation, it will also make the logic more coherent.&lt;/p>
&lt;p>For QP, the IB specification also designs several states for it. The functions of a QP in different states vary. For example, only after entering the Ready to Send state can the QP perform Post Send data operations. State transitions between normal states (in green) are actively triggered by the user through the Modify QP user interface introduced above; whereas error states (in red) often automatically transition after an error occurs. When a QP is in an error state, it cannot perform normal operations and needs to be reconfigured to a normal state by the upper layer through Modify QP.&lt;/p>
&lt;p>In the above diagram, we only focus on the part of QP. EE (End-to-End Context) is a concept specifically used for RD service types, which we will not cover for now. We enter this state diagram through the Create QP interface and exit this state diagram through the Destroy QP interface.&lt;/p>
&lt;p>QP has the following states, we will only introduce some important points:&lt;/p>
&lt;h3 id="rst-reset">
&lt;a href="#rst-reset" class="header-anchor">#&lt;/a>
RST (Reset)
&lt;/h3>
&lt;p>Reset state. When a QP is created through Create QP, it is in this state. The related resources have already been allocated, but this QP cannot do anything at the moment. It cannot receive WQEs issued by the user, nor can it receive messages from a QP on the peer end.&lt;/p>
&lt;h3 id="initinitialized">
&lt;a href="#initinitialized" class="header-anchor">#&lt;/a>
INIT（Initialized）
&lt;/h3>
&lt;p>Initialized state. In this state, the user can issue Receive WR to this QP via Post Receive, but the received messages will not be processed and will be silently discarded; if the user issues a Post Send WR, an error will occur.&lt;/p>
&lt;h3 id="rtrready-to-receive">
&lt;a href="#rtrready-to-receive" class="header-anchor">#&lt;/a>
RTR（Ready to Receive）
&lt;/h3>
&lt;p>Ready to receive status. Based on the INIT state, RQ can function normally, meaning it can move data to the specified memory location according to the instructions in the received message&amp;rsquo;s WQE. In this state, SQ still cannot function.&lt;/p>
&lt;h3 id="rts-ready-to-send">
&lt;a href="#rts-ready-to-send" class="header-anchor">#&lt;/a>
RTS (Ready to Send)
&lt;/h3>
&lt;p>Ready to send status. Based on RTR, SQ can work normally, meaning the user can perform Post Send, and the hardware will also send the data according to the content of SQ. Before entering this state, QP must have already established a connection with the peer.&lt;/p>
&lt;h3 id="sqd-send-queue-drain">
&lt;a href="#sqd-send-queue-drain" class="header-anchor">#&lt;/a>
SQD (Send Queue Drain)
&lt;/h3>
&lt;p>SQ emptying state. As the name suggests, this state will process all the existing unprocessed WQEs in the SQ queue. At this time, the user can still submit new WQEs, but these WQEs will be processed only after all the old WQEs have been processed.&lt;/p>
&lt;h3 id="sqer-send-queue-error">
&lt;a href="#sqer-send-queue-error" class="header-anchor">#&lt;/a>
SQEr (Send Queue Error)
&lt;/h3>
&lt;p>SQ error state. When a Send WR encounters a completion error (i.e., an error reported to the driver by the hardware through CQE), it causes the QP to enter this state.&lt;/p>
&lt;h3 id="err-error">
&lt;a href="#err-error" class="header-anchor">#&lt;/a>
ERR (Error)
&lt;/h3>
&lt;p>Error state. If an error occurs in other states, they may enter this state. In the Error state, the QP will stop processing WQE, and any WQE that is halfway processed will also stop. The upper layer needs to switch the QP back to the initial RST state after fixing the error.&lt;/p>
&lt;h2 id="summary">
&lt;a href="#summary" class="header-anchor">#&lt;/a>
Summary
&lt;/h2>
&lt;p>This article first reviews some important basic concepts of QP, then explains QPC, QPN, and other concepts closely related to QP, and finally introduces the interfaces commonly used by users to operate QP and the QP state machine. I believe that after reading this article, readers will have a deeper understanding of QP.&lt;/p>
&lt;p>In fact, as a core concept of RDMA, there is a lot of content regarding QP, and this article cannot cover everything. I will gradually complete the related content in future articles. For example, the concept of QKey will be explained in detail in subsequent articles dedicated to various Keys.&lt;/p>
&lt;p>Alright, this is the end of the article. Thank you for reading. A preview of the next article will provide a detailed explanation of CQ.&lt;/p>
&lt;h2 id="relevant-sections-of-the-agreement">
&lt;a href="#relevant-sections-of-the-agreement" class="header-anchor">#&lt;/a>
Relevant sections of the agreement
&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>3.5.1 10.2.4 Basic Concepts of QP&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.3 QP State Machine&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.2.5 Software interfaces related to QP&lt;/p>
&lt;/li>
&lt;li>
&lt;p>11.4 Post Send Post Recv&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>RDMA: Address Handle</title><link>https://cuterwrite.top/en/p/rdma-address-handle/</link><pubDate>Sat, 15 Jun 2024 01:00:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/rdma-address-handle/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116373922_p9_master1200.webp" alt="Featured image of post RDMA: Address Handle" />&lt;h1 id="rdmas-address-handle">
&lt;a href="#rdmas-address-handle" class="header-anchor">#&lt;/a>
RDMA&amp;rsquo;s Address Handle
&lt;/h1>
&lt;p>&lt;strong>This article welcomes non-commercial reprints, please indicate the source when reprinting.&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>Statement: For collection only, for easy reading&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/163552044">&lt;cite>Zhihu Column: 8. RDMA Address Handle&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>It has already been introduced that the basic unit of RDMA communication is the QP. Let&amp;rsquo;s consider a question: Suppose a QP on node A wants to exchange information with a QP on node B. Besides knowing the QP number—QPN—of node B, what other information is needed? It should be noted that the QPN is a number maintained independently by each node and is not unique across the entire network. For instance, if QP 3 on A wants to communicate with QP 5 on B, there is not just one QP5 in the network; many nodes might have their own QP 5. Therefore, we can naturally think of the need to find a way for each node to have a unique identifier.&lt;/p>
&lt;p>In the traditional TCP-IP protocol stack, the well-known IP address is used to identify each node at the network layer. In the IB protocol, this identifier is called the &lt;strong>GID (Global Identifier)&lt;/strong>, which is a 128-bit sequence. This article will not discuss GID in detail and will introduce it later.&lt;/p>
&lt;h2 id="what-is-ah">
&lt;a href="#what-is-ah" class="header-anchor">#&lt;/a>
What is AH
&lt;/h2>
&lt;p>AH stands for Address Handle. I couldn&amp;rsquo;t think of a particularly suitable Chinese translation, so let&amp;rsquo;s directly translate it as &amp;ldquo;地址句柄&amp;rdquo; for now. The address here refers to a set of information used to locate a remote node. In the IB protocol, the address refers to information such as GID, port number, etc. As for the so-called handle, we can understand it as a pointer to an object.&lt;/p>
&lt;p>Does everyone still remember that there are four basic service types in the IB protocol—RC, UD, RD, and UC, with RC and UD being the most commonly used. The characteristic of RC is that a reliable connection is established between the QPs of two nodes, and once the connection is established, it is not easily changed. The information of the peer is stored in the QP Context when creating the QP.&lt;/p>
&lt;p>As for UD, there is no connection relationship between QPs. The user can fill in the peer&amp;rsquo;s address information in the WQE for whoever they want to send to. &lt;strong>The user does not directly fill the peer&amp;rsquo;s address information into the WQE but prepares an &amp;ldquo;address book&amp;rdquo; in advance, specifying the peer node&amp;rsquo;s address information each time through an index, which is the AH.&lt;/strong>&lt;/p>
&lt;p>The concept of AH can be roughly represented by the diagram below:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_8_1.webp"
alt="2024-06-16_8_1" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Address Handle Function Diagram&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>For each destination node, a corresponding AH will be created at this end, and the same AH can be shared by multiple QPs.&lt;/p>
&lt;h2 id="the-role-of-ah">
&lt;a href="#the-role-of-ah" class="header-anchor">#&lt;/a>
The role of AH
&lt;/h2>
&lt;p>Before each communication of the UD service type, the user needs to first use the interface provided by the IB framework to &lt;strong>create an AH for each possible peer node&lt;/strong>, then these AHs are driven into a &amp;ldquo;secure&amp;rdquo; area, and an index (pointer/handle) is returned to the user. When the user actually issues a WR (Work Request), they just need to pass in this index.&lt;/p>
&lt;p>The above process is shown in the diagram below. Node A receives a task from the user to exchange data using its QP4 with Node B&amp;rsquo;s QP3 (specified through AH):&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_8_2.webp"
alt="2024-06-16_8_2" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>UD service type uses AH to specify peer node&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>The IB protocol does not explain why AH is used. I believe there are three reasons for defining the concept of AH:&lt;/p>
&lt;ol>
&lt;li>Ensure the destination address is available, improve efficiency&lt;/li>
&lt;/ol>
&lt;p>Due to the connectionless nature of UD, users can directly specify the destination through WR in user mode. However, if users are allowed to fill in address information at will, and then hardware packages the data according to this information, it may lead to problems. For example, there is a scenario like this: a user tells the hardware through WR to send data to port Z of a node with GID X and MAC address Y. However, X, Y, and Z may not be a valid combination, or a node with GID X may not even exist in the network, and the hardware is unable to verify this content, so it can only obediently package and send the data. This results in the unnecessary sending of a data packet to an invalid destination.&lt;/p>
&lt;p>And preparing the address information in advance can avoid the above situation. When the user creates AH, they will enter kernel mode. If the parameters passed by the user are valid, the kernel will store this destination node information, generate a pointer, and return it to the user; if the parameters passed by the user are invalid, AH creation will fail. This process ensures that the address information is valid. The user can quickly specify the destination node through the pointer, speeding up the data interaction process.&lt;/p>
&lt;p>Some may ask, since the kernel is trusted, why not switch to kernel mode to verify the address information passed by the user when sending data? Please do not forget where one of the major advantages of RDMA technology lies—the data flow can go directly from user space to hardware, completely bypassing the kernel, thus avoiding the overhead of system calls and copying. If the legality of the address has to be checked every time data is sent, it will inevitably reduce the communication rate.&lt;/p>
&lt;ol start="2">
&lt;li>Hide underlying address details from the user&lt;/li>
&lt;/ol>
&lt;p>When the user creates AH, they only need to provide information such as gid, port number, static rate, etc., while other address information required for communication (mainly MAC addresses) is resolved by the kernel driver through querying the system neighbor table and other methods. There is no need to expose this additional information to the user layer.&lt;/p>
&lt;ol start="3">
&lt;li>You can use PD to manage the destination address.&lt;/li>
&lt;/ol>
&lt;p>In the previous text, when we introduced protection domains, we mentioned that besides QP and MR, AH is also resource partitioned by PD. Once the software entity AH is defined, we can isolate and manage all destinations reachable by QP.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_8_3.webp"
alt="2024-06-16_8_3" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Use PD to isolate AH&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>For example, in the image above, AH1~3 can only be used by QP3 and QP9 under the same PD, while AH4 can only be used by QP5.&lt;/p>
&lt;h2 id="relevant-sections-of-the-agreement">
&lt;a href="#relevant-sections-of-the-agreement" class="header-anchor">#&lt;/a>
Relevant sections of the agreement
&lt;/h2>
&lt;p>There is not much coverage about AH in the agreement, and there is not even a chapter dedicated to introducing its concept:&lt;/p>
&lt;p>[1] 9.8.3 What components make up the destination address in UD service type: including AH, QPN, and Q_key&lt;/p>
&lt;p>[2] 10.2.2.2 Relevant Considerations for the Destination Address&lt;/p>
&lt;p>[3] 11.2.2.1 AH Related Verbs Interface&lt;/p>
&lt;p>AH is introduced here, thank you for reading. In the next article, I plan to describe more details about QP.&lt;/p></description></item><item><title>RDMA: Protection Domain</title><link>https://cuterwrite.top/en/p/rdma-protection-domain/</link><pubDate>Thu, 18 Apr 2024 21:42:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/rdma-protection-domain/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/d31a474af07682028ca085f871bc5d07195413-2024-04-19.webp" alt="Featured image of post RDMA: Protection Domain" />&lt;h1 id="protection-domain-in-rdma">
&lt;a href="#protection-domain-in-rdma" class="header-anchor">#&lt;/a>
Protection Domain in RDMA
&lt;/h1>
&lt;p>&lt;strong>This article welcomes non-commercial reproduction, please indicate the source when reproducing.&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>Statement: For collection only, for easy reading&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/159493100">&lt;cite>Zhihu Column: 7. RDMA and Protection Domain&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>In the previous text, we briefly introduced some of the most common resources in RDMA, including various Queues and the concept of MR, among others. MR is used to control and manage HCA&amp;rsquo;s access rights to local and remote memory, ensuring that the HCA can only read and write to memory regions that the user has registered after obtaining the correct Key. To better ensure security, the IB protocol also introduced the concept of Protection Domain (PD) to ensure mutual isolation among RDMA resources. This article will introduce the concept of PD.&lt;/p>
&lt;h2 id="what-is-pd">
&lt;a href="#what-is-pd" class="header-anchor">#&lt;/a>
What is PD?
&lt;/h2>
&lt;p>PD stands for Protection Domain. The concept of a domain is often seen, from mathematical &amp;ldquo;real number fields&amp;rdquo; and &amp;ldquo;complex number fields&amp;rdquo; to geographical &amp;ldquo;airspace&amp;rdquo; and &amp;ldquo;sea area,&amp;rdquo; representing a space/range. In RDMA, a PD is like a &amp;ldquo;container&amp;rdquo; that holds various resources (QP, MR, etc.), bringing these resources under its protection to prevent unauthorized access. Multiple protection domains can be defined within a node, and the resources contained within each PD are isolated from each other and cannot be used together.&lt;/p>
&lt;p>The concept is still somewhat abstract, let&amp;rsquo;s take a look at what role PD plays and what specific problems it solves.&lt;/p>
&lt;h2 id="the-function-of-pd">
&lt;a href="#the-function-of-pd" class="header-anchor">#&lt;/a>
The function of PD
&lt;/h2>
&lt;p>A user may create multiple QPs and multiple MRs, and each QP may establish connections with different remote QPs, as shown in the diagram below (gray arrows indicate the connection relationships between QPs):&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/7_1-2024-04-19.webp"
alt="7_1-2024-04-19" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Figure 1: RDMA Resources Without PD Concept&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>Since there is no binding relationship between MR and QP, this means that once a remote QP establishes a connection with a local QP and the conditions for communication are met, theoretically, as long as the remote node knows the VA and R_key (it can even keep guessing until it gets a valid pair of values), it can access the contents of an MR on the local node.&lt;/p>
&lt;p>In general, the virtual address VA of MR and the key R_Key are difficult to guess, which already ensures a certain level of security. However, to better protect the data in memory and further isolate and divide the permissions of various resources, we have defined PD in each node, as shown in the diagram below.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/7_2-2024-04-19.webp"
alt="7_2-2024-04-19" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Figure 2: RDMA resources when adding the PD concept&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>In the diagram, Node 0 has two PDs, with 3 QPs and 2 MRs divided into two groups. Additionally, Node 1 and Node 2 each have a PD containing all QPs and MRs. The resources in the two PDs on Node 0 cannot be used together, meaning QP3 and QP9 cannot access the data of MR1, and QP6 cannot access the data of MR0. If we specify the hardware to use QP3 and MR1 during data transmission, the hardware will verify that they do not belong to the same PD and return an error.&lt;/p>
&lt;p>For the remote node, Node1 can only access Node0&amp;rsquo;s memory through QP3 connected via QP8. However, because Node0&amp;rsquo;s QP3 is &amp;ldquo;encircled&amp;rdquo; in the protection domain PD0, Node1&amp;rsquo;s QP8 can only access the memory corresponding to MR0, &lt;strong>and cannot access the data in MR1 under any circumstances&lt;/strong>. This is restricted from two aspects:&lt;/p>
&lt;ol>
&lt;li>Node 1&amp;rsquo;s QP8 is only connected to Node 0&amp;rsquo;s QP3, and cannot access memory through Node 0&amp;rsquo;s QP6.&lt;/li>
&lt;li>MR1 and QP3 of Node 0 belong to different PDs, so even if QP8 of Node 1 obtains the VA and R_key of MR1, the hardware will refuse to provide service due to different PDs.&lt;/li>
&lt;/ol>
&lt;p>As mentioned at the beginning of this article, a PD is like a container that protects some RDMA resources, isolating them from each other to enhance security. In fact, RDMA includes not only resources like QP and MR, but also Address Handle, Memory Window, etc., which are also isolated and protected by PD, as will be introduced later.&lt;/p>
&lt;h2 id="how-to-use-pd">
&lt;a href="#how-to-use-pd" class="header-anchor">#&lt;/a>
How to use PD
&lt;/h2>
&lt;p>Still looking at the above diagram, we notice that Node 0 has two PDs for resource isolation, whereas Node 1 and Node 2 have only one PD containing all the resources.&lt;/p>
&lt;p>The reason I draw it this way is to illustrate that the number of PDs divided on a node is entirely up to the user. &lt;strong>If you want to enhance security, then each QP connected to a remote node and the MR provided for remote access should be isolated as much as possible by dividing PDs. If higher security is not a concern, then creating one PD that encompasses all resources is also acceptable.&lt;/strong>&lt;/p>
&lt;p>The IB protocol specifies: &lt;strong>Each node must have at least one PD, each QP must belong to a PD, and each MR must also belong to a PD&lt;/strong>.&lt;/p>
&lt;p>So how is the inclusion relationship of PD reflected in the software? It itself has a software entity (structure) that records some information about this protection domain. Before users create resources like QP and MR, they must first create a PD through the interface of the IB framework to get its pointer/handle. Then, when creating QP and MR, this PD&amp;rsquo;s pointer/handle needs to be passed in, and PD information will be included in QP and MR. When the hardware sends and receives packets, it will verify the PD of QP and MR. I will introduce more about the software protocol stack in later articles.&lt;/p>
&lt;p>Additionally, it is important to emphasize that &lt;strong>PD is a local concept and only exists within the node&lt;/strong>, it is not visible to other nodes; whereas MR is visible to both the local and remote ends.&lt;/p>
&lt;p>For everyone&amp;rsquo;s convenience in referencing and learning, I will list the protocol sections involved in the articles. I will also supplement the previous content when I have time.&lt;/p>
&lt;h2 id="pd-related-protocol-chapter">
&lt;a href="#pd-related-protocol-chapter" class="header-anchor">#&lt;/a>
PD related protocol chapter
&lt;/h2>
&lt;ul>
&lt;li>Basic concepts and functions of 3.5.5 PD&lt;/li>
&lt;li>10.2.3 introduces the relationship between PD and some other RDMA resources, as well as the software interfaces related to PD.&lt;/li>
&lt;li>10.6.3.5 Emphasize again the relationship between PD, MR, and QP.&lt;/li>
&lt;li>11.2.1.5 Detailed introduction to the Verbs interface of PD, including functions, input parameters, output parameters, and return values, etc.&lt;/li>
&lt;/ul>
&lt;p>Alright, that concludes the introduction to PD. In the following text, I will introduce the concept of Address Handle used for UD service types.&lt;/p></description></item><item><title>RDMA: Memory Region</title><link>https://cuterwrite.top/en/p/rdma-mr/</link><pubDate>Wed, 03 Apr 2024 16:17:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/rdma-mr/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/8fa232626b76940fddc8cc52a49c49e9195413-2024-04-04.webp" alt="Featured image of post RDMA: Memory Region" />&lt;h1 id="memory-region-of-rdma">
&lt;a href="#memory-region-of-rdma" class="header-anchor">#&lt;/a>
Memory Region of RDMA
&lt;/h1>
&lt;p>&lt;strong>This article welcomes non-commercial reproduction, please indicate the source.&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>Statement: For collection only, for easy reading&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/156975042">&lt;cite>Zhihu Column: 6. RDMA Memory Region&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>We assume a scenario and also take the opportunity to review the RDMA WRITE operation process:&lt;/p>
&lt;p>As shown in the figure below, Node A wants to write a piece of data into Node B&amp;rsquo;s memory via the IB protocol. The upper-layer application issues a WQE to the RDMA network card of the local node. The WQE contains information such as source memory address, destination memory address, data length, and key. Then the hardware retrieves the data from memory, packages it, and sends it to the remote network card. After Node B&amp;rsquo;s network card receives the data, it parses the destination memory address and writes the data into the local node&amp;rsquo;s memory.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/6_1-2024-04-04.webp"
alt="6_1-2024-04-04" width="auto" loading="lazy">
&lt;/figure>
&lt;p>So the question arises, the addresses provided by the APP are all virtual addresses (Virtual Address, referred to as VA below), which need to be converted by the MMU to obtain the real physical address (Physical Address, referred to as PA below). &lt;strong>How does our RDMA network card obtain the PA to fetch data from memory&lt;/strong>? Even if the network card knows where to fetch the data, &lt;strong>if a user maliciously specifies an illegal VA, wouldn&amp;rsquo;t the network card possibly be &amp;ldquo;instructed&amp;rdquo; to read and write critical memory&lt;/strong>?&lt;/p>
&lt;p>To solve the above problem, the IB protocol proposed the concept of MR.&lt;/p>
&lt;h2 id="what-is-mr">
&lt;a href="#what-is-mr" class="header-anchor">#&lt;/a>
What is MR
&lt;/h2>
&lt;p>MR stands for Memory Region, which refers to a region designated by the RDMA software layer in memory for storing transmitted and received data. In the IB protocol, after a user requests a memory region for storing data, they must register the MR by calling the API provided by the IB framework to allow the RDMA network card to access this memory region. As can be seen from the diagram below, MR is just a special piece of memory:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/6_2-2024-04-04.webp"
alt="6_2-2024-04-04" width="auto" loading="lazy">
&lt;/figure>
&lt;p>When describing the IB protocol, we usually refer to the RDMA hardware as &lt;strong>HCA (Host Channel Adapter)&lt;/strong>. The IB protocol defines it as &amp;ldquo;an IB device in processors and I/O units capable of generating and consuming packets.&amp;rdquo; To remain consistent with the protocol, we will refer to the hardware part as HCA in this and subsequent articles.&lt;/p>
&lt;h2 id="why-register-mr">
&lt;a href="#why-register-mr" class="header-anchor">#&lt;/a>
Why register MR
&lt;/h2>
&lt;p>Let&amp;rsquo;s take a look at how MR addresses the two questions raised at the beginning of this article:&lt;/p>
&lt;h3 id="1-register-mr-to-achieve-virtual-to-physical-address-translation">
&lt;a href="#1-register-mr-to-achieve-virtual-to-physical-address-translation" class="header-anchor">#&lt;/a>
1. Register MR to achieve virtual-to-physical address translation
&lt;/h3>
&lt;p>We all know that an APP can only see virtual addresses, and it will directly pass the VA to the HCA in the WQE (including both the source VA on the local end and the destination VA on the remote end). Modern CPUs have the &amp;ldquo;tool&amp;rdquo; of MMU and page tables to perform the conversion between VA and PA, while the HCA is either directly connected to the bus or connected to the bus after address translation through IOMMU/SMMU. It cannot &amp;ldquo;understand&amp;rdquo; the real physical memory address corresponding to the VA provided by the APP.&lt;/p>
&lt;p>So during the process of registering MR, the hardware will create and fill a VA to PA mapping table in memory, so that when needed, VA can be converted to PA by looking up the table. Let&amp;rsquo;s provide a specific example to explain this process:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/6_3-2024-04-04.webp"
alt="6_3-2024-04-04" width="auto" loading="lazy">
&lt;/figure>
&lt;p>Now assume that the node on the left initiates an RDMA WRITE operation to the node on the right, directly writing data into the memory area of the right node. Assume that both ends in the diagram have already completed the registration of MR, which corresponds to the &amp;ldquo;data Buffer&amp;rdquo; in the diagram, and have also created the VA-&amp;gt;PA mapping table.&lt;/p>
&lt;ul>
&lt;li>First, this end&amp;rsquo;s APP will issue a WQE to the HCA, informing the HCA of the virtual address of the local buffer used to store the data to be sent, as well as the virtual address of the peer data buffer that will be written to.&lt;/li>
&lt;li>This end HCA queries the VA-&amp;gt;PA mapping table to obtain the physical address of the data to be sent, then retrieves the data from memory, assembles the data packet, and sends it out.&lt;/li>
&lt;li>The remote HCA received the packet and parsed the destination VA from it.&lt;/li>
&lt;li>The peer HCA uses the VA-&amp;gt;PA mapping table stored in local memory to find the real physical address, verifies the permissions, and then stores the data in memory.&lt;/li>
&lt;/ul>
&lt;p>Emphasize once again, for the right-side node, &lt;strong>whether it&amp;rsquo;s address translation or writing to memory, it does not require any involvement of its CPU&lt;/strong>.&lt;/p>
&lt;h3 id="2-mr-can-control-hcas-access-to-memory-permissions">
&lt;a href="#2-mr-can-control-hcas-access-to-memory-permissions" class="header-anchor">#&lt;/a>
2. MR can control HCA&amp;rsquo;s access to memory permissions
&lt;/h3>
&lt;p>Because the memory address accessed by the HCA comes from the user, if the user provides an illegal address (such as system memory or memory used by another process), HCA reading or writing to it may cause information leakage or memory corruption. Therefore, we need a mechanism to ensure that HCA can only access authorized and safe memory addresses. In the IB protocol, during the preparation stage for data interaction, the APP needs to perform the action of registering MR.&lt;/p>
&lt;p>When a user registers MR, two keys are generated—L_KEY (Local Key) and R_KEY (Remote Key). Although they are called keys, their entities are actually just a sequence. They will be used to ensure access permissions for the local and remote memory regions, respectively. The following two diagrams are schematic representations describing the functions of L_Key and R_Key:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/6_4-2024-04-04.webp"
alt="6_4-2024-04-04" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>L_Key&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/6_5-2024-04-04.webp"
alt="6_5-2024-04-04" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>R_Key&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>Here, everyone might have a question: how does this end know the available VA and the corresponding R_Key of the peer node? In fact, before the actual RDMA communication, both nodes establish a link through some means (it could be a Socket connection or a CM connection) and exchange some necessary information for RDMA communication (VA, Key, QPN, etc.) through this link. We call this process &amp;ldquo;link establishment&amp;rdquo; and &amp;ldquo;handshake.&amp;rdquo; I will introduce this in detail in the following articles.&lt;/p>
&lt;p>In addition to the two points above, registering MR has another important function:&lt;/p>
&lt;h3 id="3-mr-can-avoid-page-swapping">
&lt;a href="#3-mr-can-avoid-page-swapping" class="header-anchor">#&lt;/a>
3. MR can avoid page swapping
&lt;/h3>
&lt;p>Because physical memory is limited, the operating system uses a paging mechanism to temporarily save the unused memory contents of a process to the hard drive. When the process needs to use it, a page fault interrupt is used to move the contents from the hard drive back to memory, and this process almost inevitably causes the VA-PA mapping relationship to change.&lt;/p>
&lt;p>Since HCA often bypasses the CPU to read and write to the physical memory areas pointed to by the VA provided by the user, if the VA-PA mapping relationship changes, then the VA-&amp;gt;PA mapping table mentioned earlier will lose its significance, and HCA will be unable to find the correct physical address.&lt;/p>
&lt;p>In order to prevent the VA-PA mapping relationship from changing due to page swapping, the memory is &amp;ldquo;Pinned&amp;rdquo; when registering MR (also known as &amp;ldquo;page locking&amp;rdquo;), which means locking the VA-PA mapping relationship. In other words, this MR memory area will remain in physical memory without being swapped out until the communication is completed, and the user actively deregisters this MR.&lt;/p>
&lt;p>Alright, we have now finished introducing the concept and function of MR. In the next article, I will introduce the concept of PD (Protection Domain).&lt;/p>
&lt;h2 id="code-example">
&lt;a href="#code-example" class="header-anchor">#&lt;/a>
Code example
&lt;/h2>
&lt;p>Below is a simple RDMA program demonstrating how to register MR:&lt;/p>
&lt;pre>&lt;code class="language-c">#include &amp;lt;infiniband/verbs.h&amp;gt;
int main() {
// Omit initialization process...
struct ibv_mr *mr;
mr = ibv_reg_mr(pd, buf, 1024, IBV_ACCESS_LOCAL_WRITE |
IBV_ACCESS_REMOTE_WRITE);
// Get L_Key and R_Key
uint32_t lkey = mr-&amp;gt;lkey;
uint32_t rkey = mr-&amp;gt;rkey;
// Omit other code...
}
&lt;/code>&lt;/pre></description></item><item><title>RDMA Basic Service Types</title><link>https://cuterwrite.top/en/p/rdma-service-types/</link><pubDate>Sun, 25 Feb 2024 22:04:01 +0000</pubDate><guid>https://cuterwrite.top/en/p/rdma-service-types/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/f71da3ec40dd74648e15471d47ba3b84195413_crop-2024-02-26.webp" alt="Featured image of post RDMA Basic Service Types" />&lt;h1 id="rdma-basic-service-types">
&lt;a href="#rdma-basic-service-types" class="header-anchor">#&lt;/a>
RDMA Basic Service Types
&lt;/h1>
&lt;p>&lt;strong>This article welcomes non-commercial reprints, please indicate the source when reprinting.&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>Statement: For collection only, for convenient reading&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/144099636">&lt;cite>Zhihu Column: 5. Basic RDMA Service Types&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>In the article &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-element/" >【“3. RDMA Basic Elements”】
&lt;/a>
, we mentioned that &lt;strong>the basic communication unit of RDMA is QP&lt;/strong>, and there are many communication models based on QP, which we refer to as &amp;ldquo;service types&amp;rdquo; in the field of RDMA. The IB protocol describes a service type through two dimensions: &amp;ldquo;reliable&amp;rdquo; and &amp;ldquo;connected&amp;rdquo;.&lt;/p>
&lt;h2 id="reliable">
&lt;a href="#reliable" class="header-anchor">#&lt;/a>
Reliable
&lt;/h2>
&lt;p>Reliability in communication refers to ensuring that the sent data packets can be properly received through some mechanisms. In the IB protocol, reliable service is described as follows:&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Reliable Service&lt;/strong> provides a guarantee that messages are delivered from a requester to a responder at most once, in order and without corruption.&lt;/p>
&lt;/blockquote>
&lt;p>&amp;ldquo;Reliable service ensures that information is transmitted at most once between the sender and receiver, and it can guarantee that it is completely received in the order it was sent.&amp;rdquo;&lt;/p>
&lt;p>IB ensures reliability through the following three mechanisms:&lt;/p>
&lt;h2 id="response-mechanism">
&lt;a href="#response-mechanism" class="header-anchor">#&lt;/a>
Response mechanism
&lt;/h2>
&lt;p>Suppose A sends a data packet to B, how can A know that B has received it? Naturally, B replies with a &amp;ldquo;I have received it&amp;rdquo; message to A. In the field of communications, we generally refer to this reply as an acknowledgment packet or ACK. In the reliable service type of the IB protocol, an acknowledgment mechanism is used to ensure that the data packet is received by the other party. In the reliable service type of IB, the receiver does not have to reply to every packet; it can also reply with an ACK for multiple packets at once. We will discuss this further later.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/ib_ack-2024-02-26.webp"
alt="ib_ack-2024-02-26" width="auto" loading="lazy">
&lt;/figure>
&lt;h2 id="data-validation-mechanism">
&lt;a href="#data-validation-mechanism" class="header-anchor">#&lt;/a>
Data validation mechanism
&lt;/h2>
&lt;p>This is relatively easy to understand. The sender will use a certain algorithm to obtain a checksum for the Header and Payload (the actual data to be sent and received) and place it at the end of the data packet. When the receiving end receives the data packet, it will also use the same algorithm to calculate the checksum and then compare it with the checksum in the data packet. If they do not match, it indicates that the data contains errors (usually caused by link issues), and the receiving end will discard this data packet. The IB protocol uses CRC for checksum, and this article does not provide an in-depth introduction to CRC.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/crc-2024-02-26.png"
alt="crc-2024-02-26" width="auto" loading="lazy">
&lt;/figure>
&lt;h2 id="order-preserving-mechanism">
&lt;a href="#order-preserving-mechanism" class="header-anchor">#&lt;/a>
Order-preserving mechanism
&lt;/h2>
&lt;p>In-order delivery refers to ensuring that data packets sent first over the physical link are received by the recipient before later sent packets. Some services have strict requirements on the order of data packets, such as voice or video. The IB protocol includes the concept of PSN (Packet Sequence Number), meaning each packet has an incrementing number. PSN can be used to detect packet loss; for example, if the receiver gets 1 but receives 3 without having received 2, it will consider an error occurred during transmission and will send a NAK back to the sender, requesting the retransmission of the lost packet.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/psn-2024-02-26.webp"
alt="psn-2024-02-26" width="auto" loading="lazy">
&lt;/figure>
&lt;p>Unreliable service, without the above mechanisms to ensure that packets are received correctly, belongs to the type of service that is &amp;ldquo;just send it out, I don&amp;rsquo;t care if it is received or not.&amp;rdquo;&lt;/p>
&lt;h2 id="connection-and-datagram">
&lt;a href="#connection-and-datagram" class="header-anchor">#&lt;/a>
Connection and Datagram
&lt;/h2>
&lt;p>&lt;strong>Connection&lt;/strong> here refers to an abstract logical concept, which needs to be distinguished from a physical connection. Readers familiar with Sockets will certainly not be unfamiliar with this. A connection is a communication &amp;ldquo;pipeline.&amp;rdquo; Once the pipeline is established, the data sent from this end of the pipeline will definitely reach the other end along this pipeline.&lt;/p>
&lt;p>There are many definitions for &amp;ldquo;connection&amp;rdquo; or &amp;ldquo;connection-oriented&amp;rdquo;, some focus on ensuring the order of messages, some emphasize the uniqueness of the message delivery path, some highlight the need for software and hardware overhead to maintain the connection, and some overlap with the concept of reliability. Since this column is about introducing RDMA technology, let&amp;rsquo;s take a look at its description in section 3.2.2 of the IB protocol:&lt;/p>
&lt;blockquote>
&lt;p>IBA supports both connection-oriented and datagram service. For connected service, each QP is associated with exactly one remote consumer. In this case, the QP context is configured with the identity of the remote consumer’s queue pair. &amp;hellip; During the communication establishment process, this and other information is exchanged between the two nodes.&lt;/p>
&lt;/blockquote>
&lt;p>That is, &amp;ldquo;IBA supports both connection-oriented and datagram-based services. For connection-oriented services, each QP is associated with another remote node. In this case, the QP Context contains the QP information of the remote node. During the process of establishing communication, the two nodes exchange peer information, including the QP that will be used for communication later.&amp;rdquo;&lt;/p>
&lt;p>In the description above, Context is generally translated as 上下文. QP Context (abbreviated as QPC) can be simply understood as a table that records information related to a QP. We know that QP consists of two queues, and in addition to these two queues, we also need to record information about the QP in a table. This information may include the depth of the queues, the queue numbers, etc. We will elaborate on this later.&lt;/p>
&lt;p>It might still be a bit abstract, let&amp;rsquo;s use a diagram to explain:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/QPC-2024-02-26.webp"
alt="QPC-2024-02-26" width="auto" loading="lazy">
&lt;/figure>
&lt;p>The network cards of nodes A, B, and A, C are physically connected. A&amp;rsquo;s QP2 and B&amp;rsquo;s QP7, A&amp;rsquo;s QP4 and B&amp;rsquo;s QP2 have established a logical connection, or are &amp;ldquo;bound together.&amp;rdquo; &lt;strong>In the connection service type, each QP is connected to a unique other QP, meaning that the destination of each WQE issued by the QP is unique.&lt;/strong> For example, for each WQE issued by A&amp;rsquo;s QP2, the hardware can know through QPC that its destination is B&amp;rsquo;s QP7, and will send the assembled packet to B. Then B will store the data according to the RQ WQE issued by QP7; similarly, for each WQE issued by A&amp;rsquo;s QP4, A&amp;rsquo;s hardware knows that the data should be sent to Node C&amp;rsquo;s QP2.&lt;/p>
&lt;p>How is a &amp;ldquo;connection&amp;rdquo; maintained? Actually, it&amp;rsquo;s just a record inside the QPC. If A&amp;rsquo;s QP2 wants to disconnect from B&amp;rsquo;s QP7 and then &amp;ldquo;connect&amp;rdquo; with another QP, it only needs to modify the QPC. During the process of establishing a connection between two nodes, they exchange the QP Number that will be used later for data interaction, and then record it in the QPC respectively.&lt;/p>
&lt;p>&lt;strong>Datagram&lt;/strong> Contrary to connection, there is no need for a &amp;ldquo;pipeline establishment&amp;rdquo; step between the sender and receiver. As long as the sender can physically reach the receiver, it is possible to send to any receiving node from any path. The IB protocol defines it as follows:&lt;/p>
&lt;blockquote>
&lt;p>For datagram service, a QP is not tied to a single remote consumer, but rather information in the WQE identifies the destination. A communication setup process similar to the connection setup process needs to occur with each destination to exchange that information.&lt;/p>
&lt;p>&amp;ldquo;For datagram services, a QP will not be bound to a unique remote node but will specify the destination node through a WQE. Similar to connection-type services, the process of establishing communication requires both ends to exchange peer information, but for datagram services, this exchange process needs to be executed once for each destination node.&amp;rdquo;&lt;/p>
&lt;/blockquote>
&lt;p>Let&amp;rsquo;s take an example:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/Datagram-2024-02-26.webp"
alt="Datagram-2024-02-26" width="auto" loading="lazy">
&lt;/figure>
&lt;p>In the context of a datagram-type QP, it does not contain peer information, meaning each QP is not bound to another QP. &lt;strong>Each WQE issued to the hardware by the QP may point to a different destination&lt;/strong>. For example, the first WQE issued by QP2 of node A instructs to send data to QP3 of node C; while the next WQE may instruct the hardware to send to QP7 of node B.&lt;/p>
&lt;p>Like the connection service type, which remote QP the local QP can send data to is mutually informed in advance during the preparation stage through certain means. This is also the meaning of the above statement &amp;ldquo;the datagram service needs to perform this exchange process once for each destination node.&amp;rdquo;&lt;/p>
&lt;h2 id="service-type">
&lt;a href="#service-type" class="header-anchor">#&lt;/a>
Service type
&lt;/h2>
&lt;p>The two dimensions mentioned above combine in pairs to form the four basic service types of IB:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">&lt;/th>
&lt;th style="text-align: left">Reliable&lt;/th>
&lt;th style="text-align: left">Unreliable&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">Connection&lt;/td>
&lt;td style="text-align: left">RC (Reliable Connection)&lt;/td>
&lt;td style="text-align: left">UC (Unreliable Connection)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Datagram&lt;/td>
&lt;td style="text-align: left">RD (Reliable Datagram)&lt;/td>
&lt;td style="text-align: left">UD (Unreliable Datagram)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>RC and UD are the most applied and fundamental types of services, and we can analogize them to the TCP and UDP of the TCP/IP protocol stack&amp;rsquo;s transport layer, respectively.&lt;/p>
&lt;p>RC is used in scenarios with high requirements for data integrity and reliability, similar to TCP, because various mechanisms are needed to ensure reliability, so the overhead will naturally be higher. Additionally, since RC service types and each node need to maintain their own QP, assuming there are N nodes that need to communicate with each other, at least &lt;strong>N * (N - 1)&lt;/strong> QPs are required. QP and QPC themselves need to occupy network card resources or memory, and when there are many nodes, the consumption of storage resources will be very large.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/RC_Connect-2024-02-26.webp"
alt="RC_Connect-2024-02-26" width="auto" loading="lazy">
&lt;/figure>
&lt;p>UD hardware overhead is small and saves storage resources. For example, if N nodes need to communicate with each other, only &lt;strong>N&lt;/strong> QPs need to be created. However, reliability cannot be guaranteed, just like UDP. If users want to implement reliability based on the UD service type, they need to implement an application-layer reliable transmission mechanism based on the IB transport layer themselves.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/UD_Connect-2024-02-26.webp"
alt="UD_Connect-2024-02-26" width="auto" loading="lazy">
&lt;/figure>
&lt;p>In addition, there are RD and UC types, as well as more complex service types like XRC (Extended Reliable Connection) and SRD (Scalable Reliable Datagram), which we will describe in detail in the protocol analysis section.&lt;/p>
&lt;p>For more information on QP type selection, you can refer to the article &lt;a class="link" href="https://www.rdmamojo.com/2013/06/01/which-queue-pair-type-to-use/" target="_blank" rel="noopener" >Which Queue Pair type to use?
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
on RDMAmojo. Thanks to &lt;a class="link" href="https://www.zhihu.com/people/fc04fe143ad43b66fabb7050dadef923" target="_blank" rel="noopener" >@sinkinben
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
for pointing it out in the comments section.&lt;/p>
&lt;h2 id="code-example">
&lt;a href="#code-example" class="header-anchor">#&lt;/a>
Code example
&lt;/h2>
&lt;p>In RDMA programming, we can create a QP using the &lt;code>ibv_create_qp&lt;/code> function, where the &lt;code>qp_type&lt;/code> field in the &lt;code>struct ibv_qp_init_attr&lt;/code> structure is used to specify the service type of the QP. Below is a simple example code:&lt;/p>
&lt;pre>&lt;code class="language-c">struct ibv_qp_init_attr qp_init_attr;
qp_init_attr.qp_type = IBV_QPT_RC; // RC type
qp_init_attr.sq_sig_all = 1; // 1 means each WQE in SQ needs a corresponding CQE
qp_init_attr.send_cq = cq; // Send CQ
qp_init_attr.recv_cq = cq; // Receive CQ
qp_init_attr.cap.max_send_wr = 1024; // Depth of SQ
struct ibv_qp *qp = ibv_create_qp(pd, &amp;amp;qp_init_attr);
&lt;/code>&lt;/pre></description></item></channel></rss>