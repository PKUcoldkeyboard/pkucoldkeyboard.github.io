<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Research Related on Cuterwrite's Blog</title><link>https://cuterwrite.top/en/categories/research/</link><description>Recent content in Research Related on Cuterwrite's Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>cuterwrite</copyright><lastBuildDate>Sun, 03 Mar 2024 01:16:00 +0000</lastBuildDate><atom:link href="https://cuterwrite.top/en/categories/research/index.xml" rel="self" type="application/rss+xml"/><item><title>Notes: Pure - Improving Message Passing to Better Utilize Intra-Node Shared Memory</title><link>https://cuterwrite.top/en/p/pure/</link><pubDate>Sun, 03 Mar 2024 01:16:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/pure/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/crop_e9af4c445d695be5002248c7c814c67d195413-2024-03-04.webp" alt="Featured image of post Notes: Pure - Improving Message Passing to Better Utilize Intra-Node Shared Memory" />&lt;h1 id="note-pure-improve-message-passing-to-better-utilize-shared-memory-within-nodes">Note: Pure: Improve message passing to better utilize shared memory within nodes&lt;/h1>
&lt;h2 id="citation">Citation&lt;/h2>
&lt;p>James Psota and Armando Solar-Lezama. 2024. Pure: Evolving Message Passing To Better Leverage Shared Memory Within Nodes. In Proceedings of the 29th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming (&lt;strong>PPoPP &amp;lsquo;24&lt;/strong>). Association for Computing Machinery, New York, NY, USA, 133–146. &lt;a class="link" href="https://doi.org/10.1145/3627535.3638503" target="_blank" rel="noopener" >https://doi.org/10.1145/3627535.3638503
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h2 id="keywords">Keywords&lt;/h2>
&lt;ul>
&lt;li>Parallel programming model&lt;/li>
&lt;li>Distributed runtime system&lt;/li>
&lt;li>Task-based parallel model&lt;/li>
&lt;li>Concurrent data structures&lt;/li>
&lt;li>Lock-free data structure&lt;/li>
&lt;/ul>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Pure is a new programming model and runtime system designed to fully utilize shared memory within nodes in environments based on the Message Passing Interface (enhancing task usage to leverage idle core capabilities). Pure leverages shared memory in two ways: (1) allowing ranks to steal work from each other while waiting for messages to arrive; (2) using efficient lock-free data structures to achieve high-performance message passing and collective operations among processes within a node. Researchers evaluated Pure&amp;rsquo;s key message passing and collective features through micro benchmark tests and demonstrated that in CoMD molecular dynamics and miniAMR adaptive mesh refinement applications, Pure can achieve up to &lt;strong>2.1x&lt;/strong> application speedup when scaling to 4096 ranks.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>In recent decades, the field of high-performance computing has transitioned from large vector computers to clusters composed of single processors interconnected through networks. MPI has become the de facto standard for parallel programming on distributed memory systems. With advancements in hardware, the emergence of multi-core clusters has allowed cores within nodes to share memory and communicate over networks, prompting the community to continually seek new paradigms to more efficiently utilize modern cluster resources. Currently, there are two main strategies: one is to maintain a unified MPI programming approach by improving the MPI runtime system to better utilize shared memory; the other is to adopt hybrid programming models like MPI+X, using shared memory parallelism within nodes while continuing to use MPI between nodes. &lt;strong>However, these approaches may either be limited by the MPI standard&amp;rsquo;s specifications on interface behavior, preventing performance maximization, or present challenges to programmers in managing two programming models.&lt;/strong>&lt;/p>
&lt;p>The community has tried many other methods, including the &lt;strong>PGAS&lt;/strong> model, which provides a shared memory abstraction across clusters, and implicitly parallel programming languages like &lt;strong>Legion&lt;/strong>, &lt;strong>Chapel&lt;/strong>, and &lt;strong>X10&lt;/strong>, which offer high-level abstractions and attempt to automatically and efficiently coordinate applications. Despite some progress, many modern HPC applications still rely on MPI. &lt;strong>MPC&lt;/strong> and &lt;strong>AMPI&lt;/strong> also attempt to improve performance by using threads as MPI Rank to leverage internal shared memory.&lt;/p>
&lt;p>However, using only MPI methods often performs better than hybrid programming methods. This may be due to the limitations of the interface and the inability to fully utilize shared memory within nodes, causing MPI to fail to fully realize its potential performance. Therefore, the Pure system proposed in this article is built based on the MPI-everywhere method, breaking some traditional assumptions of MPI, more effectively utilizing shared memory, while avoiding the need for major restructuring of existing programs. Pure adopts a programming model similar to MPI, thus enabling the use of the existing MPI knowledge and application base of the HPC community.&lt;/p>
&lt;p>The design inspiration for Pure comes from MPI, with its core programming model based on message passing, and optionally integrating task parallelism. Unlike MPI, Pure abandons the use of process-level ranks and the limitation of supporting legacy languages, opting instead to implement ranks using threads rather than traditional processes. This shift allows Pure to efficiently adopt lightweight lock-free synchronization mechanisms to coordinate between threads within the same node. Utilizing this threaded rank architecture, Pure constructs efficient intra-node collective operations and optimizes the performance of these operations through lock-free algorithms. Additionally, Pure supports running portions of parallel code blocks in the application as standard C++ lambda expressions, which can be executed automatically and concurrently by the current rank-holding thread as well as other idle ranks, all of which are automatically scheduled by the Pure Runtime system.&lt;/p>
&lt;p>The optimization strategies proposed in the paper cover the following points:&lt;/p>
&lt;ul>
&lt;li>A lock-free messaging method suitable for the transmission of small messages and large data messages.&lt;/li>
&lt;li>Lock-free data structure, used for efficient implementation of collective communication algorithms.&lt;/li>
&lt;li>A lock-free task scheduler that allows idle threads to efficiently &amp;ldquo;steal&amp;rdquo; workloads from other threads.&lt;/li>
&lt;/ul>
&lt;p>The author uses the standard C++ library to ensure the wide compatibility of Pure and demonstrates that Pure has a significant performance improvement compared to highly optimized MPI benchmarks. In addition, the author shows that the Pure programming model is semantically very similar to MPI, which means that migrating from existing applications to Pure is straightforward and simple, further demonstrated by the source-to-source conversion tool mpi2pure. Overall, the main contributions of the paper can be summarized as follows:&lt;/p>
&lt;ol>
&lt;li>A new programming model and runtime system have been proposed, which effectively combines message passing and task parallelism, and utilizes features of standard C++ to implement it.&lt;/li>
&lt;li>Demonstrates how modern C++ supports more flexible parallel runtime system interfaces.&lt;/li>
&lt;li>Describes a well-designed lock-free, multithreaded, and distributed runtime system that shows significant speed improvements over MPI within nodes.&lt;/li>
&lt;li>It has been proven that by making minimal source code modifications to existing MPI applications, significant performance improvements can be achieved in micro benchmark tests and three real-world applications compared to state-of-the-art MPI implementations.&lt;/li>
&lt;/ol>
&lt;h2 id="2-pure-usage-example">2. Pure Usage Example&lt;/h2>
&lt;p>This section illustrates the use of Pure through a simple 1-D Stencil algorithm example. Although this example is simple, it clearly demonstrates the core concepts of Pure and its similarities with MPI, laying the foundation for developers to write more complex applications.&lt;/p>
&lt;p>In the MPI version implementation code &lt;code>rand_stencil_mpi&lt;/code>, the main computational work is performed in the function &lt;code>random_work&lt;/code>. In simple terms, the &lt;code>rand_stencil_mpi&lt;/code> function first enters a loop, iterating &lt;code>iters&lt;/code> times, calculating &lt;code>random_work&lt;/code> on each element of the array &lt;code>a&lt;/code>. It is noteworthy that the execution time of &lt;code>random_work&lt;/code> is variable and unknown, which introduces load imbalance. Moreover, &lt;code>random_work&lt;/code> does not modify the contents of the array &lt;code>a&lt;/code>, but instead updates the array &lt;code>a&lt;/code> by averaging adjacent elements. Finally, the program uses &lt;code>MPI_Send&lt;/code> and &lt;code>MPI_Recv&lt;/code> to exchange the first and last elements of the &lt;code>temp&lt;/code> array to compute the first and last elements of the array &lt;code>a&lt;/code>. Due to the varying time required by &lt;code>random_work&lt;/code>, some processing units will complete tasks early and sometimes become blocked while waiting for the slower sender in an &lt;code>MPI_Recv&lt;/code> call.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/1D_stencil-2024-03-14.webp"
alt="1D_stencil-2024-03-14" width="auto" loading="lazy">
&lt;/figure>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>Example 1: 1-D Stencil with Random Work, MPI Version&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-cpp">void rand_stencil_mpi(double* const a, size_t arr_sz, size_t iters, int my_rank,
int n_ranks) {
double temp[arr_sz];
for (auto it = 0; it &amp;lt; iters; ++it) {
for (auto i = 0; i &amp;lt; arr_sz; ++i) {
temp[i] = random_work(a[i]);
}
for (auto i = 1; i &amp;lt; arr_sz - 1; ++i) {
a[i] = (temp[i - 1] + temp[i] + temp[i + 1]) / 3.0;
}
if (my_rank &amp;gt; 0) {
MPI_Send(&amp;amp;temp[0], 1, MPI_DOUBLE, my_rank - 1, 0, MPI_COMM_WORLD);
double neighbor_hi_val;
MPI_Recv(&amp;amp;neighbor_hi_val, 1, MPI_DOUBLE, my_rank - 1, 0, MPI_COMM_WORLD,
MPI_STATUS_IGNORE);
a[0] = (neighbor_hi_val + temp[0] + temp[1]) / 3.0;
} // ends if not first rank
if (my_rank &amp;lt; n_ranks - 1) {
MPI_Send(&amp;amp;temp[arr_sz - 1], 1, MPI_DOUBLE, my_rank + 1, 0,
MPI_COMM_WORLD);
double neighbor_lo_val;
MPI_Recv(&amp;amp;neighbor_lo_val, 1, MPI_DOUBLE, my_rank + 1, 0, MPI_COMM_WORLD,
MPI_STATUS_IGNORE);
a[arr_sz - 1] =
(temp[arr_sz - 2] + temp[arr_sz - 1] + neighbor_lo_val) / 3.0;
} // ends if not last rank
} // ends for all iterations
}
&lt;/code>&lt;/pre>
&lt;p>Example 2 demonstrates the Pure version that achieves the same functionality. There are some key differences. First, the message calling function interface is different, using the corresponding Pure message passing functions &lt;code>pure_send_msg&lt;/code> and &lt;code>pure_recv_msg&lt;/code>, instead of MPI calls, but the parameters are essentially the same as the corresponding MPI functions. The message passing semantics of Pure are similar to MPI: the sender&amp;rsquo;s buffer is copied to the receiver&amp;rsquo;s buffer. The main implementation difference is that Pure uses &lt;strong>a lightweight message passing method within the node&lt;/strong>, resulting in lower latency for message passing within the node compared to MPI.&lt;/p>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>Example 2: Pure Version&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-cpp">void rand_stencil_pure(double* a, const int arr_sz, const int n_iter,
const int my_rank, const int n_ranks) {
double temp[arr_sz];
PureTask rand_work_task = [a, temp, arr_sz, my_rank](
chunk_id_t start_chunk, chunk_id_t end_chunk,
std::optinal&amp;lt;void&amp;gt; cont_params) {
auto [min_idx, max_idx] =
pure_aligned_idx_range&amp;lt;double&amp;gt;(arr_sz, start_chunk, end_chunk);
for (auto i = min_idx; i &amp;lt; max_idx; i++) {
temp[i] = random_work(a[i]);
}
}; // ends defining the Pure Task for rand_work_task
for (auto it = 0; it &amp;lt; n_iter; it++) {
rand_work_task.execute(); // execute all chunks of rand_work_task
for (auto i = 1; i &amp;lt; arr_sz - 1; ++i) {
a[i] = (temp[i - 1] + temp[i] + temp[i + 1]) / 3.0;
}
if (my_rank &amp;gt; 0) {
pure_send_msg(&amp;amp;temp[0], 1, MPI_DOUBLE, my_rank - 1, 0, PURE_COMM_WORLD);
double neighbor_hi_val;
pure_recv_msg(&amp;amp;neighbor_hi_val, 1, MPI_DOUBLE, my_rank - 1, 0,
PURE_COMM_WORLD);
a[0] = (neighbor_hi_val + temp[0] + temp[1]) / 3.0;
} // ends if not first rank
if (my_rank &amp;lt; n_ranks - 1) {
pure_send_msg(&amp;amp;temp[arr_sz - 1], 1, MPI_DOUBLE, my_rank + 1, 0,
PURE_COMM_WORLD);
double neighbor_lo_val;
pure_recv_msg(&amp;amp;neighbor_lo_val, 1, MPI_DOUBLE, my_rank + 1, 0,
PURE_COMM_WORLD);
a[arr_sz - 1] =
(temp[arr_sz - 2] + temp[arr_sz - 1] + neighbor_lo_val) / 3.0;
} // ends if not last rank
} // ends defining the Pure Task for rand_work_task
}
&lt;/code>&lt;/pre>
&lt;p>The more important difference is the addition of &lt;strong>Pure Task&lt;/strong> in Pure, which uses a lambda expression defined with a set of specific parameters. It leverages the capture parameter feature of lambda, allowing variables external to the lambda body to be captured by value or reference and used when the lambda is executed. Pure Task can be viewed as a snippet of application code executed by the Pure Runtime system and can be executed concurrently through multithreading. Therefore, Pure tasks should be structured in a data-parallel-like form. Additionally, Pure Task requires the programmer to ensure thread safety.&lt;/p>
&lt;p>In the above Pure implementation, programmers can use chunk ranges to describe concurrency. These subranges or chunks are passed to the Pure Task through the &lt;code>start_chunk&lt;/code> and &lt;code>end_chunk&lt;/code> parameters, and they are provided by the Pure Runtime system. The Pure Runtime system is responsible for ensuring that all work is completed smoothly. Since multiple different threads may be involved, the Pure Runtime system achieves this by tracking which chunks have been allocated and completed.&lt;/p>
&lt;p>Secondly, programmers need to map the &lt;code>start_chunk&lt;/code> and &lt;code>end_chunk&lt;/code> parameters provided by the Pure Runtime system to specific content related to application computation. Here, the code uses the &lt;code>pure_aligned_idx_range&lt;/code> helper function to convert them into loop subranges. This helper function takes cache lines into account, which helps avoid false sharing issues.&lt;/p>
&lt;p>Due to random_work potentially causing uneven load distribution, some ranks may be idle while waiting for messages. The Pure task scheduler will automatically utilize these idle ranks to execute other pending Pure task blocks within the same node. Take the example of three ranks within the same node in the diagram below: &lt;strong>rank 0&lt;/strong> is executing a Pure Task divided into 6 chunks, while &lt;strong>rank 1&lt;/strong> and &lt;strong>rank 2&lt;/strong> are blocked due to receiving messages.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/timeline-2024-03-14.png"
alt="timeline-2024-03-14" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Example Pure code timeline diagram&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>From the diagram, the following execution flow can be clearly seen:&lt;/p>
&lt;ul>
&lt;li>rank 0 begins processing the first chunk (chunk 0).&lt;/li>
&lt;li>At the same time, rank 1 steals and executes the second chunk (chunk 1) in parallel.&lt;/li>
&lt;li>The task scheduler then assigns the third chunk (chunk 2) to rank 0 and the fourth chunk (chunk 3) to rank 1.&lt;/li>
&lt;li>rank 2 attempts to steal a task and successfully executes the fifth chunk (chunk 4). Due to the randomness of &lt;code>random_work&lt;/code> execution, chunk 2 and chunk 4 might be time-consuming tasks.&lt;/li>
&lt;li>rank 0 completed the processing of chunk 5, which is a smaller task block, and it finished before rank 2 completed chunk 4.&lt;/li>
&lt;li>The task scheduler ensures that rank 0 does not finish execution before all chunks are completed. In fact, rank 0 has to wait until chunk 4 is completed before it can continue.&lt;/li>
&lt;li>During the process of waiting for messages at rank 1 and rank 2, they will attempt to steal more chunks from any other available rank.&lt;/li>
&lt;li>Thanks to the variable capture feature of lambda expressions, context information can be efficiently shared between different ranks.&lt;/li>
&lt;/ul>
&lt;p>The experimental results show that on a single node configured with 32 ranks, the Pure version achieves a 10% performance improvement compared to the MPI version due to faster message passing and parallel execution of Pure Tasks. In scenarios with uneven load distribution, the acceleration ratio of Pure even exceeds 200%. Although the degree of these performance improvements is affected by load imbalance, in practical application scenarios, Pure still demonstrates significant performance enhancements. This is attributed to the capability of the Pure Runtime system, which can automatically detect and efficiently utilize underutilized computational resources.&lt;/p>
&lt;h2 id="3-programming-model">3. Programming Model&lt;/h2>
&lt;p>The core of Pure&amp;rsquo;s programming model is &amp;ldquo;message passing combined with optional task parallelism.&amp;rdquo; Semantically, Pure&amp;rsquo;s message passing and collective communication operations are equivalent to MPI, with differences mainly in some syntactical details.&lt;/p>
&lt;p>Although Pure uses threads within nodes, its rank namespace remains non-hierarchical across the entire cluster. During the execution cycle of a Pure program, the number of ranks remains unchanged.&lt;/p>
&lt;p>The Pure application is written in C++ and runs using the SPMD (Single Program Multiple Data) model, achieving internal multithreading. On the same node, all ranks are implemented through kernel threads.&lt;/p>
&lt;p>&lt;strong>It is important to note that Pure applications do not support global variables&lt;/strong>. Therefore, developers should remove global variables or use the &lt;code>thread_local&lt;/code> keyword to limit the scope of variables, ensuring thread safety.&lt;/p>
&lt;p>For applications with load imbalance issues, developers can use Pure Task in parts of the program that meet the following specific conditions:&lt;/p>
&lt;ol>
&lt;li>Compute-intensive hotspot areas.&lt;/li>
&lt;li>Tasks that can be executed concurrently.&lt;/li>
&lt;/ol>
&lt;h3 id="message-passing-and-collective-communication-operations">Message passing and collective communication operations&lt;/h3>
&lt;p>In Pure, the &lt;code>pure_send_msg&lt;/code> and &lt;code>pure_recv_msg&lt;/code> functions correspond functionally to MPI&amp;rsquo;s &lt;code>MPI_Send&lt;/code> and &lt;code>MPI_Recv&lt;/code>, and Pure also provides corresponding non-blocking versions.&lt;/p>
&lt;p>Pure Runtime system ensures that all messages are delivered and in the order they are sent. Pure also implements a series of collective communication operations, including:&lt;/p>
&lt;ul>
&lt;li>Reduce&lt;/li>
&lt;li>All-Reduce&lt;/li>
&lt;li>Barrier&lt;/li>
&lt;li>Broadcast&lt;/li>
&lt;/ul>
&lt;p>In addition, Pure introduced the concept of a communication subgroup, allowing developers to further subdivide a communication subset into smaller subsets through the &lt;code>pure_comm_split&lt;/code> function.&lt;/p>
&lt;p>In order to use Pure, the application needs to be written using modern C++ standards, and it is recommended to compile using &lt;code>std=c++11&lt;/code> or a higher version. Pure provides a Make-based build system that automatically configures appropriate compiler options and links to the Pure Runtime system (libpure), while defining a series of targets for debugging and performance analysis.&lt;/p>
&lt;h3 id="pure-task">Pure Task&lt;/h3>
&lt;p>Pure Task allows developers to define the computational parts of an application and break them down into chunks that can be executed in parallel. These chunks can be automatically executed concurrently by the Pure Runtime system.&lt;/p>
&lt;p>However, Pure Task is not necessary and is only recommended when a task can be divided into multiple smaller chunks and doing so helps alleviate load imbalance issues.&lt;/p>
&lt;p>Pure Task is implemented through C++ Lambda expressions and synchronously executed when the &lt;code>execute&lt;/code> method is called on the rank that owns the task. Each rank can only execute one Pure Task at a time. The variable capture feature of Lambda expressions allows different ranks to efficiently share context information when executing different chunks. Typically, a Pure Task is defined once during the application&amp;rsquo;s runtime and then executed multiple times at each timestep or other iterations.&lt;/p>
&lt;p>When defining a Pure Task, you need to specify the number of chunks and additional application parameters. Tasks should avoid interdependence; however, because they are fully executed during the &lt;code>execute&lt;/code> call, they will not conflict with code outside of the tasks.&lt;/p>
&lt;p>Pure Task contains an &lt;code>execute&lt;/code> method, which accepts a parameter &lt;code>per_exe_args&lt;/code> of type &lt;code>optional&amp;lt;void*&amp;gt;&lt;/code>, used to pass additional arguments each time the task is executed. This is very useful when the input values of the task body change during consecutive executions. For example, a developer can pass a pointer to a local structure to the &lt;code>execute&lt;/code> method.&lt;/p>
&lt;p>The first two parameters of a Pure Task, &lt;code>start_chunk&lt;/code> and &lt;code>end_chunk&lt;/code>, are unsigned integers used to specify the range of chunks to execute. These chunks are allocated by the Pure Runtime system, ensuring that each chunk is executed only once, even if they may be executed concurrently.&lt;/p>
&lt;p>Pure Task uses chunk ranges to provide flexibility to the scheduler, allowing multiple chunks to be allocated at once. The number of chunks is determined by the Pure task scheduler, but will not exceed the &lt;code>PURE_MAX_TASK_CHUNKS&lt;/code> predefined in the Makefile.&lt;/p>
&lt;p>Currently, the Pure Task interface requires manually mapping chunk numbers to array indices, which can be cumbersome when dealing with multidimensional arrays. Therefore, the future goal is to extend the interface to provide a more concise and higher-level interface similar to TBB&amp;rsquo;s &lt;code>parallel_for&lt;/code>.&lt;/p>
&lt;p>Finally, developers need to ensure that the internal implementation of Pure Task is thread-safe to avoid mutual contention between chunks of the same task being executed concurrently. For example, in the CoMD molecular dynamics benchmark, the issue of multiple threads writing to the same memory location simultaneously needs to be addressed, and in such cases, an &lt;code>std::atomic&lt;/code> array can be used to replace a regular &lt;code>int&lt;/code> array.&lt;/p>
&lt;h2 id="4-runtime-system">4. Runtime System&lt;/h2>
&lt;p>The Pure runtime system is a dynamic library for multithreaded and distributed runtime, used to support the development of Pure applications. Developers need to include the &lt;code>pure.h&lt;/code> header file when using it, compile with the C++17 standard, and link to the &lt;code>libpure&lt;/code> library. The Pure runtime system can automatically find and exploit opportunities for overlapping execution between computation and communication operations, especially in cases of high communication latency.&lt;/p>
&lt;p>The main functions of the Pure runtime system include:&lt;/p>
&lt;ul>
&lt;li>Initialize and configure the necessary processes and threads, start the application.&lt;/li>
&lt;li>Communication and collective operations between ranks within the management node.&lt;/li>
&lt;li>Manage internal memory buffers and data structures.&lt;/li>
&lt;li>If a Pure Task is defined in the application, the runtime system is also responsible for scheduling and executing these tasks.&lt;/li>
&lt;/ul>
&lt;h3 id="rank-initialization-and-mapping">Rank initialization and mapping&lt;/h3>
&lt;p>In Pure, rank is implemented as the kernel thread of an MPI process. In multi-node applications, Pure runs MPI to handle cross-node communication, whereas in single-node applications, MPI is not used. Nonetheless, Pure applications do not directly call MPI functions. Through Makefile configuration, a Pure program can start an MPI process on a node or NUMA node and create a corresponding number of threads based on the number of cores per node or NUMA node. For application developers, they only need to understand the non-hierarchical rank namespace, while underlying concepts such as nodes, threads, MPI processes, and communication latency are abstracted and transparent to the developers.&lt;/p>
&lt;p>Pure supports flexible rank-to-node mapping strategies and defaults to using an SMP-style allocation strategy. Additionally, Pure supports custom rank mapping, including the use of CrayPAT rank reordering files. While these hardware-related details are invisible to developers, Pure internally utilizes this information to optimize key functionalities.&lt;/p>
&lt;p>When a Pure application starts, the original &lt;code>main&lt;/code> function of the application is not executed directly. Instead, the underlying MPI program calls the &lt;code>main&lt;/code> function defined in the Pure runtime system, which is responsible for initializing Pure&amp;rsquo;s core data structures. It then creates and binds threads, each executing an &lt;code>original_main&lt;/code> function, which is a renamed version of the original &lt;code>main&lt;/code> function from the application code. After the application completes execution, the &lt;code>original_main&lt;/code> function returns to the Pure runtime system, which is responsible for completing the MPI cleanup and termination process.&lt;/p>
&lt;h3 id="spin-steal-waiting-loop-ssw-loop">Spin-Steal Waiting Loop (SSW-Loop)&lt;/h3>
&lt;p>When a Pure rank encounters a blocking event, such as waiting for a message to arrive, it will execute a mechanism called the &lt;strong>Spin, Steal, Wait Loop (SSW-Loop)&lt;/strong> instead of simply entering an idle state. In this loop, the rank checks if the blocking condition is met, such as whether a message has arrived, and if not, it will attempt to steal tasks from other ranks. If a blocking rank can assist in completing tasks that other threads in its process are concurrently executing, it will participate in such assistance work.&lt;/p>
&lt;p>Since threads are bound to specific CPUs and each rank runs only one application, Pure chooses to have ranks actively spin-wait rather than relinquish the CPU. The SSW-Loop gives ranks in computation &amp;ldquo;polymorphism&amp;rdquo;: they can act as computing nodes for the main program, assist other ranks in executing stolen task blocks, and then return to check their own blocking events.&lt;/p>
&lt;p>Pure follows the strategy of prioritizing the stolen workload of the current rank and adheres to the scheduling principle of workload priority.&lt;/p>
&lt;p>Unlike systems that use auxiliary threads to achieve workload stealing or communication, Pure is characterized by allowing application-level compute nodes to directly perform task stealing operations.&lt;/p>
&lt;h3 id="implementation-instructions">Implementation Instructions&lt;/h3>
&lt;p>Pure is written using the C++17 standard library. The Pure runtime system consists of about 21,000 lines of source code, and the Pure tools contain about 14,000 lines of source code. Pure has been tested in various environments, including laptops and clusters, and only requires a C++17 supporting compiler, a Unix-like operating system, and an MPI environment to run. The source code for Pure is publicly available on GitHub at the following link: &lt;a class="link" href="https://github.com/psota/pure" target="_blank" rel="noopener" >https://github.com/psota/pure
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;h3 id="point-to-point-communication">Point-to-point communication&lt;/h3>
&lt;p>Pure provides blocking and non-blocking point-to-point message passing functionality, consistent with the message passing semantics of MPI.&lt;/p>
&lt;p>Pure internally uses three different strategies for message passing, and the choice of strategy depends on the size of the message and whether the sender and receiver are located on the same node.&lt;/p>
&lt;p>Pure allocates and reuses a persistent Channel object throughout the program&amp;rsquo;s lifecycle, which is stored in the runtime system. The internal Channel Manager is responsible for mapping message parameters to the appropriate data structures and creating these structures as needed.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/strategy-2024-03-15.webp"
alt="strategy-2024-03-15" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Pure Message Passing Strategy&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>&lt;strong>Short message (less than 8KB)&lt;/strong>:
&lt;ul>
&lt;li>Use a lock-free circular queue (PureBufferQueue, PBQ), with acquire-release memory semantics. The sending thread copies the message to the PBQ when there is available space, and the receiving thread retrieves it when the message is ready.
&lt;ul>
&lt;li>In short message passing, the overhead of copying is relatively small, allowing the sender to immediately perform other useful work after the call returns.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Both sending and receiving threads use SSW-Loop to wait in order to achieve as much overlap of computation and communication as possible.&lt;/li>
&lt;li>The slots for all messages are stored in a contiguous buffer, with pointer arithmetic ensuring that each slot is aligned with cache line boundaries, avoiding false sharing between sending and receiving threads.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Big Message (greater than or equal to 8KB)&lt;/strong>:
&lt;ul>
&lt;li>A strategy similar to PBQ, but using a single memory copy directly from sender to receiver, inspired by the rendezvous mode of MPI.&lt;/li>
&lt;li>Use a lock-free fixed-size circular buffer to store the receiver&amp;rsquo;s receive call parameters.&lt;/li>
&lt;li>The sender waits for a metadata queue item via SSW-Loop and then copies the message content directly to the receiver&amp;rsquo;s buffer. The sender notifies the receiver that the transfer is complete by inserting the number of bytes transferred into the lock-free queue.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Cross-node message&lt;/strong>
&lt;ul>
&lt;li>Transparently use the MPI interface for message passing.&lt;/li>
&lt;li>During Pure initialization, use a distributed consensus algorithm to create a &lt;code>thread-rank-process-node&lt;/code> mapping data structure that maps Pure rank to MPI rank.&lt;/li>
&lt;li>To ensure that the correct receiving thread on the receiving node can receive the message, encode the sending and receiving thread IDs in &lt;code>MPI_TAG&lt;/code> to solve the multithreading routing problem.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="collective-communication">Collective communication&lt;/h3>
&lt;p>The collective communication operations of Pure are semantically the same as MPI, but they are implemented within nodes using a bottom-up constructed data structure. This has shown significant performance improvements in both single-node and multi-node benchmarks, even though it still relies on MPI&amp;rsquo;s collective operations for cross-node communication.&lt;/p>
&lt;p>Pure uses a leader thread to coordinate the collective communication process, while other threads assist with computation and call MPI collective functions as needed.&lt;/p>
&lt;ul>
&lt;li>Pure uses a static leader election method, which is more efficient than the compare-and-swap based &amp;ldquo;first-come&amp;rdquo; method.&lt;/li>
&lt;/ul>
&lt;p>The following is an example using All-Reduce, other collective communication operations have similar concepts.&lt;/p>
&lt;p>For small data All-Reduce operations, Pure designed a concurrent data structure called Sequenced Per-Thread Dropbox (SPTD), providing an efficient lock-free mechanism for pairwise synchronization and optionally sharing data between leader threads and other non-leader threads.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/SPTD-2024-03-15.webp"
alt="SPTD-2024-03-15" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Sequenced Per-Thread Dropbox (SPTD)&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>This method draws on the flat-combinding technique, using thread 0 in the communicator as the leader thread.&lt;/p>
&lt;ul>
&lt;li>For small arrays not exceeding 2KB:
&lt;ul>
&lt;li>Non-leader threads first copy data to SPTD, then synchronize with the leader thread, indicating that the input data is ready (using atomic sequence numbers instead of a shared atomic counter).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>The leader thread performs an element-wise Reduce operation on all input arrays.
&lt;ul>
&lt;li>Each node&amp;rsquo;s leader thread uses &lt;code>MPI_Allreduce&lt;/code> to perform a global Reduce on local Reduce results.&lt;/li>
&lt;li>Leader thread synchronization, non-leader threads copy the final Reduce result to a private buffer.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>All threads execute the SSW-Loop while waiting.&lt;/li>
&lt;li>For large arrays exceeding 2KB, Reduce computation may become a performance bottleneck. Therefore, it is necessary for all threads to execute Reduce computation concurrently and read from or write to data directly from each thread&amp;rsquo;s buffer through shared memory.
&lt;ul>
&lt;li>Reduce work is divided into equally sized blocks to avoid false sharing and enable vectorized computation.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Threads report readiness status using SPTD and mark computation completion with atomic sequence numbers.
&lt;ul>
&lt;li>The leader thread calls &lt;code>MPI_Allreduce&lt;/code> to perform an All-Reduce operation across nodes and propagates the final result through another atomic sequence number.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="task-scheduler">Task Scheduler&lt;/h3>
&lt;p>The Pure runtime system has meticulously designed a task scheduler that maintains an array named &lt;code>active_tasks&lt;/code> in shared memory. This array stores a series of atomic pointers, each corresponding to a task being executed, and allocates an entry for each node and each rank in the system. These entries are initially set to &lt;code>nullptr&lt;/code>, indicating that a task has not yet been assigned.&lt;/p>
&lt;p>When a task is created and prepared for execution, the system initializes its state and updates the corresponding entry in the &lt;code>active_tasks&lt;/code> array through an atomic operation to reflect that the task has been assigned. This update process ensures that the execution state of the task is visible to all threads in the system, allowing the task to be &amp;ldquo;stolen&amp;rdquo; by other threads.&lt;/p>
&lt;p>During the execution of the task, the rank owning the task will begin executing a series of chunks, which are the subdivided work units of the task. Meanwhile, other threads will continuously check the &lt;code>active_tasks&lt;/code> array during their SSL-Loop, using atomic load operations to look for executable non-empty tasks.&lt;/p>
&lt;p>The execution of the task is coordinated by two atomic integers, &lt;code>curr_chunk&lt;/code> and &lt;code>chunks_done&lt;/code>. The owner rank of the task and the possible thief ranks will both run the same concurrent execution function. The thief threads will return after executing one chunk, while the owner thread will continue executing until all chunks are completed. By using the &lt;code>fetch_add&lt;/code> operation, a thread can determine which chunk it should execute. If the value of &lt;code>curr_chunk&lt;/code> has already exceeded the total number of chunks, the thread will stop executing.&lt;/p>
&lt;p>Each time a chunk is successfully completed, the thread atomically increments the value of &lt;code>chunks_done&lt;/code>. The owner thread updates its local storage to avoid cache misses. Finally, the owner rank will wait until all chunks are executed, ensuring the complete execution of the task.&lt;/p>
&lt;p>It is worth noting that the task&amp;rsquo;s chunk and the application&amp;rsquo;s rank are executed on the same hardware thread. In Pure applications, each hardware thread is assigned to a specific rank. Although currently Pure does not utilize hardware accelerators (such as GPUs) to accelerate task execution, the designers believe that Pure&amp;rsquo;s architecture is fully capable of supporting such acceleration.&lt;/p>
&lt;p>The Pure task scheduler provides various execution modes and stealing algorithms to accommodate different execution needs. For example, the author implemented a single chunk execution mode and a guided self-scheduling mode, the latter being a work partitioning algorithm that prioritizes the allocation of larger work chunks followed by smaller ones. Additionally, the scheduler includes a NUMA-aware stealing mode, which prioritizes stealing tasks from threads on the same NUMA node, and a &amp;ldquo;sticky&amp;rdquo; stealing mode, allowing thief threads to return to tasks they recently stole that are still active. These features collectively ensure the efficiency and flexibility of task scheduling.&lt;/p>
&lt;h2 id="evaluation">Evaluation&lt;/h2>
&lt;p>The performance evaluation of Pure was conducted on the Cori HPC cluster at Berkeley NERSC. This cluster consists of 2388 nodes, each configured with 2 sockets, 16 cores, and 128GB of memory, interconnected via Cray Aires. The experimental configuration enabled hyper-threading and adopted a 256-bit vector width. Two processes were run on each node, totaling 32 threads. The evaluation used the Intel compiler and Cray MPICH as the performance baseline.&lt;/p>
&lt;h3 id="nas-dt-benchmark-results">NAS DT benchmark results&lt;/h3>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/nasdt-2024-03-15.webp"
alt="nasdt-2024-03-15" width="auto" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>By optimizing the message-passing mechanism alone, Pure achieved a performance boost of 11% to 25%.&lt;/li>
&lt;li>After introducing Pure Tasks, the performance acceleration ratio increased to 1.7 times to 2.6 times.&lt;/li>
&lt;li>Auxiliary threads can slightly improve performance, but only if there are remaining unused CPU cores available. Here, except in the case of 80 ranks where 24 cores were idle, the CPU cores were fully utilized in other cases.&lt;/li>
&lt;/ul>
&lt;h3 id="comd-and-miniamr-benchmarks">CoMD and miniAMR benchmarks&lt;/h3>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/benchmark-pure-2024-03-15.webp"
alt="benchmark-pure-2024-03-15" width="auto" loading="lazy">
&lt;/figure>
&lt;p>-In the CoMD molecular dynamics application, Pure outperforms using only MPI and MPI+OpenMP in terms of performance across all ranks, achieving speedups of 7% to 25% and 35% to 50% respectively, even in the absence of load imbalance.&lt;/p>
&lt;ul>
&lt;li>In the miniAMR adaptive mesh refinement application, Pure achieved at least a 20% and at most a 50% performance acceleration.&lt;/li>
&lt;/ul>
&lt;h3 id="collective-communication-performance">Collective communication performance&lt;/h3>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/benchmark-pure-msg-2024-03-15.webp"
alt="benchmark-pure-msg-2024-03-15" width="auto" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>Pure exhibits outstanding performance in collective communication operations, where its internal optimization mechanisms and data structure design allow Pure to demonstrate significant efficiency and advantages when handling large-scale parallel computing tasks.&lt;/li>
&lt;/ul>
&lt;h2 id="related-work">Related work&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Category&lt;/th>
&lt;th style="text-align:left">Related Work&lt;/th>
&lt;th style="text-align:left">Advantages of Pure&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">MPI&lt;/td>
&lt;td style="text-align:left">1. Utilize shared memory within multi-core nodes to enhance performance; 2. XPMEM significantly enhances intra-node communication efficiency; 3. ch4 network library optimizes MPI shared memory communication; 4. Improved MPI collective communication algorithms; 5. DMAPP library is optimized for specific collective communications, but with many limitations; 6. Addressed the challenges of large-scale all-to-all collective communications; 7. One-sided message API achieves decoupling; 8. Optimized data movement and process synchronization&lt;/td>
&lt;td style="text-align:left">1. Pure demonstrates excellent performance across all collective communications and load sizes; 2. Provides advanced communication-computation overlap mechanisms, surpassing traditional one-sided message API&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">MPI Multithreading&lt;/td>
&lt;td style="text-align:left">1. Supports multithreading within ranks in MPI_THREAD_MULTIPLE mode; 2. Most MPI implementations achieve thread safety through global locks, leading to performance bottlenecks; 3. MPI 4.0 introduces MPI+X method to enhance multithreading support; 4. Introduces the concepts of MPI Fine-points and Endpoints to support threading&lt;/td>
&lt;td style="text-align:left">1. Pure emphasizes the importance of MPI calls in multithreaded code; 2. Provides a unified programming model to simplify the introduction of parallel tasks&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">AMPI&lt;/td>
&lt;td style="text-align:left">1. MPI compatible library based on Charm++; 2. Provides advanced parallel programming abstractions; 3. Achieves performance improvement with minimal code changes&lt;/td>
&lt;td style="text-align:left">1. Pure outperforms AMIP in practical tests due to its optimized message passing and collective communication, as well as more refined and low-overhead load balancing strategies; 2. Compared to the thread-based model of AMIP SMP, Pure offers more efficient parallel processing&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">PGAS language and parallel frameworks&lt;/td>
&lt;td style="text-align:left">1. PGAS language provides an abstraction of global memory address space; 2. Chapel and X10 extend the PGAS approach, supporting local and remote asynchronous tasks; 3. HPX adds distributed operation support to the modern C++ standard; 4. Legion as a data center parallel programming system; 5. Frameworks like Kokkos, STAPL, BCL provide an abstraction layer between applications and hardware&lt;/td>
&lt;td style="text-align:left">1. Similar to Pure, the PGAS model adopts the SPMD style to improve performance through locality reference; 2. Although these frameworks utilize modern C++ features, they usually require extensive rewriting of existing applications, whereas Pure offers a more direct optimization path&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="summary-1">Summary&lt;/h2>
&lt;p>For decades, message passing has been regarded as the standard model for parallel programming due to its relative simplicity and performance advantages. However, this paper demonstrates that message passing and shared memory are not incompatible. In fact, by designing appropriate libraries, shared memory can be fully utilized without sacrificing most of the advantages of message passing.&lt;/p></description></item><item><title>Scientific Research Chart Drawing</title><link>https://cuterwrite.top/en/p/science-plot/</link><pubDate>Tue, 27 Feb 2024 00:14:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/science-plot/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/crop_ee40c9cb9e33ffe888365e66e0a104dc195413-2024-02-28.webp" alt="Featured image of post Scientific Research Chart Drawing" />&lt;h1 id="scientific-chart-plotting">Scientific chart plotting&lt;/h1>
&lt;h2 id="prerequisite-knowledge">Prerequisite knowledge&lt;/h2>
&lt;h3 id="bitmap">Bitmap&lt;/h3>
&lt;p>Also known as bitmap image, pixel image, or raster image, it is composed of pixels. These points can be arranged and colored differently to form an image.&lt;/p>
&lt;p>&lt;strong>Bitmap Characteristics&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>Bitmap images are good at reproducing subtle gradations of color, capable of creating images with rich variations in color and brightness, realistic colors, large files, and cannot be scaled arbitrarily;&lt;/li>
&lt;li>The larger the image size, the larger the file; the richer the image color, the larger the file.&lt;/li>
&lt;li>The precision of printing and output is limited;&lt;/li>
&lt;li>&lt;strong>Bitmap file formats&lt;/strong>: such as .tiff, .bmp, .gif, .jpg, .png, .psd, etc.&lt;/li>
&lt;li>&lt;strong>Common bitmap editing software&lt;/strong>: Photoshop, etc.&lt;/li>
&lt;/ul>
&lt;h3 id="vector-graphics">Vector graphics&lt;/h3>
&lt;p>&lt;strong>Vector&lt;/strong> is also known as &amp;ldquo;vector,&amp;rdquo; and the graphic elements (points and line segments) in vector images are called objects. Each object is an individual entity with attributes such as size, direction, outline, color, and screen position. Simply put, vector graphic software uses mathematical methods to draw basic shapes like rectangles.&lt;/p>
&lt;p>&lt;strong>Vector Graphic Features&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>Can be infinitely enlarged without worrying about distortion;&lt;/li>
&lt;li>Vector graphics can be easily converted to bitmap, while converting bitmap to vector graphics requires methods like image tracing, but perfectly converting to vector graphics is still somewhat challenging.&lt;/li>
&lt;li>&lt;strong>Vector graphic file formats&lt;/strong>: Such as Adobe Illustrator&amp;rsquo;s .AI, .EPS, .SVG, .PDF, AutoCAD&amp;rsquo;s .dwg and .dxf, Windows standard metafile *.wmf and enhanced metafile *.emf, etc.&lt;/li>
&lt;li>&lt;strong>Common vector graphics editing software&lt;/strong>: Illustrator, CorelDraw, AutoCAD, etc.&lt;/li>
&lt;/ul>
&lt;h3 id="the-relationship-between-pixels-dpi-and-print-size">The relationship between pixels, DPI, and print size&lt;/h3>
&lt;p>The mathematical relationship between image resolution, number of pixels, and print size is: Pixels = Resolution (DPI) × Print Size (in inches).&lt;/p>
&lt;p>Among them, DPI is the number of pixels per square inch, which is a measure of the image&amp;rsquo;s level of detail. Understanding the above concepts, we can infer the image size using these concepts. For example, if I want to print an 8-inch * 10-inch, 300 DPI image, how should I set the pixel width and height of the image? You simply multiply the two together, $8 \times 300=2400$, $10 \times 300=3000$, so the pixel dimensions of this image are $2400 \times 3000$.&lt;/p>
&lt;h3 id="magazine-requirements">Magazine requirements&lt;/h3>
&lt;p>Here, taking the example of the requirements from the well-known publisher &lt;a class="link" href="https://www.elsevier.com/authors/author-schemas/artwork-and-media-instructions/artwork-sizing" target="_blank" rel="noopener" >Elsevier
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">TARGET SIZE&lt;/th>
&lt;th style="text-align:center">Image Width&lt;/th>
&lt;th style="text-align:center">Pixels at 300 dpi&lt;/th>
&lt;th style="text-align:center">Pixels at 500 dpi&lt;/th>
&lt;th style="text-align:center">Pixels at 1000 dpi&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">Minimal size&lt;/td>
&lt;td style="text-align:center">30 mm (85 pt)&lt;/td>
&lt;td style="text-align:center">354&lt;/td>
&lt;td style="text-align:center">591&lt;/td>
&lt;td style="text-align:center">1181&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Single column&lt;/td>
&lt;td style="text-align:center">90 mm (255 pt)&lt;/td>
&lt;td style="text-align:center">1063&lt;/td>
&lt;td style="text-align:center">1772&lt;/td>
&lt;td style="text-align:center">3543&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">1.5 column&lt;/td>
&lt;td style="text-align:center">140 mm (397 pt)&lt;/td>
&lt;td style="text-align:center">1654&lt;/td>
&lt;td style="text-align:center">2756&lt;/td>
&lt;td style="text-align:center">5512&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Double column (full width)&lt;/td>
&lt;td style="text-align:center">190 mm (539 pt)&lt;/td>
&lt;td style="text-align:center">2244&lt;/td>
&lt;td style="text-align:center">3740&lt;/td>
&lt;td style="text-align:center">7480&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>By studying the content on image dimensions above, we can understand the relationship between print size, pixels, and dpi. For example, the table shows that the minimum image size required in red is $30 \mathrm{mm}$. We can use the formula to verify whether the printed size of 354 pixels wide at 300dpi resolution is indeed $30 \mathrm{mm}$: $354 \div 300 \times 2.54 \times 10 = 29.97 \mathrm{mm}$, the two numbers multiplied at the end convert inches to millimeters, which is exactly $30 \mathrm{mm}$. Knowing the above relationship, we can use Photoshop to edit our images.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/attachments-2018-07-4Fzft7fc5b5ace58ae9dd-2024-02-28.webp"
alt="attachments-2018-07-4Fzft7fc5b5ace58ae9dd-2024-02-28" width="auto" loading="lazy">
&lt;/figure>
&lt;p>For example, an image from Mapman, opened with Photoshop, shows the following dimensions:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/attachments-2018-08-ZdVKUJMx5b63ec8d64919-2024-02-28.webp"
alt="attachments-2018-08-ZdVKUJMx5b63ec8d64919-2024-02-28" width="auto" loading="lazy">
&lt;/figure>
&lt;p>Due to the image size being too large, with a width of $124.99 \mathrm{cm}$, and a resolution of $72$, it does not meet the magazine requirements. Here, we will use the knowledge learned above to adjust the image size without losing image pixels;&lt;/p>
&lt;p>Now we need to adjust the image width to a double-column size, which is $19\mathrm{cm}$; using the formula: pixels = resolution (DPI) × print size (in inches)&lt;/p>
&lt;p>With the pixels unchanged, we need to increase the resolution to reduce the print size of the image. According to the ratio calculation, it should be increased to how much dpi: $124.99 \div 19 \times 72=473.6 \mathrm{dpi}$;&lt;/p>
&lt;p>So, modifying these two values of width and resolution is sufficient, and the number of pixels in the image remains unchanged, achieving a lossless change in the image size; moreover, 473dpi is greater than the minimum 300dpi.&lt;/p>
&lt;h2 id="matplotlib-python-library">Matplotlib Python library&lt;/h2>
&lt;p>As the most fundamental and widely used data visualization library in the Python ecosystem, Matplotlib offers rich 2D and 3D plotting capabilities, particularly suitable for creating common scientific charts such as line charts, bar charts, and scatter plots. It also allows for highly customizable output styles to meet the standards of various academic journals.&lt;/p>
&lt;p>It can be used to draw various static, dynamic, and interactive charts. We can use this tool to present a lot of data more intuitively in the form of charts, including drawing line charts, scatter plots, contour plots, bar charts, histograms, 3D graphics, and even graphic animations, etc.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/matplotlib-graphs-2048x1153-2024-02-28.webp"
alt="matplotlib-graphs-2048x1153-2024-02-28" width="auto" loading="lazy">
&lt;/figure>
&lt;hr>
&lt;ul>
&lt;li>Matplotlib Cheat Sheet&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/cheatsheets-1-2024-02-28.webp"
alt="cheatsheets-1-2024-02-28" width="auto" loading="lazy">
&lt;/figure>
&lt;h2 id="seaborn-python-library">Seaborn Python library&lt;/h2>
&lt;p>Built on top of Matplotlib, Seaborn further enhances the functionality of statistical charts. It comes with many advanced statistical chart styles, such as heatmaps, box plots, and time series analysis charts, making the presentation of complex data relationships more intuitive and readable. Since it is based on Matplotlib, many of Seaborn&amp;rsquo;s chart interfaces and parameter settings are quite similar to it, making plotting more convenient and fast. Even those without much foundation can create aesthetically pleasing graphics with analytical value through minimal code.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/Seaborn-gallery-2024-02-28.webp"
alt="Seaborn-gallery-2024-02-28" width="auto" loading="lazy">
&lt;/figure>
&lt;hr>
&lt;ul>
&lt;li>Seaborn Cheat Sheet
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/Python_Seaborn_Cheat_Sheet_q074wv-2024-02-28.webp"
alt="Python_Seaborn_Cheat_Sheet_q074wv-2024-02-28" width="auto" loading="lazy">
&lt;/figure>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;ul>
&lt;li>Excellent tutorial: &lt;a class="link" href="https://zhuanlan.zhihu.com/p/81553421" target="_blank" rel="noopener" >Data Visualization, Seaborn makes plotting so beautiful
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;/ul>
&lt;h2 id="visio-vector-graphics-software-framework-flowcharting-and-algorithm-structure">Visio vector graphics software (framework flowcharting and algorithm structure)&lt;/h2>
&lt;p>For chart designs that are not data-intensive but have rigorous logic, such as experimental flowcharts, system architecture diagrams, or algorithm flowcharts, Microsoft Visio, with its powerful vector editing capabilities and vast array of preset templates, has become the ideal choice for constructing clear and standardized flowcharts.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/data-graphics-2024-02-28.webp"
alt="data-graphics-2024-02-28" width="auto" loading="lazy">
&lt;/figure>
&lt;h2 id="origin-vector-graphics-software-mathematical-analysis-and-function-plotting">Origin Vector Graphics Software (Mathematical Analysis and Function Plotting)&lt;/h2>
&lt;p>Origin is a scientific graphing and data analysis software developed by OriginLab Corporation, supporting operation under Microsoft Windows. Origin supports a variety of 2D/3D graphics. The data analysis features in Origin include statistics, signal processing, curve fitting, and peak analysis. Curve fitting in Origin uses the nonlinear least squares fitting based on the Levenberg-Marquardt algorithm (LMA). Origin&amp;rsquo;s powerful data import capabilities support various data formats, including ASCII, Excel, NI TDM, DIADem, NetCDF, SPC, and more. The graphic output formats are diverse, such as JPEG, GIF, EPS, TIFF, etc. The built-in query tool can access database data through ADO.&lt;/p>
&lt;p>In the fields of physics, chemistry, and biology, Origin is renowned for its powerful mathematical analysis and function plotting capabilities, specifically designed for scientific data analysis. It is particularly suitable for plotting precise signal curves, spectrum analysis graphs, and other complex scientific graphics.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/Picture1_revised%20text-2024-02-28.webp"
alt="Picture1_revised text-2024-02-28" width="auto" loading="lazy">
&lt;/figure>
&lt;h2 id="ai-adobe-illustrator-vector-graphics-software">AI (Adobe Illustrator) vector graphics software&lt;/h2>
&lt;p>As an industry-standard vector graphics processing software, Illustrator is not only suitable for high-precision, publication-grade chart design but also capable of creating high-quality scientific illustrations, ensuring clear and delicate effects at any size. It is an industry-standard vector illustration software used in publishing, multimedia, and online images. The software is mainly used in print publishing, poster and book layout, professional illustration, multimedia image processing, and web page production. It can also provide high precision and control for line art, suitable for producing any small design to large and complex projects.&lt;/p>
&lt;p>In chart plotting, it is mainly applied in: direct plotting - rarely used in computer science and control classes, but students in the biochemical, environmental, and material fields use AI to achieve operations like cell structure and ventricle highlighting; integrating previously exported individual vector graphics; converting non-vector graphics to vector graphics.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/attachments-2018-07-NTyyj78C5b500737d0ac8-2024-02-28.webp"
alt="attachments-2018-07-NTyyj78C5b500737d0ac8-2024-02-28" width="auto" loading="lazy">
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/attachments-2018-07-Mz7UCtYZ5b500821eb556-2024-02-28.webp"
alt="attachments-2018-07-Mz7UCtYZ5b500821eb556-2024-02-28" width="auto" loading="lazy">
&lt;/figure>
&lt;h2 id="inkscape-vector-graphics-software">Inkscape vector graphics software&lt;/h2>
&lt;p>AI&amp;rsquo;s alternative version, with the advantage of being &lt;strong>open source and free&lt;/strong>. As a leader in the open-source vector graphics editor community, Inkscape offers a complete set of SVG editing tools, allowing researchers to use it for free to create complex vector diagrams, ensuring cross-platform compatibility and lossless scalability. Official Chinese site: &lt;a class="link" href="https://inkscape.org/zh-hans/" target="_blank" rel="noopener" >Inkscape
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/INKSCAPE-2024-02-28.webp"
alt="INKSCAPE-2024-02-28" width="auto" loading="lazy">
&lt;/figure>
&lt;hr>
&lt;ul>
&lt;li>
&lt;p>Detailed introduction: &lt;a class="link" href="https://zhuanlan.zhihu.com/p/642526806" target="_blank" rel="noopener" >Inkscape - Free open-source, cross-platform vector graphics design software, alternative to Adobe Illustrator (AI) and CorelDRAW
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Recommended video tutorial:&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="video-wrapper">
&lt;iframe src="https://player.bilibili.com/player.html?autoplay=0&amp;as_wide=1&amp;amp;high_quality=1&amp;amp;page=1&amp;bvid=BV1mA411e7FM"
scrolling="no"
frameborder="no"
framespacing="0"
allowfullscreen="true"
>
&lt;/iframe>
&lt;/div></description></item></channel></rss>