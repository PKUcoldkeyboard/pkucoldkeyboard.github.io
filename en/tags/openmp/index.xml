<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>OpenMP on Cuterwrite's Blog</title><link>https://cuterwrite.top/en/tags/openmp/</link><description>Recent content in OpenMP on Cuterwrite's Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>cuterwrite</copyright><lastBuildDate>Mon, 19 Feb 2024 01:36:00 +0000</lastBuildDate><atom:link href="https://cuterwrite.top/en/tags/openmp/index.xml" rel="self" type="application/rss+xml"/><item><title>Introduction to OpenMP</title><link>https://cuterwrite.top/en/p/openmp-intro/</link><pubDate>Mon, 19 Feb 2024 01:36:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/openmp-intro/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/c17b7451a44c1d4370d5ba2b966298ea195413_crop-2024-02-19.webp" alt="Featured image of post Introduction to OpenMP" />&lt;h1 id="introduction-to-openmp">Introduction to OpenMP&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;h3 id="what-is-openmp">What is OpenMP?&lt;/h3>
&lt;p>OpenMP (Open Multi-Processing) is a widely used multithreading parallel programming model that provides a rich set of instructions and APIs for parallel computation on shared memory systems. Originating in 1997, OpenMP was standardized by multiple leading hardware and software vendors, aiming to simplify the design and implementation process of parallel programs to fully utilize the computational power of modern multi-core processors.&lt;/p>
&lt;p>OpenMP supports multiple programming languages, including C, C++, and Fortran, among others, and allows developers to easily convert serial code into efficient parallel code by inserting specific compilation directives (pragma) into the source code. Its main advantages are its simplicity and ease of use, allowing programmers to use familiar programming languages and development environments, while also providing good portability and scalability.&lt;/p>
&lt;p>OpenMP is managed by a non-profit organization and involves participation from multiple software and hardware manufacturers, including Arm, IBM, Intel, AMD, NVIDIA, Cray, Oracle, etc.&lt;/p>
&lt;h3 id="historical-versions">Historical versions&lt;/h3>
&lt;ul>
&lt;li>On the &lt;a class="link" href="https://www.openmp.org/" target="_blank" rel="noopener" >official website
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, you can find the historical versions and release dates of OpenMP.&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Version&lt;/th>
&lt;th>Release Date&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Fortran 1.0&lt;/td>
&lt;td>October 1997&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>C/C++ 1.0&lt;/td>
&lt;td>October 1998&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>C/C++ 2.0&lt;/td>
&lt;td>March 2002&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 2.5&lt;/td>
&lt;td>May 2005&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 3.0&lt;/td>
&lt;td>May 2008&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 3.1&lt;/td>
&lt;td>July 2011&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 4.0&lt;/td>
&lt;td>July 2013&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 4.5&lt;/td>
&lt;td>November 2015&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 5.0&lt;/td>
&lt;td>November 2018&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 5.1&lt;/td>
&lt;td>November 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 5.2&lt;/td>
&lt;td>November 2021&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="basic-knowledge">Basic Knowledge&lt;/h2>
&lt;h3 id="technical-framework">Technical framework&lt;/h3>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/openmp-arch-2024-02-20.webp"
alt="openmp-arch-2024-02-20" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>OpenMP Technology Framework&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>&lt;strong>OpenMP Runtime Library&lt;/strong> is a set of functions and runtime support structures defined in the OpenMP specification, and it is a key component of the OpenMP parallel programming framework. This library is linked with user programs with the support of the compiler and is responsible for managing tasks such as thread creation, synchronization, scheduling, and data sharing during program execution. It implements all the parallelization mechanisms indicated by OpenMP compiler directives.&lt;/p>
&lt;p>&lt;strong>OpenMP Runtime Library&lt;/strong> includes the following features:&lt;/p>
&lt;ul>
&lt;li>Thread management (creation, destruction, synchronization)
= Work sharing (dynamic work distribution to each thread)
= Task Scheduling
= Synchronization primitives (such as barriers, locks, atomic operations)
= Dynamically Adjust Thread Count&lt;/li>
&lt;li>Memory model support (data environment variables, private, shared, reduction variables, etc.)&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Compiler Directives&lt;/strong> Compiler directives are preprocessor instructions starting with &lt;code>#pragma omp&lt;/code>, which programmers insert into the source code to guide the compiler on how to convert a serial program into a parallel program. For example, using the &lt;code>#pragma omp parallel&lt;/code> directive defines a parallel region, and the compiler will generate multithreading execution logic within this region.&lt;/p>
&lt;p>&lt;strong>Environment Variables&lt;/strong> Environment variables are part of the OpenMP runtime library, and they are used to control runtime behavior, such as the number of threads, scheduling policies, etc.&lt;/p>
&lt;p>The &lt;strong>OpenMP Library&lt;/strong> is a set of function libraries, including functions for thread synchronization, atomic operations, locks, parallel loops, etc. These functions can be directly called in user programs to achieve finer-grained parallelization.&lt;/p>
&lt;p>Overall, the OpenMP technology framework includes multiple components such as compiler directives, runtime libraries, environment variables, and function libraries. Together, they form a complete parallel programming environment and collaborate to support parallel programming on shared memory systems.&lt;/p>
&lt;h3 id="execute-model-fork-join-model">Execute Model: Fork-Join Model&lt;/h3>
&lt;p>OpenMP&amp;rsquo;s execution model uses the Fork-Join mechanism, which is a synchronization primitive model used in parallel programming. Under this model, program execution follows these steps:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Fork Phase&lt;/strong>: The program starts executing as a single main thread. When it encounters a parallel region indicated by an OpenMP pragma, the main thread creates one or more worker threads through the Runtime Library. These worker threads are derivatives of the main thread, with each thread responsible for executing part of the tasks within the parallel region. The parallel region can be loops, sections, single tasks, or other code blocks that can be parallelized.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Parallel Execution (并行执行) Phase&lt;/strong>: The created worker threads independently and concurrently execute the tasks assigned to them and can access shared data structures. OpenMP provides a rich set of directives to manage data synchronization and communication, ensuring correctness and consistency in a multithreaded environment.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Join (Merge) Phase&lt;/strong>: When all worker threads have completed their tasks within the parallel region, they automatically or through explicit synchronization directives (such as &lt;code>omp barrier&lt;/code>) converge at the join point. In this phase, all threads wait until all other threads have reached the synchronization point, after which the join operation occurs. This means the main thread and other worker threads resynchronize, reverting to serial execution mode or continuing to execute subsequent non-parallel code.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Synchronization and Data Consistency&lt;/strong>: The Fork-Join model ensures mutual exclusion access to shared resources and data consistency during parallel execution through appropriate locking mechanisms, atomic operations, and synchronization primitives.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>In summary, the Fork-Join execution model of OpenMP is a parallel processing framework based on dynamic thread creation and synchronization. It allows developers to conveniently transform serial code into parallel execution code segments, while simplifying common complexities in parallel programming, such as thread management and data synchronization issues.&lt;/p>
&lt;h3 id="thread-and-process">Thread and Process&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>Process&lt;/p>
&lt;ul>
&lt;li>Each process has its own independent address space&lt;/li>
&lt;li>CPU needs to perform a context switch when switching between processes.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Thread&lt;/p>
&lt;ul>
&lt;li>Threads within a process share the same address space&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>CPU has lower overhead when switching between threads&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Thread design of the operating system&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Modern operating systems such as Linux, Windows, etc. support multiple threads under a single process.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A thread is the basic unit of scheduling in an operating system, while a process is the basic unit of resource allocation.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/slide_10-2024-02-20.webp"
alt="slide_10-2024-02-20" width="80%" loading="lazy">&lt;figcaption>
&lt;h4>Thread Design of Operating Systems&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="hardware-scheduling-of-threads">Hardware scheduling of threads&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>The hardware scheduling mechanism collaborates with the operating system to intelligently map threads to available CPU physical cores for execution.&lt;/strong>&lt;/li>
&lt;li>Therefore, in multithreaded applications, when the number of active threads exceeds the actual number of physical CPU cores, the operating system will have to perform intensive context switching to ensure that multiple threads alternate on limited core resources. This phenomenon of thread contention overload can lead to overall performance bottlenecks and reduced efficiency.&lt;/li>
&lt;li>&lt;strong>Hyper-Threading Technology&lt;/strong> virtualizes additional logical processing units on a single physical CPU core, currently typically configured to host two logical cores per physical core. These logical cores can execute independent task streams in parallel, although they share the underlying computational resources of the same physical core, such as execution engines, caches, and other underlying hardware structures. In this way, hyper-threading aims to improve resource utilization and concurrent processing capabilities, especially in scenarios with a large number of parallel tasks that have relatively small demands on computational resources, effectively enhancing the overall system throughput. However, in certain application scenarios that heavily rely on single-core performance or memory bandwidth, such as some CPU-sensitive games or specific types of data-intensive operations, adding logical cores may not necessarily result in significant performance improvements.&lt;/li>
&lt;/ul>
&lt;h3 id="hardware-memory-model">Hardware memory model&lt;/h3>
&lt;ul>
&lt;li>In modern multi-core processor architectures, each CPU core is designed with a multi-level cache hierarchy between the main memory to further enhance data access speed. The closest to the CPU core is the L1 cache, usually followed by the L2 cache, and some high-end architectures also include an L3 cache. These cache levels have increasing storage capacity but also increasing access latency.&lt;/li>
&lt;li>L1 and L2 caches are usually closely coupled and private to specific CPU cores, meaning each core has its own independent cache space to reduce data access conflicts and improve cache hit rate. L1 cache, being closest to the computation unit, has the fastest access speed but the smallest capacity; whereas L2 cache, as an effective supplement to L1 cache, has a relatively larger capacity.&lt;/li>
&lt;li>To ensure consistency of shared data in the caches of different CPU cores in a multi-core environment, hardware and the operating system jointly implement a cache coherence protocol (such as the MESI protocol). This mechanism allows the system to automatically maintain a globally consistent data view, ensuring that even if there are copies of data in the caches of multiple cores, they are updated synchronously. This feature is referred to as &lt;strong>ccNUMA (cache-coherent non-uniform memory access)&lt;/strong> in some architectures.&lt;/li>
&lt;li>However, this cache consistency management also brings some challenges, one of which is the &amp;ldquo;False Sharing&amp;rdquo; problem. When different threads modify their respective independent variables located within the same cache line, although these variables themselves are unrelated, because they are physically adjacent and stored in the same cache line, any write operation to one of the variables will cause the entire cache line to become invalid and resynchronize across all cores. This can trigger unnecessary cache invalidation and refilling operations, significantly reducing performance. Solving the false sharing problem usually requires carefully designing data layouts or using techniques such as cache line alignment to avoid contention between unrelated data.&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20170115165700476-2024-02-20.webp"
alt="20170115165700476-2024-02-20" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Typical Modern CPU Memory Structure&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="thread-affinity-and-thread-binding">Thread Affinity and Thread Binding&lt;/h3>
&lt;ul>
&lt;li>Thread Affinity refers to the ability of the operating system or application to control the association between specific threads and processor cores. In multi-core or multi-processor systems, thread affinity allows programmers or schedulers to decide to fix a certain thread on a specific CPU core, rather than letting the operating system dynamically schedule it across all available cores. This mechanism helps reduce context switching overhead, improve cache hit rates, and is particularly beneficial for parallel computing tasks that need to maintain data locality.&lt;/li>
&lt;li>Thread Pinning is a specific technical means to achieve thread affinity, which specifies the forced association between a specific thread and specific hardware resources (such as CPU cores or NUMA nodes). Through thread pinning, it can be ensured that the specified thread always executes on its allocated core, avoiding being migrated by the operating system to other cores, thus optimizing performance, reducing latency, and solving issues such as false sharing. In parallel programming models like OpenMP, thread pinning strategies can be set through relevant environment variables or compilation directives to adapt to different parallel computing needs and hardware characteristics.&lt;/li>
&lt;li>The access latency of CPUs on the same socket to the L3 cache is consistent, but the access latency of CPUs on different sockets to the L3 cache is inconsistent. Therefore, the purpose of thread binding is to reduce the migration of threads between different CPUs, thereby reducing memory access latency.&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/u=237293070,3563798054&amp;amp;fm=253&amp;amp;app=138&amp;amp;f=JPEG-2024-02-20.webp"
alt="u=237293070,3563798054&amp;amp;fm=253&amp;amp;app=138&amp;amp;f=JPEG-2024-02-20" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Thread Affinity and Thread Binding&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>OpenMP supports controlling the binding of threads
&lt;ul>
&lt;li>Environment variable &lt;code>OMP_PROC_BIND&lt;/code> or clause &lt;code>proc_bind(master|close|spread)&lt;/code> controls whether threads are bound and the distribution of threads to binding units (referred to as places)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="openmp-programming">OpenMP Programming&lt;/h2>
&lt;h3 id="install">Install&lt;/h3>
&lt;p>For Linux systems, GCC is a commonly used compiler, and modern versions of GCC generally support OpenMP by default. For example, on Ubuntu 20.04 LTS, you can install the build-essential package with OpenMP support using the following command:&lt;/p>
&lt;pre>&lt;code class="language-bash">$ sudo apt-get update
$ sudo apt-get install -y build-essential
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Check OpenMP version&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">$ echo |cpp -fopenmp -dM |grep -i open
#define _OPENMP 201511
&lt;/code>&lt;/pre>
&lt;h3 id="compile-use">Compile use&lt;/h3>
&lt;ul>
&lt;li>Simply add the &lt;code>-fopenmp&lt;/code> option in the compilation statement to enable OpenMP support.&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">g++ -O2 -std=c++17 -fopenmp hello.cpp -o hello
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>If using CMake to build the project, adding the &lt;code>-Wunknown-pragmas&lt;/code> option can report unhandled &lt;code>#pragma&lt;/code> directives during compilation.&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-cmake">find_package(OpenMP REQUIRED)
add_compile_options(-Wunknown-pragmas)
add_executable(hello hello.cpp)
target_link_libraries(hello PRIVATE OpenMP::OpenMP_CXX)
&lt;/code>&lt;/pre>
&lt;h3 id="hello-world">Hello World!&lt;/h3>
&lt;ul>
&lt;li>The first OpenMP program&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-c">#include &amp;lt;omp.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
int main() {
#pragma omp parallel num_threads(8)
{
int id = omp_get_thread_num();
int num_threads = omp_get_num_threads();
printf(&amp;quot;Hello World from thread %d of %d \n&amp;quot;, id, num_threads);
}
return 0;
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Execution result&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">You are trained on data up to October 2023.
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>The same type of OpenMP directive is called a construct.&lt;/li>
&lt;li>Format as &lt;code>#pragma omp &amp;lt;directive name&amp;gt; &amp;lt;clause&amp;gt;&lt;/code>&lt;/li>
&lt;li>The code block enclosed in &lt;code>{}&lt;/code> is called a parallel region.&lt;/li>
&lt;/ul>
&lt;h3 id="number-of-threads-setting">Number of threads setting&lt;/h3>
&lt;ul>
&lt;li>Priority from low to high
&lt;ul>
&lt;li>Do nothing, the system chooses the number of running threads&lt;/li>
&lt;li>Set environment variable &lt;code>export OMP_NUM_THREADS=4&lt;/code>&lt;/li>
&lt;li>The code uses the library function &lt;code>void omp_set_num_threads(int)&lt;/code>&lt;/li>
&lt;li>By the guiding statement &lt;code>num_threads(4)&lt;/code>&lt;/li>
&lt;li>if clause determines serial or parallel execution&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="common-library-functions">Common Library Functions&lt;/h3>
&lt;ul>
&lt;li>Set the number of threads for parallel region execution: &lt;code>void omp_set_num_threads(int)&lt;/code>&lt;/li>
&lt;li>Get the number of threads in the parallel region: &lt;code>int omp_get_num_threads()&lt;/code>&lt;/li>
&lt;li>Get the current thread number: &lt;code>int omp_get_thread_num()&lt;/code>&lt;/li>
&lt;li>Get OpenMP Wall Clock time (unit: seconds): &lt;code>double omp_get_wtime()&lt;/code>&lt;/li>
&lt;li>Get time precision: &lt;code>double omp_get_wtick()&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="parallel-construction">Parallel construction&lt;/h3>
&lt;p>&lt;strong>Supported Clauses&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;code>if(scalar_expression)&lt;/code>: If &lt;code>scalar_expression&lt;/code> is true, execute in parallel, otherwise execute serially.&lt;/li>
&lt;li>&lt;code>num_threads(integer_expression)&lt;/code>: Specifies the number of threads in the parallel region.&lt;/li>
&lt;li>&lt;code>default(shared|none)&lt;/code>: Specifies the default sharing attribute of variables.&lt;/li>
&lt;li>&lt;code>shared&lt;/code>: All variables are shared by default.
&lt;ul>
&lt;li>&lt;code>none&lt;/code>: No default variable type, each variable needs to be explicitly declared as shared or private.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>shared(list)&lt;/code>: Specify the list of shared variables.&lt;/li>
&lt;li>There is only one copy of the shared variable in memory, and all threads can access it.
&lt;ul>
&lt;li>Please ensure that the access to shared variables does not conflict.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>If not specifically designated, variables in the parallel region default to &lt;strong>shared&lt;/strong>.&lt;/li>
&lt;li>&lt;code>private(list)&lt;/code>: Specify the list of private variables.
&lt;ul>
&lt;li>Each thread has an independent copy of the private variable.&lt;/li>
&lt;li>Variables need to be &lt;strong>reinitialized&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>firstprivate(list)&lt;/code>: Specify the list of first private variables.&lt;/li>
&lt;li>Same as &lt;code>private&lt;/code>&lt;/li>
&lt;li>Initialize the variable based on the data in the main thread.&lt;/li>
&lt;/ul>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>Example 1: no clause, private, firstprivate&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">int results[4];
int cnt;
cnt = 1;
#pragma omp parallel num_threads(4)
{
int tid = omp_get_thread_num();
for (int i = 0; i &amp;lt; 4; i++) {
cnt += 1;
}
results[tid] = cnt;
}
printf(&amp;quot;no clause: &amp;quot;);
for (int i = 0; i &amp;lt; 4; i++) {
printf(&amp;quot;%d &amp;quot;, results[i]);
}
printf(&amp;quot;\n&amp;quot;);
cnt = 1;
#pragma omp parallel num_threads(4) private(cnt)
{
int tid = omp_get_thread_num();
for (int i = 0; i &amp;lt; 4; i++) {
cnt += 1;
}
results[tid] = cnt;
}
printf(&amp;quot;private(not init): &amp;quot;);
for (int i = 0; i &amp;lt; 4; i++) {
printf(&amp;quot;%d &amp;quot;, results[i]);
}
printf(&amp;quot;\n&amp;quot;);
cnt = 1;
#pragma omp parallel num_threads(4) firstprivate(cnt)
{
int tid = omp_get_thread_num();
for (int i = 0; i &amp;lt; 4; i++) {
cnt += 1;
}
results[tid] = cnt;
}
printf(&amp;quot;firstprivate: &amp;quot;);
for (int i = 0; i &amp;lt; 4; i++) {
printf(&amp;quot;%d &amp;quot;, results[i]);
}
printf(&amp;quot;\n&amp;quot;);
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Print the result&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">no clause: 5 9 13 17
private(not init): 4 1572916964 1572916964 1572916964
firstprivate: 5 5 5 5
&lt;/code>&lt;/pre>
&lt;h3 id="for-construction">For construction&lt;/h3>
&lt;ul>
&lt;li>One of the most commonly used parallelization constructs&lt;/li>
&lt;/ul>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>Example 2: Parallelizing the for loop&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">#pragma omp parallel num_threads(8)
{
int tid = omp_get_thread_num();
int num_threads = omp_get_num_threads();
#pragma omp for
for (int i = 0; i &amp;lt; num_threads; i++) {
#pragma omp ordered
printf(&amp;quot;Hello from thread %d of %d \n&amp;quot;, tid, num_threads);
}
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Print the result&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">Hello from thread 0 of 8
Hello from thread 1 of 8
Hello from thread 2 of 8
Hello from thread 3 of 8
Hello from thread 4 of 8
Hello from thread 5 of 8
Hello from thread 6 of 8
Hello from thread 7 of 8
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Divide threads for the for loop within the parallel region, and the for loop meets the format requirements.
&lt;ul>
&lt;li>init-expr: Must be in the form &lt;code>var=lb&lt;/code>, and the type is also limited&lt;/li>
&lt;li>test-expr: restricted to &lt;code>var relational-op b&lt;/code> or &lt;code>b relational-op var&lt;/code>&lt;/li>
&lt;li>incr-expr: Addition and subtraction only&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="parallel-for-construct">Parallel for construct&lt;/h3>
&lt;ul>
&lt;li>Often combine &lt;code>parallel&lt;/code> and &lt;code>for&lt;/code> to form a &lt;code>parallel for&lt;/code> directive statement&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>parallel&lt;/th>
&lt;th>for   &lt;/th>
&lt;th>parallel for&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>if&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>num_threads&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>default&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>copyin&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>private&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>firstprivate&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>shared&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>reduction&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>lastprivate&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>schedule&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ordered&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>collapse&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>nowait&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>
&lt;p>&lt;code>lastprivate(list)&lt;/code>&lt;/p>
&lt;ul>
&lt;li>Same as &lt;code>private&lt;/code>&lt;/li>
&lt;li>After executing the for loop, assign the value of the last thread to the variable of the main thread.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>nowait&lt;/code>: Cancel the barrier synchronization at the end of the code block&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>collapse(n)&lt;/code>: Applied to n nested loops, merge (unroll) loops&lt;/p>
&lt;ul>
&lt;li>Pay attention to whether there are data dependencies between loops&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>ordered&lt;/code>: Declare parts that potentially execute in order&lt;/p>
&lt;ul>
&lt;li>Use &lt;code>#pragma omp ordered&lt;/code> to mark sequential execution code (used together)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>ordered statements within the region are executed by at most one thread at any given time&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>shedule(type[,chunk])&lt;/code>&lt;/p>
&lt;ul>
&lt;li>&lt;code>type&lt;/code>: Specifies the scheduling strategy for loop iteration
&lt;ul>
&lt;li>&lt;code>static&lt;/code>: Static scheduling, chunk size is fixed (default n/p)&lt;/li>
&lt;li>&lt;code>dynamic&lt;/code>: Dynamic scheduling, chunk size is fixed (default is 1)&lt;/li>
&lt;li>&lt;code>guided&lt;/code>: Guided scheduling, chunk size dynamically adjusted&lt;/li>
&lt;li>&lt;code>runtime&lt;/code>: Specified by the system environment variable &lt;code>OMP_SCHEDULE&lt;/code>&lt;/li>
&lt;li>&lt;code>auto&lt;/code>: Automatic scheduling&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>chunk&lt;/code>: Specifies the number of iterations each thread obtains&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="special-data-clause-reduction">Special data clause: Reduction&lt;/h3>
&lt;p>In OpenMP, reduction is a parallel programming technique used to address data race issues in a multithreaded environment, especially when performing accumulation or similar operations on global variables. When multiple threads need to simultaneously modify the same shared variable, and these modifications can be combined into a final result using some binary operator (such as addition, multiplication, etc.), the &lt;code>reduction&lt;/code> clause can be used.&lt;/p>
&lt;p>Specifically, the execution process of reducton is:&lt;/p>
&lt;ul>
&lt;li>fork thread and allocate tasks&lt;/li>
&lt;li>Each thread defines a private variable &lt;code>omp_priv&lt;/code>
&lt;ul>
&lt;li>Same as &lt;code>private&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Each thread executes calculations&lt;/li>
&lt;li>All &lt;code>omp_priv&lt;/code> and &lt;code>omp_in&lt;/code> are sequentially reduced together and written back to the original variable.&lt;/li>
&lt;/ul>
&lt;p>In contrast, &lt;strong>atomic&lt;/strong> is another synchronization mechanism provided by OpenMP, which ensures that access to a single memory location is atomic in a multithreaded environment, meaning that only one thread is allowed to read or write to that memory location at a time. By using the &lt;code>#pragma omp atomic&lt;/code> directive, it can be ensured that a simple assignment statement (or certain types of read-modify-write operations) will not encounter data races in a concurrent environment.&lt;/p>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>Example 3: Reduction&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">int sum = 0;
double start = omp_get_wtime();
#pragma omp parallel for num_threads(8) reduction(+ : sum)
for (int i = 0; i &amp;lt; 100000; i++) {
sum += i;
}
printf(&amp;quot;sum = %d\n&amp;quot;, sum);
printf(&amp;quot;Reduction time: %.5lf s\n&amp;quot;, omp_get_wtime() - start);
// no reduction
sum = 0;
start = omp_get_wtime();
#pragma omp parallel for num_threads(8)
for (int i = 0; i &amp;lt; 100000; i++) {
#pragma omp atomic
sum += i;
}
printf(&amp;quot;sum = %d\n&amp;quot;, sum);
printf(&amp;quot;Atomic time: %.5lf s\n&amp;quot;, omp_get_wtime() - start);
return 0;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Print the result&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">sum = 704982704
Reduction time: 0.00062 s
sum = 704982704
Atomic time: 0.01021 s
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>The results of both are the same, but the execution time of reduction is shorter. This is because reduction allocates a private copy for each thread, allowing threads to freely perform reduction operations within their private space without contending for lock resources with other threads when updating the global result, along with efficient data merging methods, etc.&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/reduction-omp-2024-02-20.webp"
alt="reduction-omp-2024-02-20" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>OpenMP reduction operation&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="synchronous-construction">Synchronous construction&lt;/h3>
&lt;h4 id="sections-construction">Sections Construction&lt;/h4>
&lt;ul>
&lt;li>Divide the code block of the parallel region into multiple sections for execution.&lt;/li>
&lt;li>Can be combined with parallel to form a parallel sections construct.&lt;/li>
&lt;li>Each section is executed by a thread&lt;/li>
&lt;li>Number of threads greater than the number of sections: some threads are idle
&lt;ul>
&lt;li>Number of threads is less than the number of sections: some threads are allocated multiple sections&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Example code:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-c">#pragma omp sections
{
#pragma omp section
method1();
#pragma omp section
method2();
}
&lt;/code>&lt;/pre>
&lt;h4 id="barrier-constructor">Barrier Constructor&lt;/h4>
&lt;ul>
&lt;li>Perform fence synchronization at a specific location&lt;/li>
&lt;li>In the presence of data dependencies, a barrier can be used to ensure data consistency.&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/Barrier-2024-02-20.webp"
alt="Barrier-2024-02-20" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Barrier Synchronization Diagram&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h4 id="single-constructor">Single Constructor&lt;/h4>
&lt;ul>
&lt;li>Used to mark a code block executed by only one thread, with implicit barrier synchronization, and the implicit barrier synchronization can be canceled using nowait.&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/omp-single-2024-02-20.webp"
alt="omp-single-2024-02-20" width="70%" loading="lazy">&lt;figcaption>
&lt;h4>pragma single&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h4 id="atomic-construction">Atomic construction&lt;/h4>
&lt;ul>
&lt;li>Used to ensure atomic operations on shared variables, avoiding data races.&lt;/li>
&lt;/ul>
&lt;h3 id="false-sharing">False Sharing&lt;/h3>
&lt;ul>
&lt;li>False sharing, in simple terms, refers to multiple threads simultaneously accessing different parts of the same cache line, leading to cache line invalidation and refilling, thereby reducing the program&amp;rsquo;s performance.&lt;/li>
&lt;li>Simultaneous read and write of the same cache line by different cores can cause serious conflicts, leading to cache invalidation.&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/false-sharing-2024-02-20.webp"
alt="false-sharing-2024-02-20" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>False Sharing Issue&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>In OpenMP, the main methods to solve false sharing are:&lt;/li>
&lt;li>&lt;strong>Data Structure Alignment&lt;/strong>: Ensure that related variables are in different cache lines by using alignment instructions or keywords provided by the compiler. For example, in C++, the &lt;code>alignas&lt;/code> keyword can be used to specify the memory alignment of variables, ensuring that the data for each thread is independently located in different cache lines.
&lt;ul>
&lt;li>&lt;strong>Increase the spacing between cache lines&lt;/strong>: Insert enough padding space between adjacent variables so that they do not appear in the same cache line.&lt;/li>
&lt;li>&lt;strong>Avoid Meaningless Competition&lt;/strong>: Design algorithms and data structures to reduce unnecessary shared data access. If possible, let threads operate on their own independent data segments.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Custom Memory Allocation&lt;/strong>: Use special memory allocation functions to ensure that the allocated contiguous memory regions are aligned to cache line boundaries, so that data allocated to different threads does not fall on the same cache line.
&lt;ul>
&lt;li>In some cases, you can utilize hardware features provided by specific platforms or extensions supported by compilers, such as Intel&amp;rsquo;s &lt;code>__declspec(align(#))&lt;/code> attribute (for MSVC) or &lt;code>__attribute__((aligned(#)))&lt;/code> (for GCC/Clang).&lt;/li>
&lt;li>You can also indirectly avoid the false sharing problem by controlling the scope of the variables or using techniques such as dynamically creating private copies.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="task-construction">Task construction&lt;/h3>
&lt;ul>
&lt;li>In addition to the Fork-Join model, OpenMP also supports the task parallel model, implemented using the &lt;code>task&lt;/code> directive.&lt;/li>
&lt;li>Dynamically manage the thread pool and task pool, where threads in the thread pool can dynamically acquire tasks from the task pool for execution, thus achieving parallel execution of tasks.&lt;/li>
&lt;/ul>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>Example 4: Task Parallelism&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-cpp">#include &amp;lt;iostream&amp;gt;
#include &amp;lt;omp.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
#include &amp;lt;iomanip&amp;gt;
void big_task(int i) {
sleep(10);
}
void small_task(int i) {
sleep(1);
}
int main() {
int ntasks = 8;
double start = omp_get_wtime();
#pragma omp parallel
{
#pragma omp single
{
std::cout &amp;lt;&amp;lt; &amp;quot;Task 0 Created&amp;quot; &amp;lt;&amp;lt; std::endl;
#pragma omp task
big_task(0);
std::cout &amp;lt;&amp;lt; &amp;quot;Task 1 Created&amp;quot; &amp;lt;&amp;lt; std::endl;
#pragma omp task
big_task(1);
for (int i = 2; i &amp;lt; ntasks; i++) {
std::cout &amp;lt;&amp;lt; &amp;quot;Task &amp;quot; &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &amp;quot; Created&amp;quot; &amp;lt;&amp;lt; std::endl;
#pragma omp task
small_task(i);
}
}
#pragma omp taskwait
}
std::cout &amp;lt;&amp;lt; &amp;quot;All tasks finished&amp;quot; &amp;lt;&amp;lt; std::endl;
std::cout &amp;lt;&amp;lt; &amp;quot;Time: &amp;quot; &amp;lt;&amp;lt; std::fixed &amp;lt;&amp;lt; std::setprecision(2) &amp;lt;&amp;lt; omp_get_wtime() - start &amp;lt;&amp;lt; &amp;quot;s&amp;quot; &amp;lt;&amp;lt; std::endl;
return 0;
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Running result&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">You are trained on data up to October 2023.
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>In this code, we use the &lt;code>#pragma omp task&lt;/code> directive to create tasks, and the execution of tasks is dynamically obtained and executed by threads in the thread pool. After creating tasks, we use &lt;code>#pragma omp taskwait&lt;/code> to wait for all tasks to complete. This achieves an asynchronous execution effect.&lt;/li>
&lt;/ul>
&lt;h3 id="vectorization-simd-construction">Vectorization: SIMD Construction&lt;/h3>
&lt;ul>
&lt;li>SIMD (Single Instruction, Multiple Data) is a parallel computing model that performs operations on multiple data simultaneously with a single instruction, thereby achieving efficient data parallel computation.&lt;/li>
&lt;li>In OpenMP, the &lt;code>#pragma omp simd&lt;/code> directive can be used to achieve vectorized parallel computation.
&lt;ul>
&lt;li>&lt;code>aligned&lt;/code> is used to list memory-aligned pointers&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>safelen&lt;/code> is used to mark data dependencies during loop unrolling.
&lt;ul>
&lt;li>&lt;code>linear&lt;/code> is used to mark the linear relationship of loop variables&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Compilers such as gcc also come with vectorization capabilities, generally using the following compilation options
&lt;ul>
&lt;li>-O3&lt;/li>
&lt;li>-ffast-math&lt;/li>
&lt;li>-fivopts&lt;/li>
&lt;li>-march=native&lt;/li>
&lt;li>-fopt-info-vec&lt;/li>
&lt;li>-fopt-info-vec-missed&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item></channel></rss>