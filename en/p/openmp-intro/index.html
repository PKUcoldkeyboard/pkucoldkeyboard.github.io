<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="OpenMP (Open Multi-Processing) is a widely used multithreading parallel programming model that provides a rich set of instructions and APIs for parallel computing on shared memory systems. Originating in 1997, OpenMP was standardized by multiple leading hardware and software vendors with the aim of simplifying the design and implementation process of parallel programs to fully utilize the computing power of modern multi-core processors. This article will introduce the basics and programming techniques of OpenMP."><title>Introduction to OpenMP</title>
<link rel=canonical href=https://cuterwrite.top/en/p/openmp-intro/><link rel=stylesheet href=/scss/style.min.9e9a820f30d9af5db6f416de7ab0f7a731a8bcab6669edcbd5a489a07906aa5d.css><meta property='og:title' content="Introduction to OpenMP"><meta property='og:description' content="OpenMP (Open Multi-Processing) is a widely used multithreading parallel programming model that provides a rich set of instructions and APIs for parallel computing on shared memory systems. Originating in 1997, OpenMP was standardized by multiple leading hardware and software vendors with the aim of simplifying the design and implementation process of parallel programs to fully utilize the computing power of modern multi-core processors. This article will introduce the basics and programming techniques of OpenMP."><meta property='og:url' content='https://cuterwrite.top/en/p/openmp-intro/'><meta property='og:site_name' content="Cuterwrite's Blog"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='OpenMP'><meta property='article:tag' content='并行计算'><meta property='article:published_time' content='2024-02-19T01:36:00+00:00'><meta property='article:modified_time' content='2024-02-19T01:36:00+00:00'><meta property='og:image' content='https://cuterwrite-1302252842.file.myqcloud.com/img/c17b7451a44c1d4370d5ba2b966298ea195413_crop-2024-02-19.webp'><meta name=twitter:title content="Introduction to OpenMP"><meta name=twitter:description content="OpenMP (Open Multi-Processing) is a widely used multithreading parallel programming model that provides a rich set of instructions and APIs for parallel computing on shared memory systems. Originating in 1997, OpenMP was standardized by multiple leading hardware and software vendors with the aim of simplifying the design and implementation process of parallel programs to fully utilize the computing power of modern multi-core processors. This article will introduce the basics and programming techniques of OpenMP."><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://cuterwrite-1302252842.file.myqcloud.com/img/c17b7451a44c1d4370d5ba2b966298ea195413_crop-2024-02-19.webp'><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><script async src=https://analytics.cuterwrite.top/uma.js data-website-id=b13594a2-4d15-4a4e-a020-5e3cc1d88c12 data-domains=cuterwrite.top></script><link rel=manifest href=/manifest.json></head><body class="article-page
line-numbers"><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/en/><img src=/img/avatar_hue4d14694a57c01a222a16c47db12c89c_369633_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>😉</span></figure><div class=site-meta><h1 class=site-name><a href=/en>Cuterwrite's Blog</a></h1><h2 class=site-description>Cuterwrite's tech blog, focusing on in-depth exploration and experience sharing in areas such as high-performance computing, operating systems, full-stack development, and artificial intelligence.</h2></div></header><ol class=menu-social><li><a href=https://analytics.cuterwrite.top/share/Ji0gm9OaLDk8gco7 target=_blank title=Analytics rel=me><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 5H7A2 2 0 005 7v12a2 2 0 002 2h10a2 2 0 002-2V7a2 2 0 00-2-2h-2"/><rect x="9" y="3" width="6" height="4" rx="2"/><path d="M9 17v-5"/><path d="M12 17v-1"/><path d="M15 17v-3"/></svg></a></li><li><a href=https://stats.uptimerobot.com/6NVhRHkSAQ target=_blank title=Uptime rel=me><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-chart-line"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 19h16"/><path d="M4 15l4-6 4 2 4-5 4 4"/></svg></a></li><li><a href=/index.xml target=_blank title=RSS rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-rss" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="5" cy="19" r="1"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg></a></li><li><a href=https://github.com/PKUcoldkeyboard target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.zhihu.com/people/kong-tiao-cheng-tai-lang-30-57 target=_blank title=zhihu rel=me><svg t="1705591931290" class="icon" viewBox="0 0 1280 1024" xmlns="http://www.w3.org/2000/svg" p-id="21048" width="32" height="32"><path d="M341.08 296.26v435.08l46.86.02 15.42 52.74 84.02-52.74h99.06V296.26H341.08zm195.5 387.86H480.7l-55.8 35.02-10.16-34.94-23.8-.08V343.5h145.64v340.62zM299.66 495.34H195c3.48-54.2 4.4-103.18 4.4-146.92h102.32s3.94-45.12-17.16-44.62h-177c6.98-26.24 15.74-53.32 26.24-81.34.0.0-48.14.0-64.54 43.14-6.78 17.8-26.42 86.28-61.4 156.24 11.78-1.28 50.74-2.36 73.68-44.42 4.22-11.78 5.02-13.32 10.28-29.06h57.74c0 21-2.4 133.76-3.36 146.88H41.66c-23.48.0-31.12 47.24-31.12 47.24H141.7C132.9 642.2 85.66 726.24.0 792.68c40.98 11.7 81.82-1.86 102-19.8.0.0 45.96-41.8 71.18-138.5L281.1 764.26s15.82-53.78-2.48-79.98c-15.16-17.84-56.12-66.12-73.58-83.62L175.8 623.9c8.72-27.96 13.98-55.1 15.74-81.34h123.3s-.18-47.24-15.18-47.24v.02zm824.04-3.2c41.66-51.28 89.96-117.14 89.96-117.14s-37.3-29.6-54.76-8.12c-12 16.3-73.66 96.4-73.66 96.4l38.46 28.86zM823.52 373.96c-18.02-16.5-51.82 4.26-51.82 4.26s79.04 110.08 82.24 114.9l38.92-27.46s-51.34-75.22-69.32-91.72h-.02zM1280 516.7c-39.56.0-261.82 1.86-262.12 1.86v-202c9.62.0 24.84-.8 45.7-2.4 81.76-4.82 140.26-8 175.54-9.62.0.0 24.44-54.38-1.18-66.88-6.14-2.36-46.34 9.16-46.34 9.16s-330.44 32.98-464.72 36.1c3.2 17.64 15.24 34.16 31.56 39.1 26.62 6.96 45.38 3.4 98.3 1.78 49.66-3.2 87.36-4.86 113.02-4.86v199.62H702.82s5.64 44.62 51.02 45.7h215.88V706.1c0 27.94-22.38 43.98-48.96 42.24-28.16.22-52.16-2.3-83.38-3.62 3.98 7.94 12.66 28.78 38.62 43.68 19.76 9.62 32.34 13.14 52.04 13.14 59.12.0 91.34-34.56 89.78-90.62V564.28h244.72c19.36.0 17.4-47.56 17.4-47.56l.06-.02z" fill="#707070" p-id="21049"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/en/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页 | Home</span></a></li><li><a href=/en/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>关于 | About</span></a></li><li><a href=/en/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>归档 | Archives</span></a></li><li><a href=/en/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>搜索 | Search</span></a></li><li><a href=https://cuterwrite.top/image-hosting target=_blank><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-album"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 4m0 2a2 2 0 012-2h12a2 2 0 012 2v12a2 2 0 01-2 2H6a2 2 0 01-2-2z"/><path d="M12 4v7l2-2 2 2V4"/></svg>
<span>图册 | Gallery</span></a></li><li><a href=https://draw.cuterwrite.top target=_blank><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-artboard"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M8 8m0 1a1 1 0 011-1h6a1 1 0 011 1v6a1 1 0 01-1 1H9a1 1 0 01-1-1z"/><path d="M3 8h1"/><path d="M3 16h1"/><path d="M8 3v1"/><path d="M16 3v1"/><path d="M20 8h1"/><path d="M20 16h1"/><path d="M8 20v1"/><path d="M16 20v1"/></svg>
<span>画板 | Canvas</span></a></li><li><a href=https://it-tools.tech target=_blank><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-tools"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 21h4L20 8a1.5 1.5.0 00-4-4L3 17v4"/><path d="M14.5 5.5l4 4"/><path d="M12 8 7 3 3 7l5 5"/><path d="M7 8 5.5 9.5"/><path d="M16 12l5 5-4 4-5-5"/><path d="M16 17l-1.5 1.5"/></svg>
<span>工具 | Tools</span></a></li><li class=menu-bottom-section><ol class=menu><li id=i18n-switch><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg>
<select name=language title=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://cuterwrite.top/>中文</option><option value=https://cuterwrite.top/en/ selected>English</option></select></li><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a><ul><li><a href=#what-is-openmp>What is OpenMP?</a></li><li><a href=#historical-versions>Historical versions</a></li></ul></li><li><a href=#basic-knowledge>Basic Knowledge</a><ul><li><a href=#technical-framework>Technical framework</a></li><li><a href=#execute-model-fork-join-model>Execute Model: Fork-Join Model</a></li><li><a href=#thread-and-process>Thread and Process</a></li><li><a href=#hardware-scheduling-of-threads>Hardware scheduling of threads</a></li><li><a href=#hardware-memory-model>Hardware memory model</a></li><li><a href=#thread-affinity-and-thread-binding>Thread Affinity and Thread Binding</a></li></ul></li><li><a href=#openmp-programming>OpenMP Programming</a><ul><li><a href=#install>Install</a></li><li><a href=#compile-use>Compile use</a></li><li><a href=#hello-world>Hello World!</a></li><li><a href=#number-of-threads-setting>Number of threads setting</a></li><li><a href=#common-library-functions>Common Library Functions</a></li><li><a href=#parallel-construction>Parallel construction</a></li><li><a href=#for-construction>For construction</a></li><li><a href=#parallel-for-construct>Parallel for construct</a></li><li><a href=#special-data-clause-reduction>Special data clause: Reduction</a></li><li><a href=#synchronous-construction>Synchronous construction</a><ul><li><a href=#sections-construction>Sections Construction</a></li><li><a href=#barrier-constructor>Barrier Constructor</a></li><li><a href=#single-constructor>Single Constructor</a></li><li><a href=#atomic-construction>Atomic construction</a></li></ul></li><li><a href=#false-sharing>False Sharing</a></li><li><a href=#task-construction>Task construction</a></li><li><a href=#vectorization-simd-construction>Vectorization: SIMD Construction</a></li></ul></li></ul></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/en/p/openmp-intro/><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/c17b7451a44c1d4370d5ba2b966298ea195413_crop-2024-02-19.webp loading=lazy alt="Featured image of post Introduction to OpenMP"></a></div><div class=article-details><header class=article-category><a href=/en/categories/hpc/ style=background-color:#ffb900;color:#fff>High Performance Computing</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/en/p/openmp-intro/>Introduction to OpenMP</a></h2><h3 class=article-subtitle>OpenMP (Open Multi-Processing) is a widely used multithreading parallel programming model that provides a rich set of instructions and APIs for parallel computing on shared memory systems. Originating in 1997, OpenMP was standardized by multiple leading hardware and software vendors with the aim of simplifying the design and implementation process of parallel programs to fully utilize the computing power of modern multi-core processors. This article will introduce the basics and programming techniques of OpenMP.</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>2024-02-19</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>8 minute read</time></div><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-keyboard"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M2 6m0 2a2 2 0 012-2h16a2 2 0 012 2v8a2 2 0 01-2 2H4a2 2 0 01-2-2z"/><path d="M6 10v.01"/><path d="M10 10v.01"/><path d="M14 10v.01"/><path d="M18 10v.01"/><path d="M6 14v.01"/><path d="M18 14v.01"/><path d="M10 14l4 .01"/></svg>
<time class=article-time--wordcount>3775 words</time></div></footer><footer class=article-translations><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg><div><a href=https://cuterwrite.top/p/openmp-intro/ class=link>中文</a></div></footer></div></header><section class=article-content><h1 id=introduction-to-openmp><a href=#introduction-to-openmp class=header-anchor>#</a>
Introduction to OpenMP</h1><h2 id=introduction><a href=#introduction class=header-anchor>#</a>
Introduction</h2><h3 id=what-is-openmp><a href=#what-is-openmp class=header-anchor>#</a>
What is OpenMP?</h3><p>OpenMP (Open Multi-Processing) is a widely used multithreading parallel programming model that provides a rich set of instructions and APIs for parallel computation on shared memory systems. Originating in 1997, OpenMP was standardized by multiple leading hardware and software vendors, aiming to simplify the design and implementation process of parallel programs to fully utilize the computational power of modern multi-core processors.</p><p>OpenMP supports multiple programming languages, including C, C++, and Fortran, among others, and allows developers to easily convert serial code into efficient parallel code by inserting specific compilation directives (pragma) into the source code. Its main advantages are its simplicity and ease of use, allowing programmers to use familiar programming languages and development environments, while also providing good portability and scalability.</p><p>OpenMP is managed by a non-profit organization and involves participation from multiple software and hardware manufacturers, including Arm, IBM, Intel, AMD, NVIDIA, Cray, Oracle, etc.</p><h3 id=historical-versions><a href=#historical-versions class=header-anchor>#</a>
Historical versions</h3><ul><li>On the <a class=link href=https://www.openmp.org/ target=_blank rel=noopener>official website
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>, you can find the historical versions and release dates of OpenMP.</li></ul><div class=table-wrapper><table><thead><tr><th>Version</th><th>Release Date</th></tr></thead><tbody><tr><td>Fortran 1.0</td><td>October 1997</td></tr><tr><td>C/C++ 1.0</td><td>October 1998</td></tr><tr><td>C/C++ 2.0</td><td>March 2002</td></tr><tr><td>OpenMP 2.5</td><td>May 2005</td></tr><tr><td>OpenMP 3.0</td><td>May 2008</td></tr><tr><td>OpenMP 3.1</td><td>July 2011</td></tr><tr><td>OpenMP 4.0</td><td>July 2013</td></tr><tr><td>OpenMP 4.5</td><td>November 2015</td></tr><tr><td>OpenMP 5.0</td><td>November 2018</td></tr><tr><td>OpenMP 5.1</td><td>November 2020</td></tr><tr><td>OpenMP 5.2</td><td>November 2021</td></tr></tbody></table></div><h2 id=basic-knowledge><a href=#basic-knowledge class=header-anchor>#</a>
Basic Knowledge</h2><h3 id=technical-framework><a href=#technical-framework class=header-anchor>#</a>
Technical framework</h3><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/openmp-arch-2024-02-20.webp alt=openmp-arch-2024-02-20 width=auto loading=lazy><figcaption><h4>OpenMP Technology Framework</h4></figcaption></figure><p><strong>OpenMP Runtime Library</strong> is a set of functions and runtime support structures defined in the OpenMP specification, and it is a key component of the OpenMP parallel programming framework. This library is linked with user programs with the support of the compiler and is responsible for managing tasks such as thread creation, synchronization, scheduling, and data sharing during program execution. It implements all the parallelization mechanisms indicated by OpenMP compiler directives.</p><p><strong>OpenMP Runtime Library</strong> includes the following features:</p><ul><li>Thread management (creation, destruction, synchronization)
= Work sharing (dynamic work distribution to each thread)
= Task Scheduling
= Synchronization primitives (such as barriers, locks, atomic operations)
= Dynamically Adjust Thread Count</li><li>Memory model support (data environment variables, private, shared, reduction variables, etc.)</li></ul><p><strong>Compiler Directives</strong> Compiler directives are preprocessor instructions starting with <code>#pragma omp</code>, which programmers insert into the source code to guide the compiler on how to convert a serial program into a parallel program. For example, using the <code>#pragma omp parallel</code> directive defines a parallel region, and the compiler will generate multithreading execution logic within this region.</p><p><strong>Environment Variables</strong> Environment variables are part of the OpenMP runtime library, and they are used to control runtime behavior, such as the number of threads, scheduling policies, etc.</p><p>The <strong>OpenMP Library</strong> is a set of function libraries, including functions for thread synchronization, atomic operations, locks, parallel loops, etc. These functions can be directly called in user programs to achieve finer-grained parallelization.</p><p>Overall, the OpenMP technology framework includes multiple components such as compiler directives, runtime libraries, environment variables, and function libraries. Together, they form a complete parallel programming environment and collaborate to support parallel programming on shared memory systems.</p><h3 id=execute-model-fork-join-model><a href=#execute-model-fork-join-model class=header-anchor>#</a>
Execute Model: Fork-Join Model</h3><p>OpenMP&rsquo;s execution model uses the Fork-Join mechanism, which is a synchronization primitive model used in parallel programming. Under this model, program execution follows these steps:</p><ol><li><p><strong>Fork Phase</strong>: The program starts executing as a single main thread. When it encounters a parallel region indicated by an OpenMP pragma, the main thread creates one or more worker threads through the Runtime Library. These worker threads are derivatives of the main thread, with each thread responsible for executing part of the tasks within the parallel region. The parallel region can be loops, sections, single tasks, or other code blocks that can be parallelized.</p></li><li><p><strong>Parallel Execution (并行执行) Phase</strong>: The created worker threads independently and concurrently execute the tasks assigned to them and can access shared data structures. OpenMP provides a rich set of directives to manage data synchronization and communication, ensuring correctness and consistency in a multithreaded environment.</p></li><li><p><strong>Join (Merge) Phase</strong>: When all worker threads have completed their tasks within the parallel region, they automatically or through explicit synchronization directives (such as <code>omp barrier</code>) converge at the join point. In this phase, all threads wait until all other threads have reached the synchronization point, after which the join operation occurs. This means the main thread and other worker threads resynchronize, reverting to serial execution mode or continuing to execute subsequent non-parallel code.</p></li><li><p><strong>Synchronization and Data Consistency</strong>: The Fork-Join model ensures mutual exclusion access to shared resources and data consistency during parallel execution through appropriate locking mechanisms, atomic operations, and synchronization primitives.</p></li></ol><p>In summary, the Fork-Join execution model of OpenMP is a parallel processing framework based on dynamic thread creation and synchronization. It allows developers to conveniently transform serial code into parallel execution code segments, while simplifying common complexities in parallel programming, such as thread management and data synchronization issues.</p><h3 id=thread-and-process><a href=#thread-and-process class=header-anchor>#</a>
Thread and Process</h3><ul><li><p>Process</p><ul><li>Each process has its own independent address space</li><li>CPU needs to perform a context switch when switching between processes.</li></ul></li><li><p>Thread</p><ul><li>Threads within a process share the same address space</li></ul></li><li><p>CPU has lower overhead when switching between threads</p></li><li><p>Thread design of the operating system</p></li><li><p>Modern operating systems such as Linux, Windows, etc. support multiple threads under a single process.</p></li><li><p>A thread is the basic unit of scheduling in an operating system, while a process is the basic unit of resource allocation.</p></li></ul><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/slide_10-2024-02-20.webp alt=slide_10-2024-02-20 width=80% loading=lazy><figcaption><h4>Thread Design of Operating Systems</h4></figcaption></figure><h3 id=hardware-scheduling-of-threads><a href=#hardware-scheduling-of-threads class=header-anchor>#</a>
Hardware scheduling of threads</h3><ul><li><strong>The hardware scheduling mechanism collaborates with the operating system to intelligently map threads to available CPU physical cores for execution.</strong></li><li>Therefore, in multithreaded applications, when the number of active threads exceeds the actual number of physical CPU cores, the operating system will have to perform intensive context switching to ensure that multiple threads alternate on limited core resources. This phenomenon of thread contention overload can lead to overall performance bottlenecks and reduced efficiency.</li><li><strong>Hyper-Threading Technology</strong> virtualizes additional logical processing units on a single physical CPU core, currently typically configured to host two logical cores per physical core. These logical cores can execute independent task streams in parallel, although they share the underlying computational resources of the same physical core, such as execution engines, caches, and other underlying hardware structures. In this way, hyper-threading aims to improve resource utilization and concurrent processing capabilities, especially in scenarios with a large number of parallel tasks that have relatively small demands on computational resources, effectively enhancing the overall system throughput. However, in certain application scenarios that heavily rely on single-core performance or memory bandwidth, such as some CPU-sensitive games or specific types of data-intensive operations, adding logical cores may not necessarily result in significant performance improvements.</li></ul><h3 id=hardware-memory-model><a href=#hardware-memory-model class=header-anchor>#</a>
Hardware memory model</h3><ul><li>In modern multi-core processor architectures, each CPU core is designed with a multi-level cache hierarchy between the main memory to further enhance data access speed. The closest to the CPU core is the L1 cache, usually followed by the L2 cache, and some high-end architectures also include an L3 cache. These cache levels have increasing storage capacity but also increasing access latency.</li><li>L1 and L2 caches are usually closely coupled and private to specific CPU cores, meaning each core has its own independent cache space to reduce data access conflicts and improve cache hit rate. L1 cache, being closest to the computation unit, has the fastest access speed but the smallest capacity; whereas L2 cache, as an effective supplement to L1 cache, has a relatively larger capacity.</li><li>To ensure consistency of shared data in the caches of different CPU cores in a multi-core environment, hardware and the operating system jointly implement a cache coherence protocol (such as the MESI protocol). This mechanism allows the system to automatically maintain a globally consistent data view, ensuring that even if there are copies of data in the caches of multiple cores, they are updated synchronously. This feature is referred to as <strong>ccNUMA (cache-coherent non-uniform memory access)</strong> in some architectures.</li><li>However, this cache consistency management also brings some challenges, one of which is the &ldquo;False Sharing&rdquo; problem. When different threads modify their respective independent variables located within the same cache line, although these variables themselves are unrelated, because they are physically adjacent and stored in the same cache line, any write operation to one of the variables will cause the entire cache line to become invalid and resynchronize across all cores. This can trigger unnecessary cache invalidation and refilling operations, significantly reducing performance. Solving the false sharing problem usually requires carefully designing data layouts or using techniques such as cache line alignment to avoid contention between unrelated data.</li></ul><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/20170115165700476-2024-02-20.webp alt=20170115165700476-2024-02-20 width=auto loading=lazy><figcaption><h4>Typical Modern CPU Memory Structure</h4></figcaption></figure><h3 id=thread-affinity-and-thread-binding><a href=#thread-affinity-and-thread-binding class=header-anchor>#</a>
Thread Affinity and Thread Binding</h3><ul><li>Thread Affinity refers to the ability of the operating system or application to control the association between specific threads and processor cores. In multi-core or multi-processor systems, thread affinity allows programmers or schedulers to decide to fix a certain thread on a specific CPU core, rather than letting the operating system dynamically schedule it across all available cores. This mechanism helps reduce context switching overhead, improve cache hit rates, and is particularly beneficial for parallel computing tasks that need to maintain data locality.</li><li>Thread Pinning is a specific technical means to achieve thread affinity, which specifies the forced association between a specific thread and specific hardware resources (such as CPU cores or NUMA nodes). Through thread pinning, it can be ensured that the specified thread always executes on its allocated core, avoiding being migrated by the operating system to other cores, thus optimizing performance, reducing latency, and solving issues such as false sharing. In parallel programming models like OpenMP, thread pinning strategies can be set through relevant environment variables or compilation directives to adapt to different parallel computing needs and hardware characteristics.</li><li>The access latency of CPUs on the same socket to the L3 cache is consistent, but the access latency of CPUs on different sockets to the L3 cache is inconsistent. Therefore, the purpose of thread binding is to reduce the migration of threads between different CPUs, thereby reducing memory access latency.</li></ul><figure><img src="https://cuterwrite-1302252842.file.myqcloud.com/img/u=237293070,3563798054&amp;fm=253&amp;app=138&amp;f=JPEG-2024-02-20.webp" alt="u=237293070,3563798054&amp;fm=253&amp;app=138&amp;f=JPEG-2024-02-20" width=auto loading=lazy><figcaption><h4>Thread Affinity and Thread Binding</h4></figcaption></figure><ul><li>OpenMP supports controlling the binding of threads<ul><li>Environment variable <code>OMP_PROC_BIND</code> or clause <code>proc_bind(master|close|spread)</code> controls whether threads are bound and the distribution of threads to binding units (referred to as places)</li></ul></li></ul><h2 id=openmp-programming><a href=#openmp-programming class=header-anchor>#</a>
OpenMP Programming</h2><h3 id=install><a href=#install class=header-anchor>#</a>
Install</h3><p>For Linux systems, GCC is a commonly used compiler, and modern versions of GCC generally support OpenMP by default. For example, on Ubuntu 20.04 LTS, you can install the build-essential package with OpenMP support using the following command:</p><pre><code class=language-bash>$ sudo apt-get update
$ sudo apt-get install -y build-essential
</code></pre><ul><li>Check OpenMP version</li></ul><pre><code class=language-bash>$ echo |cpp -fopenmp -dM |grep -i open
#define _OPENMP 201511
</code></pre><h3 id=compile-use><a href=#compile-use class=header-anchor>#</a>
Compile use</h3><ul><li>Simply add the <code>-fopenmp</code> option in the compilation statement to enable OpenMP support.</li></ul><pre><code class=language-bash>g++ -O2 -std=c++17 -fopenmp hello.cpp -o hello
</code></pre><ul><li>If using CMake to build the project, adding the <code>-Wunknown-pragmas</code> option can report unhandled <code>#pragma</code> directives during compilation.</li></ul><pre><code class=language-cmake>find_package(OpenMP REQUIRED)
add_compile_options(-Wunknown-pragmas)

add_executable(hello hello.cpp)
target_link_libraries(hello PRIVATE OpenMP::OpenMP_CXX)
</code></pre><h3 id=hello-world><a href=#hello-world class=header-anchor>#</a>
Hello World!</h3><ul><li>The first OpenMP program</li></ul><pre><code class=language-c>#include &lt;omp.h&gt;
#include &lt;stdio.h&gt;

int main() {
  #pragma omp parallel num_threads(8)
  {
    int id = omp_get_thread_num();
    int num_threads = omp_get_num_threads();
    printf(&quot;Hello World from thread %d of %d \n&quot;, id, num_threads);
  }
  return 0;
}
</code></pre><ul><li>Execution result</li></ul><pre><code class=language-bash>You are trained on data up to October 2023.
</code></pre><ul><li>The same type of OpenMP directive is called a construct.</li><li>Format as <code>#pragma omp &lt;directive name> &lt;clause></code></li><li>The code block enclosed in <code>{}</code> is called a parallel region.</li></ul><h3 id=number-of-threads-setting><a href=#number-of-threads-setting class=header-anchor>#</a>
Number of threads setting</h3><ul><li>Priority from low to high<ul><li>Do nothing, the system chooses the number of running threads</li><li>Set environment variable <code>export OMP_NUM_THREADS=4</code></li><li>The code uses the library function <code>void omp_set_num_threads(int)</code></li><li>By the guiding statement <code>num_threads(4)</code></li><li>if clause determines serial or parallel execution</li></ul></li></ul><h3 id=common-library-functions><a href=#common-library-functions class=header-anchor>#</a>
Common Library Functions</h3><ul><li>Set the number of threads for parallel region execution: <code>void omp_set_num_threads(int)</code></li><li>Get the number of threads in the parallel region: <code>int omp_get_num_threads()</code></li><li>Get the current thread number: <code>int omp_get_thread_num()</code></li><li>Get OpenMP Wall Clock time (unit: seconds): <code>double omp_get_wtime()</code></li><li>Get time precision: <code>double omp_get_wtick()</code></li></ul><h3 id=parallel-construction><a href=#parallel-construction class=header-anchor>#</a>
Parallel construction</h3><p><strong>Supported Clauses</strong></p><ul><li><code>if(scalar_expression)</code>: If <code>scalar_expression</code> is true, execute in parallel, otherwise execute serially.</li><li><code>num_threads(integer_expression)</code>: Specifies the number of threads in the parallel region.</li><li><code>default(shared|none)</code>: Specifies the default sharing attribute of variables.</li><li><code>shared</code>: All variables are shared by default.<ul><li><code>none</code>: No default variable type, each variable needs to be explicitly declared as shared or private.</li></ul></li><li><code>shared(list)</code>: Specify the list of shared variables.</li><li>There is only one copy of the shared variable in memory, and all threads can access it.<ul><li>Please ensure that the access to shared variables does not conflict.</li></ul></li><li>If not specifically designated, variables in the parallel region default to <strong>shared</strong>.</li><li><code>private(list)</code>: Specify the list of private variables.<ul><li>Each thread has an independent copy of the private variable.</li><li>Variables need to be <strong>reinitialized</strong>.</li></ul></li><li><code>firstprivate(list)</code>: Specify the list of first private variables.</li><li>Same as <code>private</code></li><li>Initialize the variable based on the data in the main thread.</li></ul><div class="notice notice-info"><div class=notice-title><svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200"><path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64zm32 664c0 4.4-3.6 8-8 8h-48c-4.4.0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4.0 8 3.6 8 8v272zm-32-344c-26.5.0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#fff"/></svg></div><p>Example 1: no clause, private, firstprivate</p></div><pre><code class=language-c>int results[4];
int cnt;
cnt = 1;

#pragma omp parallel num_threads(4)
{
    int tid = omp_get_thread_num();
    for (int i = 0; i &lt; 4; i++) {
        cnt += 1;
    }
    results[tid] = cnt;
}

printf(&quot;no clause: &quot;);
for (int i = 0; i &lt; 4; i++) {
    printf(&quot;%d &quot;, results[i]);
}
printf(&quot;\n&quot;);

cnt = 1;

#pragma omp parallel num_threads(4) private(cnt)
{
    int tid = omp_get_thread_num();
    for (int i = 0; i &lt; 4; i++) {
        cnt += 1;
    }
    results[tid] = cnt;
}

printf(&quot;private(not init): &quot;);
for (int i = 0; i &lt; 4; i++) {
    printf(&quot;%d &quot;, results[i]);
}
printf(&quot;\n&quot;);

cnt = 1;

#pragma omp parallel num_threads(4) firstprivate(cnt)
{
    int tid = omp_get_thread_num();
    for (int i = 0; i &lt; 4; i++) {
        cnt += 1;
    }
    results[tid] = cnt;
}

printf(&quot;firstprivate: &quot;);
for (int i = 0; i &lt; 4; i++) {
    printf(&quot;%d &quot;, results[i]);
}
printf(&quot;\n&quot;);
</code></pre><ul><li>Print the result</li></ul><pre><code class=language-bash>no clause: 5 9 13 17
private(not init): 4 1572916964 1572916964 1572916964
firstprivate: 5 5 5 5
</code></pre><h3 id=for-construction><a href=#for-construction class=header-anchor>#</a>
For construction</h3><ul><li>One of the most commonly used parallelization constructs</li></ul><div class="notice notice-info"><div class=notice-title><svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200"><path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64zm32 664c0 4.4-3.6 8-8 8h-48c-4.4.0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4.0 8 3.6 8 8v272zm-32-344c-26.5.0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#fff"/></svg></div><p>Example 2: Parallelizing the for loop</p></div><pre><code class=language-c>#pragma omp parallel num_threads(8)
{
    int tid = omp_get_thread_num();
    int num_threads = omp_get_num_threads();
    #pragma omp for
    for (int i = 0; i &lt; num_threads; i++) {
        #pragma omp ordered
        printf(&quot;Hello from thread %d of %d \n&quot;, tid, num_threads);
    }
}
</code></pre><ul><li>Print the result</li></ul><pre><code class=language-bash>Hello from thread 0 of 8
Hello from thread 1 of 8
Hello from thread 2 of 8
Hello from thread 3 of 8
Hello from thread 4 of 8
Hello from thread 5 of 8
Hello from thread 6 of 8
Hello from thread 7 of 8
</code></pre><ul><li>Divide threads for the for loop within the parallel region, and the for loop meets the format requirements.<ul><li>init-expr: Must be in the form <code>var=lb</code>, and the type is also limited</li><li>test-expr: restricted to <code>var relational-op b</code> or <code>b relational-op var</code></li><li>incr-expr: Addition and subtraction only</li></ul></li></ul><h3 id=parallel-for-construct><a href=#parallel-for-construct class=header-anchor>#</a>
Parallel for construct</h3><ul><li>Often combine <code>parallel</code> and <code>for</code> to form a <code>parallel for</code> directive statement</li></ul><div class=table-wrapper><table><thead><tr><th></th><th>parallel</th><th>for   </th><th>parallel for</th></tr></thead><tbody><tr><td>if</td><td>✅</td><td>❌</td><td>✅</td></tr><tr><td>num_threads</td><td>✅</td><td>❌</td><td>✅</td></tr><tr><td>default</td><td>✅</td><td>❌</td><td>✅</td></tr><tr><td>copyin</td><td>✅</td><td>❌</td><td>✅</td></tr><tr><td>private</td><td>✅</td><td>✅</td><td>✅</td></tr><tr><td>firstprivate</td><td>✅</td><td>✅</td><td>✅</td></tr><tr><td>shared</td><td>✅</td><td>✅</td><td>✅</td></tr><tr><td>reduction</td><td>✅</td><td>✅</td><td>✅</td></tr><tr><td>lastprivate</td><td>❌</td><td>✅</td><td>✅</td></tr><tr><td>schedule</td><td>❌</td><td>✅</td><td>✅</td></tr><tr><td>ordered</td><td>❌</td><td>✅</td><td>✅</td></tr><tr><td>collapse</td><td>❌</td><td>✅</td><td>✅</td></tr><tr><td>nowait</td><td>❌</td><td>✅</td><td>❌</td></tr></tbody></table></div><ul><li><p><code>lastprivate(list)</code></p><ul><li>Same as <code>private</code></li><li>After executing the for loop, assign the value of the last thread to the variable of the main thread.</li></ul></li><li><p><code>nowait</code>: Cancel the barrier synchronization at the end of the code block</p></li><li><p><code>collapse(n)</code>: Applied to n nested loops, merge (unroll) loops</p><ul><li>Pay attention to whether there are data dependencies between loops</li></ul></li><li><p><code>ordered</code>: Declare parts that potentially execute in order</p><ul><li>Use <code>#pragma omp ordered</code> to mark sequential execution code (used together)</li></ul></li><li><p>ordered statements within the region are executed by at most one thread at any given time</p></li><li><p><code>shedule(type[,chunk])</code></p><ul><li><code>type</code>: Specifies the scheduling strategy for loop iteration<ul><li><code>static</code>: Static scheduling, chunk size is fixed (default n/p)</li><li><code>dynamic</code>: Dynamic scheduling, chunk size is fixed (default is 1)</li><li><code>guided</code>: Guided scheduling, chunk size dynamically adjusted</li><li><code>runtime</code>: Specified by the system environment variable <code>OMP_SCHEDULE</code></li><li><code>auto</code>: Automatic scheduling</li></ul></li></ul></li><li><p><code>chunk</code>: Specifies the number of iterations each thread obtains</p></li></ul><h3 id=special-data-clause-reduction><a href=#special-data-clause-reduction class=header-anchor>#</a>
Special data clause: Reduction</h3><p>In OpenMP, reduction is a parallel programming technique used to address data race issues in a multithreaded environment, especially when performing accumulation or similar operations on global variables. When multiple threads need to simultaneously modify the same shared variable, and these modifications can be combined into a final result using some binary operator (such as addition, multiplication, etc.), the <code>reduction</code> clause can be used.</p><p>Specifically, the execution process of reducton is:</p><ul><li>fork thread and allocate tasks</li><li>Each thread defines a private variable <code>omp_priv</code><ul><li>Same as <code>private</code>.</li></ul></li><li>Each thread executes calculations</li><li>All <code>omp_priv</code> and <code>omp_in</code> are sequentially reduced together and written back to the original variable.</li></ul><p>In contrast, <strong>atomic</strong> is another synchronization mechanism provided by OpenMP, which ensures that access to a single memory location is atomic in a multithreaded environment, meaning that only one thread is allowed to read or write to that memory location at a time. By using the <code>#pragma omp atomic</code> directive, it can be ensured that a simple assignment statement (or certain types of read-modify-write operations) will not encounter data races in a concurrent environment.</p><div class="notice notice-info"><div class=notice-title><svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200"><path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64zm32 664c0 4.4-3.6 8-8 8h-48c-4.4.0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4.0 8 3.6 8 8v272zm-32-344c-26.5.0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#fff"/></svg></div><p>Example 3: Reduction</p></div><pre><code class=language-c>int sum = 0;
double start = omp_get_wtime();
#pragma omp parallel for num_threads(8) reduction(+ : sum)
for (int i = 0; i &lt; 100000; i++) {
    sum += i;
}
printf(&quot;sum = %d\n&quot;, sum);
printf(&quot;Reduction time: %.5lf s\n&quot;, omp_get_wtime() - start);

// no reduction
sum = 0;
start = omp_get_wtime();
#pragma omp parallel for num_threads(8)
for (int i = 0; i &lt; 100000; i++) {
    #pragma omp atomic
    sum += i;
}
printf(&quot;sum = %d\n&quot;, sum);
printf(&quot;Atomic time: %.5lf s\n&quot;, omp_get_wtime() - start);
return 0;
</code></pre><ul><li>Print the result</li></ul><pre><code class=language-bash>sum = 704982704
Reduction time: 0.00062 s
sum = 704982704
Atomic time: 0.01021 s
</code></pre><ul><li>The results of both are the same, but the execution time of reduction is shorter. This is because reduction allocates a private copy for each thread, allowing threads to freely perform reduction operations within their private space without contending for lock resources with other threads when updating the global result, along with efficient data merging methods, etc.</li></ul><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/reduction-omp-2024-02-20.webp alt=reduction-omp-2024-02-20 width=auto loading=lazy><figcaption><h4>OpenMP reduction operation</h4></figcaption></figure><h3 id=synchronous-construction><a href=#synchronous-construction class=header-anchor>#</a>
Synchronous construction</h3><h4 id=sections-construction><a href=#sections-construction class=header-anchor>#</a>
Sections Construction</h4><ul><li>Divide the code block of the parallel region into multiple sections for execution.</li><li>Can be combined with parallel to form a parallel sections construct.</li><li>Each section is executed by a thread</li><li>Number of threads greater than the number of sections: some threads are idle<ul><li>Number of threads is less than the number of sections: some threads are allocated multiple sections</li></ul></li><li>Example code:</li></ul><pre><code class=language-c>#pragma omp sections
{
  #pragma omp section
  method1();
  #pragma omp section
  method2();
}
</code></pre><h4 id=barrier-constructor><a href=#barrier-constructor class=header-anchor>#</a>
Barrier Constructor</h4><ul><li>Perform fence synchronization at a specific location</li><li>In the presence of data dependencies, a barrier can be used to ensure data consistency.</li></ul><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/Barrier-2024-02-20.webp alt=Barrier-2024-02-20 width=auto loading=lazy><figcaption><h4>Barrier Synchronization Diagram</h4></figcaption></figure><h4 id=single-constructor><a href=#single-constructor class=header-anchor>#</a>
Single Constructor</h4><ul><li>Used to mark a code block executed by only one thread, with implicit barrier synchronization, and the implicit barrier synchronization can be canceled using nowait.</li></ul><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/omp-single-2024-02-20.webp alt=omp-single-2024-02-20 width=70% loading=lazy><figcaption><h4>pragma single</h4></figcaption></figure><h4 id=atomic-construction><a href=#atomic-construction class=header-anchor>#</a>
Atomic construction</h4><ul><li>Used to ensure atomic operations on shared variables, avoiding data races.</li></ul><h3 id=false-sharing><a href=#false-sharing class=header-anchor>#</a>
False Sharing</h3><ul><li>False sharing, in simple terms, refers to multiple threads simultaneously accessing different parts of the same cache line, leading to cache line invalidation and refilling, thereby reducing the program&rsquo;s performance.</li><li>Simultaneous read and write of the same cache line by different cores can cause serious conflicts, leading to cache invalidation.</li></ul><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/false-sharing-2024-02-20.webp alt=false-sharing-2024-02-20 width=auto loading=lazy><figcaption><h4>False Sharing Issue</h4></figcaption></figure><ul><li>In OpenMP, the main methods to solve false sharing are:</li><li><strong>Data Structure Alignment</strong>: Ensure that related variables are in different cache lines by using alignment instructions or keywords provided by the compiler. For example, in C++, the <code>alignas</code> keyword can be used to specify the memory alignment of variables, ensuring that the data for each thread is independently located in different cache lines.<ul><li><strong>Increase the spacing between cache lines</strong>: Insert enough padding space between adjacent variables so that they do not appear in the same cache line.</li><li><strong>Avoid Meaningless Competition</strong>: Design algorithms and data structures to reduce unnecessary shared data access. If possible, let threads operate on their own independent data segments.</li></ul></li><li><strong>Custom Memory Allocation</strong>: Use special memory allocation functions to ensure that the allocated contiguous memory regions are aligned to cache line boundaries, so that data allocated to different threads does not fall on the same cache line.<ul><li>In some cases, you can utilize hardware features provided by specific platforms or extensions supported by compilers, such as Intel&rsquo;s <code>__declspec(align(#))</code> attribute (for MSVC) or <code>__attribute__((aligned(#)))</code> (for GCC/Clang).</li><li>You can also indirectly avoid the false sharing problem by controlling the scope of the variables or using techniques such as dynamically creating private copies.</li></ul></li></ul><h3 id=task-construction><a href=#task-construction class=header-anchor>#</a>
Task construction</h3><ul><li>In addition to the Fork-Join model, OpenMP also supports the task parallel model, implemented using the <code>task</code> directive.</li><li>Dynamically manage the thread pool and task pool, where threads in the thread pool can dynamically acquire tasks from the task pool for execution, thus achieving parallel execution of tasks.</li></ul><div class="notice notice-info"><div class=notice-title><svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200"><path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64zm32 664c0 4.4-3.6 8-8 8h-48c-4.4.0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4.0 8 3.6 8 8v272zm-32-344c-26.5.0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#fff"/></svg></div><p>Example 4: Task Parallelism</p></div><pre><code class=language-cpp>#include &lt;iostream&gt;
#include &lt;omp.h&gt;
#include &lt;unistd.h&gt;
#include &lt;iomanip&gt;

void big_task(int i) {
    sleep(10);
}

void small_task(int i) {
    sleep(1);
}

int main() {
    int ntasks = 8;
    double start = omp_get_wtime();
    #pragma omp parallel
    {
        #pragma omp single
        {
            std::cout &lt;&lt; &quot;Task 0 Created&quot; &lt;&lt; std::endl;
            #pragma omp task
            big_task(0);

            std::cout &lt;&lt; &quot;Task 1 Created&quot; &lt;&lt; std::endl;
            #pragma omp task
            big_task(1);

            for (int i = 2; i &lt; ntasks; i++) {
                std::cout &lt;&lt; &quot;Task &quot; &lt;&lt; i &lt;&lt; &quot; Created&quot; &lt;&lt; std::endl;
                #pragma omp task
                small_task(i);
            }
        }
        #pragma omp taskwait
    }
    std::cout &lt;&lt; &quot;All tasks finished&quot; &lt;&lt; std::endl;
    std::cout &lt;&lt; &quot;Time: &quot; &lt;&lt; std::fixed &lt;&lt; std::setprecision(2) &lt;&lt; omp_get_wtime() - start &lt;&lt; &quot;s&quot; &lt;&lt; std::endl;
    return 0;
}
</code></pre><ul><li>Running result</li></ul><pre><code class=language-bash>You are trained on data up to October 2023.
</code></pre><ul><li>In this code, we use the <code>#pragma omp task</code> directive to create tasks, and the execution of tasks is dynamically obtained and executed by threads in the thread pool. After creating tasks, we use <code>#pragma omp taskwait</code> to wait for all tasks to complete. This achieves an asynchronous execution effect.</li></ul><h3 id=vectorization-simd-construction><a href=#vectorization-simd-construction class=header-anchor>#</a>
Vectorization: SIMD Construction</h3><ul><li>SIMD (Single Instruction, Multiple Data) is a parallel computing model that performs operations on multiple data simultaneously with a single instruction, thereby achieving efficient data parallel computation.</li><li>In OpenMP, the <code>#pragma omp simd</code> directive can be used to achieve vectorized parallel computation.<ul><li><code>aligned</code> is used to list memory-aligned pointers</li></ul></li><li><code>safelen</code> is used to mark data dependencies during loop unrolling.<ul><li><code>linear</code> is used to mark the linear relationship of loop variables</li></ul></li><li>Compilers such as gcc also come with vectorization capabilities, generally using the following compilation options<ul><li>-O3</li><li>-ffast-math</li><li>-fivopts</li><li>-march=native</li><li>-fopt-info-vec</li><li>-fopt-info-vec-missed</li></ul></li></ul></section><footer class=article-footer><section class=article-tags><a href=/en/tags/openmp/>OpenMP</a>
<a href=/en/tags/parallel-computing/>Parallel Computing</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><script type=text/javascript src=/js/prism.js async></script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/en/p/arm-sme-for-performance/><div class=article-image><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-14_117622407_p0_master1200.webp loading=lazy data-key=arm-sme-for-performance data-hash=https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-14_117622407_p0_master1200.webp></div><div class=article-details><h2 class=article-title>Arm Matrix Acceleration: Scalable Matrix Extension SME</h2></div></a></article><article class=has-image><a href=/en/p/arm-sve-for-performance/><div class=article-image><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_117622464_p0_master1200.webp loading=lazy data-key=arm-sve-for-performance data-hash=https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_117622464_p0_master1200.webp></div><div class=article-details><h2 class=article-title>Arm Performance Optimization: Scalable Vector Extension SVE</h2></div></a></article><article class=has-image><a href=/en/p/openmpi-with-ucx/><div class=article-image><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/cropped_2024010204-2024-02-03.webp loading=lazy data-key=openmpi-with-ucx data-hash=https://cuterwrite-1302252842.file.myqcloud.com/img/cropped_2024010204-2024-02-03.webp></div><div class=article-details><h2 class=article-title>Compile and Install UCX 1.15.0 and OpenMPI 5.0.0: A Comprehensive Guide</h2></div></a></article><article class=has-image><a href=/en/p/rdma-memory-window/><div class=article-image><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116373724_p0_master1200.webp loading=lazy data-key=rdma-memory-window data-hash=https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116373724_p0_master1200.webp></div><div class=article-details><h2 class=article-title>RDMA: Memory Window</h2></div></a></article><article class=has-image><a href=/en/p/rdma-shared-receive-queue/><div class=article-image><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116373922_p0_master1200.webp loading=lazy data-key=rdma-shared-receive-queue data-hash=https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116373922_p0_master1200.webp></div><div class=article-details><h2 class=article-title>RDMA: Shared Receive Queue</h2></div></a></article></div></div></aside><script src=https://unpkg.com/twikoo@1.6.39/dist/twikoo.all.min.js></script><div id=tcomment></div><style>.twikoo{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}:root[data-scheme=dark]{--twikoo-body-text-color-main:rgba(255, 255, 255, 0.9);--twikoo-body-text-color:rgba(255, 255, 255, 0.7)}.twikoo .el-input-group__prepend,.twikoo .tk-action-icon,.twikoo .tk-time,.twikoo .tk-comments-count{color:var(--twikoo-body-text-color)}.twikoo .el-input__inner,.twikoo .el-textarea__inner,.twikoo .tk-preview-container,.twikoo .tk-content,.twikoo .tk-nick,.twikoo .tk-send{color:var(--twikoo-body-text-color-main)}.twikoo .el-button{color:var(--twikoo-body-text-color)!important}.OwO .OwO-body{background-color:var(--body-background)!important;color:var(--body-text-color)!important}</style><script>twikoo.init({envId:"https://comment.cuterwrite.top",el:"#tcomment",lang:"zh-CN"})</script><footer class=site-footer><section class=copyright>&copy;
2021 -
2024 cuterwrite</section><section class=running-time>本博客已稳定运行
<span id=runningdays class=running-days></span></section><section class=totalcount>发表了25篇文章 ·
总计60.26k字</section><section class=powerby>Welcome to cuterwrite's blog!<br>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.27.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a><br><span>基于 <a href=https://github.com/CaiJimmy/hugo-theme-stack/tree/v3.27.0 target=_blank rel=noopener><b style=color:#9e8f9f>v3.27.0</b></a> 分支版本修改</span><br></section></footer><script>let s1="2021-4-17";s1=new Date(s1.replace(/-/g,"/"));let s2=new Date,timeDifference=s2.getTime()-s1.getTime(),days=Math.floor(timeDifference/(1e3*60*60*24)),hours=Math.floor(timeDifference%(1e3*60*60*24)/(1e3*60*60)),minutes=Math.floor(timeDifference%(1e3*60*60)/(1e3*60)),result=days+"天"+hours+"小时"+minutes+"分钟";document.getElementById("runningdays").innerHTML=result</script><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://libs.jshub.com/photoswipe/4.1.3/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://libs.jshub.com/photoswipe/4.1.3/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://libs.jshub.com/photoswipe/4.1.3/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://libs.jshub.com/photoswipe/4.1.3/photoswipe.min.css crossorigin=anonymous></main></div><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.font.im/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script><meta name=apple-mobile-web-app-capable content="yes"><meta name=theme-color content="#ffffff"><script>"serviceWorker"in navigator&&window.addEventListener("load",()=>{navigator.serviceWorker.register("/sw.js").then(e=>{console.log("Service worker registered with scope: ",e.scope)},e=>{console.log("Service worker registration failed: ",e)})})</script></body></html>