<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="This article focuses on introducing the basic concepts of Ollama and its notable advantages, including its open-source and free nature, ease of use, rich models, and low resource consumption. It then provides a detailed installation and usage guide, covering detailed installation steps for different operating systems and Docker environments, as well as how to download and run models. Additionally, the article introduces how to deploy Ollama on HPC clusters and how to create a local code completion assistant by integrating IDE plugins."><title>Ollama: From Beginner to Advanced</title>
<link rel=canonical href=https://cuterwrite.top/en/p/ollama/><link rel=stylesheet href=/scss/style.min.e82b84120b43b340665e4b3c6b625144c63ea87ec3a8572acc62bbd314e4204b.css><meta property='og:title' content="Ollama: From Beginner to Advanced"><meta property='og:description' content="This article focuses on introducing the basic concepts of Ollama and its notable advantages, including its open-source and free nature, ease of use, rich models, and low resource consumption. It then provides a detailed installation and usage guide, covering detailed installation steps for different operating systems and Docker environments, as well as how to download and run models. Additionally, the article introduces how to deploy Ollama on HPC clusters and how to create a local code completion assistant by integrating IDE plugins."><meta property='og:url' content='https://cuterwrite.top/en/p/ollama/'><meta property='og:site_name' content="Cuterwrite's Blog"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='llm'><meta property='article:published_time' content='2024-06-15T23:10:00+00:00'><meta property='article:modified_time' content='2024-06-15T23:10:00+00:00'><meta property='og:image' content='https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116903387_p0_master1200.webp'><meta name=twitter:title content="Ollama: From Beginner to Advanced"><meta name=twitter:description content="This article focuses on introducing the basic concepts of Ollama and its notable advantages, including its open-source and free nature, ease of use, rich models, and low resource consumption. It then provides a detailed installation and usage guide, covering detailed installation steps for different operating systems and Docker environments, as well as how to download and run models. Additionally, the article introduces how to deploy Ollama on HPC clusters and how to create a local code completion assistant by integrating IDE plugins."><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116903387_p0_master1200.webp'><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><script async src=https://analytics.cuterwrite.top/uma.js data-website-id=b13594a2-4d15-4a4e-a020-5e3cc1d88c12 data-domains=cuterwrite.top></script><link rel=manifest href=/manifest.json></head><body class="article-page
line-numbers"><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/en/><img src=/img/avatar_hu7627246953874065940.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>üòâ</span></figure><div class=site-meta><h1 class=site-name><a href=/en>Cuterwrite's Blog</a></h1><h2 class=site-description>Cuterwrite's tech blog, focusing on in-depth exploration and experience sharing in areas such as high-performance computing, operating systems, full-stack development, and artificial intelligence.</h2></div></header><ol class=menu-social><li><a href=https://analytics.cuterwrite.top/share/Ji0gm9OaLDk8gco7 target=_blank title=Analytics rel=me><svg width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 5H7A2 2 0 005 7v12a2 2 0 002 2h10a2 2 0 002-2V7a2 2 0 00-2-2h-2"/><rect x="9" y="3" width="6" height="4" rx="2"/><path d="M9 17v-5"/><path d="M12 17v-1"/><path d="M15 17v-3"/></svg></a></li><li><a href=https://stats.uptimerobot.com/6NVhRHkSAQ target=_blank title=Uptime rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-chart-line"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 19h16"/><path d="M4 15l4-6 4 2 4-5 4 4"/></svg></a></li><li><a href=/index.xml target=_blank title=RSS rel=me><svg class="icon icon-tabler icon-tabler-rss" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="5" cy="19" r="1"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg></a></li><li><a href=https://github.com/PKUcoldkeyboard target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.zhihu.com/people/kong-tiao-cheng-tai-lang-30-57 target=_blank title=zhihu rel=me><svg t="1705591931290" class="icon" viewBox="0 0 1280 1024" p-id="21048" width="32" height="32"><path d="M341.08 296.26v435.08l46.86.02 15.42 52.74 84.02-52.74h99.06V296.26H341.08zm195.5 387.86H480.7l-55.8 35.02-10.16-34.94-23.8-.08V343.5h145.64v340.62zM299.66 495.34H195c3.48-54.2 4.4-103.18 4.4-146.92h102.32s3.94-45.12-17.16-44.62h-177c6.98-26.24 15.74-53.32 26.24-81.34.0.0-48.14.0-64.54 43.14-6.78 17.8-26.42 86.28-61.4 156.24 11.78-1.28 50.74-2.36 73.68-44.42 4.22-11.78 5.02-13.32 10.28-29.06h57.74c0 21-2.4 133.76-3.36 146.88H41.66c-23.48.0-31.12 47.24-31.12 47.24H141.7C132.9 642.2 85.66 726.24.0 792.68c40.98 11.7 81.82-1.86 102-19.8.0.0 45.96-41.8 71.18-138.5L281.1 764.26s15.82-53.78-2.48-79.98c-15.16-17.84-56.12-66.12-73.58-83.62L175.8 623.9c8.72-27.96 13.98-55.1 15.74-81.34h123.3s-.18-47.24-15.18-47.24v.02zm824.04-3.2c41.66-51.28 89.96-117.14 89.96-117.14s-37.3-29.6-54.76-8.12c-12 16.3-73.66 96.4-73.66 96.4l38.46 28.86zM823.52 373.96c-18.02-16.5-51.82 4.26-51.82 4.26s79.04 110.08 82.24 114.9l38.92-27.46s-51.34-75.22-69.32-91.72h-.02zM1280 516.7c-39.56.0-261.82 1.86-262.12 1.86v-202c9.62.0 24.84-.8 45.7-2.4 81.76-4.82 140.26-8 175.54-9.62.0.0 24.44-54.38-1.18-66.88-6.14-2.36-46.34 9.16-46.34 9.16s-330.44 32.98-464.72 36.1c3.2 17.64 15.24 34.16 31.56 39.1 26.62 6.96 45.38 3.4 98.3 1.78 49.66-3.2 87.36-4.86 113.02-4.86v199.62H702.82s5.64 44.62 51.02 45.7h215.88V706.1c0 27.94-22.38 43.98-48.96 42.24-28.16.22-52.16-2.3-83.38-3.62 3.98 7.94 12.66 28.78 38.62 43.68 19.76 9.62 32.34 13.14 52.04 13.14 59.12.0 91.34-34.56 89.78-90.62V564.28h244.72c19.36.0 17.4-47.56 17.4-47.56l.06-.02z" fill="#707070" p-id="21049"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/en/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>‰∏ªÈ°µ | Home</span></a></li><li><a href=/en/about/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>ÂÖ≥‰∫é | About</span></a></li><li><a href=/en/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>ÂΩíÊ°£ | Archives</span></a></li><li><a href=/en/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>ÊêúÁ¥¢ | Search</span></a></li><li><a href=https://cuterwrite.top/image-hosting target=_blank><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-album"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 4m0 2a2 2 0 012-2h12a2 2 0 012 2v12a2 2 0 01-2 2H6a2 2 0 01-2-2z"/><path d="M12 4v7l2-2 2 2V4"/></svg>
<span>ÂõæÂÜå | Gallery</span></a></li><li><a href=https://draw.cuterwrite.top target=_blank><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-artboard"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M8 8m0 1a1 1 0 011-1h6a1 1 0 011 1v6a1 1 0 01-1 1H9a1 1 0 01-1-1z"/><path d="M3 8h1"/><path d="M3 16h1"/><path d="M8 3v1"/><path d="M16 3v1"/><path d="M20 8h1"/><path d="M20 16h1"/><path d="M8 20v1"/><path d="M16 20v1"/></svg>
<span>ÁîªÊùø | Canvas</span></a></li><li><a href=https://it-tools.tech target=_blank><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-tools"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 21h4L20 8a1.5 1.5.0 00-4-4L3 17v4"/><path d="M14.5 5.5l4 4"/><path d="M12 8 7 3 3 7l5 5"/><path d="M7 8 5.5 9.5"/><path d="M16 12l5 5-4 4-5-5"/><path d="M16 17l-1.5 1.5"/></svg>
<span>Â∑•ÂÖ∑ | Tools</span></a></li><li class=menu-bottom-section><ol class=menu><li id=i18n-switch><svg class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg>
<select name=language title=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://cuterwrite.top/>‰∏≠Êñá</option><option value=https://cuterwrite.top/en/ selected>English</option></select></li><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ul><li><a href=#what-is-ollama>What is Ollama?</a></li><li><a href=#advantages-of-ollama>Advantages of Ollama</a></li><li><a href=#how-to-use-ollama>How to use Ollama?</a><ul><li><a href=#install-ollama>Install Ollama</a><ul><li><a href=#macos>macOS</a></li><li><a href=#windows>Windows</a></li><li><a href=#linux>Linux</a></li><li><a href=#docker>Docker</a></li></ul></li><li><a href=#launch-ollama>Launch Ollama</a></li><li><a href=#download-model>Download model</a></li><li><a href=#run-model>Run model</a><ul><li><a href=#run-model-in-docker-container>Run model in Docker container</a></li></ul></li><li><a href=#configure-ollama>Configure Ollama</a></li></ul></li><li><a href=#advanced-usage-deploying-ollama-on-an-hpc-cluster>Advanced Usage: Deploying Ollama on an HPC Cluster</a></li><li><a href=#advanced-usage-local-code-completion-assistant>Advanced Usage: Local Code Completion Assistant</a><ul><li><a href=#codestral-22b-model>Codestral 22B Model</a><ul><li><a href=#download-and-run-the-codestral-model>Download and run the Codestral model</a></li><li><a href=#configure-configjson>Configure config.json</a></li></ul></li><li><a href=#deepseek-coder-67b-model--llama-3-8b-model>DeepSeek Coder 6.7B model + Llama 3 8B model</a><ul><li><a href=#download-and-run-the-deepseek-coder-model>Download and run the DeepSeek Coder model</a></li><li><a href=#download-and-run-the-llama-3-model>Download and run the Llama 3 model</a></li><li><a href=#configure-configjson-1>Configure config.json</a></li></ul></li><li><a href=#codeqwen-7b-model--qwen2-7b-model>Codeqwen 7B model + Qwen2 7B model</a><ul><li><a href=#download-and-run-the-codeqwen-model>Download and run the Codeqwen model</a></li><li><a href=#download-and-run-the-qwen2-model>Download and run the Qwen2 model</a></li><li><a href=#configure-configjson-2>Configure config.json</a></li></ul></li><li><a href=#optimize-chat-using-rag-vector-retrieval>Optimize chat using RAG vector retrieval</a><ul><li><a href=#download-and-run-the-nomic-embed-text-model>Download and run the Nomic Embed Text model</a></li><li><a href=#configure-configjson-3>Configure config.json</a></li></ul></li><li><a href=#code-completion-effect>Code completion effect</a></li><li><a href=#chat-with-ollama>Chat with Ollama</a></li><li><a href=#code-auto-comment>Code auto-comment</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/en/p/ollama/><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116903387_p0_master1200.webp loading=lazy alt="Featured image of post Ollama: From Beginner to Advanced"></a></div><div class=article-details><header class=article-category><a href=/en/categories/ai/ style=background-color:#eaaa60;color:#fff>Artificial Intelligence and Data Science</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/en/p/ollama/>Ollama: From Beginner to Advanced</a></h2><h3 class=article-subtitle>This article focuses on introducing the basic concepts of Ollama and its notable advantages, including its open-source and free nature, ease of use, rich models, and low resource consumption. It then provides a detailed installation and usage guide, covering detailed installation steps for different operating systems and Docker environments, as well as how to download and run models. Additionally, the article introduces how to deploy Ollama on HPC clusters and how to create a local code completion assistant by integrating IDE plugins.</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>2024-06-15</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>4 minute read</time></div><div><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-keyboard"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M2 6m0 2a2 2 0 012-2h16a2 2 0 012 2v8a2 2 0 01-2 2H4a2 2 0 01-2-2z"/><path d="M6 10v.01"/><path d="M10 10v.01"/><path d="M14 10v.01"/><path d="M18 10v.01"/><path d="M6 14v.01"/><path d="M18 14v.01"/><path d="M10 14l4 .01"/></svg>
<time class=article-time--wordcount>1722 words</time></div></footer><footer class=article-translations><svg class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg><div><a href=https://cuterwrite.top/p/ollama/ class=link>‰∏≠Êñá</a></div></footer></div></header><section class=article-content><p>In recent years, large language models (LLM) have become a cornerstone of the artificial intelligence field due to their powerful text generation and understanding capabilities. Commercial LLMs are often expensive and have closed-source code, limiting the exploration space for researchers and developers. Fortunately, the open-source community offers excellent alternatives like Ollama, allowing everyone to easily experience the charm of LLMs and combine them with HPC and IDE plugins to create more powerful personal assistants.</p><h2 id=what-is-ollama><a href=#what-is-ollama class=header-anchor>#</a>
What is Ollama?</h2><p>Ollama is a tool for building large language model applications. It offers a simple and easy-to-use command line interface and server, allowing you to easily download, run, and manage various open-source LLMs. Unlike traditional LLMs that require complex configurations and powerful hardware, Ollama allows you to conveniently experience the powerful features of LLMs as if you were using a mobile app.</p><h2 id=advantages-of-ollama><a href=#advantages-of-ollama class=header-anchor>#</a>
Advantages of Ollama</h2><p>Ollama has the following significant advantages:</p><ul><li><strong>Open source and free</strong>: Ollama and its supported models are completely open source and free, allowing anyone to use, modify, and distribute them freely.</li><li><strong>Simple and Easy to Use</strong>: No need for complex configuration and installation processes, Ollama can be started and run with just a few commands.</li><li><strong>Rich Model</strong>: Ollama supports many popular open-source LLMs such as Llama 3, Mistral, Qwen2, and provides one-click download and switching features.</li><li><strong>Low resource consumption</strong>: Compared to commercial LLMs, Ollama has lower hardware requirements and can run smoothly even on ordinary laptops.</li><li><strong>Community Activity</strong>: Ollama has a large and active community where users can easily get help, share experiences, and participate in model development.</li></ul><h2 id=how-to-use-ollama><a href=#how-to-use-ollama class=header-anchor>#</a>
How to use Ollama?</h2><p>Using Ollama is very simple, just follow these steps:</p><ol><li><strong>Install Ollama</strong>: Download and install the latest version from the <a class=link href=https://ollama.com/ target=_blank rel=noopener>Ollama official website
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>according to your operating system.</li><li><strong>Start Ollama</strong>: Open the terminal or command line and enter the <code>ollama serve</code> command to start the Ollama server.</li><li><strong>Download Model</strong>: Find the desired model in the <a class=link href=https://ollama.com/library target=_blank rel=noopener>model library
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>, then use the <code>ollama pull</code> command to download it, for example, <code>ollama pull llama3:70b</code>.</li><li><strong>Run the model</strong>: Use the <code>ollama run</code> command to start the model, for example <code>ollama run llama3:70b</code>.</li><li><strong>Start chatting</strong>: Enter your question or command in the terminal, and Ollama will generate a corresponding response based on the model.</li></ol><h3 id=install-ollama><a href=#install-ollama class=header-anchor>#</a>
Install Ollama</h3><h4 id=macos><a href=#macos class=header-anchor>#</a>
macOS</h4><p><a class=link href=https://ollama.com/download/Ollama-darwin.zip target=_blank rel=noopener>Download Ollama for macOS
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a></p><h4 id=windows><a href=#windows class=header-anchor>#</a>
Windows</h4><p><a class=link href=https://ollama.com/download/OllamaSetup.exe target=_blank rel=noopener>Download Ollama for Windows
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a></p><h4 id=linux><a href=#linux class=header-anchor>#</a>
Linux</h4><pre><code class=language-bash>curl -fsSL https://ollama.com/install.sh | sh
</code></pre><h4 id=docker><a href=#docker class=header-anchor>#</a>
Docker</h4><h5 id=cpu-version><a href=#cpu-version class=header-anchor>#</a>
CPU version</h5><pre><code class=language-bash>docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
</code></pre><h5 id=gpu-version><a href=#gpu-version class=header-anchor>#</a>
GPU version</h5><ol><li>Install <a class=link href=https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation target=_blank rel=noopener>Nvidia container toolkit
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a></li><li>Run Ollama in a Docker container</li></ol><pre><code class=language-bash>docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
</code></pre><h3 id=launch-ollama><a href=#launch-ollama class=header-anchor>#</a>
Launch Ollama</h3><pre><code class=language-bash>ollama serve
</code></pre><p>Output the following information indicating that the Ollama server has successfully started (V100 machine):</p><pre><code class=language-bash>$ ollama serve
### Omitted log output ###
Listening on [::]:11434 (version 0.1.42)
</code></pre><h3 id=download-model><a href=#download-model class=header-anchor>#</a>
Download model</h3><pre><code class=language-bash>ollama pull qwen2:72b
</code></pre><h3 id=run-model><a href=#run-model class=header-anchor>#</a>
Run model</h3><pre><code class=language-bash>ollama run qwen2:72b
</code></pre><p>For example, after running the following command:</p><pre><code class=language-bash>You are trained on data up to October 2023.
</code></pre><h4 id=run-model-in-docker-container><a href=#run-model-in-docker-container class=header-anchor>#</a>
Run model in Docker container</h4><pre><code class=language-bash>docker exec -it ollama ollama run qwen2:72b
</code></pre><h3 id=configure-ollama><a href=#configure-ollama class=header-anchor>#</a>
Configure Ollama</h3><p>Ollama provides a variety of environment variables for configuration:</p><ul><li><code>OLLAMA_DEBUG</code>: Whether to enable debug mode, default is <code>false</code>.</li><li><code>OLLAMA_FLASH_ATTENTION</code>: Whether to flash attention, default is <code>true</code>.</li><li><code>OLLAMA_HOST</code>: Host address of the Ollama server, default is empty.</li><li><code>OLLAMA_KEEP_ALIVE</code>: Time to keep the connection alive, default is <code>5m</code>.</li><li><code>OLLAMA_LLM_LIBRARY</code>: LLM library, default is empty.</li><li><code>OLLAMA_MAX_LOADED_MODELS</code>: Maximum number of loaded models, default is <code>1</code>.</li><li><code>OLLAMA_MAX_QUEUE</code>: Maximum number of queues, default is empty.</li><li><code>OLLAMA_MAX_VRAM</code>: Maximum virtual memory, default is empty.</li><li><code>OLLAMA_MODELS</code>: Model directory, default is empty.</li><li><code>OLLAMA_NOHISTORY</code>: Whether to save history, defaults to <code>false</code>.</li><li><code>OLLAMA_NOPRUNE</code>: Whether to enable pruning, default is <code>false</code>.</li><li><code>OLLAMA_NUM_PARALLEL</code>: Number of parallels, default is <code>1</code>.</li><li><code>OLLAMA_ORIGINS</code>: Allowed origins, default is empty.</li><li><code>OLLAMA_RUNNERS_DIR</code>: Runner directory, default is empty.</li><li><code>OLLAMA_SCHED_SPREAD</code>: Scheduling distribution, default is empty.</li><li><code>OLLAMA_TMPDIR</code>: Temporary file directory, defaults to empty. Here is the optimized list in the desired format:</li><li><code>OLLAMA_DEBUG</code>: Whether to enable debug mode, default is <code>false</code>.</li><li><code>OLLAMA_FLASH_ATTENTION</code>: Whether to flash attention, default is <code>true</code>.</li><li><code>OLLAMA_HOST</code>: Host address of the Ollama server, default is empty.</li><li><code>OLLAMA_KEEP_ALIVE</code>: Duration to keep the connection alive, default is <code>5m</code>.</li><li><code>OLLAMA_LLM_LIBRARY</code>: LLM library, default is empty.</li><li><code>OLLAMA_MAX_LOADED_MODELS</code>: Maximum number of loaded models, default is <code>1</code>.</li><li><code>OLLAMA_MAX_QUEUE</code>: Maximum queue number, default is empty.</li><li><code>OLLAMA_MAX_VRAM</code>: Maximum virtual memory, default is empty.</li><li><code>OLLAMA_MODELS</code>: Model directory, default is empty.</li><li><code>OLLAMA_NOHISTORY</code>: Whether to save history, default is <code>false</code>.</li><li><code>OLLAMA_NOPRUNE</code>: Whether to enable pruning, default is <code>false</code>.</li><li><code>OLLAMA_NUM_PARALLEL</code>: Number of parallels, default is <code>1</code>.</li><li><code>OLLAMA_ORIGINS</code>: Allowed origins, default is empty.</li><li><code>OLLAMA_RUNNERS_DIR</code>: Runner directory, default is empty.</li><li><code>OLLAMA_SCHED_SPREAD</code>: Scheduling distribution, default is empty.</li><li><code>OLLAMA_TMPDIR</code>: Temporary file directory, default is empty.</li></ul><h2 id=advanced-usage-deploying-ollama-on-an-hpc-cluster><a href=#advanced-usage-deploying-ollama-on-an-hpc-cluster class=header-anchor>#</a>
Advanced Usage: Deploying Ollama on an HPC Cluster</h2><p>For large models or situations requiring higher performance, the powerful computing power of an HPC cluster can be used to run Ollama. By combining with Slurm for task management and using port mapping to expose the service locally, remote access and use can be conveniently achieved:</p><ol><li>Configure the Ollama environment on the login node: Install Ollama and download the required models.</li><li><strong>Write a slurm script</strong>: Specify resource requirements (CPU, memory, GPU, etc.), and use the <code>ollama serve</code> command to start the model service and bind it to a specific port.</li></ol><pre><code class=language-bash>#!/bin/bash
#SBATCH --job-name=ollama
#SBATCH -N 1
#SBATCH -p GPU
#SBATCH --gres=gpu:1

module load CUDA
ollama serve
</code></pre><ol start=3><li><strong>Submit slurm task</strong>: Use the <code>sbatch</code> command to submit the script, Slurm will allocate the task to compute nodes for execution.</li><li><strong>Local Port Mapping</strong>: Use the ssh -L command to map the compute node&rsquo;s port to the local machine, for example:</li></ol><pre><code class=language-bash>ssh -t -t  username@login node ip -L 11434:localhost:11434 -i login node private key ssh compute node IP  -L 11434:127.0.0.1:11434
</code></pre><ol start=5><li><strong>Local Access</strong>: Access http://localhost:11434 in a browser or application to use the Ollama service.</li></ol><blockquote class="alert-blockquote alert-important"><p class=alert-heading><svg viewBox="0 0 16 16" width="16" height="16"><path d="M0 1.75C0 .784.784.0 1.75.0h12.5C15.216.0 16 .784 16 1.75v9.5A1.75 1.75.0 0114.25 13H8.06l-2.573 2.573A1.458 1.458.0 013 14.543V13H1.75A1.75 1.75.0 010 11.25zm1.75-.25a.25.25.0 00-.25.25v9.5c0 .138.112.25.25.25h2a.75.75.0 01.75.75v2.19l2.72-2.72a.749.749.0 01.53-.22h6.5a.25.25.0 00.25-.25v-9.5a.25.25.0 00-.25-.25zm7 2.25v2.5a.75.75.0 01-1.5.0v-2.5a.75.75.0 011.5.0zM9 9A1 1 0 117 9a1 1 0 012 0z"/></svg>
<span>Important</span></p><p>Note: Since the compute node is not connected to the internet, you need to use <code>ollama pull</code> on the login node in advance to download the required model. Additionally, you need to set <code>OLLAMA_ORIGINS</code> to <code>*</code> and <code>OLLAMA_HOST</code> to <code>0.0.0.0</code> to allow all sources to access the service.</p></blockquote><h2 id=advanced-usage-local-code-completion-assistant><a href=#advanced-usage-local-code-completion-assistant class=header-anchor>#</a>
Advanced Usage: Local Code Completion Assistant</h2><p>Ollama can not only be used for chatting and text creation but also for creating a powerful code completion assistant by combining code generation models and IDE plugins. For example, using the Codeqwen 7B model and the VS Code plugin <a class=link href=https://continue.dev/ target=_blank rel=noopener>Continue
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>, efficient and convenient code completion functionality can be achieved.</p><p>First, let me introduce Continue:<blockquote><p><a class=link href=https://continue.dev/ target=_blank rel=noopener>Continue
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>allows you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs. All of this can run entirely on your own laptop or have Ollama deployed on a server to remotely power code completion and chat experiences based on your needs.</p><span class=cite><span>‚Äï </span><span>Continue</span><cite></cite></span></blockquote></p><p>Before starting, you need to install the following tools:</p><ul><li><a class=link href=https://docs.continue.dev/quickstart target=_blank rel=noopener>Continue
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>: <a class=link href="https://marketplace.visualstudio.com/items?itemName=Continue.continue" target=_blank rel=noopener>VS Code Version
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>or <a class=link href=https://plugins.jetbrains.com/plugin/22707-continue target=_blank rel=noopener>JetBrains Version
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a></li><li><a class=link href=https://ollama.com/ target=_blank rel=noopener>Ollama
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a></li></ul><p>Next, using VS Code as an example, we will introduce how to use Ollama + Continue to implement code completion functionality:</p><h3 id=codestral-22b-model><a href=#codestral-22b-model class=header-anchor>#</a>
Codestral 22B Model</h3><p>Codestral is capable of both code auto-completion and chat functionality. However, given that it has 22 billion parameters and lacks a production license, it requires a significant amount of video memory and is limited to research and testing use, so it may not be suitable for everyday local applications.</p><h4 id=download-and-run-the-codestral-model><a href=#download-and-run-the-codestral-model class=header-anchor>#</a>
Download and run the Codestral model</h4><pre><code class=language-bash>ollama run codestral
</code></pre><h4 id=configure-configjson><a href=#configure-configjson class=header-anchor>#</a>
Configure config.json</h4><ul><li>In the VS Code sidebar, click the Continue plugin icon, then click the &ldquo;gear&rdquo; icon at the bottom right of the panel to open the <code>config.json</code> file. Then copy the following configuration into the <code>config.json</code> file:</li></ul><pre><code class=language-json>{
  &quot;models&quot;: [
    {
      &quot;title&quot;: &quot;Codestral&quot;,
      &quot;provider&quot;: &quot;ollama&quot;,
      &quot;model&quot;: &quot;codestral&quot;
    }
  ],
  &quot;tabAutocompleteModel&quot;: {
    &quot;title&quot;: &quot;Codestral&quot;,
    &quot;provider&quot;: &quot;ollama&quot;,
    &quot;model&quot;: &quot;codestral&quot;
  }
}
</code></pre><h3 id=deepseek-coder-67b-model--llama-3-8b-model><a href=#deepseek-coder-67b-model--llama-3-8b-model class=header-anchor>#</a>
DeepSeek Coder 6.7B model + Llama 3 8B model</h3><p>Depending on the machine&rsquo;s VRAM size, you can utilize Ollama&rsquo;s ability to run multiple models simultaneously and handle multiple concurrent requests, using <code>DeepSeek Coder 6.7B</code> for auto-completion, and <code>Llama 3 8B</code> for chatting. If your machine cannot run both at the same time, you can try them separately to decide whether you prefer the local auto-completion or the local chat experience.</p><h4 id=download-and-run-the-deepseek-coder-model><a href=#download-and-run-the-deepseek-coder-model class=header-anchor>#</a>
Download and run the DeepSeek Coder model</h4><pre><code class=language-bash>ollama run deepseek-coder:6.7b-base
</code></pre><h4 id=download-and-run-the-llama-3-model><a href=#download-and-run-the-llama-3-model class=header-anchor>#</a>
Download and run the Llama 3 model</h4><pre><code class=language-bash>ollama run llama3:8b
</code></pre><h4 id=configure-configjson-1><a href=#configure-configjson-1 class=header-anchor>#</a>
Configure config.json</h4><pre><code class=language-json>{
  &quot;models&quot;: [
    {
      &quot;title&quot;: &quot;Llama 3 8B&quot;,
      &quot;provider&quot;: &quot;ollama&quot;,
      &quot;model&quot;: &quot;llama3:8b&quot;,
      &quot;apiBase&quot;: &quot;http://127.0.0.1:11434&quot;
    }
  ],
  &quot;tabAutocompleteModel&quot;: {
    &quot;title&quot;: &quot;DeepSeek Coder 6.7B&quot;,
    &quot;provider&quot;: &quot;ollama&quot;,
    &quot;model&quot;: &quot;deepseek-coder:6.7b-base&quot;,
    &quot;apiBase&quot;: &quot;http://127.0.0.1:11434&quot;
  }
}
</code></pre><h3 id=codeqwen-7b-model--qwen2-7b-model><a href=#codeqwen-7b-model--qwen2-7b-model class=header-anchor>#</a>
Codeqwen 7B model + Qwen2 7B model</h3><p>The Codeqwen 7B model is a model specifically designed for code completion, while the Qwen2 7B model is a general-purpose chat model. These two models can be well combined to achieve both code completion and chat functions.</p><h4 id=download-and-run-the-codeqwen-model><a href=#download-and-run-the-codeqwen-model class=header-anchor>#</a>
Download and run the Codeqwen model</h4><pre><code class=language-bash>ollama run codeqwen
</code></pre><h4 id=download-and-run-the-qwen2-model><a href=#download-and-run-the-qwen2-model class=header-anchor>#</a>
Download and run the Qwen2 model</h4><pre><code class=language-bash>ollama run qwen2:7b
</code></pre><h4 id=configure-configjson-2><a href=#configure-configjson-2 class=header-anchor>#</a>
Configure config.json</h4><pre><code class=language-json>{
  &quot;models&quot;: [
    {
      &quot;title&quot;: &quot;Codeqwen 7B&quot;,
      &quot;provider&quot;: &quot;ollama&quot;,
      &quot;model&quot;: &quot;codeqwen&quot;,
      &quot;apiBase&quot;: &quot;http://127.0.0.1:11434&quot;
    }
  ],
  &quot;tabAutocompleteModel&quot;: {
    &quot;title&quot;: &quot;Qwen2 7B&quot;,
    &quot;provider&quot;: &quot;ollama&quot;,
    &quot;model&quot;: &quot;qwen2:7b&quot;,
    &quot;apiBase&quot;: &quot;http://127.0.0.1:11434&quot;
  }
}
</code></pre><h3 id=optimize-chat-using-rag-vector-retrieval><a href=#optimize-chat-using-rag-vector-retrieval class=header-anchor>#</a>
Optimize chat using RAG vector retrieval</h3><p>Continue has a built-in <a class=link href=https://docs.continue.dev/customization/context-providers#codebase-retrieval target=_blank rel=noopener>@codebase
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>context provider that can automatically retrieve the most relevant code snippets from the codebase. If you have set up a chat model (such as Codestral, Llama 3), then with the help of Ollama and <a class=link href=https://blog.lancedb.com/lancedb-x-continue/ target=_blank rel=noopener>LanceDB
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>&rsquo;s vectorization technology, you can achieve more efficient code retrieval and chat experience.</p><p>Here, we use the <code>nomic-embed-text</code> model as the vector retrieval model:</p><h4 id=download-and-run-the-nomic-embed-text-model><a href=#download-and-run-the-nomic-embed-text-model class=header-anchor>#</a>
Download and run the Nomic Embed Text model</h4><pre><code class=language-bash>ollama run nomic-embed-text
</code></pre><h4 id=configure-configjson-3><a href=#configure-configjson-3 class=header-anchor>#</a>
Configure config.json</h4><ul><li>Add the following content to the file:</li></ul><pre><code class=language-json>{
    &quot;embeddingsProvider&quot;: {
        &quot;provider&quot;: &quot;ollama&quot;,
        &quot;model&quot;: &quot;nomic-embed-text&quot;,
        &quot;apiBase&quot;: &quot;http://127.0.0.1:11434&quot;
    }
}
</code></pre><h3 id=code-completion-effect><a href=#code-completion-effect class=header-anchor>#</a>
Code completion effect</h3><ul><li><code>Ctrl + I</code>: Generate code snippet based on instructions.</li></ul><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_1-2024-06-17.webp alt=codeqwen_1-2024-06-17 width=90% loading=lazy></figure><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_2-2024-06-17.webp alt=codeqwen_2-2024-06-17 width=90% loading=lazy></figure><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_3-2024-06-17.webp alt=codeqwen_3-2024-06-17 width=90% loading=lazy></figure><ul><li>Cursor hover auto-complete code</li></ul><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_4-2024-06-17.webp alt=codeqwen_4-2024-06-17 width=90% loading=lazy></figure><h3 id=chat-with-ollama><a href=#chat-with-ollama class=header-anchor>#</a>
Chat with Ollama</h3><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_5-2024-06-17.webp alt=codeqwen_5-2024-06-17 width=90% loading=lazy></figure><h3 id=code-auto-comment><a href=#code-auto-comment class=header-anchor>#</a>
Code auto-comment</h3><ul><li>Select code to open the right-click menu</li></ul><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_6-2024-06-17.webp alt=codeqwen_6-2024-06-17 width=90% loading=lazy></figure><figure><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_7-2024-06-17.webp alt=codeqwen_7-2024-06-17 width=90% loading=lazy></figure><h2 id=summary><a href=#summary class=header-anchor>#</a>
Summary</h2><p>Ollama has opened the door to the world of open-source LLM, allowing everyone to easily experience the powerful features of LLM and customize applications according to their own needs. Whether for research, development, or daily use, Ollama can provide you with a platform to explore the limitless possibilities of LLM. With the continuous development of Ollama, it is believed that it will bring us more surprises and promote the application and development of LLM technology in various fields.</p></section><footer class=article-footer><section class=article-tags><a href=/en/tags/llm/>llm</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><script type=text/javascript src=/js/prism.js async></script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/en/p/integrate-open-webui-ollama-qwen25-local-rag/><div class=article-image><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_116903285_p0_master1200.webp loading=lazy data-key=integrate-open-webui-ollama-qwen25-local-rag data-hash=https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_116903285_p0_master1200.webp></div><div class=article-details><h2 class=article-title>Implementing Local RAG Service: Integrating Open WebUI, Ollama, and Qwen2.5</h2></div></a></article><article class=has-image><a href=/en/p/llm-ecosystem/><div class=article-image><img src=https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_119269220_p0_master1200.webp loading=lazy data-key=llm-ecosystem data-hash=https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_119269220_p0_master1200.webp></div><div class=article-details><h2 class=article-title>Introduction to LLM Ecosystem: From Model Fine-tuning to Application Implementation</h2></div></a></article></div></div></aside><script src=https://unpkg.com/twikoo@1.6.39/dist/twikoo.all.min.js></script><div id=tcomment></div><style>.twikoo{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}:root[data-scheme=dark]{--twikoo-body-text-color-main:rgba(255, 255, 255, 0.9);--twikoo-body-text-color:rgba(255, 255, 255, 0.7)}.twikoo .el-input-group__prepend,.twikoo .tk-action-icon,.twikoo .tk-time,.twikoo .tk-comments-count{color:var(--twikoo-body-text-color)}.twikoo .el-input__inner,.twikoo .el-textarea__inner,.twikoo .tk-preview-container,.twikoo .tk-content,.twikoo .tk-nick,.twikoo .tk-send{color:var(--twikoo-body-text-color-main)}.twikoo .el-button{color:var(--twikoo-body-text-color)!important}.OwO .OwO-body{background-color:var(--body-background)!important;color:var(--body-text-color)!important}</style><script>twikoo.init({envId:"https://comment.cuterwrite.top",el:"#tcomment",lang:"zh-CN"})</script><footer class=site-footer><section class=copyright>&copy;
2021 -
2024 cuterwrite</section><section class=running-time>Êú¨ÂçöÂÆ¢Â∑≤Á®≥ÂÆöËøêË°å
<span id=runningdays class=running-days></span><br>ÊÄªËÆøÂÆ¢Êï∞Ôºö
<span id=busuanzi_value_site_uv class=running-days>Loading</span><br>ÊÄªËÆøÈóÆÈáèÔºö
<span id=busuanzi_value_site_pv class=running-days>Loading</span></section><section class=totalcount>ÂèëË°®‰∫Ü
<span class=running-days>25</span> ÁØáÊñáÁ´† ¬∑
ÊÄªËÆ°
<span class=running-days>60.67k</span> Â≠ó</section><section class=powerby><hr>Built with <b><a style=color:#9e8f9f href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a></b><br>Theme <b><a style=color:#9e8f9f href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.27.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a><br><span>Âü∫‰∫é <a href=https://github.com/CaiJimmy/hugo-theme-stack/tree/v3.27.0 target=_blank rel=noopener><b style=color:#9e8f9f>v3.27.0</b></a> ÂàÜÊîØÁâàÊú¨‰øÆÊîπ</span><br></section></footer><script>let s1="2021-4-17";s1=new Date(s1.replace(/-/g,"/"));let s2=new Date,timeDifference=s2.getTime()-s1.getTime(),days=Math.floor(timeDifference/(1e3*60*60*24)),hours=Math.floor(timeDifference%(1e3*60*60*24)/(1e3*60*60)),minutes=Math.floor(timeDifference%(1e3*60*60)/(1e3*60)),result=days+"Â§©"+hours+"Â∞èÊó∂"+minutes+"ÂàÜÈíü";document.getElementById("runningdays").innerHTML=result</script><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://libs.jshub.com/photoswipe/4.1.3/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://libs.jshub.com/photoswipe/4.1.3/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://libs.jshub.com/photoswipe/4.1.3/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://libs.jshub.com/photoswipe/4.1.3/photoswipe.min.css crossorigin=anonymous></main></div><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.font.im/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script><meta name=apple-mobile-web-app-capable content="yes"><meta name=theme-color content="#ffffff"><script>"serviceWorker"in navigator&&window.addEventListener("load",()=>{navigator.serviceWorker.register("/sw.js").then(e=>{console.log("Service worker registered with scope: ",e.scope)},e=>{console.log("Service worker registration failed: ",e)})})</script><script defer src=https://cn.vercount.one/js></script></body></html>