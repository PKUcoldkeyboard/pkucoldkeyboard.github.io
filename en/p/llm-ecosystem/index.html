<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Building a complete LLM application requires more than just having a powerful model. A thriving LLM ecosystem needs to cover all aspects from model training and optimization to deployment and application. This article will take you through various aspects of the LLM ecosystem, exploring how to truly apply LLM to real-world scenarios."><title>Introduction to LLM Ecosystem: From Model Fine-tuning to Application Implementation</title>
<link rel=canonical href=https://cuterwrite.top/en/p/llm-ecosystem/><link rel=stylesheet href=/scss/style.min.e82b84120b43b340665e4b3c6b625144c63ea87ec3a8572acc62bbd314e4204b.css><meta property='og:title' content="Introduction to LLM Ecosystem: From Model Fine-tuning to Application Implementation"><meta property='og:description' content="Building a complete LLM application requires more than just having a powerful model. A thriving LLM ecosystem needs to cover all aspects from model training and optimization to deployment and application. This article will take you through various aspects of the LLM ecosystem, exploring how to truly apply LLM to real-world scenarios."><meta property='og:url' content='https://cuterwrite.top/en/p/llm-ecosystem/'><meta property='og:site_name' content="Cuterwrite's Blog"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='llm'><meta property='article:published_time' content='2024-07-05T22:46:00+00:00'><meta property='article:modified_time' content='2024-07-05T22:46:00+00:00'><meta property='og:image' content='https://cloud.cuterwrite.fun/img/2024-06-29_119269220_p0_master1200.webp'><meta name=twitter:title content="Introduction to LLM Ecosystem: From Model Fine-tuning to Application Implementation"><meta name=twitter:description content="Building a complete LLM application requires more than just having a powerful model. A thriving LLM ecosystem needs to cover all aspects from model training and optimization to deployment and application. This article will take you through various aspects of the LLM ecosystem, exploring how to truly apply LLM to real-world scenarios."><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://cloud.cuterwrite.fun/img/2024-06-29_119269220_p0_master1200.webp'><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><script async src=https://analytics.cuterwrite.top/uma.js data-website-id=b13594a2-4d15-4a4e-a020-5e3cc1d88c12 data-domains=cuterwrite.top></script><link rel=manifest href=/manifest.json></head><body class="article-page
line-numbers"><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/en/><img src=/img/avatar_hu7627246953874065940.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>😉</span></figure><div class=site-meta><h1 class=site-name><a href=/en>Cuterwrite's Blog</a></h1><h2 class=site-description>Cuterwrite's tech blog, focusing on in-depth exploration and experience sharing in areas such as high-performance computing, operating systems, full-stack development, and artificial intelligence.</h2></div></header><ol class=menu-social><li><a href=https://analytics.cuterwrite.top/share/Ji0gm9OaLDk8gco7 target=_blank title=Analytics rel=me><svg width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 5H7A2 2 0 005 7v12a2 2 0 002 2h10a2 2 0 002-2V7a2 2 0 00-2-2h-2"/><rect x="9" y="3" width="6" height="4" rx="2"/><path d="M9 17v-5"/><path d="M12 17v-1"/><path d="M15 17v-3"/></svg></a></li><li><a href=https://stats.uptimerobot.com/6NVhRHkSAQ target=_blank title=Uptime rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-chart-line"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 19h16"/><path d="M4 15l4-6 4 2 4-5 4 4"/></svg></a></li><li><a href=/index.xml target=_blank title=RSS rel=me><svg class="icon icon-tabler icon-tabler-rss" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="5" cy="19" r="1"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg></a></li><li><a href=https://github.com/PKUcoldkeyboard target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.zhihu.com/people/kong-tiao-cheng-tai-lang-30-57 target=_blank title=zhihu rel=me><svg t="1705591931290" class="icon" viewBox="0 0 1280 1024" p-id="21048" width="32" height="32"><path d="M341.08 296.26v435.08l46.86.02 15.42 52.74 84.02-52.74h99.06V296.26H341.08zm195.5 387.86H480.7l-55.8 35.02-10.16-34.94-23.8-.08V343.5h145.64v340.62zM299.66 495.34H195c3.48-54.2 4.4-103.18 4.4-146.92h102.32s3.94-45.12-17.16-44.62h-177c6.98-26.24 15.74-53.32 26.24-81.34.0.0-48.14.0-64.54 43.14-6.78 17.8-26.42 86.28-61.4 156.24 11.78-1.28 50.74-2.36 73.68-44.42 4.22-11.78 5.02-13.32 10.28-29.06h57.74c0 21-2.4 133.76-3.36 146.88H41.66c-23.48.0-31.12 47.24-31.12 47.24H141.7C132.9 642.2 85.66 726.24.0 792.68c40.98 11.7 81.82-1.86 102-19.8.0.0 45.96-41.8 71.18-138.5L281.1 764.26s15.82-53.78-2.48-79.98c-15.16-17.84-56.12-66.12-73.58-83.62L175.8 623.9c8.72-27.96 13.98-55.1 15.74-81.34h123.3s-.18-47.24-15.18-47.24v.02zm824.04-3.2c41.66-51.28 89.96-117.14 89.96-117.14s-37.3-29.6-54.76-8.12c-12 16.3-73.66 96.4-73.66 96.4l38.46 28.86zM823.52 373.96c-18.02-16.5-51.82 4.26-51.82 4.26s79.04 110.08 82.24 114.9l38.92-27.46s-51.34-75.22-69.32-91.72h-.02zM1280 516.7c-39.56.0-261.82 1.86-262.12 1.86v-202c9.62.0 24.84-.8 45.7-2.4 81.76-4.82 140.26-8 175.54-9.62.0.0 24.44-54.38-1.18-66.88-6.14-2.36-46.34 9.16-46.34 9.16s-330.44 32.98-464.72 36.1c3.2 17.64 15.24 34.16 31.56 39.1 26.62 6.96 45.38 3.4 98.3 1.78 49.66-3.2 87.36-4.86 113.02-4.86v199.62H702.82s5.64 44.62 51.02 45.7h215.88V706.1c0 27.94-22.38 43.98-48.96 42.24-28.16.22-52.16-2.3-83.38-3.62 3.98 7.94 12.66 28.78 38.62 43.68 19.76 9.62 32.34 13.14 52.04 13.14 59.12.0 91.34-34.56 89.78-90.62V564.28h244.72c19.36.0 17.4-47.56 17.4-47.56l.06-.02z" fill="#707070" p-id="21049"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/en/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页 | Home</span></a></li><li><a href=/en/about/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>关于 | About</span></a></li><li><a href=/en/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>归档 | Archives</span></a></li><li><a href=/en/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>搜索 | Search</span></a></li><li><a href=https://gallery.cuterwrite.top target=_blank><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-album"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 4m0 2a2 2 0 012-2h12a2 2 0 012 2v12a2 2 0 01-2 2H6a2 2 0 01-2-2z"/><path d="M12 4v7l2-2 2 2V4"/></svg>
<span>图册 | Gallery</span></a></li><li><a href=https://draw.cuterwrite.top target=_blank><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-artboard"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M8 8m0 1a1 1 0 011-1h6a1 1 0 011 1v6a1 1 0 01-1 1H9a1 1 0 01-1-1z"/><path d="M3 8h1"/><path d="M3 16h1"/><path d="M8 3v1"/><path d="M16 3v1"/><path d="M20 8h1"/><path d="M20 16h1"/><path d="M8 20v1"/><path d="M16 20v1"/></svg>
<span>画板 | Canvas</span></a></li><li><a href=https://it-tools.tech target=_blank><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-tools"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 21h4L20 8a1.5 1.5.0 00-4-4L3 17v4"/><path d="M14.5 5.5l4 4"/><path d="M12 8 7 3 3 7l5 5"/><path d="M7 8 5.5 9.5"/><path d="M16 12l5 5-4 4-5-5"/><path d="M16 17l-1.5 1.5"/></svg>
<span>工具 | Tools</span></a></li><li class=menu-bottom-section><ol class=menu><li id=i18n-switch><svg class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg>
<select name=language title=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://cuterwrite.top/>中文</option><option value=https://cuterwrite.top/en/ selected>English</option></select></li><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ul><li><a href=#model-fine-tuning>Model fine-tuning</a><ul><li><a href=#axolotl>Axolotl</a></li><li><a href=#llama-factory>Llama-Factory</a></li><li><a href=#firefly>Firefly</a></li><li><a href=#xtuner>XTuner</a></li></ul></li><li><a href=#model-quantization>Model quantization</a><ul><li><a href=#autogptq>AutoGPTQ</a></li><li><a href=#autoawq>AutoAWQ</a></li><li><a href=#neural-compressor>Neural Compressor</a></li></ul></li><li><a href=#model-deployment>Model deployment</a><ul><li><a href=#vllm>vLLM</a></li><li><a href=#sgl>SGL</a></li><li><a href=#skypilot>SkyPilot</a></li><li><a href=#tensorrt-llm>TensorRT-LLM</a></li><li><a href=#openvino>OpenVino</a></li><li><a href=#tgi>TGI</a></li></ul></li><li><a href=#local-run>Local run</a><ul><li><a href=#mlx>MLX</a></li><li><a href=#llamacpp>Llama.cpp</a></li><li><a href=#ollama>Ollama</a></li></ul></li><li><a href=#agent-and-rag-framework>Agent and RAG framework</a><ul><li><a href=#llamaindex>LlamaIndex</a></li><li><a href=#crewai>CrewAI</a></li><li><a href=#opendevin>OpenDevin</a></li></ul></li><li><a href=#model-evaluation>Model evaluation</a><ul><li><a href=#lmsys>LMSys</a></li><li><a href=#opencompass>OpenCompass</a></li><li><a href=#open-llm-leaderboard>Open LLM Leaderboard</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/en/p/llm-ecosystem/><img src=https://cloud.cuterwrite.fun/img/2024-06-29_119269220_p0_master1200.webp loading=lazy alt="Featured image of post Introduction to LLM Ecosystem: From Model Fine-tuning to Application Implementation"></a></div><div class=article-details><header class=article-category><a href=/en/categories/ai/ style=background-color:#eaaa60;color:#fff>Artificial Intelligence and Data Science</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/en/p/llm-ecosystem/>Introduction to LLM Ecosystem: From Model Fine-tuning to Application Implementation</a></h2><h3 class=article-subtitle>Building a complete LLM application requires more than just having a powerful model. A thriving LLM ecosystem needs to cover all aspects from model training and optimization to deployment and application. This article will take you through various aspects of the LLM ecosystem, exploring how to truly apply LLM to real-world scenarios.</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>2024-07-05</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>8 minute read</time></div><div><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-keyboard"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M2 6m0 2a2 2 0 012-2h16a2 2 0 012 2v8a2 2 0 01-2 2H4a2 2 0 01-2-2z"/><path d="M6 10v.01"/><path d="M10 10v.01"/><path d="M14 10v.01"/><path d="M18 10v.01"/><path d="M6 14v.01"/><path d="M18 14v.01"/><path d="M10 14l4 .01"/></svg>
<time class=article-time--wordcount>3806 words</time></div></footer><footer class=article-translations><svg class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg><div><a href=https://cuterwrite.top/p/llm-ecosystem/ class=link>中文</a></div></footer></div></header><section class=article-content><h1 id=llm-ecosystem-introduction-from-model-fine-tuning-to-application-implementation><a href=#llm-ecosystem-introduction-from-model-fine-tuning-to-application-implementation class=header-anchor>#</a>
LLM Ecosystem Introduction: From Model Fine-Tuning to Application Implementation</h1><h2 id=model-fine-tuning><a href=#model-fine-tuning class=header-anchor>#</a>
Model fine-tuning</h2><p>Pre-trained LLMs typically possess broad knowledge, but fine-tuning is essential for them to excel in specific tasks. Here are some commonly used LLM fine-tuning tools:</p><h3 id=axolotl><a href=#axolotl class=header-anchor>#</a>
Axolotl</h3><a href=https://github.com/OpenAccess-AI-Collective/axolotl target=_blank class="card-github fetch-waiting no-styling" repo=OpenAccess-AI-Collective/axolotl id=repo-eezYQpBiLBqzPYLy-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-eezYQpBiLBqzPYLy-avatar class=gc-avatar></div><div class=gc-user>OpenAccess-AI-Collective</div></div><div class=gc-divider>/</div><div class=gc-repo>axolotl</div></div><div class=github-logo></div></div><div id=repo-eezYQpBiLBqzPYLy-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-eezYQpBiLBqzPYLy-stars class=gc-stars>0</div><div id=repo-eezYQpBiLBqzPYLy-forks class=gc-forks>0</div><div id=repo-eezYQpBiLBqzPYLy-license class=gc-license>unkown</div><div id=repo-eezYQpBiLBqzPYLy-language class=gc-language>Waiting...</div></div></a><script id=repo-eezYQpBiLBqzPYLy-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/OpenAccess-AI-Collective/axolotl",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-eezYQpBiLBqzPYLy-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-eezYQpBiLBqzPYLy-language").innerText=e.language,document.getElementById("repo-eezYQpBiLBqzPYLy-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-eezYQpBiLBqzPYLy-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-eezYQpBiLBqzPYLy-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-eezYQpBiLBqzPYLy-license").innerText=e.license.spdx_id:document.getElementById("repo-eezYQpBiLBqzPYLy-license").classList.add="no-license",document.getElementById("repo-eezYQpBiLBqzPYLy-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for OpenAccess-AI-Collective/axolotl.")}).catch(e=>{const t=document.getElementById("repo-eezYQpBiLBqzPYLy-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for OpenAccess-AI-Collective/axolotl.")})</script><p>Axolotl is a tool designed to simplify the fine-tuning of various AI models, supporting multiple configurations and architectures.</p><p><strong>Main Features:</strong></p><ul><li>Train various Huggingface models, such as llama, pythia, falcon, mpt</li><li>Supports fullfinetune, lora, qlora, relora, and gptq</li><li>Customize configuration using simple yaml files or CLI rewrite functions</li><li>Load different dataset formats, use custom formats, or built-in tokenized datasets</li><li>Integrated with xformer, flash attention, rope scaling, and multi-packing</li><li>Can work with a single GPU or multiple GPUs through FSDP or Deepspeed.</li><li>Easily run locally or in the cloud using Docker</li><li>Record the results and optional checkpoints to wandb or mlflow</li></ul><p><strong>Quick Start:</strong>
Requirements: Python >=3.10 and Pytorch >=2.1.1</p><pre><code class=language-bash>git clone https://github.com/OpenAccess-AI-Collective/axolotl
cd axolotl

pip3 install packaging ninja
pip3 install -e '.[flash-attn,deepspeed]'
</code></pre><p><strong>Usage:</strong></p><pre><code class=language-bash># preprocess datasets - optional but recommended
CUDA_VISIBLE_DEVICES=&quot;&quot; python -m axolotl.cli.preprocess examples/openllama-3b/lora.yml

# finetune lora
accelerate launch -m axolotl.cli.train examples/openllama-3b/lora.yml

# inference
accelerate launch -m axolotl.cli.inference examples/openllama-3b/lora.yml \
    --lora_model_dir=&quot;./outputs/lora-out&quot;

# gradio
accelerate launch -m axolotl.cli.inference examples/openllama-3b/lora.yml \
    --lora_model_dir=&quot;./outputs/lora-out&quot; --gradio

# remote yaml files - the yaml config can be hosted on a public URL
# Note: the yaml config must directly link to the **raw** yaml
accelerate launch -m axolotl.cli.train https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/examples/openllama-3b/lora.yml
</code></pre><p>For more detailed information, please visit the <a class=link href=https://github.com/OpenAccess-AI-Collective/axolotl target=_blank rel=noopener>Axolotl
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>project homepage.</p><h3 id=llama-factory><a href=#llama-factory class=header-anchor>#</a>
Llama-Factory</h3><figure><img src=https://cloud.cuterwrite.fun/img/2024-07-07_llama-factory-logo.webp alt="Llama Factory Logo" width=80% loading=lazy></figure><a href=https://github.com/hiyouga/LLaMA-Factory target=_blank class="card-github fetch-waiting no-styling" repo=hiyouga/LLaMA-Factory id=repo-EO5eJZNCdoj1Qcz5-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-EO5eJZNCdoj1Qcz5-avatar class=gc-avatar></div><div class=gc-user>hiyouga</div></div><div class=gc-divider>/</div><div class=gc-repo>LLaMA-Factory</div></div><div class=github-logo></div></div><div id=repo-EO5eJZNCdoj1Qcz5-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-EO5eJZNCdoj1Qcz5-stars class=gc-stars>0</div><div id=repo-EO5eJZNCdoj1Qcz5-forks class=gc-forks>0</div><div id=repo-EO5eJZNCdoj1Qcz5-license class=gc-license>unkown</div><div id=repo-EO5eJZNCdoj1Qcz5-language class=gc-language>Waiting...</div></div></a><script id=repo-EO5eJZNCdoj1Qcz5-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/hiyouga/LLaMA-Factory",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-EO5eJZNCdoj1Qcz5-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-EO5eJZNCdoj1Qcz5-language").innerText=e.language,document.getElementById("repo-EO5eJZNCdoj1Qcz5-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-EO5eJZNCdoj1Qcz5-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-EO5eJZNCdoj1Qcz5-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-EO5eJZNCdoj1Qcz5-license").innerText=e.license.spdx_id:document.getElementById("repo-EO5eJZNCdoj1Qcz5-license").classList.add="no-license",document.getElementById("repo-EO5eJZNCdoj1Qcz5-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for hiyouga/LLaMA-Factory.")}).catch(e=>{const t=document.getElementById("repo-EO5eJZNCdoj1Qcz5-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for hiyouga/LLaMA-Factory.")})</script><p>Llama-Factory is launched by Meta and is a framework focused on fine-tuning Llama models. It is built on top of the PyTorch ecosystem and provides efficient training and evaluation tools.</p><p><strong>Main Features:</strong></p><ul><li><strong>Multiple models</strong>: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Yi, Gemma, Baichuan, ChatGLM, Phi, etc.</li><li><strong>Integration Methods</strong>: (Incremental) Pre-training, (Multimodal) Instruction Supervised Fine-tuning, Reward Model Training, PPO Training, DPO Training, KTO Training, ORPO Training, etc.</li><li><strong>Multiple Precisions</strong>: 16-bit full parameter fine-tuning, frozen fine-tuning, LoRA fine-tuning, and 2/3/4/5/6/8-bit QLoRA fine-tuning based on AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.</li><li><strong>Advanced Algorithms</strong>: GaLore, BAdam, DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ, PiSSA, and Agent fine-tuning.</li><li><strong>Practical Tips</strong>: FlashAttention-2, Unsloth, RoPE scaling, NEFTune, and rsLoRA.</li><li><strong>Experiment Monitoring</strong>: LlamaBoard, TensorBoard, Wandb, MLflow, etc.</li><li><strong>Rapid Reasoning</strong>: OpenAI-style API, browser interface, and command-line interface based on vLLM.</li></ul><link href=https://web.sdk.qcloud.com/player/tcplayer/release/v4.2.1/tcplayer.min.css rel=stylesheet><script src=https://web.sdk.qcloud.com/player/tcplayer/release/v4.2.1/libs/hls.min.0.13.2m.js></script><script src=https://web.sdk.qcloud.com/player/tcplayer/release/v4.2.1/tcplayer.v4.2.1.min.js></script><style>.tcplayer{position:absolute;width:100%;height:100%;left:0;top:0;border:0}</style><div class=video-wrapper><video id=player-container-id preload=auto width=100% height=100% playsinline webkit-playsinline></video></div><script>var tcplayer=TCPlayer("player-container-id",{reportable:!1,poster:""});tcplayer.src("https://cloud.cuterwrite.fun/img/2024-07-07_309488092-ec36a9dd-37f4-4f72-81bd-d76c6d0a6594_output.mp4")</script><p><strong>Performance Metrics</strong></p><p>Compared to the <a class=link href=https://github.com/THUDM/ChatGLM2-6B/tree/main/ptuning target=_blank rel=noopener>P-Tuning
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>fine-tuning by ChatGLM official, the LoRA fine-tuning by LLaMA Factory provides a <strong>3.7 times</strong> speedup and achieves higher Rouge scores in advertising copy generation tasks. Combined with 4-bit quantization technology, LLaMA Factory&rsquo;s QLoRA fine-tuning further reduces GPU memory consumption.</p><figure><img src=https://cloud.cuterwrite.fun/img/2024-07-07_benchmark.webp alt=Performance width=auto loading=lazy></figure><details><summary>Variable Definition</summary><ul><li><strong>Training Speed</strong>: Number of samples processed per second during the training phase. (Batch size=4, truncation length=1024)</li><li><strong>Rouge Score</strong>: Rouge-2 score on the validation set of the <a class=link href=https://aclanthology.org/D19-1321.pdf target=_blank rel=noopener>advertising copy generation
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>task. (Batch size=4, truncation length=1024)</li><li><strong>GPU Memory</strong>: Peak GPU memory for 4-bit quantization training. (Batch size=1, truncation length=1024)</li><li>We use <code>pre_seq_len=128</code> in the P-Tuning of ChatGLM, and <code>lora_rank=32</code> in the LoRA fine-tuning of LLaMA Factory.</li></ul></details><p><strong>Quick Start</strong></p><pre><code class=language-bash>git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e &quot;.[torch,metrics]&quot;
</code></pre><p>Optional additional dependencies: torch, torch-npu, metrics, deepspeed, bitsandbytes, hqq, eetq, gptq, awq, aqlm, vllm, galore, badam, qwen, modelscope, quality</p><blockquote class="alert-blockquote alert-tip"><p class=alert-heading><svg viewBox="0 0 16 16" width="16" height="16"><path d="M8 1.5c-2.363.0-4 1.69-4 3.75.0.984.424 1.625.984 2.304l.214.253c.223.264.47.556.673.848.284.411.537.896.621 1.49a.75.75.0 01-1.484.211c-.04-.282-.163-.547-.37-.847a8.456 8.456.0 00-.542-.68c-.084-.1-.173-.205-.268-.32C3.201 7.75 2.5 6.766 2.5 5.25 2.5 2.31 4.863.0 8 0s5.5 2.31 5.5 5.25c0 1.516-.701 2.5-1.328 3.259-.095.115-.184.22-.268.319-.207.245-.383.453-.541.681-.208.3-.33.565-.37.847a.751.751.0 01-1.485-.212c.084-.593.337-1.078.621-1.489.203-.292.45-.584.673-.848.075-.088.147-.173.213-.253.561-.679.985-1.32.985-2.304.0-2.06-1.637-3.75-4-3.75zM5.75 12h4.5a.75.75.0 010 1.5h-4.5a.75.75.0 010-1.5zM6 15.25a.75.75.0 01.75-.75h2.5a.75.75.0 010 1.5h-2.5A.75.75.0 016 15.25z"/></svg>
<span>Tip</span></p><p>When encountering package conflicts, you can use pip install &ndash;no-deps -e . to resolve them.</p></blockquote><details><summary>Windows User Guide</summary><p>If you want to enable Quantized LoRA (QLoRA) on the Windows platform, you need to install the precompiled <code>bitsandbytes</code> library, which supports CUDA 11.1 to 12.2. Please choose the appropriate <a class=link href=https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels target=_blank rel=noopener>release version
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>according to your CUDA version.</p><pre><code class=language-bash>pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl
</code></pre><p>If you want to enable FlashAttention-2 on the Windows platform, you need to install the precompiled <code>flash-attn</code> library, which supports CUDA 12.1 to 12.2. Please download and install the corresponding version according to your needs from <a class=link href=https://github.com/bdashore3/flash-attention/releases target=_blank rel=noopener>flash-attention
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>.</p></details><details><summary>Ascend NPU User Guide</summary><p>When installing LLaMA Factory on Ascend NPU devices, you need to specify additional dependencies and use the command <code>pip install -e ".[torch-npu,metrics]"</code> to install them. Additionally, you need to install the <strong><a class=link href="https://www.hiascend.com/developer/download/community/result?module=cann" target=_blank rel=noopener>Ascend CANN Toolkit and Kernels
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a></strong>. Please refer to the <a class=link href=https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/80RC2alpha002/quickstart/quickstart/quickstart_18_0004.html target=_blank rel=noopener>installation tutorial
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>or use the following command:</p><pre><code class=language-bash># Please replace URL with the URL corresponding to the CANN version and device model
# Install CANN Toolkit
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-toolkit_8.0.RC1.alpha001_linux-&quot;$(uname -i)&quot;.run
bash Ascend-cann-toolkit_8.0.RC1.alpha001_linux-&quot;$(uname -i)&quot;.run --install

# Install CANN Kernels
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run
bash Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run --install

# Set environment variables
source /usr/local/Ascend/ascend-toolkit/set_env.sh
</code></pre><div class=table-wrapper><table><thead><tr><th style=text-align:left>Dependencies</th><th style=text-align:left>Minimum</th><th style=text-align:left>Recommended</th></tr></thead><tbody><tr><td style=text-align:left>CANN</td><td style=text-align:left>8.0.RC1</td><td style=text-align:left>8.0.RC1</td></tr><tr><td style=text-align:left>torch</td><td style=text-align:left>2.1.0</td><td style=text-align:left>2.1.0</td></tr><tr><td style=text-align:left>torch-npu</td><td style=text-align:left>2.1.0</td><td style=text-align:left>2.1.0.post3</td></tr><tr><td style=text-align:left>deepspeed</td><td style=text-align:left>0.13.2</td><td style=text-align:left>0.13.2</td></tr></tbody></table></div><p>Please use <code>ASCEND_RT_VISIBLE_DEVICES</code> instead of <code>CUDA_VISIBLE_DEVICES</code> to specify the computation device.</p><p>If you encounter a situation where reasoning cannot proceed normally, try setting <code>do_sample: false</code>.</p><p>Download pre-built Docker image: <a class=link href=http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html target=_blank rel=noopener>32GB
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>| <a class=link href=http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html target=_blank rel=noopener>64GB
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a></p></details><p>The following three commands perform LoRA fine-tuning, inference, and merging on the Llama3-8B-Instruct model.</p><pre><code class=language-bash>llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml
</code></pre><p>For more detailed information, please visit the <a class=link href=https://github.com/hiyouga/LLaMA-Factory target=_blank rel=noopener>Llama-Factory
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>project homepage.</p><h3 id=firefly><a href=#firefly class=header-anchor>#</a>
Firefly</h3><a href=https://github.com/yangjianxin1/Firefly target=_blank class="card-github fetch-waiting no-styling" repo=yangjianxin1/Firefly id=repo-5HEMtlG7x2XNTMVf-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-5HEMtlG7x2XNTMVf-avatar class=gc-avatar></div><div class=gc-user>yangjianxin1</div></div><div class=gc-divider>/</div><div class=gc-repo>Firefly</div></div><div class=github-logo></div></div><div id=repo-5HEMtlG7x2XNTMVf-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-5HEMtlG7x2XNTMVf-stars class=gc-stars>0</div><div id=repo-5HEMtlG7x2XNTMVf-forks class=gc-forks>0</div><div id=repo-5HEMtlG7x2XNTMVf-license class=gc-license>unkown</div><div id=repo-5HEMtlG7x2XNTMVf-language class=gc-language>Waiting...</div></div></a><script id=repo-5HEMtlG7x2XNTMVf-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/yangjianxin1/Firefly",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-5HEMtlG7x2XNTMVf-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-5HEMtlG7x2XNTMVf-language").innerText=e.language,document.getElementById("repo-5HEMtlG7x2XNTMVf-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-5HEMtlG7x2XNTMVf-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-5HEMtlG7x2XNTMVf-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-5HEMtlG7x2XNTMVf-license").innerText=e.license.spdx_id:document.getElementById("repo-5HEMtlG7x2XNTMVf-license").classList.add="no-license",document.getElementById("repo-5HEMtlG7x2XNTMVf-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for yangjianxin1/Firefly.")}).catch(e=>{const t=document.getElementById("repo-5HEMtlG7x2XNTMVf-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for yangjianxin1/Firefly.")})</script><p><strong>Firefly</strong> is an open-source large model training project that supports pre-training, instruction fine-tuning, and DPO for mainstream large models, including but not limited to Qwen2, Yi-1.5, Llama3, Gemma, Qwen1.5, MiniCPM, Llama, InternLM, Baichuan, ChatGLM, Yi, Deepseek, Qwen, Orion, Ziya, Xverse, Mistral, Mixtral-8x7B, Zephyr, Vicuna, Bloom, etc.
This project supports <strong>full parameter training, LoRA, QLoRA efficient training</strong>, and supports <strong>pre-training, SFT, DPO</strong>. If your training resources are limited, we strongly recommend using QLoRA for instruction fine-tuning, as we have validated the effectiveness of this method on the Open LLM Leaderboard and achieved very good results.</p><p><strong>Main Features:</strong></p><ul><li>📗 Supports pre-training, instruction fine-tuning, DPO, full parameter training, LoRA, QLoRA efficient training. Train different models through configuration files, allowing beginners to quickly get started with model training.</li><li>📗 Supports using <a class=link href=https://github.com/yangjianxin1/unsloth target=_blank rel=noopener>Unsloth
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>to accelerate training and save video memory.</li><li>📗 Supports most mainstream open-source large models, such as Llama3, Gemma, MiniCPM, Llama, InternLM, Baichuan, ChatGLM, Yi, Deepseek, Qwen, Orion, Ziya, Xverse, Mistral, Mixtral-8x7B, Zephyr, Vicuna, Bloom, aligning with the templates of each official chat model during training.</li><li>📗 Organize and open-source instruction fine-tuning datasets: firefly-train-1.1M, moss-003-sft-data, ultrachat, WizardLM_evol_instruct_V2_143k, school_math_0.25M.</li><li>📗 Open source <a class=link href=https://huggingface.co/YeungNLP target=_blank rel=noopener>Firefly series instruction fine-tuning model weights
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>.</li><li>📗 Validated the effectiveness of the QLoRA training process on the Open LLM Leaderboard.</li></ul><p>The README of the project contains detailed usage instructions, including how to install, how to train, how to fine-tune, and how to evaluate, etc. Please visit the <a class=link href=https://github.com/yangjianxin1/Firefly target=_blank rel=noopener>Firefly
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>project homepage.</p><h3 id=xtuner><a href=#xtuner class=header-anchor>#</a>
XTuner</h3><a href=https://github.com/InternLM/xtuner target=_blank class="card-github fetch-waiting no-styling" repo=InternLM/xtuner id=repo-WSfnWmlVvppcLPA0-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-WSfnWmlVvppcLPA0-avatar class=gc-avatar></div><div class=gc-user>InternLM</div></div><div class=gc-divider>/</div><div class=gc-repo>xtuner</div></div><div class=github-logo></div></div><div id=repo-WSfnWmlVvppcLPA0-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-WSfnWmlVvppcLPA0-stars class=gc-stars>0</div><div id=repo-WSfnWmlVvppcLPA0-forks class=gc-forks>0</div><div id=repo-WSfnWmlVvppcLPA0-license class=gc-license>unkown</div><div id=repo-WSfnWmlVvppcLPA0-language class=gc-language>Waiting...</div></div></a><script id=repo-WSfnWmlVvppcLPA0-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/InternLM/xtuner",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-WSfnWmlVvppcLPA0-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-WSfnWmlVvppcLPA0-language").innerText=e.language,document.getElementById("repo-WSfnWmlVvppcLPA0-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-WSfnWmlVvppcLPA0-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-WSfnWmlVvppcLPA0-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-WSfnWmlVvppcLPA0-license").innerText=e.license.spdx_id:document.getElementById("repo-WSfnWmlVvppcLPA0-license").classList.add="no-license",document.getElementById("repo-WSfnWmlVvppcLPA0-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for InternLM/xtuner.")}).catch(e=>{const t=document.getElementById("repo-WSfnWmlVvppcLPA0-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for InternLM/xtuner.")})</script><p>XTuner is an efficient, flexible, and versatile lightweight large model fine-tuning tool library.</p><p><strong>Main Features:</strong></p><ul><li><strong>Efficient</strong><ul><li>Supports pre-training and lightweight fine-tuning of large language models (LLM) and multimodal image-text models (VLM). XTuner supports fine-tuning a 7B model with 8GB of video memory and also supports multi-node cross-device fine-tuning of larger scale models (70B+).</li><li>Automatically distribute high-performance operators (such as FlashAttention, Triton kernels, etc.) to accelerate training throughput.</li><li>Compatible with <a class=link href=https://github.com/microsoft/DeepSpeed target=_blank rel=noopener>DeepSpeed
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>🚀, easily apply various ZeRO training optimization strategies.</li></ul></li><li><strong>Flexible</strong><ul><li>Supports multiple large language models, including but not limited to <a class=link href=https://huggingface.co/internlm target=_blank rel=noopener>InternLM
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>, <a class=link href=https://huggingface.co/mistralai target=_blank rel=noopener>Mixtral-8x7B
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>, <a class=link href=https://huggingface.co/meta-llama target=_blank rel=noopener>Llama 2
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>, <a class=link href=https://huggingface.co/THUDM target=_blank rel=noopener>ChatGLM
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>, <a class=link href=https://huggingface.co/Qwen target=_blank rel=noopener>Qwen
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>, <a class=link href=https://huggingface.co/baichuan-inc target=_blank rel=noopener>Baichuan
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>.</li></ul></li><li>Supports pre-training and fine-tuning of the multimodal text-image model LLaVA. The model <a class=link href=https://huggingface.co/xtuner/llava-internlm2-20b target=_blank rel=noopener>LLaVA-InternLM2-20B
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>trained with XTuner performs excellently.<ul><li>Carefully designed data pipeline, compatible with any data format, both open-source data and custom data can be quickly adopted.</li><li>Supports multiple fine-tuning algorithms such as <a class=link href=http://arxiv.org/abs/2305.14314 target=_blank rel=noopener>QLoRA
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>, <a class=link href=http://arxiv.org/abs/2106.09685 target=_blank rel=noopener>LoRA
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>, and full parameter fine-tuning, allowing users to make the optimal choice based on specific needs.</li></ul></li><li><strong>Omnipotent</strong><ul><li>Supports incremental pre-training, instruction fine-tuning, and Agent fine-tuning.</li><li>Predefined numerous open-source dialogue templates, supporting conversation with open-source or trained models.</li><li>The trained model can be seamlessly integrated with the deployment toolkit <a class=link href=https://github.com/InternLM/lmdeploy target=_blank rel=noopener>LMDeploy
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>, the large-scale evaluation toolkit <a class=link href=https://github.com/open-compass/opencompass target=_blank rel=noopener>OpenCompass
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>, and <a class=link href=https://github.com/open-compass/VLMEvalKit target=_blank rel=noopener>VLMEvalKit
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>.</li></ul></li></ul><p><strong>Quick Start:</strong><details><summary>Installation</summary><ul><li>It is recommended to use conda to first build a Python-3.10 virtual environment</li></ul><pre><code class=language-bash>  conda create --name xtuner-env python=3.10 -y
  conda activate xtuner-env
</code></pre><ul><li>Install XTuner via pip:</li></ul><pre><code class=language-shell>  pip install -U xtuner
</code></pre><p>Can also integrate DeepSpeed installation:</p><pre><code class=language-shell>  pip install -U 'xtuner[deepspeed]'
</code></pre><ul><li>Install XTuner from source:</li></ul><pre><code class=language-shell>  git clone https://github.com/InternLM/xtuner.git
  cd xtuner
  pip install -e '.[all]'

</code></pre></details></p><details><summary>Fine-tuning</summary><p>XTuner supports fine-tuning large language models. For dataset preprocessing guidelines, please refer to the <a class=link href=./docs/zh_cn/user_guides/dataset_prepare.md>documentation
</a>.</p><ul><li><strong>Step 0</strong>, prepare the configuration file. XTuner provides multiple out-of-the-box configuration files, which users can view with the following command:</li></ul><pre><code class=language-shell>  xtuner list-cfg
</code></pre><p>Or, if the provided configuration file does not meet the usage requirements, please export the provided configuration file and make the corresponding changes:</p><pre><code class=language-shell>  xtuner copy-cfg ${CONFIG_NAME} ${SAVE_PATH}
  vi ${SAVE_PATH}/${CONFIG_NAME}_copy.py
</code></pre><ul><li><strong>Step 1</strong>, start fine-tuning.</li></ul><pre><code class=language-shell>  xtuner train ${CONFIG_NAME_OR_PATH}
</code></pre><p>For example, we can use the QLoRA algorithm to fine-tune InternLM2.5-Chat-7B on the oasst1 dataset:</p><pre><code class=language-shell># Single card
xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2
# Multi-card
  (DIST) NPROC_PER_NODE=${GPU_NUM} xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2
(SLURM) srun ${SRUN_ARGS} xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --launcher slurm --deepspeed deepspeed_zero2
</code></pre><ul><li><p><code>--deepspeed</code> indicates using <a class=link href=https://github.com/microsoft/DeepSpeed target=_blank rel=noopener>DeepSpeed
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>🚀 to optimize the training process. XTuner has multiple built-in strategies, including ZeRO-1, ZeRO-2, ZeRO-3, etc. If the user wishes to disable this feature, please remove this parameter directly.</p><ul><li>For more examples, please refer to the <a class=link href=./docs/zh_cn/user_guides/finetune.md>documentation
</a>.</li></ul></li><li><p><strong>Step 2</strong>, convert the saved PTH model (if using DeepSpeed, it will be a folder) to a HuggingFace model:</p></li></ul><pre><code class=language-shell>  xtuner convert pth_to_hf ${CONFIG_NAME_OR_PATH} ${PTH} ${SAVE_PATH}
</code></pre></details><p>For more detailed information, please visit the <a class=link href=https://github.com/InternLM/xtuner target=_blank rel=noopener>XTuner
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>project homepage.</p><h2 id=model-quantization><a href=#model-quantization class=header-anchor>#</a>
Model quantization</h2><p>LLM is usually large in volume and requires high computational resources. Model quantization techniques can compress model size, improve operational efficiency, and make it easier to deploy:</p><h3 id=autogptq><a href=#autogptq class=header-anchor>#</a>
AutoGPTQ</h3><a href=https://github.com/PanQiWei/AutoGPTQ target=_blank class="card-github fetch-waiting no-styling" repo=PanQiWei/AutoGPTQ id=repo-RjpVZsTlHp61KbZJ-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-RjpVZsTlHp61KbZJ-avatar class=gc-avatar></div><div class=gc-user>PanQiWei</div></div><div class=gc-divider>/</div><div class=gc-repo>AutoGPTQ</div></div><div class=github-logo></div></div><div id=repo-RjpVZsTlHp61KbZJ-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-RjpVZsTlHp61KbZJ-stars class=gc-stars>0</div><div id=repo-RjpVZsTlHp61KbZJ-forks class=gc-forks>0</div><div id=repo-RjpVZsTlHp61KbZJ-license class=gc-license>unkown</div><div id=repo-RjpVZsTlHp61KbZJ-language class=gc-language>Waiting...</div></div></a><script id=repo-RjpVZsTlHp61KbZJ-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/PanQiWei/AutoGPTQ",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-RjpVZsTlHp61KbZJ-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-RjpVZsTlHp61KbZJ-language").innerText=e.language,document.getElementById("repo-RjpVZsTlHp61KbZJ-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-RjpVZsTlHp61KbZJ-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-RjpVZsTlHp61KbZJ-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-RjpVZsTlHp61KbZJ-license").innerText=e.license.spdx_id:document.getElementById("repo-RjpVZsTlHp61KbZJ-license").classList.add="no-license",document.getElementById("repo-RjpVZsTlHp61KbZJ-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for PanQiWei/AutoGPTQ.")}).catch(e=>{const t=document.getElementById("repo-RjpVZsTlHp61KbZJ-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for PanQiWei/AutoGPTQ.")})</script><p>AutoGPTQ is a large language model quantization toolkit based on the GPTQ algorithm, simple to use and with a user-friendly interface.</p><p><strong>Quick Installation</strong></p><ul><li>For CUDA 11.7:</li></ul><pre><code class=language-bash>pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/
</code></pre><ul><li>For CUDA 11.8:</li></ul><pre><code class=language-bash>pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/
</code></pre><ul><li>For RoCm 5.4.2:</li></ul><pre><code class=language-bash>pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/rocm542/
</code></pre><p>For more detailed information, please visit the project homepage of <a class=link href=https://github.com/AutoGPTQ/AutoGPTQ/ target=_blank rel=noopener>AutoGPTQ
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>.</p><h3 id=autoawq><a href=#autoawq class=header-anchor>#</a>
AutoAWQ</h3><a href=https://github.com/casper-hansen/AutoAWQ target=_blank class="card-github fetch-waiting no-styling" repo=casper-hansen/AutoAWQ id=repo-lcqZRbdUrvaJrSQR-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-lcqZRbdUrvaJrSQR-avatar class=gc-avatar></div><div class=gc-user>casper-hansen</div></div><div class=gc-divider>/</div><div class=gc-repo>AutoAWQ</div></div><div class=github-logo></div></div><div id=repo-lcqZRbdUrvaJrSQR-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-lcqZRbdUrvaJrSQR-stars class=gc-stars>0</div><div id=repo-lcqZRbdUrvaJrSQR-forks class=gc-forks>0</div><div id=repo-lcqZRbdUrvaJrSQR-license class=gc-license>unkown</div><div id=repo-lcqZRbdUrvaJrSQR-language class=gc-language>Waiting...</div></div></a><script id=repo-lcqZRbdUrvaJrSQR-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/casper-hansen/AutoAWQ",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-lcqZRbdUrvaJrSQR-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-lcqZRbdUrvaJrSQR-language").innerText=e.language,document.getElementById("repo-lcqZRbdUrvaJrSQR-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-lcqZRbdUrvaJrSQR-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-lcqZRbdUrvaJrSQR-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-lcqZRbdUrvaJrSQR-license").innerText=e.license.spdx_id:document.getElementById("repo-lcqZRbdUrvaJrSQR-license").classList.add="no-license",document.getElementById("repo-lcqZRbdUrvaJrSQR-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for casper-hansen/AutoAWQ.")}).catch(e=>{const t=document.getElementById("repo-lcqZRbdUrvaJrSQR-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for casper-hansen/AutoAWQ.")})</script><p>AutoAWQ is another automated model quantization tool that supports multiple quantization precisions and offers flexible configuration options, allowing adjustments based on different hardware platforms and performance requirements.</p><p>AutoAWQ is an easy-to-use 4-bit quantization model package. Compared to FP16, AutoAWQ can increase model speed by 3 times and reduce memory requirements by 3 times. AutoAWQ implements the activation-aware weight quantization (AWQ) algorithm for quantizing LLMs. AutoAWQ was created and improved based on the original work <a class=link href=https://github.com/mit-han-lab/llm-awq target=_blank rel=noopener>AWQ
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>from MIT.</p><p><strong>Installation Method:</strong></p><p>Before installing, ensure that CUDA >= 12.1 is installed (Note: The following is just the quickest installation method)</p><pre><code class=language-bash>pip install autoawq
</code></pre><p>For more details and examples, please visit the project homepage of <a class=link href=https://github.com/casper-hansen/AutoAWQ target=_blank rel=noopener>AutoAWQ
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>.</p><h3 id=neural-compressor><a href=#neural-compressor class=header-anchor>#</a>
Neural Compressor</h3><a href=https://github.com/intel/neural-compressor target=_blank class="card-github fetch-waiting no-styling" repo=intel/neural-compressor id=repo-mgduZaNKZa0Z80Uy-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-mgduZaNKZa0Z80Uy-avatar class=gc-avatar></div><div class=gc-user>intel</div></div><div class=gc-divider>/</div><div class=gc-repo>neural-compressor</div></div><div class=github-logo></div></div><div id=repo-mgduZaNKZa0Z80Uy-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-mgduZaNKZa0Z80Uy-stars class=gc-stars>0</div><div id=repo-mgduZaNKZa0Z80Uy-forks class=gc-forks>0</div><div id=repo-mgduZaNKZa0Z80Uy-license class=gc-license>unkown</div><div id=repo-mgduZaNKZa0Z80Uy-language class=gc-language>Waiting...</div></div></a><script id=repo-mgduZaNKZa0Z80Uy-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/intel/neural-compressor",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-mgduZaNKZa0Z80Uy-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-mgduZaNKZa0Z80Uy-language").innerText=e.language,document.getElementById("repo-mgduZaNKZa0Z80Uy-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-mgduZaNKZa0Z80Uy-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-mgduZaNKZa0Z80Uy-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-mgduZaNKZa0Z80Uy-license").innerText=e.license.spdx_id:document.getElementById("repo-mgduZaNKZa0Z80Uy-license").classList.add="no-license",document.getElementById("repo-mgduZaNKZa0Z80Uy-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for intel/neural-compressor.")}).catch(e=>{const t=document.getElementById("repo-mgduZaNKZa0Z80Uy-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for intel/neural-compressor.")})</script><p>Neural Compressor is a model compression toolkit developed by Intel, supporting popular model compression techniques on all major deep learning frameworks (TensorFlow, PyTorch, ONNX Runtime, and MXNet).</p><p><strong>Installation Method:</strong></p><pre><code class=language-bash>pip install &quot;neural-compressor&gt;=2.3&quot; &quot;transformers&gt;=4.34.0&quot; torch torchvision
</code></pre><p>For more detailed information and examples, please visit the project homepage of <a class=link href=https://github.com/intel/neural-compressor target=_blank rel=noopener>Neural Compressor
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>.</p><h2 id=model-deployment><a href=#model-deployment class=header-anchor>#</a>
Model deployment</h2><p>Deploying a trained LLM to a production environment is crucial. Here are some commonly used LLM deployment tools:</p><h3 id=vllm><a href=#vllm class=header-anchor>#</a>
vLLM</h3><a href=https://github.com/vllm-project/vllm target=_blank class="card-github fetch-waiting no-styling" repo=vllm-project/vllm id=repo-7qIBPjsHkBqA6FnA-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-7qIBPjsHkBqA6FnA-avatar class=gc-avatar></div><div class=gc-user>vllm-project</div></div><div class=gc-divider>/</div><div class=gc-repo>vllm</div></div><div class=github-logo></div></div><div id=repo-7qIBPjsHkBqA6FnA-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-7qIBPjsHkBqA6FnA-stars class=gc-stars>0</div><div id=repo-7qIBPjsHkBqA6FnA-forks class=gc-forks>0</div><div id=repo-7qIBPjsHkBqA6FnA-license class=gc-license>unkown</div><div id=repo-7qIBPjsHkBqA6FnA-language class=gc-language>Waiting...</div></div></a><script id=repo-7qIBPjsHkBqA6FnA-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/vllm-project/vllm",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-7qIBPjsHkBqA6FnA-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-7qIBPjsHkBqA6FnA-language").innerText=e.language,document.getElementById("repo-7qIBPjsHkBqA6FnA-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-7qIBPjsHkBqA6FnA-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-7qIBPjsHkBqA6FnA-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-7qIBPjsHkBqA6FnA-license").innerText=e.license.spdx_id:document.getElementById("repo-7qIBPjsHkBqA6FnA-license").classList.add="no-license",document.getElementById("repo-7qIBPjsHkBqA6FnA-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for vllm-project/vllm.")}).catch(e=>{const t=document.getElementById("repo-7qIBPjsHkBqA6FnA-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for vllm-project/vllm.")})</script><p>vLLM is a fast and easy-to-use LLM inference service library.</p><p><strong>Main Features:</strong></p><ul><li><strong>Fast</strong><ul><li>SOTA service throughput</li><li>Efficiently manage attention key-value memory using PagedAttention</li><li>Continuously batch process received requests</li><li>Use CUDA/HIP graphs for acceleration</li><li>Quantization: Supports GPTQ, AWQ, SqueezeLLM, FP8 KV cache</li><li>Optimized CUDA kernel</li></ul></li><li><strong>Flexible</strong><ul><li>Seamless integration with popular Hugging Face models</li><li>Provide high-throughput services using various decoding algorithms (including parallel sampling, beam search, etc.)</li><li>Provide tensor parallel support for distributed inference</li><li>Stream output</li><li>Compatible with OpenAI&rsquo;s application programming interface server</li><li>Supports NVIDIA GPU, AMD GPU, Intel CPU, and GPU</li><li>(Experimental) Support prefix caching</li><li>(Experimental) Support for multiple languages</li></ul></li><li><strong>Seamless Support</strong><ul><li>Transformer-based models, such as Llama</li><li>MoE-based model, such as Mixtral</li><li>Multimodal models, such as LLaVA</li></ul></li></ul><p><strong>Quick Installation:</strong></p><pre><code class=language-bash>pip install vllm
</code></pre><p>For more detailed information, please refer to the <a class=link href=https://vllm.readthedocs.io/en/latest/ target=_blank rel=noopener>vLLM
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>official documentation.</p><h3 id=sgl><a href=#sgl class=header-anchor>#</a>
SGL</h3><a href=https://github.com/sgl-project/sglang target=_blank class="card-github fetch-waiting no-styling" repo=sgl-project/sglang id=repo-sJn5YVpNmlSXC8LZ-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-sJn5YVpNmlSXC8LZ-avatar class=gc-avatar></div><div class=gc-user>sgl-project</div></div><div class=gc-divider>/</div><div class=gc-repo>sglang</div></div><div class=github-logo></div></div><div id=repo-sJn5YVpNmlSXC8LZ-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-sJn5YVpNmlSXC8LZ-stars class=gc-stars>0</div><div id=repo-sJn5YVpNmlSXC8LZ-forks class=gc-forks>0</div><div id=repo-sJn5YVpNmlSXC8LZ-license class=gc-license>unkown</div><div id=repo-sJn5YVpNmlSXC8LZ-language class=gc-language>Waiting...</div></div></a><script id=repo-sJn5YVpNmlSXC8LZ-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/sgl-project/sglang",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-sJn5YVpNmlSXC8LZ-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-sJn5YVpNmlSXC8LZ-language").innerText=e.language,document.getElementById("repo-sJn5YVpNmlSXC8LZ-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-sJn5YVpNmlSXC8LZ-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-sJn5YVpNmlSXC8LZ-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-sJn5YVpNmlSXC8LZ-license").innerText=e.license.spdx_id:document.getElementById("repo-sJn5YVpNmlSXC8LZ-license").classList.add="no-license",document.getElementById("repo-sJn5YVpNmlSXC8LZ-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for sgl-project/sglang.")}).catch(e=>{const t=document.getElementById("repo-sJn5YVpNmlSXC8LZ-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for sgl-project/sglang.")})</script><p>SGLang is a structured generation language designed specifically for large language models (LLMs). By co-designing the front-end language and the runtime system, it makes your interactions with LLMs faster and more controllable.</p><p><strong>Main Features:</strong></p><ul><li><strong>Flexible front-end language</strong>: Easily write LLM applications through chainable generation calls, advanced prompts, control flow, multiple modes, concurrency, and external interaction.</li><li><strong>High-performance backend runtime</strong>: Features RadixAttention capability, which can accelerate complex LLM programs by reusing KV cache across multiple calls. It can also function as a standalone inference engine, implementing all common techniques (such as continuous batching and tensor parallelism).</li></ul><p>For more detailed information, please visit the <a class=link href=https://github.com/sgl-project/ target=_blank rel=noopener>SGL
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>project homepage.</p><h3 id=skypilot><a href=#skypilot class=header-anchor>#</a>
SkyPilot</h3><a href=https://github.com/skypilot-org/skypilot target=_blank class="card-github fetch-waiting no-styling" repo=skypilot-org/skypilot id=repo-4K3DZwLsUSWBxz6q-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-4K3DZwLsUSWBxz6q-avatar class=gc-avatar></div><div class=gc-user>skypilot-org</div></div><div class=gc-divider>/</div><div class=gc-repo>skypilot</div></div><div class=github-logo></div></div><div id=repo-4K3DZwLsUSWBxz6q-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-4K3DZwLsUSWBxz6q-stars class=gc-stars>0</div><div id=repo-4K3DZwLsUSWBxz6q-forks class=gc-forks>0</div><div id=repo-4K3DZwLsUSWBxz6q-license class=gc-license>unkown</div><div id=repo-4K3DZwLsUSWBxz6q-language class=gc-language>Waiting...</div></div></a><script id=repo-4K3DZwLsUSWBxz6q-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/skypilot-org/skypilot",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-4K3DZwLsUSWBxz6q-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-4K3DZwLsUSWBxz6q-language").innerText=e.language,document.getElementById("repo-4K3DZwLsUSWBxz6q-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-4K3DZwLsUSWBxz6q-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-4K3DZwLsUSWBxz6q-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-4K3DZwLsUSWBxz6q-license").innerText=e.license.spdx_id:document.getElementById("repo-4K3DZwLsUSWBxz6q-license").classList.add="no-license",document.getElementById("repo-4K3DZwLsUSWBxz6q-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for skypilot-org/skypilot.")}).catch(e=>{const t=document.getElementById("repo-4K3DZwLsUSWBxz6q-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for skypilot-org/skypilot.")})</script><p>SkyPilot is a flexible cloud LLM deployment tool launched by UC Berkeley RISELab, supporting multiple cloud platforms and hardware accelerators. It can automatically select the optimal deployment plan and provide cost optimization features.</p><p><strong>Main Features:</strong></p><ul><li><strong>Multi-cloud support:</strong> Supports various cloud platforms such as AWS, GCP, Azure, allowing users to choose the appropriate deployment environment.</li><li><strong>Easy to expand</strong>: Queue and run multiple jobs, automatic management</li><li><strong>Easy Access to Object Storage</strong>: Easily access object storage (S3, GCS, R2)</li></ul><p>For more detailed information, please visit the <a class=link href=https://github.com/skypilot-org/skypilot target=_blank rel=noopener>SkyPilot
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>project homepage.</p><h3 id=tensorrt-llm><a href=#tensorrt-llm class=header-anchor>#</a>
TensorRT-LLM</h3><a href=https://github.com/NVIDIA/TensorRT-LLM target=_blank class="card-github fetch-waiting no-styling" repo=NVIDIA/TensorRT-LLM id=repo-hYxwEuuFKyxztBth-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-hYxwEuuFKyxztBth-avatar class=gc-avatar></div><div class=gc-user>NVIDIA</div></div><div class=gc-divider>/</div><div class=gc-repo>TensorRT-LLM</div></div><div class=github-logo></div></div><div id=repo-hYxwEuuFKyxztBth-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-hYxwEuuFKyxztBth-stars class=gc-stars>0</div><div id=repo-hYxwEuuFKyxztBth-forks class=gc-forks>0</div><div id=repo-hYxwEuuFKyxztBth-license class=gc-license>unkown</div><div id=repo-hYxwEuuFKyxztBth-language class=gc-language>Waiting...</div></div></a><script id=repo-hYxwEuuFKyxztBth-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/NVIDIA/TensorRT-LLM",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-hYxwEuuFKyxztBth-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-hYxwEuuFKyxztBth-language").innerText=e.language,document.getElementById("repo-hYxwEuuFKyxztBth-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-hYxwEuuFKyxztBth-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-hYxwEuuFKyxztBth-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-hYxwEuuFKyxztBth-license").innerText=e.license.spdx_id:document.getElementById("repo-hYxwEuuFKyxztBth-license").classList.add="no-license",document.getElementById("repo-hYxwEuuFKyxztBth-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for NVIDIA/TensorRT-LLM.")}).catch(e=>{const t=document.getElementById("repo-hYxwEuuFKyxztBth-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for NVIDIA/TensorRT-LLM.")})</script><p>TensorRT-LLM is a high-performance LLM inference engine launched by NVIDIA, capable of fully utilizing GPU accelerated computation and optimized for the Transformer model architecture, significantly improving inference speed.</p><p>TensorRT-LLM provides users with an easy-to-use Python API for defining large language models (LLMs) and building TensorRT engines, which incorporate state-of-the-art optimization techniques for efficient inference execution on NVIDIA® graphics processors. TensorRT-LLM also includes components for creating Python and C++ runtimes that execute these TensorRT engines.</p><p>For more details, please visit the <a class=link href=https://github.com/NVIDIA/TensorRT-LLM target=_blank rel=noopener>TensorRT-LLM
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>project homepage.</p><h3 id=openvino><a href=#openvino class=header-anchor>#</a>
OpenVino</h3><a href=https://github.com/openvinotoolkit/openvino target=_blank class="card-github fetch-waiting no-styling" repo=openvinotoolkit/openvino id=repo-lgySZNaLlFaz7h8Q-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-lgySZNaLlFaz7h8Q-avatar class=gc-avatar></div><div class=gc-user>openvinotoolkit</div></div><div class=gc-divider>/</div><div class=gc-repo>openvino</div></div><div class=github-logo></div></div><div id=repo-lgySZNaLlFaz7h8Q-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-lgySZNaLlFaz7h8Q-stars class=gc-stars>0</div><div id=repo-lgySZNaLlFaz7h8Q-forks class=gc-forks>0</div><div id=repo-lgySZNaLlFaz7h8Q-license class=gc-license>unkown</div><div id=repo-lgySZNaLlFaz7h8Q-language class=gc-language>Waiting...</div></div></a><script id=repo-lgySZNaLlFaz7h8Q-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/openvinotoolkit/openvino",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-lgySZNaLlFaz7h8Q-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-lgySZNaLlFaz7h8Q-language").innerText=e.language,document.getElementById("repo-lgySZNaLlFaz7h8Q-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-lgySZNaLlFaz7h8Q-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-lgySZNaLlFaz7h8Q-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-lgySZNaLlFaz7h8Q-license").innerText=e.license.spdx_id:document.getElementById("repo-lgySZNaLlFaz7h8Q-license").classList.add="no-license",document.getElementById("repo-lgySZNaLlFaz7h8Q-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for openvinotoolkit/openvino.")}).catch(e=>{const t=document.getElementById("repo-lgySZNaLlFaz7h8Q-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for openvinotoolkit/openvino.")})</script><p>OpenVINO™ is an open-source toolkit for optimizing and deploying artificial intelligence inference.</p><p><strong>Main Features:</strong></p><ul><li><strong>Inference Optimization</strong>: Enhance the performance of deep learning in computer vision, automatic speech recognition, generative AI, natural language processing using large and small language models, and many other common tasks.</li><li><strong>Flexible Model Support</strong>: Models trained with popular frameworks such as TensorFlow, PyTorch, ONNX, Keras, and PaddlePaddle. Convert and deploy models without the need for the original framework.</li><li><strong>Broad Platform Compatibility</strong>: Reduce resource requirements and efficiently deploy across a range of platforms from edge to cloud. OpenVINO™ supports inference on CPUs (x86, ARM), GPUs (integrated and discrete GPUs supporting OpenCL), and AI accelerators (Intel NPU).</li><li><strong>Community and Ecosystem</strong>: Join an active community contributing to improving deep learning performance in various fields.</li></ul><p>For more information, please visit the project homepage of <a class=link href=https://github.com/openvinotoolkit/openvino target=_blank rel=noopener>OpenVino
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>.</p><h3 id=tgi><a href=#tgi class=header-anchor>#</a>
TGI</h3><a href=https://github.com/huggingface/text-generation-inference target=_blank class="card-github fetch-waiting no-styling" repo=huggingface/text-generation-inference id=repo-RgN5Ks7irDPykXSV-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-RgN5Ks7irDPykXSV-avatar class=gc-avatar></div><div class=gc-user>huggingface</div></div><div class=gc-divider>/</div><div class=gc-repo>text-generation-inference</div></div><div class=github-logo></div></div><div id=repo-RgN5Ks7irDPykXSV-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-RgN5Ks7irDPykXSV-stars class=gc-stars>0</div><div id=repo-RgN5Ks7irDPykXSV-forks class=gc-forks>0</div><div id=repo-RgN5Ks7irDPykXSV-license class=gc-license>unkown</div><div id=repo-RgN5Ks7irDPykXSV-language class=gc-language>Waiting...</div></div></a><script id=repo-RgN5Ks7irDPykXSV-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/huggingface/text-generation-inference",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-RgN5Ks7irDPykXSV-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-RgN5Ks7irDPykXSV-language").innerText=e.language,document.getElementById("repo-RgN5Ks7irDPykXSV-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-RgN5Ks7irDPykXSV-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-RgN5Ks7irDPykXSV-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-RgN5Ks7irDPykXSV-license").innerText=e.license.spdx_id:document.getElementById("repo-RgN5Ks7irDPykXSV-license").classList.add="no-license",document.getElementById("repo-RgN5Ks7irDPykXSV-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for huggingface/text-generation-inference.")}).catch(e=>{const t=document.getElementById("repo-RgN5Ks7irDPykXSV-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for huggingface/text-generation-inference.")})</script><p>Text Generation Inference (TGI) is a toolkit for deploying and serving large language models (LLMs). TGI enables high-performance text generation for the most popular open-source LLMs, including Llama, Falcon, StarCoder, BLOOM, GPT-NeoX, and others.</p><p>TGI has implemented many features, and detailed information can be found on the project homepage of <a class=link href=https://github.com/huggingface/text-generation-inference target=_blank rel=noopener>TGI
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>.</p><h2 id=local-run><a href=#local-run class=header-anchor>#</a>
Local run</h2><p>Thanks to model compression and optimization techniques, we can also run LLM on personal devices:</p><h3 id=mlx><a href=#mlx class=header-anchor>#</a>
MLX</h3><a href=https://github.com/ml-explore/mlx target=_blank class="card-github fetch-waiting no-styling" repo=ml-explore/mlx id=repo-88dtx15eudIHwdJV-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-88dtx15eudIHwdJV-avatar class=gc-avatar></div><div class=gc-user>ml-explore</div></div><div class=gc-divider>/</div><div class=gc-repo>mlx</div></div><div class=github-logo></div></div><div id=repo-88dtx15eudIHwdJV-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-88dtx15eudIHwdJV-stars class=gc-stars>0</div><div id=repo-88dtx15eudIHwdJV-forks class=gc-forks>0</div><div id=repo-88dtx15eudIHwdJV-license class=gc-license>unkown</div><div id=repo-88dtx15eudIHwdJV-language class=gc-language>Waiting...</div></div></a><script id=repo-88dtx15eudIHwdJV-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/ml-explore/mlx",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-88dtx15eudIHwdJV-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-88dtx15eudIHwdJV-language").innerText=e.language,document.getElementById("repo-88dtx15eudIHwdJV-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-88dtx15eudIHwdJV-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-88dtx15eudIHwdJV-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-88dtx15eudIHwdJV-license").innerText=e.license.spdx_id:document.getElementById("repo-88dtx15eudIHwdJV-license").classList.add="no-license",document.getElementById("repo-88dtx15eudIHwdJV-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for ml-explore/mlx.")}).catch(e=>{const t=document.getElementById("repo-88dtx15eudIHwdJV-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for ml-explore/mlx.")})</script><p>MLX is a framework specifically designed to support running LLM on Apple devices, fully utilizing Metal to accelerate computation, and providing easy-to-use APIs to facilitate developers in integrating LLM into iOS applications.</p><p><strong>Main Features:</strong></p><ul><li><strong>Similar Application Programming Interface</strong>: MLX&rsquo;s Python API is very similar to NumPy. MLX also has fully functional C++, C, and Swift APIs, which are very similar to the Python API. MLX has higher-level packages like <code>mlx.nn</code> and <code>mlx.optimizers</code>, whose APIs are very close to PyTorch, simplifying the construction of more complex models.</li><li><strong>Composable Function Transformations</strong>: MLX supports composable function transformations for automatic differentiation, automatic vectorization, and computation graph optimization.</li><li><strong>Lazy Evaluation</strong>: In MLX, computations only materialize arrays when needed.</li><li><strong>Dynamic Graph Construction</strong>: In MLX, the computational graph is dynamically constructed. Changing the shape of function parameters does not slow down the compilation speed, and debugging is simple and intuitive.</li><li><strong>Multi-device</strong>: Operations can run on any supported device (currently CPU and GPU).</li><li><strong>Unified Memory</strong>: The unified memory model is a significant difference between MLX and other frameworks. Arrays in MLX reside in shared memory. Operations on MLX arrays can be performed on any supported device type without the need to transfer data.</li></ul><p>MLX is designed by machine learning researchers for machine learning researchers. The framework aims to be user-friendly while still efficiently training and deploying models. The design concept of the framework itself is also very simple. Our goal is to enable researchers to easily expand and improve MLX, thus quickly exploring new ideas. For more details, please visit the project homepage of <a class=link href=https://github.com/ml-explore/mlx target=_blank rel=noopener>MLX
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>.</p><h3 id=llamacpp><a href=#llamacpp class=header-anchor>#</a>
Llama.cpp</h3><p>Llama.cpp is a Llama model inference engine implemented in C++, capable of running efficiently on CPUs, and supports multiple operating systems and hardware platforms, allowing developers to run LLM on resource-constrained devices.</p><a href=https://github.com/ggerganov/llama.cpp target=_blank class="card-github fetch-waiting no-styling" repo=ggerganov/llama.cpp id=repo-IGEAT5UcIUDvS911-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-IGEAT5UcIUDvS911-avatar class=gc-avatar></div><div class=gc-user>ggerganov</div></div><div class=gc-divider>/</div><div class=gc-repo>llama.cpp</div></div><div class=github-logo></div></div><div id=repo-IGEAT5UcIUDvS911-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-IGEAT5UcIUDvS911-stars class=gc-stars>0</div><div id=repo-IGEAT5UcIUDvS911-forks class=gc-forks>0</div><div id=repo-IGEAT5UcIUDvS911-license class=gc-license>unkown</div><div id=repo-IGEAT5UcIUDvS911-language class=gc-language>Waiting...</div></div></a><script id=repo-IGEAT5UcIUDvS911-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/ggerganov/llama.cpp",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-IGEAT5UcIUDvS911-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-IGEAT5UcIUDvS911-language").innerText=e.language,document.getElementById("repo-IGEAT5UcIUDvS911-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-IGEAT5UcIUDvS911-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-IGEAT5UcIUDvS911-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-IGEAT5UcIUDvS911-license").innerText=e.license.spdx_id:document.getElementById("repo-IGEAT5UcIUDvS911-license").classList.add="no-license",document.getElementById("repo-IGEAT5UcIUDvS911-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for ggerganov/llama.cpp.")}).catch(e=>{const t=document.getElementById("repo-IGEAT5UcIUDvS911-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for ggerganov/llama.cpp.")})</script><p><strong>Main Features:</strong></p><ul><li><strong>CPU Inference:</strong> Optimized for CPU platforms, allowing LLM to run on devices without a GPU.</li><li><strong>Cross-platform support:</strong> Supports multiple operating systems such as Linux, macOS, Windows, making it convenient for users to use on different platforms.</li><li><strong>Lightweight Deployment:</strong> The compiled binary files are small, making it convenient for users to deploy and use.</li></ul><p>For more detailed information, please visit the project homepage of <a class=link href=https://github.com/ggerganov/llama.cpp target=_blank rel=noopener>Llama.cpp
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>.</p><h3 id=ollama><a href=#ollama class=header-anchor>#</a>
Ollama</h3><a href=https://github.com/ollama/ollama target=_blank class="card-github fetch-waiting no-styling" repo=ollama/ollama id=repo-XQfvMSwzeQhfBX58-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-XQfvMSwzeQhfBX58-avatar class=gc-avatar></div><div class=gc-user>ollama</div></div><div class=gc-divider>/</div><div class=gc-repo>ollama</div></div><div class=github-logo></div></div><div id=repo-XQfvMSwzeQhfBX58-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-XQfvMSwzeQhfBX58-stars class=gc-stars>0</div><div id=repo-XQfvMSwzeQhfBX58-forks class=gc-forks>0</div><div id=repo-XQfvMSwzeQhfBX58-license class=gc-license>unkown</div><div id=repo-XQfvMSwzeQhfBX58-language class=gc-language>Waiting...</div></div></a><script id=repo-XQfvMSwzeQhfBX58-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/ollama/ollama",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-XQfvMSwzeQhfBX58-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-XQfvMSwzeQhfBX58-language").innerText=e.language,document.getElementById("repo-XQfvMSwzeQhfBX58-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-XQfvMSwzeQhfBX58-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-XQfvMSwzeQhfBX58-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-XQfvMSwzeQhfBX58-license").innerText=e.license.spdx_id:document.getElementById("repo-XQfvMSwzeQhfBX58-license").classList.add="no-license",document.getElementById("repo-XQfvMSwzeQhfBX58-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for ollama/ollama.")}).catch(e=>{const t=document.getElementById("repo-XQfvMSwzeQhfBX58-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for ollama/ollama.")})</script><p>In the article <a class=link href=/en/p/ollama/>【Ollama: From Beginner to Advanced】
</a>, it is introduced that Ollama is a tool for building large language model applications. It provides a simple and easy-to-use command line interface and server, allowing you to easily download, run, and manage various open-source LLMs. Unlike traditional LLMs that require complex configuration and powerful hardware, Ollama allows you to conveniently experience the powerful features of LLMs as easily as using a mobile app.</p><p><strong>Main Features:</strong></p><ul><li><strong>Simple and Easy to Use</strong>: Ollama provides a simple and easy-to-use command line tool for users to download, run, and manage LLM.</li><li><strong>Multiple models</strong>: Ollama supports various open-source LLMs, including Qwen2, Llama3, Mistral, etc.</li><li><strong>Compatible with OpenAI Interface</strong>: Ollama supports the OpenAI API interface, making it easy to switch existing applications to Ollama.</li></ul><p>For more details, please visit the project homepage of <a class=link href=https://github.com/ollama/ollama target=_blank rel=noopener>Ollama
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>.</p><h2 id=agent-and-rag-framework><a href=#agent-and-rag-framework class=header-anchor>#</a>
Agent and RAG framework</h2><p>Combining LLM with external data and tools can build more powerful applications. Here are some commonly used Agent and RAG frameworks:</p><h3 id=llamaindex><a href=#llamaindex class=header-anchor>#</a>
LlamaIndex</h3><a href=https://github.com/run-llama/llama_index target=_blank class="card-github fetch-waiting no-styling" repo=run-llama/llama_index id=repo-5PixVVMWkIgyjcRX-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-5PixVVMWkIgyjcRX-avatar class=gc-avatar></div><div class=gc-user>run-llama</div></div><div class=gc-divider>/</div><div class=gc-repo>llama_index</div></div><div class=github-logo></div></div><div id=repo-5PixVVMWkIgyjcRX-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-5PixVVMWkIgyjcRX-stars class=gc-stars>0</div><div id=repo-5PixVVMWkIgyjcRX-forks class=gc-forks>0</div><div id=repo-5PixVVMWkIgyjcRX-license class=gc-license>unkown</div><div id=repo-5PixVVMWkIgyjcRX-language class=gc-language>Waiting...</div></div></a><script id=repo-5PixVVMWkIgyjcRX-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/run-llama/llama_index",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-5PixVVMWkIgyjcRX-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-5PixVVMWkIgyjcRX-language").innerText=e.language,document.getElementById("repo-5PixVVMWkIgyjcRX-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-5PixVVMWkIgyjcRX-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-5PixVVMWkIgyjcRX-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-5PixVVMWkIgyjcRX-license").innerText=e.license.spdx_id:document.getElementById("repo-5PixVVMWkIgyjcRX-license").classList.add="no-license",document.getElementById("repo-5PixVVMWkIgyjcRX-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for run-llama/llama_index.")}).catch(e=>{const t=document.getElementById("repo-5PixVVMWkIgyjcRX-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for run-llama/llama_index.")})</script><p>LlamaIndex (GPT Index) is a data framework for LLM applications. Building applications with LlamaIndex typically requires using LlamaIndex core and a selected set of integrations (or plugins). There are two ways to build applications with LlamaIndex in Python:</p><ul><li>Launcher: <code>llama-index</code> ( <a class=link href=https://pypi.org/project/llama-index/%29 target=_blank rel=noopener>https://pypi.org/project/llama-index/)
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>. Python starter package, includes core LlamaIndex and some integrations.</li><li>Customization: <code>llama-index-core</code> ( <a class=link href=https://pypi.org/project/llama-index-core/%29 target=_blank rel=noopener>https://pypi.org/project/llama-index-core/)
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>. Install the core LlamaIndex and add the necessary LlamaIndex integration packages for your application on LlamaHub. There are currently over 300 LlamaIndex integration packages that can seamlessly collaborate with the core, allowing you to build using your preferred LLM, embeddings, and vector storage databases.</li></ul><p>The LlamaIndex Python library is named as such, so import statements containing <code>core</code> mean that the core package is being used. Conversely, those statements without <code>core</code> mean that the integration package is being used.</p><pre><code class=language-python># typical pattern
from llama_index.core.xxx import ClassABC  # core submodule xxx
from llama_index.xxx.yyy import (
    SubclassABC,
)  # integration yyy for submodule xxx

# concrete example
from llama_index.core.llms import LLM
from llama_index.llms.openai import OpenAI
</code></pre><h3 id=crewai><a href=#crewai class=header-anchor>#</a>
CrewAI</h3><a href=https://github.com/joaomdmoura/crewAI target=_blank class="card-github fetch-waiting no-styling" repo=joaomdmoura/crewAI id=repo-fu4SPI0dmQx5shp6-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-fu4SPI0dmQx5shp6-avatar class=gc-avatar></div><div class=gc-user>joaomdmoura</div></div><div class=gc-divider>/</div><div class=gc-repo>crewAI</div></div><div class=github-logo></div></div><div id=repo-fu4SPI0dmQx5shp6-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-fu4SPI0dmQx5shp6-stars class=gc-stars>0</div><div id=repo-fu4SPI0dmQx5shp6-forks class=gc-forks>0</div><div id=repo-fu4SPI0dmQx5shp6-license class=gc-license>unkown</div><div id=repo-fu4SPI0dmQx5shp6-language class=gc-language>Waiting...</div></div></a><script id=repo-fu4SPI0dmQx5shp6-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/joaomdmoura/crewAI",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-fu4SPI0dmQx5shp6-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-fu4SPI0dmQx5shp6-language").innerText=e.language,document.getElementById("repo-fu4SPI0dmQx5shp6-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-fu4SPI0dmQx5shp6-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-fu4SPI0dmQx5shp6-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-fu4SPI0dmQx5shp6-license").innerText=e.license.spdx_id:document.getElementById("repo-fu4SPI0dmQx5shp6-license").classList.add="no-license",document.getElementById("repo-fu4SPI0dmQx5shp6-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for joaomdmoura/crewAI.")}).catch(e=>{const t=document.getElementById("repo-fu4SPI0dmQx5shp6-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for joaomdmoura/crewAI.")})</script><p>CrewAI is a framework for building AI Agents that can integrate LLM with other tools and APIs to accomplish more complex tasks, such as automating web operations, generating code, and more.</p><p><strong>Main Features:</strong></p><ul><li><strong>Role-Based Agent Design</strong>: You can customize agents using specific roles, goals, and tools.</li><li><strong>Delegation between Autonomous Agents</strong>: Agents can autonomously delegate tasks to other agents and query information from each other, thereby improving problem-solving efficiency.</li><li><strong>Flexible task management</strong>: Customizable tools can be used to define tasks and dynamically assign tasks to agents.</li><li><strong>Process-Driven</strong>: The system is process-centered, currently supporting sequential task execution and hierarchical processes. In the future, it will also support more complex processes, such as negotiation and autonomous processes.</li><li><strong>Save output as file</strong>: Allows saving the output of a single task as a file for later use.</li><li><strong>Parse output to Pydantic or Json</strong>: It is possible to parse the output of a single task into a Pydantic model or Json format for easy subsequent processing and analysis.</li><li><strong>Support for Open Source Models</strong>: You can use OpenAI or other open source models to run your agent team. For more information on configuring agent and model connections, including how to connect to a locally running model, see <a class=link href=https://docs.crewai.com/how-to/LLM-Connections/ target=_blank rel=noopener>Connecting crewAI to Large Language Models
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>.</li></ul><p>For more detailed information, please visit the project homepage of <a class=link href=https://github.com/joaomdmoura/crewAI target=_blank rel=noopener>CrewAI
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>.</p><h3 id=opendevin><a href=#opendevin class=header-anchor>#</a>
OpenDevin</h3><a href=https://github.com/opendevin/opendevin target=_blank class="card-github fetch-waiting no-styling" repo=opendevin/opendevin id=repo-mXqJRSCjLP0N2QHR-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-mXqJRSCjLP0N2QHR-avatar class=gc-avatar></div><div class=gc-user>opendevin</div></div><div class=gc-divider>/</div><div class=gc-repo>opendevin</div></div><div class=github-logo></div></div><div id=repo-mXqJRSCjLP0N2QHR-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-mXqJRSCjLP0N2QHR-stars class=gc-stars>0</div><div id=repo-mXqJRSCjLP0N2QHR-forks class=gc-forks>0</div><div id=repo-mXqJRSCjLP0N2QHR-license class=gc-license>unkown</div><div id=repo-mXqJRSCjLP0N2QHR-language class=gc-language>Waiting...</div></div></a><script id=repo-mXqJRSCjLP0N2QHR-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/opendevin/opendevin",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-mXqJRSCjLP0N2QHR-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-mXqJRSCjLP0N2QHR-language").innerText=e.language,document.getElementById("repo-mXqJRSCjLP0N2QHR-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-mXqJRSCjLP0N2QHR-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-mXqJRSCjLP0N2QHR-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-mXqJRSCjLP0N2QHR-license").innerText=e.license.spdx_id:document.getElementById("repo-mXqJRSCjLP0N2QHR-license").classList.add="no-license",document.getElementById("repo-mXqJRSCjLP0N2QHR-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for opendevin/opendevin.")}).catch(e=>{const t=document.getElementById("repo-mXqJRSCjLP0N2QHR-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for opendevin/opendevin.")})</script><p>OpenDevin is an autonomous software engineer platform powered by artificial intelligence and LLMs.</p><p>OpenDevin agents collaborate with human developers to write code, fix bugs, and release features.</p><p>For more information, please visit the project homepage of <a class=link href=https://github.com/opendevin/opendevin target=_blank rel=noopener>OpenDevin
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span>
</a>.</p><h2 id=model-evaluation><a href=#model-evaluation class=header-anchor>#</a>
Model evaluation</h2><p>In order to select a suitable LLM and evaluate its performance, we need to conduct model evaluation:</p><h3 id=lmsys><a href=#lmsys class=header-anchor>#</a>
LMSys</h3><p>LMSys Org is an open research organization founded by students and faculty from the University of California, Berkeley, in collaboration with the University of California, San Diego, and Carnegie Mellon University.</p><p>The goal is to make large models accessible to everyone by jointly developing open models, datasets, systems, and evaluation tools. Train large language models and make their applications widely available, while also developing distributed systems to accelerate their training and inference process.</p><p>Currently, the LMSys Chatbot Area is one of the most recognized large model rankings, acknowledged by many companies and research institutions.</p><p>Leaderboard address: <a class=link href=https://arena.lmsys.org/ target=_blank rel=noopener>https://arena.lmsys.org/
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a></p><h3 id=opencompass><a href=#opencompass class=header-anchor>#</a>
OpenCompass</h3><p>OpenCompass is an LLM evaluation platform that supports various models (Llama3, Mistral, InternLM2, GPT-4, LLaMa2, Qwen, GLM, Claude, etc.) on over 100 datasets.</p><a href=https://github.com/open-compass/opencompass target=_blank class="card-github fetch-waiting no-styling" repo=open-compass/opencompass id=repo-2ahjPwRIE2BUkuLp-card><div class=gc-titlebar><div class=gc-titlebar-left><div class=gc-owner><div id=repo-2ahjPwRIE2BUkuLp-avatar class=gc-avatar></div><div class=gc-user>open-compass</div></div><div class=gc-divider>/</div><div class=gc-repo>opencompass</div></div><div class=github-logo></div></div><div id=repo-2ahjPwRIE2BUkuLp-description class=gc-description>Waiting for api.github.com...</div><div class=gc-infobar><div id=repo-2ahjPwRIE2BUkuLp-stars class=gc-stars>0</div><div id=repo-2ahjPwRIE2BUkuLp-forks class=gc-forks>0</div><div id=repo-2ahjPwRIE2BUkuLp-license class=gc-license>unkown</div><div id=repo-2ahjPwRIE2BUkuLp-language class=gc-language>Waiting...</div></div></a><script id=repo-2ahjPwRIE2BUkuLp-script type=text/javascript defer>fetch("https://api.cuterwrite.top/api/repos/open-compass/opencompass",{referrerPolicy:"no-referrer"}).then(e=>e.json()).then(e=>{document.getElementById("repo-2ahjPwRIE2BUkuLp-description").innerText=e.description.replace(/:[a-zA-Z0-9_]+:/g,""),document.getElementById("repo-2ahjPwRIE2BUkuLp-language").innerText=e.language,document.getElementById("repo-2ahjPwRIE2BUkuLp-forks").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.forks).replaceAll(" ",""),document.getElementById("repo-2ahjPwRIE2BUkuLp-stars").innerText=Intl.NumberFormat("en-us",{notation:"compact",maximumFractionDigits:1}).format(e.stargazers_count).replaceAll(" ","");const t=document.getElementById("repo-2ahjPwRIE2BUkuLp-avatar");t.style.backgroundImage="url("+e.owner.avatar_url+")",t.style.backgroundColor="transparent",e.license?.spdx_id?document.getElementById("repo-2ahjPwRIE2BUkuLp-license").innerText=e.license.spdx_id:document.getElementById("repo-2ahjPwRIE2BUkuLp-license").classList.add="no-license",document.getElementById("repo-2ahjPwRIE2BUkuLp-card").classList.remove("fetch-waiting"),console.log("[GITHUB-CARD] Loaded card for open-compass/opencompass.")}).catch(e=>{const t=document.getElementById("repo-2ahjPwRIE2BUkuLp-card");t.classList.add("fetch-error"),console.warn("[GITHUB-CARD] (Error) Loading card for open-compass/opencompass.")})</script><h3 id=open-llm-leaderboard><a href=#open-llm-leaderboard class=header-anchor>#</a>
Open LLM Leaderboard</h3><p>Open LLM Leaderboard is a continuously updated LLM ranking list that ranks different models based on multiple evaluation metrics, making it convenient for developers to understand the latest model performance and development trends.</p><p>Leaderboard address: <a class=link href=https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard target=_blank rel=noopener>https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard
<span style=white-space:nowrap><svg width=".8em" height=".8em" viewBox="0 0 21 21"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z" fill="currentcolor"/><path d="M19 19H5V5h7l-2-2H5c-1.103.0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103.0 2-.897 2-2v-5l-2-2v7z" fill="currentcolor"/></span></a></p><h2 id=summary><a href=#summary class=header-anchor>#</a>
Summary</h2><p>The LLM ecosystem is thriving, covering all aspects from model training to application implementation. With continuous technological advancements, it is believed that LLM will play a more important role in more fields, bringing us a more intelligent application experience.</p></section><footer class=article-footer><section class=article-tags><a href=/en/tags/llm/>llm</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><script type=text/javascript src=/js/prism.js async></script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/en/p/integrate-open-webui-ollama-qwen25-local-rag/><div class=article-image><img src=https://cloud.cuterwrite.fun/img/2024-06-29_116903285_p0_master1200.webp loading=lazy data-key=integrate-open-webui-ollama-qwen25-local-rag data-hash=https://cloud.cuterwrite.fun/img/2024-06-29_116903285_p0_master1200.webp></div><div class=article-details><h2 class=article-title>Implementing Local RAG Service: Integrating Open WebUI, Ollama, and Qwen2.5</h2></div></a></article><article class=has-image><a href=/en/p/ollama/><div class=article-image><img src=https://cloud.cuterwrite.fun/img/2024-06-16_116903387_p0_master1200.webp loading=lazy data-key=ollama data-hash=https://cloud.cuterwrite.fun/img/2024-06-16_116903387_p0_master1200.webp></div><div class=article-details><h2 class=article-title>Ollama: From Beginner to Advanced</h2></div></a></article></div></div></aside><script src=https://unpkg.com/twikoo@1.6.39/dist/twikoo.all.min.js></script><div id=tcomment></div><style>.twikoo{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}:root[data-scheme=dark]{--twikoo-body-text-color-main:rgba(255, 255, 255, 0.9);--twikoo-body-text-color:rgba(255, 255, 255, 0.7)}.twikoo .el-input-group__prepend,.twikoo .tk-action-icon,.twikoo .tk-time,.twikoo .tk-comments-count{color:var(--twikoo-body-text-color)}.twikoo .el-input__inner,.twikoo .el-textarea__inner,.twikoo .tk-preview-container,.twikoo .tk-content,.twikoo .tk-nick,.twikoo .tk-send{color:var(--twikoo-body-text-color-main)}.twikoo .el-button{color:var(--twikoo-body-text-color)!important}.OwO .OwO-body{background-color:var(--body-background)!important;color:var(--body-text-color)!important}</style><script>twikoo.init({envId:"https://comment.cuterwrite.top",el:"#tcomment",lang:"zh-CN"})</script><footer class=site-footer><section class=copyright>&copy;
2021 -
2024 cuterwrite</section><section class=running-time>本博客已稳定运行
<span id=runningdays class=running-days></span><br>总访客数：
<span id=busuanzi_value_site_uv class=running-days>Loading</span><br>总访问量：
<span id=busuanzi_value_site_pv class=running-days>Loading</span></section><section class=totalcount>发表了
<span class=running-days>25</span> 篇文章 ·
总计
<span class=running-days>60.67k</span> 字</section><section class=powerby><hr>Built with <b><a style=color:#9e8f9f href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a></b><br>Theme <b><a style=color:#9e8f9f href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.27.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a><br><span>基于 <a href=https://github.com/CaiJimmy/hugo-theme-stack/tree/v3.27.0 target=_blank rel=noopener><b style=color:#9e8f9f>v3.27.0</b></a> 分支版本修改</span><br></section></footer><script>let s1="2021-4-17";s1=new Date(s1.replace(/-/g,"/"));let s2=new Date,timeDifference=s2.getTime()-s1.getTime(),days=Math.floor(timeDifference/(1e3*60*60*24)),hours=Math.floor(timeDifference%(1e3*60*60*24)/(1e3*60*60)),minutes=Math.floor(timeDifference%(1e3*60*60)/(1e3*60)),result=days+"天"+hours+"小时"+minutes+"分钟";document.getElementById("runningdays").innerHTML=result</script><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://libs.jshub.com/photoswipe/4.1.3/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://libs.jshub.com/photoswipe/4.1.3/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://libs.jshub.com/photoswipe/4.1.3/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://libs.jshub.com/photoswipe/4.1.3/photoswipe.min.css crossorigin=anonymous></main></div><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.font.im/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script><meta name=apple-mobile-web-app-capable content="yes"><meta name=theme-color content="#ffffff"><script>"serviceWorker"in navigator&&window.addEventListener("load",()=>{navigator.serviceWorker.register("/sw.js").then(e=>{console.log("Service worker registered with scope: ",e.scope)},e=>{console.log("Service worker registration failed: ",e)})})</script><script defer src=https://cn.vercount.one/js></script></body></html>