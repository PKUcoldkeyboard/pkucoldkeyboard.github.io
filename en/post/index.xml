<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Cuterwrite's Blog</title><link>https://cuterwrite.top/en/post/</link><description>Recent content in Posts on Cuterwrite's Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>cuterwrite</copyright><lastBuildDate>Fri, 20 Sep 2024 22:44:00 +0000</lastBuildDate><atom:link href="https://cuterwrite.top/en/post/index.xml" rel="self" type="application/rss+xml"/><item><title>Implementing Local RAG Service: Integrating Open WebUI, Ollama, and Qwen2.5</title><link>https://cuterwrite.top/en/p/integrate-open-webui-ollama-qwen25-local-rag/</link><pubDate>Fri, 20 Sep 2024 22:44:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/integrate-open-webui-ollama-qwen25-local-rag/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_116903285_p0_master1200.webp" alt="Featured image of post Implementing Local RAG Service: Integrating Open WebUI, Ollama, and Qwen2.5" />&lt;h1 id="implement-local-rag-service-integrate-open-webui-ollama-and-qwen25">
&lt;a href="#implement-local-rag-service-integrate-open-webui-ollama-and-qwen25" class="header-anchor">#&lt;/a>
Implement Local RAG Service: Integrate Open WebUI, Ollama, and Qwen2.5
&lt;/h1>
&lt;h2 id="introduction">
&lt;a href="#introduction" class="header-anchor">#&lt;/a>
Introduction
&lt;/h2>
&lt;p>When building information retrieval and generative AI applications, the Retrieval-Augmented Generation (RAG) model is increasingly favored by developers for its powerful ability to retrieve relevant information from a knowledge base and generate accurate answers. However, to implement an end-to-end local RAG service, not only is an appropriate model required, but also the integration of a robust user interface and an efficient inference framework.&lt;/p>
&lt;p>When building a local RAG service, using the easily deployable Docker method can greatly simplify model management and service integration. Here, we rely on the user interface and model inference service provided by Open WebUI, and then introduce the &lt;code>bge-m3&lt;/code> embedding model through Ollama to achieve document vectorization-based retrieval functionality, thereby helping Qwen2.5 generate more accurate answers.&lt;/p>
&lt;p>In this article, we will discuss how to quickly start Open WebUI through Docker, synchronize Ollama&amp;rsquo;s RAG capabilities, and combine the Qwen2.5 model to achieve an efficient document retrieval and generation system.&lt;/p>
&lt;h2 id="project-overview">
&lt;a href="#project-overview" class="header-anchor">#&lt;/a>
Project Overview
&lt;/h2>
&lt;p>This project will use the following key tools:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Open WebUI&lt;/strong>: Provides a web interface for user interaction with the model.&lt;/li>
&lt;li>&lt;strong>Ollama&lt;/strong>: Used for managing embedding and large language model inference tasks. Among them, the &lt;code>bge-m3&lt;/code> model in Ollama will be used for document retrieval, and Qwen2.5 will be responsible for answer generation.&lt;/li>
&lt;li>&lt;strong>Qwen2.5&lt;/strong>: The model part uses the Qwen 2.5 series launched by Alibaba, providing natural language generation for retrieval-augmented generation services.&lt;/li>
&lt;/ol>
&lt;p>In order to implement the RAG service, we need the following steps:&lt;/p>
&lt;ol>
&lt;li>Deploy Open WebUI as the user interaction interface.&lt;/li>
&lt;li>Configure Ollama for efficient scheduling of the Qwen2.5 series models.&lt;/li>
&lt;li>Use the embedding model named &lt;code>bge-m3&lt;/code> configured by Ollama to implement retrieval vectorization.&lt;/li>
&lt;/ol>
&lt;h2 id="deploy-open-webui">
&lt;a href="#deploy-open-webui" class="header-anchor">#&lt;/a>
Deploy Open WebUI
&lt;/h2>
&lt;p>Open WebUI provides a simple Docker-based solution, allowing users to launch the web interface directly via Docker without manually configuring numerous dependencies.&lt;/p>
&lt;p>First, make sure that &lt;a class="link" href="https://docs.docker.com/get-docker/" target="_blank" rel="noopener" >Docker
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
is installed on the server. If it is not installed, you can quickly install it using the following command:&lt;/p>
&lt;pre>&lt;code class="language-bash">curl https://get.docker.com | sh
&lt;/code>&lt;/pre>
&lt;p>Then create a directory to save the Open WebUI data, so the data will not be lost after the project is updated:&lt;/p>
&lt;pre>&lt;code class="language-bash">sudo mkdir -p /DATA/open-webui
&lt;/code>&lt;/pre>
&lt;p>Next, we can start Open WebUI with the following command:&lt;/p>
&lt;pre>&lt;code class="language-bash">docker run -d -p 3000:8080 \
--add-host=host.docker.internal:host-gateway \
-v /DATA/open-webui:/app/backend/data \
--name open-webui \
--restart always \
ghcr.io/open-webui/open-webui:main
&lt;/code>&lt;/pre>
&lt;p>If you want to run Open WebUI with Nvidia GPU support, you can use the following command:&lt;/p>
&lt;pre>&lt;code class="language-bash">docker run -d -p 3000:8080 \
--gpus all \
--add-host=host.docker.internal:host-gateway \
-v /DATA/open-webui:/app/backend/data \
--name open-webui \
--restart always \
ghcr.io/open-webui/open-webui:cuda
&lt;/code>&lt;/pre>
&lt;p>Here we expose the Open WebUI service on port 3000 of the machine, which can be accessed via a browser at &lt;code>http://localhost:3000&lt;/code> (for remote access, use the public IP and open port 3000). /DATA/open-webui is the data storage directory, you can adjust this path as needed.&lt;/p>
&lt;p>Of course, besides the Docker installation method, you can also install Open WebUI via pip, source code compilation, Podman, and other methods. For more installation methods, please refer to the &lt;a class="link" href="https://docs.openwebui.com/getting-started" target="_blank" rel="noopener" >Open WebUI official documentation
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;h3 id="basic-settings">
&lt;a href="#basic-settings" class="header-anchor">#&lt;/a>
Basic Settings
&lt;/h3>
&lt;ol>
&lt;li>Enter the account information to be registered, &lt;strong>set a strong password!!!&lt;/strong>&lt;/li>
&lt;/ol>
&lt;blockquote class="alert-blockquote alert-important">
&lt;p class="alert-heading">
&lt;svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16">
&lt;path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v9.5A1.75 1.75 0 0 1 14.25 13H8.06l-2.573 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25Zm7 2.25v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 9a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z">&lt;/path>
&lt;/svg>
&lt;span>Important&lt;/span>
&lt;/p>
&lt;p>The first registered user will be automatically set as the system administrator, so please ensure you are the first registered user.&lt;/p>
&lt;/blockquote>
&lt;ol start="2">
&lt;li>Click the avatar in the lower left corner, select the Admin Panel&lt;/li>
&lt;li>Click Settings in the panel&lt;/li>
&lt;li>Disable allowing new user registrations (optional)&lt;/li>
&lt;li>Click Save in the lower right corner&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-09-21_openwebui_admin.webp"
alt="Open WebUI" width="85%" loading="lazy">
&lt;/figure>
&lt;h2 id="configure-ollama-and-qwen25">
&lt;a href="#configure-ollama-and-qwen25" class="header-anchor">#&lt;/a>
Configure Ollama and Qwen2.5
&lt;/h2>
&lt;h3 id="deploy-ollama">
&lt;a href="#deploy-ollama" class="header-anchor">#&lt;/a>
Deploy Ollama
&lt;/h3>
&lt;p>Install Ollama on the local server. Currently, Ollama offers multiple installation methods. Please refer to Ollama&amp;rsquo;s &lt;a class="link" href="https://ollama.com/" target="_blank" rel="noopener" >official documentation
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
to download and install the latest version &lt;code>0.3.11&lt;/code> (Qwen2.5 is only supported starting from this version). For installation details, you can refer to an article I wrote earlier: &lt;a class="link" href="https://cuterwrite.top/en/p/ollama/" >Ollama: From Beginner to Advanced
&lt;/a>
.&lt;/p>
&lt;p>Start the Ollama service (not needed if started via Docker, but the 11434 port must be exposed):&lt;/p>
&lt;pre>&lt;code class="language-bash">ollama serve
&lt;/code>&lt;/pre>
&lt;p>After the Ollama service starts, you can connect to the Ollama service by visiting &lt;code>http://localhost:11434&lt;/code>.&lt;/p>
&lt;p>&lt;a class="link" href="https://ollama.com/library" target="_blank" rel="noopener" >Ollama Library
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
provides semantic vector models (&lt;code>bge-m3&lt;/code>) as well as major text generation models (including Qwen2.5). Next, we will configure Ollama to meet the needs of this project for document retrieval and question-answer generation.&lt;/p>
&lt;h3 id="download-qwen25-model">
&lt;a href="#download-qwen25-model" class="header-anchor">#&lt;/a>
Download Qwen2.5 model
&lt;/h3>
&lt;p>To install Qwen2.5 through Ollama, you can directly run the &lt;code>ollama pull&lt;/code> command in the command line to download the Qwen2.5 model. For example, to download the 72B model of Qwen2.5, you can use the following command:&lt;/p>
&lt;pre>&lt;code class="language-bash">ollama pull qwen2.5:72b
&lt;/code>&lt;/pre>
&lt;p>This command will fetch the Qwen2.5 model from Ollama&amp;rsquo;s model repository and prepare the runtime environment.&lt;/p>
&lt;p>Qwen2.5 offers multiple model sizes, including 72B, 32B, 14B, 7B, 3B, 1.5B, 0.5B, etc. You can choose the appropriate model based on your needs and GPU memory size. I am using a server with 4x V100, so I can directly choose the 72B model. If you require faster token generation speed and can tolerate a slight performance loss, you can use the &lt;code>q4_0&lt;/code> quantized version &lt;code>qwen2.5:72b-instruct-q4_0&lt;/code>; if you can tolerate slower token generation speed, you can use &lt;code>qwen2.5:72b-instruct-q5_K_M&lt;/code>. For a server with 4x V100, although the &lt;code>q5_K_M&lt;/code> model&amp;rsquo;s token generation is noticeably laggy, I still chose the &lt;code>q5_K_M&lt;/code> model to test the performance of Qwen2.5.&lt;/p>
&lt;p>For personal computers with less video memory, it is recommended to use the 14B or 7B model, which can be downloaded using the following command:&lt;/p>
&lt;pre>&lt;code class="language-bash">ollama pull qwen2.5:14b
&lt;/code>&lt;/pre>
&lt;p>Or&lt;/p>
&lt;pre>&lt;code class="language-bash">ollama pull qwen2.5:7b
&lt;/code>&lt;/pre>
&lt;p>If you have started both Open WebUI and Ollama services, you can also download the model in the admin panel.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-09-21_download_qwen2-5-7b.webp"
alt="Download Ollama Model in Open WebUI" width="85%" loading="lazy">
&lt;/figure>
&lt;h3 id="download-bge-m3-model">
&lt;a href="#download-bge-m3-model" class="header-anchor">#&lt;/a>
Download bge-m3 model
&lt;/h3>
&lt;p>Download the &lt;code>bge-m3&lt;/code> model in Ollama, which is used for document vectorization. Run the following command in the command line to download the model (or download it in the Open WebUI interface):&lt;/p>
&lt;pre>&lt;code class="language-bash">ollama pull bge-m3:latest
&lt;/code>&lt;/pre>
&lt;p>Up to this point, we have completed the configuration of Ollama. Next, we will configure the RAG service in Open WebUI.&lt;/p>
&lt;h2 id="rag-integration-and-configuration">
&lt;a href="#rag-integration-and-configuration" class="header-anchor">#&lt;/a>
RAG Integration and Configuration
&lt;/h2>
&lt;h3 id="configure-ollamas-rag-interface-in-open-webui">
&lt;a href="#configure-ollamas-rag-interface-in-open-webui" class="header-anchor">#&lt;/a>
Configure Ollama&amp;rsquo;s RAG interface in Open WebUI
&lt;/h3>
&lt;h4 id="access-open-webui-management-interface">
&lt;a href="#access-open-webui-management-interface" class="header-anchor">#&lt;/a>
Access Open WebUI management interface
&lt;/h4>
&lt;p>After starting Open WebUI, you can directly access the service address through a web browser, log in to your administrator account, and then enter the administrator panel.&lt;/p>
&lt;h4 id="set-up-ollama-interface">
&lt;a href="#set-up-ollama-interface" class="header-anchor">#&lt;/a>
Set up Ollama interface
&lt;/h4>
&lt;p>In the Open WebUI admin panel, click &lt;strong>Settings&lt;/strong>, you will see the option for external connections, ensure that the address for the Ollama API is &lt;code>host.docker.internal:11434&lt;/code>, then click the &lt;strong>verify connection&lt;/strong> button on the right to confirm whether the Ollama service is properly connected.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-09-21_ollama_api.webp"
alt="Open WebUI Ollama Setting" width="85%" loading="lazy">
&lt;/figure>
&lt;h4 id="set-up-semantic-vector-model">
&lt;a href="#set-up-semantic-vector-model" class="header-anchor">#&lt;/a>
Set up semantic vector model
&lt;/h4>
&lt;p>In the Open WebUI admin panel, click &lt;strong>Settings&lt;/strong>, then click &lt;strong>Documents&lt;/strong>, and follow these steps:&lt;/p>
&lt;ol>
&lt;li>Set the semantic vector model engine to Ollama.&lt;/li>
&lt;li>Set the semantic vector model to &lt;code>bge-m3:latest&lt;/code>.&lt;/li>
&lt;li>The remaining settings can be kept as default. Here, I set the maximum file upload size to 10MB, the maximum number of uploads to 3, Top K to 5, block size and block overlap to 1500 and 100 respectively, and enabled PDF image processing.&lt;/li>
&lt;li>Click the bottom right corner to save.&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-09-21_openwebui_embedding.webp"
alt="Open WebUI Embedding Setting" width="85%" loading="lazy">
&lt;/figure>
&lt;h3 id="test-rag-service">
&lt;a href="#test-rag-service" class="header-anchor">#&lt;/a>
Test RAG Service
&lt;/h3>
&lt;p>Now, you have implemented a complete local RAG system. You can enter any natural language question in the main interface of Open WebUI, then upload the corresponding document. The system will call the semantic vector model to vectorize the document, then use the Qwen2.5 model to retrieve the document, generate an answer, and return it to the user.&lt;/p>
&lt;p>In the Open WebUI user chat interface, upload the document you want to retrieve, then enter your question and click send. Open WebUI will call Ollama&amp;rsquo;s &lt;code>bge-m3&lt;/code> model for document vectorization processing, and then call the Qwen2.5 model for question and answer generation.&lt;/p>
&lt;p>Here I uploaded a simple &lt;code>txt&lt;/code> file (text generated by GPT), the content is as follows:&lt;/p>
&lt;pre>&lt;code class="language-md"># 奇幻森林的冒险
## 引言
在一个遥远的王国边界，有一片神秘的奇幻森林，传说中栖息着许多奇异的生物和古老的魔法。很少有人敢于进入，因为进入森林的人都没有再回来过。故事的主人公是一个年轻的冒险者，他名叫艾文。
## 第一章：艾文的决定
艾文是一个热爱冒险和探索的年轻人，他从小就听过很多关于奇幻森林的故事。尽管家人和朋友都劝他不要去，但他坚定地认为，自己注定要揭开这片森林的秘密。一天清晨，他收拾好行囊，带着勇气和好奇心，向森林进发。
### 1.1 出发前的准备
在出发前，艾文去了城里最有名的图书馆，查阅了关于奇幻森林的资料。他发现，有一本古老的手稿记录了进入森林的路线，以及如何避开其中一些危险的生物。艾文将这本手稿复印在自己的笔记本上，准备在需要的时候参考。
### 1.2 第一次穿越
艾文刚进入森林就感觉到这里的气息与外界完全不同。空气中弥漫着浓郁的花香，还有隐隐约约的奇怪声音。穿越森林的第一天，艾文没有遇到什么危险，但他能感觉到，有什么东西在暗中观察他。
## 第二章：神秘生物
第二天，艾文继续深入森林。然而，他没走多远，就遇到了一只奇异的生物。这是一只会发光的小鹿，全身散发着柔和的蓝色光芒。起初，艾文感到惊讶和畏惧，但这只小鹿却没有攻击他的意思，还带着他走向一个隐秘的洞穴。
### 2.1 洞穴中的秘密
在洞穴中，艾文发现了一块古老的石板，石板上刻有一些奇怪的符号。小鹿似乎知道这些符号的含义，带着艾文一步一步地解读。原来，这些符号记载着一种强大的魔法，可以帮助他在森林中找到失落的宝藏。
### 2.2 获得帮助
艾文决定接受小鹿的帮助，解开这些符号的秘密。他们在洞穴中度过了几天，艾文学会了如何利用森林中的资源制作药剂和武器。通过这些，他在森林中的生存能力大大提高。
## 第三章：最终的试炼
在小鹿的指引下，艾文终于来到了森林的深处，那里有一个古老的祭坛。据说，只有最勇敢的冒险者才能通过祭坛的试炼，获得最终的宝藏。
### 3.1 面对恐惧
祭坛周围布满了各种陷阱和幻觉。艾文必须面对自己内心深处的恐惧，才能通过这些障碍。最终，他用智慧和勇气克服了一切，获得了进入祭坛的资格。
### 3.2 发现宝藏
在祭坛的中心，艾文发现了一颗闪闪发光的宝石。据传，这颗宝石拥有改变命运的力量。艾文拿起宝石，感受到了其中的强大力量。他知道，这不仅仅是一件珍宝，还有可能是破解奇幻森林秘密的关键。
## 结论
艾文成功地揭开了奇幻森林的部分秘密，成为了传说中的英雄。他的冒险故事也激励了更多年轻的冒险者，带着勇气和智慧，踏上探索未知世界的旅程。
&lt;/code>&lt;/pre>
&lt;p>Then three questions were asked separately(in Chinese):&lt;/p>
&lt;ol>
&lt;li>艾文在森林中遇到的奇异生物是什么？&lt;/li>
&lt;li>艾文在洞穴中找到的古老石板上刻的是什么？&lt;/li>
&lt;li>艾文在祭坛中心发现了什么宝藏？&lt;/li>
&lt;/ol>
&lt;p>The following image is the answer:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-09-21_qwen2-5-QA.webp"
alt="Open WebUI Qwen2.5 Answer" width="90%" loading="lazy">
&lt;/figure>
&lt;h2 id="summary">
&lt;a href="#summary" class="header-anchor">#&lt;/a>
Summary
&lt;/h2>
&lt;p>With the help of Open WebUI and Ollama, we can easily build an efficient and intuitive local RAG system. By using the &lt;code>bge-m3&lt;/code> semantic vector model for text vectorization, combined with the Qwen2.5 generation model, users can efficiently interact with document retrieval and enhanced generation tasks within a unified web interface. This not only protects data privacy but also significantly enhances the localization capabilities of generative AI.&lt;/p></description></item><item><title>Arm Matrix Acceleration: Scalable Matrix Extension SME</title><link>https://cuterwrite.top/en/p/arm-sme-for-performance/</link><pubDate>Tue, 13 Aug 2024 22:44:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/arm-sme-for-performance/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-14_117622407_p0_master1200.webp" alt="Featured image of post Arm Matrix Acceleration: Scalable Matrix Extension SME" />&lt;h1 id="arm-matrix-acceleration-scalable-matrix-extension-sme">
&lt;a href="#arm-matrix-acceleration-scalable-matrix-extension-sme" class="header-anchor">#&lt;/a>
Arm Matrix Acceleration: Scalable Matrix Extension SME
&lt;/h1>
&lt;h2 id="1-sme-introduction">
&lt;a href="#1-sme-introduction" class="header-anchor">#&lt;/a>
1. SME Introduction
&lt;/h2>
&lt;p>Scalable Matrix Extension SME is built on the basis of Scalable Vector Extensions (SVE and SVE2) and adds the capability to efficiently handle matrices. The main features include:&lt;/p>
&lt;ul>
&lt;li>Calculate the SVE vector&amp;rsquo;s outer product&lt;/li>
&lt;li>Matrix tile storage&lt;/li>
&lt;li>Loading, storing, inserting, and extracting tile vectors (including dynamic transposition)&lt;/li>
&lt;li>Streaming SVE mode&lt;/li>
&lt;/ul>
&lt;p>The table below summarizes the main features of SME, SVE, and SVE2:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">SME&lt;/th>
&lt;th style="text-align: left">SVE&lt;/th>
&lt;th style="text-align: left">SVE2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">Streaming SVE Mode&lt;/td>
&lt;td style="text-align: left">NEON DSP++&lt;/td>
&lt;td style="text-align: left">Scalable Vector&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Dynamic Matrix Transpose&lt;/td>
&lt;td style="text-align: left">Multi-Precision Arithmetic&lt;/td>
&lt;td style="text-align: left">Per-Lane Predication&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Vector Cross Product&lt;/td>
&lt;td style="text-align: left">Match Detection and Histogram&lt;/td>
&lt;td style="text-align: left">Gather-load and Scatter-store&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Load, store, insert, and extract matrix vectors&lt;/td>
&lt;td style="text-align: left">Non-temporal scatter/gather&lt;/td>
&lt;td style="text-align: left">Predict vectorization&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">&lt;/td>
&lt;td style="text-align: left">Bitwise Permute&lt;/td>
&lt;td style="text-align: left">ML Extension (FP16 + DOT)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">&lt;/td>
&lt;td style="text-align: left">AE, SHA3, SM4, Crypto&lt;/td>
&lt;td style="text-align: left">V8.6 BF16, FP and Int8 support&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>SME has defined the following new features:&lt;/p>
&lt;ul>
&lt;li>New architecture state, can be used to store two-dimensional matrix tile&lt;/li>
&lt;li>Streaming SVE mode, supports SVE2 instructions where the execution vector length matches the tile length.&lt;/li>
&lt;li>New instruction to accumulate (or decrement) the outer product of two vectors into a matrix tile.&lt;/li>
&lt;li>New load, store, and move instructions: Vectors can be written to a row or column of a matrix tile, or a row or column of a matrix tile can be read into a vector.&lt;/li>
&lt;/ul>
&lt;p>Similar to SVE2, SME is also an extension that supports scalable vector length, enabling vector length agnosticism (VLA), per-lane predication, predication-driven loop control, and management functions.&lt;/p>
&lt;h2 id="2-streaming-sve-mode">
&lt;a href="#2-streaming-sve-mode" class="header-anchor">#&lt;/a>
2. Streaming SVE mode
&lt;/h2>
&lt;p>SME introduced the Streaming SVE mode, which implements a subset of the SVE2 instruction set and adds new SME-specific instructions.&lt;/p>
&lt;p>Streaming SVE mode supports high-throughput streaming data processing for large datasets, and streaming data usually has simple loop control flow and limited conditionality.&lt;/p>
&lt;p>In Non-streaming SVE mode, the complete SVE2 instruction set is supported, typically handling complex data structures and complex judgments.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-14_3443.webp"
alt="Streaming SVE Mode and Non-streaming SVE Mode" width="80%" loading="lazy">&lt;figcaption>
&lt;h4>Streaming SVE Mode and Non-streaming SVE Mode&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>Most SME instructions are only available in Streaming SVE mode. The streaming vector length (SVL) in Streaming SVE mode may differ from the non-streaming vector length (NSVL).&lt;/p>
&lt;p>The expectation is: SVL should be longer than or equal to NSVL, that is, SVL &amp;gt;= NSVL. For example, the length of NSVL can be 128-bit, while the length of SVL can be 512-bit.&lt;/p>
&lt;p>The SVL of SME can be 128-bit, 256-bit, 512-bit, 1024-bit, or 2048-bit. SVL needs to be a power of 2, and NSVL needs to be a multiple of 128.&lt;/p>
&lt;p>Similar to SVE2, the software can control the &lt;code>SMCR_ELx.LEN&lt;/code> register bit to set the effective SVL length that EL1, EL2, EL3 want to use (it can be set shorter than the SVL supported by the hardware).&lt;/p>
&lt;p>For more information on the Streaming SVE mode, refer to section B1.4.6 of the Arm Architecture Reference Manual (A-profile architecture).&lt;/p>
&lt;h2 id="3-switch-between-non-streaming-and-streaming-sve-modes">
&lt;a href="#3-switch-between-non-streaming-and-streaming-sve-modes" class="header-anchor">#&lt;/a>
3. Switch between Non-streaming and Streaming SVE modes
&lt;/h2>
&lt;p>If the CPU hardware implementation supports both Streaming SVE mode of SME and Non-streaming SVE mode of SVE2, applications can dynamically switch between these two operation modes based on their needs.&lt;/p>
&lt;p>Provide an independent operating mode for SME, allowing CPU hardware implementations to offer different vector lengths for the same application. For example, a CPU hardware implementation can choose to support a longer Streaming SVE mode vector length and optimize the hardware for stream operations suitable for high throughput.&lt;/p>
&lt;p>Applications can easily switch dynamically between Streaming SVE mode and Non-streaming SVE mode. The &lt;code>PSTATE.{SM, ZA}&lt;/code> bits introduced by SME can enable and disable Streaming SVE mode and SME ZA storage:&lt;/p>
&lt;ul>
&lt;li>SM: Enable and disable Streaming SVE mode&lt;/li>
&lt;li>ZA: Enable and disable ZA storage access&lt;/li>
&lt;/ul>
&lt;p>You can use the &lt;code>MSR/MRS&lt;/code> instructions to operate the Streaming Vector Control Register (SVCR) to set and read the &lt;code>PSTATE.{SM, ZA}&lt;/code> bits, with specific operations as follows:&lt;/p>
&lt;ul>
&lt;li>&lt;code>MSR SVCRSM, #&amp;lt;imm&amp;gt; MSR SVCRSM, #&lt;/code>&lt;/li>
&lt;li>&lt;code>MSR SVCRZA, #&amp;lt;imm&amp;gt;&lt;/code>&lt;/li>
&lt;li>&lt;code>MSR SVCRSMZA, #&amp;lt;imm&amp;gt;&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>The SMSTART instruction is an alias for the &lt;code>MSR&lt;/code> instruction that sets &lt;code>PSTATE.SM&lt;/code> and &lt;code>PSTATE.ZA&lt;/code>.&lt;/p>
&lt;ul>
&lt;li>&lt;code>SMSTART&lt;/code>: Simultaneously enable Streaming SVE mode and ZA storage access&lt;/li>
&lt;li>&lt;code>SMSTART SM&lt;/code>: Enable Streaming SVE mode&lt;/li>
&lt;li>&lt;code>SMSTART ZA&lt;/code>: Enable ZA storage access&lt;/li>
&lt;/ul>
&lt;p>The SMSTOP instruction is an alias for the &lt;code>MSR&lt;/code> instruction that clears &lt;code>PSTATE.SM&lt;/code> and &lt;code>PSTATE.ZA&lt;/code>.&lt;/p>
&lt;ul>
&lt;li>&lt;code>SMSTOP&lt;/code>: Simultaneously disable Streaming SVE mode and ZA storage access&lt;/li>
&lt;li>&lt;code>SMSTOP SM&lt;/code>: Disable Streaming SVE mode&lt;/li>
&lt;li>&lt;code>SMSTOP ZA&lt;/code>: Disable ZA storage access&lt;/li>
&lt;/ul>
&lt;p>The diagram below shows how the application switches between Streaming SVE mode and Non-streaming SVE mode:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_Scalable_Matrix_p1.webp"
alt="Application switching Streaming SVE mode and Non-streaming SVE mode" width="50%" loading="lazy">&lt;figcaption>
&lt;h4>Application switching Streaming SVE mode and Non-streaming SVE mode&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>For more information on switching between Streaming SVE mode and Non-Streaming SVE mode using SMSTART and SMSTOP, please refer to sections C6.2.327 and C6.2.328 of the Arm Architecture Reference Manual on A-profile architecture.&lt;/p>
&lt;h2 id="4-sme-architecture-status">
&lt;a href="#4-sme-architecture-status" class="header-anchor">#&lt;/a>
4. SME Architecture Status
&lt;/h2>
&lt;p>Similar to SVE2, in Streaming SVE mode, it has &lt;code>Z0-Z31&lt;/code> vector registers and &lt;code>P0-P15&lt;/code> predicate registers.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_4130_ARM2799_3_Scalable_Matrix_p1.webp"
alt="Streaming mode registers" width="70%" loading="lazy">
&lt;/figure>
&lt;p>The lowest numbered SVE vector register &lt;code>Zn&lt;/code> also holds fixed-length &lt;code>Vn&lt;/code>, &lt;code>Qn&lt;/code>, &lt;code>Dn&lt;/code>, &lt;code>Sn&lt;/code>, &lt;code>Hn&lt;/code>, and &lt;code>Bn&lt;/code> registers.&lt;/p>
&lt;p>When entering Streaming SVE mode (&lt;code>PSTATE.SM&lt;/code> changes from 0 to 1) or exiting Streaming SVE mode (&lt;code>PSTATE.SM&lt;/code> changes from 1 to 0), all these registers will be zeroed.&lt;/p>
&lt;p>Most non-streaming SVE2 instructions can be used in Streaming SVE mode, but &lt;strong>may use different vector lengths&lt;/strong> (streaming mode uses VSL length, non-streaming mode uses NVSL length). The &lt;code>RDSVL&lt;/code> instruction can be used to read the current effective vector length VL.&lt;/p>
&lt;pre>&lt;code class="language-armasm">// Read multiple of Streaming SVE vector register size to Xd
RDSVL &amp;lt;Xd&amp;gt;, #&amp;lt;imm&amp;gt;
&lt;/code>&lt;/pre>
&lt;blockquote class="alert-blockquote alert-note">
&lt;p class="alert-heading">
&lt;svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16">
&lt;path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z">&lt;/path>
&lt;/svg>
&lt;span>Note&lt;/span>
&lt;/p>
&lt;p>Because SME supports Vector Length Agnostic (VLA), in Streaming SVE mode, software rarely needs to explicitly read the SVL vector length. In Non-streaming SVE mode, the RDSVL instruction is usually used to determine the value of SVL.&lt;/p>
&lt;/blockquote>
&lt;h2 id="5-za-array">
&lt;a href="#5-za-array" class="header-anchor">#&lt;/a>
5. ZA array
&lt;/h2>
&lt;p>The newly introduced ZA (Z Array, ZA Storage) in SME is a two-dimensional (2D) square array with a size of SVL x SVL. It is called Z Array because the length of its rows and columns is consistent with the Zn registers in Streaming SVE mode.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_4314_ARM2799_4_Scalable_Matrix_p1.webp"
alt="ZA array" width="50%" loading="lazy">&lt;figcaption>
&lt;h4>ZA array&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>For example: If the vector length in Streaming SVE mode is 256-bit, i.e., the length of the Zn register is 256-bit, then the size of ZA is 256/8 bytes x 256/8 bytes.&lt;/p>
&lt;p>The ZA array can be accessed in the following way:&lt;/p>
&lt;ul>
&lt;li>ZA array vector access&lt;/li>
&lt;li>ZA tiles&lt;/li>
&lt;li>ZA tile slices&lt;/li>
&lt;/ul>
&lt;h3 id="51-za-array-vector-access">
&lt;a href="#51-za-array-vector-access" class="header-anchor">#&lt;/a>
5.1 ZA array vector access
&lt;/h3>
&lt;p>A row of the ZA array can be accessed as a vector of SVL length, and this vector can contain elements with data type lengths of 8-bit, 16-bit, 32-bit, 64-bit, or 128-bit, such as 32-bit fp32 floating-point numbers.&lt;/p>
&lt;pre>&lt;code class="language-c">ZA.B[N], ZA.H[N], ZA.S[N], ZA.D[N], ZA.Q[N]
&lt;/code>&lt;/pre>
&lt;p>Among them, &lt;code>B, H, S, D, Q&lt;/code> represent 8-bit, 16-bit, 32-bit, 64-bit, 128-bit, respectively.&lt;/p>
&lt;p>The number of ZA array vectors is the same as the number of bytes in SVL. For example, if SLV is 256-bit, then the number of ZA array vectors is 32, and the range of N is from 0 to 31.&lt;/p>
&lt;p>To support context switching, SME introduces new &lt;code>LDR&lt;/code> and &lt;code>STR&lt;/code> instructions for loading and storing a ZA array vector from memory.&lt;/p>
&lt;pre>&lt;code class="language-armasm">LDR ZA[&amp;lt;Wv&amp;gt;, &amp;lt;imm&amp;gt;], [&amp;lt;Xn|SP&amp;gt;{, #&amp;lt;imm&amp;gt;, MUL VL}]
STR ZA[&amp;lt;Wv&amp;gt;, &amp;lt;imm&amp;gt;], [&amp;lt;Xn|SP&amp;gt;{, #&amp;lt;imm&amp;gt;, MUL VL}]
&lt;/code>&lt;/pre>
&lt;h3 id="52-za-tiles">
&lt;a href="#52-za-tiles" class="header-anchor">#&lt;/a>
5.2 ZA tiles
&lt;/h3>
&lt;p>ZA tile is a square two-dimensional submatrix within ZA. The width of a ZA tile is always SVL, which is the same as the width of the ZA array.&lt;/p>
&lt;p>How many usable ZA tiles ZA can be divided into is determined by the size of the data type of the elements:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Element Data Type Size&lt;/th>
&lt;th style="text-align: left">Tile Quantity&lt;/th>
&lt;th style="text-align: left">Tile Name&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">8-bit&lt;/td>
&lt;td style="text-align: left">1&lt;/td>
&lt;td style="text-align: left">ZA0.B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">16-bit&lt;/td>
&lt;td style="text-align: left">2&lt;/td>
&lt;td style="text-align: left">ZA0.H-ZA1.H&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">32-bit&lt;/td>
&lt;td style="text-align: left">4&lt;/td>
&lt;td style="text-align: left">ZA0.S-ZA3.S&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">64-bit&lt;/td>
&lt;td style="text-align: left">8&lt;/td>
&lt;td style="text-align: left">ZA0.D-ZA7.D&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">128-bit&lt;/td>
&lt;td style="text-align: left">16&lt;/td>
&lt;td style="text-align: left">ZA0.Q-ZA15.Q&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>When the element data type is 8-bit, ZA can only be accessed as a ZA tile (ZA0.B).&lt;/li>
&lt;li>When the element data type is 16-bit, ZA can be accessed as 2 ZA tiles (ZA0.H and ZA1.H).&lt;/li>
&lt;li>When the element data type is 32-bit, ZA can be accessed as 4 ZA tiles (ZA0.S to ZA3.S).&lt;/li>
&lt;li>When the element data type is 64-bit, ZA can be accessed as 8 ZA tiles (ZA0.D to ZA7.D).&lt;/li>
&lt;li>When the element data type is 128-bit, ZA can be accessed as 16 ZA tiles (ZA0.Q to ZA15.Q).&lt;/li>
&lt;/ul>
&lt;p>For example, if SVL is 256-bit and the element data type size is 8-bit, then ZA can be considered as ZA0.B, or it can be seen as 32 vectors (32 rows, each row size is 32 x 8-bit, i.e., 32 elements per row).&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_ZA0B.webp"
alt="ZA0.B" width="50%" loading="lazy">
&lt;/figure>
&lt;p>If SVL is 256-bit and the element data type size is 16-bit, then ZA can be considered as 2 ZA tiles (ZA0.H and ZA1.H), with each tile considered as 16 vectors (16 rows, each row size is 16 x 16-bit, i.e., 16 elements per row).&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_ZA0H_ZA1H.webp"
alt="ZA0.H and ZA1.H" width="40%" loading="lazy">
&lt;/figure>
&lt;p>The advantage of doing this is to fully utilize ZA storage. In practical applications, for example, when the SVL is 256-bit, the element data type size is 32-bit, and the size of ZA is 256-bit x 256-bit, &lt;strong>to perform an outer product operation on vectors in two Z registers&lt;/strong>, the outer product result is a 2D array of 8 x 8 floating-point numbers. This outer product only requires 1/4 of the storage space of ZA. By dividing ZA into 4 ZA tiles, ZA storage can be fully utilized.&lt;/p>
&lt;h3 id="53-za-tile-slices">
&lt;a href="#53-za-tile-slices" class="header-anchor">#&lt;/a>
5.3 ZA tile slices
&lt;/h3>
&lt;p>A ZA tile can be accessed as a whole or in the form of individual ZA tile slices.&lt;/p>
&lt;p>When accessed as a whole, instructions can be accessed using the name of the tile:&lt;/p>
&lt;pre>&lt;code class="language-text">ZA0.B, ZA0.H-ZA1.H, ZA0.S-ZA3.S, ZA0.D-ZA7.D or ZA0.Q-ZA15.Q
&lt;/code>&lt;/pre>
&lt;p>A ZA tile slice is a one-dimensional array composed of &lt;strong>continuous elements in the horizontal or vertical direction of its ZA tile&lt;/strong>, that is, a row or a column in the ZA tile.&lt;/p>
&lt;p>Accessing a vector of a ZA tile is reading and writing a ZA tile slice:&lt;/p>
&lt;ul>
&lt;li>Horizontal or vertical ZA tile slice access is indicated by the &lt;code>H&lt;/code> or &lt;code>V&lt;/code> suffix following the ZA tile name.&lt;/li>
&lt;li>A specific ZA tile slice is represented by an index, indicated by the slice index &lt;code>[N]&lt;/code> following the ZA tile name.&lt;/li>
&lt;/ul>
&lt;p>For example, if the SVL is 128 bits and the element data type size is 8-bit, then its horizontal and vertical ZA tile slice can be represented as shown in the figure below:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_6724_ARM2799_7_Scalable_Matrix_p1.webp"
alt="ZA tile slices" width="50%" loading="lazy">
&lt;/figure>
&lt;p>For example, if the SVL is 128 bits and the element data type size is 16-bit, then its horizontal and vertical ZA tile slice can be represented as shown in the figure below:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_6888_ARM2799_8_Scalable_Matrix_p1.webp"
alt="ZA tile slices" width="50%" loading="lazy">
&lt;/figure>
&lt;p>In order to improve the efficiency of hardware access to ZA tile and ZA tile slices, the ZA tile slices of a ZA tile are interleaved.&lt;/p>
&lt;p>The image below shows an example of this interleaved arrangement. In this example, SVL is 256 bits, and the element data type size is 16 bits. This means that ZA can be viewed as two ZA tiles (ZA0H and ZA1H) and has interleaved horizontal tile slices:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_4885_SME_interleave.webp"
alt="ZA tile slices" width="auto" loading="lazy">
&lt;/figure>
&lt;p>The figure below shows a mixed view of the horizontal and vertical ZA tile slice sizes for different element data types:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-15_7673_SME_V_H.webp"
alt="ZA tile slices" width="auto" loading="lazy">
&lt;/figure>
&lt;p>The left columns show the different processing methods for each row of the ZA memory.&lt;/p>
&lt;p>Set SIZE as the size of vector elements, where SIZE is 1, 2, 4, 8, 16, representing data types B, H, S, D, or Q, respectively.&lt;/p>
&lt;p>Set NUM_OF_ELEMENTS as the number of elements in the vector, i.e., bytes_of(SVL)/SIZE.&lt;/p>
&lt;p>Horizontal tile slice, &lt;code>ZAnH.&amp;lt;B|H|S|D|Q&amp;gt;[m]&lt;/code> accesses a vector that contains a whole row (m x SIZE + n) in ZA storage. The vector contains elements of data type B, H, S, D, or Q.&lt;/p>
&lt;p>Vertical tile slice, &lt;code>ZAnV.&amp;lt;B|H|S|D|Q&amp;gt;[m] &lt;/code> accesses a vector that contains the entire column (m x SIZE) in ZA storage. This vector contains elements of data type B, H, S, D, or Q.&lt;/p>
&lt;p>&lt;code>ZAnV.[m]&lt;/code> accesses a vector containing column (m x SIZE) and row elements (i x SIZE + n), where i ranges from 0 to NUM_OF_ELEMENTS-1. This vector contains elements of data types B, H, S, D, or Q.&lt;/p>
&lt;p>Be careful with overlapping when applying mixed element data type sizes and horizontal and vertical tile slices.&lt;/p>
&lt;p>For more information on ZA Array, ZA array vectors, tile, and tile slices, refer to sections B1.4.8 to B1.4.12 of the Arm Architecture Reference Manual for the A-profile architecture.&lt;/p>
&lt;h2 id="6-instructions-supported-in-steaming-sve-mode">
&lt;a href="#6-instructions-supported-in-steaming-sve-mode" class="header-anchor">#&lt;/a>
6. Instructions supported in Steaming SVE mode
&lt;/h2>
&lt;p>Some instructions have limitations in Streaming SVE mode:&lt;/p>
&lt;ul>
&lt;li>Some SVE/SVE2 instructions become illegal to execute
&lt;ul>
&lt;li>Gather-load and Scatter-store instructions&lt;/li>
&lt;li>Use the SVE2 instruction of the First Fault register&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Most NEON instructions become UNDEFINED&lt;/li>
&lt;/ul>
&lt;p>For more information about instructions affected by the Streaming SVE mode, please refer to the document &amp;ldquo;Arm Architecture Reference Manual.&amp;rdquo;&lt;/p>
&lt;p>SME has added several new instructions, including:&lt;/p>
&lt;ul>
&lt;li>Matrix outer product and accumulate or subtract instructions, including FMOPA, UMOPA, and BFMOPA.
&lt;ul>
&lt;li>SVE2 vector registers (Z0-Z31) serve as the row and column inputs for outer product operations.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>ZA storage stores the output results of the two-dimensional matrix tile.&lt;/li>
&lt;li>Instructions for performing addition operations with the SVE2 Z vector and the rows or columns of ZA&lt;/li>
&lt;li>Instruction for clearing ZA tiles&lt;/li>
&lt;li>Added some instructions that can be used in both Streaming and Non-streaming modes.&lt;/li>
&lt;/ul>
&lt;h2 id="7-sme-directive">
&lt;a href="#7-sme-directive" class="header-anchor">#&lt;/a>
7. SME Directive
&lt;/h2>
&lt;p>The main SME commands for operating ZA storage include:&lt;/p>
&lt;ul>
&lt;li>Calculate the cross product of two vectors, and then accumulate or decrement, and place the result into an instruction of a ZA tile.&lt;/li>
&lt;li>Instructions to store or load SVE vectors (Z registers) into or from rows or columns of the ZA tile&lt;/li>
&lt;li>In the horizontal or vertical direction, an SVE vector and ZA tile addition instruction&lt;/li>
&lt;li>An instruction to add a multiple of the vector length in Streaming SVE mode to a scalar register&lt;/li>
&lt;/ul>
&lt;h3 id="71-outer-product-and-accumulate-or-subtract-instructions">
&lt;a href="#71-outer-product-and-accumulate-or-subtract-instructions" class="header-anchor">#&lt;/a>
7.1 Outer Product and Accumulate or Subtract Instructions
&lt;/h3>
&lt;p>In order to help understand outer product and accumulate or subtract instructions, let&amp;rsquo;s see how to use the outer product operation to perform matrix multiplication.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_2313_Picture1_png-1280x960.webp"
alt="Outer product" width="auto" loading="lazy">
&lt;/figure>
&lt;p>Calculating the outer product of two vectors a and b will yield a result matrix C containing the outer product:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_1665_Picture2_png-1280x960.webp"
alt="Outer product" width="auto" loading="lazy">
&lt;/figure>
&lt;p>Now consider the matrix multiplication operation of two matrices a and b:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_8117_Picture3_png-1280x960.webp"
alt="Matrix multiplication" width="auto" loading="lazy">
&lt;/figure>
&lt;p>This matrix multiplication can be achieved by calculating two outer product operations and accumulating the two resulting matrices (which is the commonly used handwritten calculation method), as shown in the diagram below:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_3731_Picture4_png-1280x960.webp"
alt="Matrix multiplication with outer product" width="auto" loading="lazy">
&lt;/figure>
&lt;p>SME introduced efficient outer product and accumulate or subtract instructions for the following data types:&lt;/p>
&lt;ul>
&lt;li>8-bit, 16-bit integers&lt;/li>
&lt;li>FP16, BF16, FP32, and FP64 floating point numbers&lt;/li>
&lt;/ul>
&lt;p>These instructions calculate the outer product of two vectors in two Z vector registers (Zn and Zm), accumulate or subtract the resulting array with the existing data in a ZA tile (ZAda), and store the result in the same ZA tile (ZAda). Each source vector is independently predicated by the corresponding control predicate registers (Pn and Pm).&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Output Array&lt;/th>
&lt;th style="text-align: left">Input Vector&lt;/th>
&lt;th style="text-align: left">Description&lt;/th>
&lt;th style="text-align: left">Example&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">INT32&lt;/td>
&lt;td style="text-align: left">INT8, INT8&lt;/td>
&lt;td style="text-align: left">Store the sum of the outer products of four INT8s into each INT32 element&lt;/td>
&lt;td style="text-align: left">SMOPA or SMOPS or UMOPA or UMOPS: Signed or unsigned integer outer product sum, and accumulate or subtract. For example: &lt;code>UMOPS &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.B, &amp;lt;Zm&amp;gt;.B&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">INT32&lt;/td>
&lt;td style="text-align: left">INT16, INT16&lt;/td>
&lt;td style="text-align: left">Store the sum of the outer product of two INT16 in each INT32 element&lt;/td>
&lt;td style="text-align: left">SMOPA or SMOPS or UMOPA or UMOPS: Signed or unsigned integer outer product sum, and accumulate or subtract. For example: &lt;code>UMOPS &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.H, &amp;lt;Zm&amp;gt;.H&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">INT64&lt;/td>
&lt;td style="text-align: left">INT16, INT16&lt;/td>
&lt;td style="text-align: left">If FEAT_SME_I16I64 is implemented, the sum of the outer products of four INT16s is stored in each INT64 element&lt;/td>
&lt;td style="text-align: left">SMOPA or SMOPS or UMOPA or UMOPS: signed or unsigned integer outer product sum, and accumulate or subtract. For example: &lt;code>UMOPS &amp;lt;ZAda&amp;gt;.D, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.H, &amp;lt;Zm&amp;gt;.H&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">FP32&lt;/td>
&lt;td style="text-align: left">BF16, BF16&lt;/td>
&lt;td style="text-align: left">Store the sum of two BF16 outer products into each FP32 element&lt;/td>
&lt;td style="text-align: left">BFMOPA or BFMOPS: BFloat16 outer product sum, with accumulation or subtraction. For example: &lt;code>BFMOPS &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.H, &amp;lt;Zm&amp;gt;.H&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">FP32&lt;/td>
&lt;td style="text-align: left">FP16, FP16&lt;/td>
&lt;td style="text-align: left">Store the sum of two FP16 outer products into each FP32 element&lt;/td>
&lt;td style="text-align: left">FMOPA or FMOPS: Half-precision floating-point outer product sum, and accumulate or subtract. For example: &lt;code>FMOPS &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.H, &amp;lt;Zm&amp;gt;.H&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">FP32&lt;/td>
&lt;td style="text-align: left">FP32, FP32&lt;/td>
&lt;td style="text-align: left">Simple FP32 outer product&lt;/td>
&lt;td style="text-align: left">FMOPA or FMOPS: Floating-point outer product and accumulate or subtract. For example: &lt;code>FMOPS &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.S, &amp;lt;Zm&amp;gt;.S&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">FP64&lt;/td>
&lt;td style="text-align: left">FP64, FP64&lt;/td>
&lt;td style="text-align: left">If FEAT_SME_F64F64 is implemented, perform a simple FP64 outer product&lt;/td>
&lt;td style="text-align: left">FMOPA or FMOPS: Floating point outer product and accumulate or subtract. For example: &lt;code>FMOPS &amp;lt;ZAda&amp;gt;.D, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.D, &amp;lt;Zm&amp;gt;.D&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="711-fp32-fp64-outer-product-and-accumulate-or-subtract-instructions">
&lt;a href="#711-fp32-fp64-outer-product-and-accumulate-or-subtract-instructions" class="header-anchor">#&lt;/a>
7.1.1 FP32, FP64 outer product and accumulate or subtract instructions
&lt;/h4>
&lt;p>Instructions where the input vectors and output arrays have the same data type (FP32, FP64) are relatively simple.&lt;/p>
&lt;p>The following example demonstrates FP32 type outer product with accumulation or subtraction instructions.&lt;/p>
&lt;pre>&lt;code class="language-armasm">FMOPA &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.S, &amp;lt;Zm&amp;gt;.S
FMOPS &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.S, &amp;lt;Zm&amp;gt;.S
&lt;/code>&lt;/pre>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_3613751670-667e5f923c64.webp"
alt="FMOPA and FMOPS" width="auto" loading="lazy">
&lt;/figure>
&lt;p>In this example, assuming the SVL vector length is 128, &lt;code>Zn.S&lt;/code> and &lt;code>Zm.S&lt;/code> contain vectors composed of 4 FP32 numbers, this instruction calculates the outer product of &lt;code>Zn.S&lt;/code> and &lt;code>Zm.S&lt;/code>, the result of the outer product is the gray matrix in the figure, then accumulates or subtracts this outer product result with the existing values in the ZA tile &lt;code>ZAda.S&lt;/code>, and stores the result in the same ZA tile.&lt;/p>
&lt;h4 id="712-fp16-bf16-int16-int8-i16i64-type-outer-product-and-accumulate-or-subtract-instructions">
&lt;a href="#712-fp16-bf16-int16-int8-i16i64-type-outer-product-and-accumulate-or-subtract-instructions" class="header-anchor">#&lt;/a>
7.1.2 FP16, BF16, INT16, INT8, I16I64 type outer product and accumulate or subtract instructions
&lt;/h4>
&lt;p>Because these instructions will expand the data type of the calculation results, these operations are not as straightforward as the previous FP32 and FP64 type instructions.&lt;/p>
&lt;ul>
&lt;li>BF16 instruction calculates the outer product of two BF16s, expands the result type to FP32, and then destructively adds or subtracts the result with the target tile.&lt;/li>
&lt;li>INT8 instructions compute the sum of the outer product of four INT8s, expanding the result type to INT32, and then perform destructive addition or subtraction of the result with the target tile.&lt;/li>
&lt;li>INT16 instruction calculates the outer product sum of two INT16s, expands the result type to INT32, and then performs a destructive add or subtract with the target tile.&lt;/li>
&lt;li>FP16 instructions calculate the sum of the outer product of two FP16s, expand the result type to FP32, and then perform destructive addition or subtraction of the result with the target tile.&lt;/li>
&lt;li>If FEAT_SME_I16I64 is implemented, the I16I64 instruction calculates the sum of the outer products of four INT16s, expands the result type to INT64, and then destructively adds or subtracts the result with the target tile.&lt;/li>
&lt;/ul>
&lt;p>The following example demonstrates the operation of the INT8 UMOPA instruction with an SVL vector length of 128:&lt;/p>
&lt;pre>&lt;code class="language-armasm">UMOPA &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.B, &amp;lt;Zm&amp;gt;.B
&lt;/code>&lt;/pre>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_1030_Picture6_png-1280x960.webp"
alt="INT8 UMOPA" width="auto" loading="lazy">
&lt;/figure>
&lt;p>Each input register (&lt;code>Zn.B&lt;/code>, &lt;code>Zm.B&lt;/code>) is treated as a matrix containing 4x4 elements, which can be seen as blocks composed of 4 consecutive elements (as marked by the red lines in the diagram) that have been transposed.&lt;/p>
&lt;p>In this example, because the SVL vector length is 128-bit:&lt;/p>
&lt;ul>
&lt;li>The first source vector &lt;code>Zn.B&lt;/code> contains a 4x4 submatrix of unsigned 8-bit integers.&lt;/li>
&lt;li>The second source vector &lt;code>Zm.B&lt;/code>, contains a 4x4 submatrix of unsigned 8-bit integers.&lt;/li>
&lt;li>UMOPA instruction calculates the sum of the 4x4 expanded 32-bit integer outer product, then destructively accumulates the integers in the target tile (ZAda).&lt;/li>
&lt;/ul>
&lt;p>More generally, the UMOPA instruction multiplies submatrices from the first source vector with submatrices from the second source vector. Each source vector contains a submatrix of unsigned 8-bit integers of size (SVL/32) x 4. The resulting (SVL/32) x (SVL/32) expanded 32-bit integer outer product is then destructively added to a 32-bit integer target tile.&lt;/p>
&lt;p>The following example demonstrates the operation of a BF16 BFMOPA with an SVL of 128-bit:&lt;/p>
&lt;pre>&lt;code class="language-armasm">BFMOPA &amp;lt;ZAda&amp;gt;.S, &amp;lt;Pn&amp;gt;/M, &amp;lt;Pm&amp;gt;/M, &amp;lt;Zn&amp;gt;.H, &amp;lt;Zm&amp;gt;.H
&lt;/code>&lt;/pre>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_6545_Picture7_png-1280x960.webp"
alt="BF16 BFMOPA" width="auto" loading="lazy">
&lt;/figure>
&lt;p>In this example, because the SVL vector length is 128-bit:&lt;/p>
&lt;ul>
&lt;li>The first source vector &lt;code>Zn.H&lt;/code>, contains a 4x2 submatrix of BF16 integers, which is expanded into single-precision floating-point numbers.&lt;/li>
&lt;li>The second source vector &lt;code>Zm.H&lt;/code>, contains a 2x4 submatrix of a BF16 integer, which is expanded into a single-precision floating-point number.&lt;/li>
&lt;li>BMOPA instruction calculates the sum of a 4x4 single-precision outer product, and then destructively accumulates it with the single-precision floating-point numbers in the target tile (ZAda).&lt;/li>
&lt;/ul>
&lt;p>More generally speaking, the BFMOPA instruction expands the type of the (SVL/32) x2 BF16 submatrix stored in the first source vector to single precision, expands the type of the 2x (SVL/32) BF16 submatrix stored in the second source vector to single precision, and multiplies these two submatrices. Then, the resulting (SVL/32) x (SVL/32) single-precision outer product is destructively added to a single-precision target tile.&lt;/p>
&lt;p>The following table shows the number of MACs (Multiply-Accumulate) for the corresponding data type performed by an outer product and accumulate or subtract instruction for several data types and SVL lengths:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">&lt;/th>
&lt;th style="text-align: left">128-bit&lt;/th>
&lt;th style="text-align: left">256-bit&lt;/th>
&lt;th style="text-align: left">512-bit&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">FP32&lt;/td>
&lt;td style="text-align: left">16&lt;/td>
&lt;td style="text-align: left">64&lt;/td>
&lt;td style="text-align: left">256&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">FP64&lt;/td>
&lt;td style="text-align: left">4&lt;/td>
&lt;td style="text-align: left">16&lt;/td>
&lt;td style="text-align: left">64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">INT8&lt;/td>
&lt;td style="text-align: left">64&lt;/td>
&lt;td style="text-align: left">256&lt;/td>
&lt;td style="text-align: left">1024&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">INT16&lt;/td>
&lt;td style="text-align: left">32&lt;/td>
&lt;td style="text-align: left">128&lt;/td>
&lt;td style="text-align: left">512&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">BF16&lt;/td>
&lt;td style="text-align: left">32&lt;/td>
&lt;td style="text-align: left">128&lt;/td>
&lt;td style="text-align: left">512&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">FP16&lt;/td>
&lt;td style="text-align: left">32&lt;/td>
&lt;td style="text-align: left">128&lt;/td>
&lt;td style="text-align: left">512&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="72-sme-instructions-with-predication">
&lt;a href="#72-sme-instructions-with-predication" class="header-anchor">#&lt;/a>
7.2 SME Instructions with Predication
&lt;/h3>
&lt;p>Each source vector can be independently predicated by its corresponding control predicate register:&lt;/p>
&lt;ul>
&lt;li>Outer product and accumulate or subtract instructions use Pn/M and Pn/M (without /Z form): Inactive source elements are treated as having a value of 0.&lt;/li>
&lt;li>Slice move command uses Pg/M: The Inactive elements in the target slice remain unchanged.&lt;/li>
&lt;li>Tile slice load instruction uses Pg/Z: Inactive elements in the target tile slice are set to 0.&lt;/li>
&lt;li>Tile slice store instruction uses Pg: Inactive elements that will not be written to memory.&lt;/li>
&lt;/ul>
&lt;p>Predication makes it easier to handle cases where the dimensions of the matrix are not a multiple of SVL.&lt;/p>
&lt;p>For example, the instructions in the image below:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_2656_Picture12_png-600x0.webp"
alt="SME prediction" width="auto" loading="lazy">
&lt;/figure>
&lt;p>The input vector &lt;code>Z0&lt;/code> is predicated by &lt;code>P0&lt;/code>, &lt;code>Z1&lt;/code> is predicated by &lt;code>P1&lt;/code>.&lt;/p>
&lt;p>In this example:&lt;/p>
&lt;ul>
&lt;li>SVL vector length is 512-bit.&lt;/li>
&lt;li>The Z register contains a vector of 16 FP32 numbers.&lt;/li>
&lt;li>The last two elements in &lt;code>P0&lt;/code> are inactive.&lt;/li>
&lt;li>The last element in &lt;code>P1&lt;/code> is inactive.&lt;/li>
&lt;/ul>
&lt;p>This instruction updates (16-2) x (16-1) FP32 elements in &lt;code>ZA0.S&lt;/code>, because &lt;code>Pn/M&lt;/code> is used, the remaining elements in &lt;code>ZA0.S&lt;/code> remain unchanged.&lt;/p>
&lt;p>The figure below shows more examples of predicated outer products with accumulation or subtraction. The underlined text in the figure indicates the parts of the calculation affected by inactive predicate elements.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_2072_Picture14_png-1280x960.webp"
alt="SME prediction FMOPA" width="auto" loading="lazy">
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_3513_Picture16_png-1280x960.webp"
alt="SME prediction UMOPA" width="auto" loading="lazy">
&lt;/figure>
&lt;h3 id="73-za-tile-and-addition-operation-with-a-z-vector">
&lt;a href="#73-za-tile-and-addition-operation-with-a-z-vector" class="header-anchor">#&lt;/a>
7.3 ZA tile and addition operation with a Z vector
&lt;/h3>
&lt;p>SME includes instructions to add a vector to the rows or columns of a ZA tile, and these instructions also support predication.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Instruction&lt;/th>
&lt;th style="text-align: left">Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">ADDHA&lt;/td>
&lt;td style="text-align: left">Add the source vector to each horizontal slice of the ZA tile&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">ADDVA&lt;/td>
&lt;td style="text-align: left">Add the source vector to each vertical slice of the ZA tile&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>For example:&lt;/p>
&lt;pre>&lt;code class="language-armasm">ADDHA ZA0.S, P0/M, P1/M, Z1.S
&lt;/code>&lt;/pre>
&lt;p>Will perform the following actions:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_ARM2799_9_Scalable_Matrix_p2_png-1200x0.webp"
alt="SME ADDHA" width="auto" loading="lazy">
&lt;/figure>
&lt;p>This ADDHA instruction adds each element of the source vector Z1 to the corresponding active element of each horizontal slice of the ZA0.S tile.&lt;/p>
&lt;p>Elements in a Tile are predicated by a pair of governing predicates. An element in a horizontal slice can be considered active under the following conditions:&lt;/p>
&lt;ul>
&lt;li>It is TRUE for the element corresponding to the second governing predicate, and&lt;/li>
&lt;li>It corresponds to TRUE at the row number of the first governing predicate&amp;rsquo;s horizontal slice, and the inactive elements in the target tile remain unchanged.&lt;/li>
&lt;/ul>
&lt;h3 id="74-tile-load-store-move-instructions">
&lt;a href="#74-tile-load-store-move-instructions" class="header-anchor">#&lt;/a>
7.4 Tile load, store, move instructions
&lt;/h3>
&lt;p>SME tile load, store, move instructions can:&lt;/p>
&lt;ul>
&lt;li>Read data from memory and place it into a row or column of the ZA tile&lt;/li>
&lt;li>Write the row or column of the ZA tile into memory&lt;/li>
&lt;li>Move the row of the ZA tile to the SVE Z vector register&lt;/li>
&lt;li>Move the SVE Z vector register to a ZA tile row or column&lt;/li>
&lt;/ul>
&lt;h4 id="741-tile-slice-load-and-store-instructions">
&lt;a href="#741-tile-slice-load-and-store-instructions" class="header-anchor">#&lt;/a>
7.4.1 Tile slice load and store instructions
&lt;/h4>
&lt;p>The LD1B, LD1H, LD1S, LD1D, and LD1Q instructions load consecutive memory values into a ZA tile slice with 8-bit, 16-bit, 32-bit, 64-bit, or 128-bit elements, respectively.&lt;/p>
&lt;p>The ST1B, ST1H, ST1S, ST1D, and ST1Q instructions store a ZA tile slice containing 8-bit, 16-bit, 32-bit, 64-bit, or 128-bit elements, respectively, into contiguous memory.&lt;/p>
&lt;p>These instructions also support predication, for example:&lt;/p>
&lt;pre>&lt;code class="language-armasm">LD1B ZA0H.B[W0, #imm], P0/Z, [X1, X2]
&lt;/code>&lt;/pre>
&lt;p>This LD1B instruction performs a predicated continuous byte read, reading data from memory at address (X1+X2) into the horizontal tile slice in ZA0 at row number (W0+imm). Inactive elements in the target tile slice are set to 0.&lt;/p>
&lt;pre>&lt;code class="language-armasm">ST1H ZA1V.H[W0, #imm], P2, [X1, X2, LSL #1]
&lt;/code>&lt;/pre>
&lt;p>This ST1H instruction executes a predicated continuous halfword store operation, storing the vertical tile slice in ZA1 with the column number (W0+imm) to the memory address (X1+X2*2), and elements that are inactive in the tile slice are not written to memory.&lt;/p>
&lt;h4 id="742-tile-slice-move-instruction">
&lt;a href="#742-tile-slice-move-instruction" class="header-anchor">#&lt;/a>
7.4.2 Tile slice move instruction
&lt;/h4>
&lt;p>The MOV instruction (alias for the MOVA instruction) moves the value of a Z vector register to a ZA tile slice, or moves the value from a ZA tile slice to a Z vector register. This instruction operates on a single horizontal or vertical tile slice of a ZA tile with a specified element size. The row number/column number of the slice is specified by the slice&amp;rsquo;s retrieval register plus an immediate offset. Inactive elements in the target slice remain unchanged.&lt;/p>
&lt;p>For example:&lt;/p>
&lt;pre>&lt;code class="language-armasm">MOV ZA0H.B[W0, #imm], P0/M, Z0.B
&lt;/code>&lt;/pre>
&lt;p>Or&lt;/p>
&lt;pre>&lt;code class="language-armasm">MOVA ZA0H.B[W0, #imm], P0/M, Z0.B
&lt;/code>&lt;/pre>
&lt;p>This instruction moves the values in vector register &lt;code>Z0.B&lt;/code> to the horizontal ZA tile slice &lt;code>ZA0H.B[W0,#imm]&lt;/code>, using &lt;code>P0&lt;/code> as the predication register. Inactive elements in the target tile slice remain unchanged.&lt;/p>
&lt;h3 id="75-za-array-vector-loadstore-instructions">
&lt;a href="#75-za-array-vector-loadstore-instructions" class="header-anchor">#&lt;/a>
7.5 ZA array vector load/store instructions
&lt;/h3>
&lt;p>SME LDR instruction reads data from memory into a ZA array vector, SME STR instruction stores the values from a ZA array vector into memory.
These instructions do not have predication functionality. They are primarily for saving/restoring ZA storage during software context switching. SME LDR/STR instructions can also be used in Non-streaming SVE mode when PSTATE.ZA is enabled.
For example, the ZA array vector in the following STR instruction is specified by a vector selection register Wv (scalar register W) plus an optional immediate number (Wv+Imm). The address for accessing memory is: a scalar register as the base, plus the same optional immediate offset multiplied by the current vector length in bytes.&lt;/p>
&lt;pre>&lt;code class="language-armasm">STR ZA[&amp;lt;Wv&amp;gt;, &amp;lt;imm&amp;gt;], [&amp;lt;Xn|SP&amp;gt;{, #&amp;lt;imm&amp;gt;, MUL VL}]
&lt;/code>&lt;/pre>
&lt;h3 id="76-za-tile-clear-instruction">
&lt;a href="#76-za-tile-clear-instruction" class="header-anchor">#&lt;/a>
7.6 ZA tile clear instruction
&lt;/h3>
&lt;p>SME ZERO instruction can clear a group of 64-bit ZA tile:&lt;/p>
&lt;pre>&lt;code class="language-armasm">ZERO { &amp;lt;mask&amp;gt;}
&lt;/code>&lt;/pre>
&lt;p>The ZERO instruction can zero out up to 8 ZA tiles named &lt;code>ZA0.D&lt;/code> to &lt;code>ZA8.D&lt;/code>. The tiles to be zeroed are specified by the mask in the instruction, while the remaining tiles remain unchanged.&lt;/p>
&lt;p>This instruction can also be used in Non-streaming SVE mode when &lt;code>PSTATE.ZA&lt;/code> is enabled.&lt;/p>
&lt;p>If you want to clear the entire ZA array, you can use an instruction alias, &lt;code>ZERO {ZA}&lt;/code>.&lt;/p>
&lt;h3 id="77-new-sve2-instructions">
&lt;a href="#77-new-sve2-instructions" class="header-anchor">#&lt;/a>
7.7 New SVE2 Instructions
&lt;/h3>
&lt;p>The SME architecture extension has added some new SVE2 instructions, which can also be used in PE that implements SVE2 when in Non-streaming SVE mode. These instructions include:&lt;/p>
&lt;ul>
&lt;li>Select a predicate register or an all-false Predicate select instruction&lt;/li>
&lt;li>Reverse 64-bit double word element instruction&lt;/li>
&lt;li>Signed/Unsigned clamp to smaller/larger value vector instructions&lt;/li>
&lt;/ul>
&lt;p>The following introduces the Predicate select instruction.&lt;/p>
&lt;h4 id="771-psel-instruction">
&lt;a href="#771-psel-instruction" class="header-anchor">#&lt;/a>
7.7.1 PSEL Instruction
&lt;/h4>
&lt;p>PSEL instruction selects a predicate register or all-false to the target predicate register, as follows:&lt;/p>
&lt;pre>&lt;code class="language-armasm">PSEL &amp;lt;Pd&amp;gt;, &amp;lt;Pn&amp;gt;, &amp;lt;Pm&amp;gt;.&amp;lt;T&amp;gt;[&amp;lt;Wv&amp;gt;, &amp;lt;imm&amp;gt;]
&lt;/code>&lt;/pre>
&lt;p>If the element specified in the second source predicate register (Pm) is True, this instruction places the content of the first source predicate register (Pn) into the destination predicate register (Pd), otherwise, it sets the value of the destination predicate register to all false.
For example, the following instruction, assuming the value of W12 is 0:&lt;/p>
&lt;pre>&lt;code class="language-armasm">PSEL P0, P1, P2.B[W12, #0]
&lt;/code>&lt;/pre>
&lt;p>The [0]th element of the second source predicate register [W12+0] is False, so the target register P0 is set to all 0 (all-false), as shown in the figure below:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_4401_Picture10_png-1280x960.webp"
alt="SME PSEL" width="auto" loading="lazy">
&lt;/figure>
&lt;p>Now look at the following instruction, still assuming the value of W12 is 0, but this time the immediate offset is 1:&lt;/p>
&lt;pre>&lt;code class="language-armasm">PSEL P0, P1, P2.B[W12, #1]
&lt;/code>&lt;/pre>
&lt;p>The [1] element of the second source predicate register [W12+1] is True, therefore select the value of the first source predicate register to the destination register P0, as shown in the diagram below:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-16_0116_Picture11_png-1280x960.webp"
alt="SME PSEL" width="auto" loading="lazy">
&lt;/figure>
&lt;h2 id="references">
&lt;a href="#references" class="header-anchor">#&lt;/a>
References
&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://community.arm.com/arm-community-blogs/b/architectures-and-processors-blog/posts/arm-scalable-matrix-extension-introduction" target="_blank" rel="noopener" >Arm Scalable Matrix Extension (SME) Introduction
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;li>&lt;a class="link" href="https://community.arm.com/arm-community-blogs/b/architectures-and-processors-blog/posts/arm-scalable-matrix-extension-introduction-p2" target="_blank" rel="noopener" >Arm Scalable Matrix Extension (SME) Instructions
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;/ul></description></item><item><title>Arm Performance Optimization: Scalable Vector Extension SVE</title><link>https://cuterwrite.top/en/p/arm-sve-for-performance/</link><pubDate>Sun, 11 Aug 2024 02:13:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/arm-sve-for-performance/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_117622464_p0_master1200.webp" alt="Featured image of post Arm Performance Optimization: Scalable Vector Extension SVE" />&lt;h1 id="arm-performance-optimization-scalable-vector-extension-sve">
&lt;a href="#arm-performance-optimization-scalable-vector-extension-sve" class="header-anchor">#&lt;/a>
ARM Performance Optimization: Scalable Vector Extension SVE
&lt;/h1>
&lt;h2 id="1-sve-introduction">
&lt;a href="#1-sve-introduction" class="header-anchor">#&lt;/a>
1. SVE Introduction
&lt;/h2>
&lt;p>After the Neon architecture extension with a fixed 128-bit vector length instruction set, Arm designed the Scalable Vector Extension (SVE) as the next-generation SIMD extension for AArch64. SVE introduces the scalable concept, allowing flexible vector length implementations and providing a range of possible values in CPU implementations. The vector length can vary from a minimum of 128 bits to a maximum of 2048 bits, in increments of 128 bits. &lt;strong>The SVE design ensures that the same application can run on different SVE-supporting implementations without recompiling the code&lt;/strong>. SVE enhances the architecture&amp;rsquo;s applicability to high-performance computing (HPC) and machine learning (ML) applications, which require very large amounts of data processing. SVE2 is a superset of SVE and Neon. SVE2 allows the use of more functional domains in data-level parallelism. SVE2 inherits the concepts, vector registers, and operation principles of SVE. SVE and SVE2 define 32 scalable vector registers. Chip partners can choose an appropriate vector length design implementation, with hardware varying between 128 bits and 2048 bits (in increments of 128 bits). The advantage of SVE and SVE2 is that only one vector instruction set uses scalable variables.&lt;/p>
&lt;p>The SVE design philosophy allows developers to write and build software once, and then run the same binary on different AArch64 hardware with various SVE vector length implementations. The portability of the binary means developers do not need to know the vector length implementation of their system. This eliminates the need to rebuild the binary, making the software easier to port. In addition to scalable vectors, SVE and SVE2 also include:&lt;/p>
&lt;ul>
&lt;li>per-lane predication&lt;/li>
&lt;li>Gather Load/Scatter Store&lt;/li>
&lt;li>Speculative Vectorization&lt;/li>
&lt;/ul>
&lt;p>These features help vectorize and optimize loops when dealing with large datasets.&lt;/p>
&lt;p>The main difference between SVE2 and SVE lies in the functional coverage of the instruction set. SVE is specifically designed for HPC and ML applications. SVE2 extends the SVE instruction set to enable accelerated data processing in areas beyond HPC and ML. The SVE2 instruction set can also accelerate common algorithms used in the following applications:&lt;/p>
&lt;ul>
&lt;li>Computer Vision&lt;/li>
&lt;li>Multimedia&lt;/li>
&lt;li>LTE Basic Processing&lt;/li>
&lt;li>Genomics&lt;/li>
&lt;li>In-memory database&lt;/li>
&lt;li>Web Service&lt;/li>
&lt;li>General software&lt;/li>
&lt;/ul>
&lt;p>SVE and SVE2 both support collecting and processing large amounts of data. SVE and SVE2 are not extensions of the Neon instruction set. Instead, SVE and SVE2 are redesigned to offer better data parallelism than Neon. However, the hardware logic of SVE and SVE2 covers the implementation of Neon hardware. When a microarchitecture supports SVE or SVE2, it also supports Neon. To use SVE and SVE2, the software running on that microarchitecture must first support Neon.&lt;/p>
&lt;h2 id="2-sve-architecture-basics">
&lt;a href="#2-sve-architecture-basics" class="header-anchor">#&lt;/a>
2. SVE Architecture Basics
&lt;/h2>
&lt;p>This section introduces the basic architectural features shared by SVE and SVE2. Like SVE, SVE2 is also based on scalable vectors. In addition to the existing register file provided by Neon, SVE and SVE2 add the following registers:&lt;/p>
&lt;ul>
&lt;li>32 scalable vector registers, &lt;code>Z0-Z31&lt;/code>&lt;/li>
&lt;li>16 scalable Predicate registers, &lt;code>P0-P15&lt;/code>
&lt;ul>
&lt;li>1 First Fault Predicate register, &lt;code>FFR&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Scalable Vector System Control Register, &lt;code>ZCR_ELx&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="21-scalable-vector-registers">
&lt;a href="#21-scalable-vector-registers" class="header-anchor">#&lt;/a>
2.1 Scalable Vector Registers
&lt;/h3>
&lt;p>Scalable vector registers &lt;code>Z0-Z31&lt;/code> can be implemented in microarchitecture as 128-2048 bits. The lowest 128 bits are shared with Neon&amp;rsquo;s fixed 128-bit vectors &lt;code>V0-V31&lt;/code>.&lt;/p>
&lt;p>The image below shows scalable vector registers &lt;code>Z0-Z31&lt;/code>:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_Z-register.webp"
alt="Z Registers-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Scalable Vector Registers Z0-Z31&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>Scalable Vector:&lt;/p>
&lt;ul>
&lt;li>Can accommodate 64, 32, 16, and 8-bit elements&lt;/li>
&lt;li>Supports integer, double precision, single precision, and half precision floating-point elements&lt;/li>
&lt;li>The vector length can be configured for each exception level (EL)&lt;/li>
&lt;/ul>
&lt;h3 id="22-scalable-predicate-register">
&lt;a href="#22-scalable-predicate-register" class="header-anchor">#&lt;/a>
2.2 Scalable Predicate Register
&lt;/h3>
&lt;p>In order to control which active elements participate in operations, Predicate registers (abbreviated as P registers) are used as masks in many SVE instructions, which also provides flexibility for vector operations. The figure below shows the scalable Predicate registers &lt;code>P0-P15&lt;/code>:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_Predicate-register.webp"
alt="P Register-2024-08-12" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Scalable Predicate Registers P0-P15&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>The P register is typically used as a bitmask for data manipulation:&lt;/p>
&lt;ul>
&lt;li>Each P register is 1/8 the length of a Z register&lt;/li>
&lt;li>&lt;code>P0-P7&lt;/code> are used for loading, storing, and arithmetic operations&lt;/li>
&lt;li>&lt;code>P8-P15&lt;/code> used for loop management&lt;/li>
&lt;li>FFR is a special P register set by the first-fault vector load and store instructions, used to indicate the success of load and store operations for each element. FFR is designed to support speculative memory access, making vectorization easier and safer in many cases.&lt;/li>
&lt;/ul>
&lt;h3 id="23-scalable-vector-system-control-register">
&lt;a href="#23-scalable-vector-system-control-register" class="header-anchor">#&lt;/a>
2.3 Scalable Vector System Control Register
&lt;/h3>
&lt;p>The figure below shows the Scalable Vector System Control Register &lt;code>ZCR_ELx&lt;/code>:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_ZCR_Elx.webp"
alt="ZCR_Elx-2024-08-12" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Scalable Vector System Control Register ZCR_Elx&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>Scalable Vector System Control Register indicates SVE implementation features:&lt;/p>
&lt;ul>
&lt;li>&lt;code>ZCR_Elx.LEN&lt;/code> field is used for the vector length of the current and lower anomaly levels.&lt;/li>
&lt;li>Most bits are currently reserved for future use.&lt;/li>
&lt;/ul>
&lt;h3 id="24-sve-assembly-syntax">
&lt;a href="#24-sve-assembly-syntax" class="header-anchor">#&lt;/a>
2.4 SVE Assembly Syntax
&lt;/h3>
&lt;p>The SVE assembly syntax format consists of an opcode, destination register, P register (if the instruction supports a Predicate mask), and input operands. The following instruction example will detail this format.&lt;/p>
&lt;p>Example 1:&lt;/p>
&lt;pre>&lt;code class="language-armasm">LDFF1D {&amp;lt;Zt&amp;gt;.D}, &amp;lt;Pg&amp;gt;/Z, [&amp;lt;Xn|SP&amp;gt;, &amp;lt;Zm&amp;gt;.D, LSL #3]
&lt;/code>&lt;/pre>
&lt;p>Among them:&lt;/p>
&lt;ul>
&lt;li>&lt;code>&amp;lt;Zt&amp;gt;&lt;/code> is the Z register, &lt;code>Z0-Z31&lt;/code>&lt;/li>
&lt;li>&lt;code>&amp;lt;Zt&amp;gt;&lt;/code>.D and &lt;code>&amp;lt;Zm&amp;gt;&lt;/code>.D specify the element type of the target and operand vectors, without needing to specify the number of elements.&lt;/li>
&lt;li>&lt;code>&amp;lt;Pg&amp;gt;&lt;/code> is the P register, &lt;code>P0-P15&lt;/code>&lt;/li>
&lt;li>&lt;code>&amp;lt;Pg&amp;gt;/Z&lt;/code> is to zero the P register.&lt;/li>
&lt;li>&lt;code>&amp;lt;Zm&amp;gt;&lt;/code> specifies the offset for the Gather Load address mode.&lt;/li>
&lt;/ul>
&lt;p>Example 2:&lt;/p>
&lt;pre>&lt;code class="language-armasm">ADD &amp;lt;Zdn&amp;gt;.&amp;lt;T&amp;gt;, &amp;lt;Pg&amp;gt;/M, &amp;lt;Zdn&amp;gt;.&amp;lt;T&amp;gt;, &amp;lt;Zm&amp;gt;.&amp;lt;T&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>Among them:&lt;/p>
&lt;ul>
&lt;li>&lt;code>&amp;lt;Pg&amp;gt;/M&lt;/code> is the merge P register.&lt;/li>
&lt;li>&lt;code>&amp;lt;Zdn&amp;gt;&lt;/code> is both the destination register and one of the input operands. The instruction syntax shows &lt;code>&amp;lt;Zdn&amp;gt;&lt;/code> in both places for convenience. In the assembly encoding, for simplification, they are only encoded once.&lt;/li>
&lt;/ul>
&lt;p>Example 3:&lt;/p>
&lt;pre>&lt;code class="language-armasm">ORRS &amp;lt;Pd&amp;gt;.B, &amp;lt;Pg&amp;gt;.Z, &amp;lt;Pn&amp;gt;.B, &amp;lt;Pm&amp;gt;.B
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>S&lt;/code> is the new interpretation of the P register condition flags &lt;code>NZCV&lt;/code>.&lt;/li>
&lt;li>&lt;code>&amp;lt;Pg&amp;gt;&lt;/code> controls the P register to act as a bitmask in the example operation.&lt;/li>
&lt;/ul>
&lt;h3 id="25-sve-architecture-features">
&lt;a href="#25-sve-architecture-features" class="header-anchor">#&lt;/a>
2.5 SVE Architecture Features
&lt;/h3>
&lt;p>SVE includes the following key architectural features:&lt;/p>
&lt;ul>
&lt;li>per-lane predication&lt;/li>
&lt;/ul>
&lt;p>In order to allow flexible operations on selected elements, SVE introduces 16 P registers, &lt;code>P0-P15&lt;/code>, to indicate valid operations on vector active channels. For example:&lt;/p>
&lt;pre>&lt;code class="language-armasm">ADD Z0.D, P0/M, Z0.D, Z1.D
&lt;/code>&lt;/pre>
&lt;p>Add the active elements &lt;code>Z0&lt;/code> and &lt;code>Z1&lt;/code> and place the result in &lt;code>Z0&lt;/code>. &lt;code>P0&lt;/code> indicates which elements of the operands are active and inactive. The &lt;strong>M&lt;/strong> following &lt;code>P0&lt;/code> stands for Merging, meaning the inactive elements of &lt;code>Z0&lt;/code> will retain their initial values after the &lt;code>ADD&lt;/code> operation. If &lt;strong>Z&lt;/strong> follows &lt;code>P0&lt;/code>, the inactive elements will be zeroed, and the inactive elements of the destination register will be zeroed after the operation.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_Per-lane_Predication.webp"
alt="Per-lane_Predication-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Per-lane predication merging&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>If &lt;strong>\Z&lt;/strong> is used, the inactive elements will be zeroed, and the inactive elements of the target register will be zeroed after the operation. For example&lt;/p>
&lt;pre>&lt;code class="language-armasm">CPY Z0.B, P0/Z, #0xFF
&lt;/code>&lt;/pre>
&lt;p>Indicates that the signed integer 0xFF will be copied to the active channel of &lt;code>Z0&lt;/code>, while the inactive channels will be cleared.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_Per-lane_Predicate_Zeroing.webp"
alt="Per-lane_Predicate_Zeroing-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Per-lane predication zeroing&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;blockquote class="alert-blockquote alert-note">
&lt;p class="alert-heading">
&lt;svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16">
&lt;path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z">&lt;/path>
&lt;/svg>
&lt;span>Note&lt;/span>
&lt;/p>
&lt;p>Not all instructions have the Predicate option. Additionally, not all Predicate operations have both merge and zeroing options. You must refer to the &lt;a class="link" href="https://developer.arm.com/documentation/ddi0487/latest/t" target="_blank" rel="noopener" >AArch64 SVE Supplement
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
to understand the specification details of each instruction.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>Gather Load and Scatter Store&lt;/li>
&lt;/ul>
&lt;p>The addressing modes in SVE allow vectors to be used as base addresses and offsets in Gather Load and Scatter Store instructions, which enables access to non-contiguous memory locations. For example:&lt;/p>
&lt;pre>&lt;code class="language-armasm">LD1SB Z0.S, P0/Z, [Z1.S] // Gather Load signed bytes from memory addresses generated by the 32-bit vector base address Z1 into the active 32-bit elements of Z0.
LD1SB Z0.D, P0/Z, [X0, Z1.D] // Gather Load signed bytes from memory addresses generated by the 64-bit scalar base address X0 plus the vector index in Z1.D into the active elements of Z0.
&lt;/code>&lt;/pre>
&lt;p>The following example shows the load operation &lt;code>LD1SB Z0.S, P0/Z, [Z1.S]&lt;/code>, where &lt;code>P0&lt;/code> contains all true elements, and &lt;code>Z1&lt;/code> contains scattered addresses. After loading, the least significant byte of each element in &lt;code>Z0.S&lt;/code> will be updated with data fetched from scattered memory locations.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_gather-load_and_scatter_store_example.webp"
alt="gather-load_and_scatter_store_example-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Gather-load and Scatter-store Example&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>Loop control and management of the P register driver&lt;/li>
&lt;/ul>
&lt;p>As a key feature of SVE, the P register not only flexibly controls individual elements of vector operations but also enables P register-driven loop control. P register-driven loop control and management make loop control efficient and flexible. This feature eliminates the overhead of extra loop heads and tails for processing partial vectors by registering active and inactive element indices in the P register. P register-driven loop control and management mean that in the subsequent loop iterations, only active elements will perform the intended operations. For example:&lt;/p>
&lt;pre>&lt;code class="language-armasm">WHILEL0 P0.S, x8, x9 // Generate a predicate in P0, starting from the lowest numbered element, true when the incremented value of the first unsigned scalar operand X8 is less than the second scalar operand X9, then false until the highest numbered element.
B.FIRST Loop_start // B.FIRST (equivalent to B.MI) or B.NFRST (equivalent to B.PL) is usually used to branch based on the test result of the above instruction, determining whether the first element of P0 is true or false as the condition to end or continue the loop.
&lt;/code>&lt;/pre>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_Predicate-driver_loop_control_and_management_example.webp"
alt="Predicate-driver_loop_control_and_management_example-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Example of loop control and management driven by P register&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>Vector partitioning for speculation in software management&lt;/li>
&lt;/ul>
&lt;p>Speculative loading can pose challenges for memory reading of traditional vectors, &lt;strong>if errors occur in certain elements during the reading process, it is difficult to reverse the load operation and track which elements failed to load&lt;/strong>. Neon does not allow speculative loading. To allow speculative loading of vectors (e.g., LDRFF), SVE introduces the first-fault vector load instruction. To allow vector access across invalid pages, SVE also introduces the FFR register. &lt;strong>When using the first-fault vector load instruction to load into an SVE vector, the FFR register updates with the success or failure result of each element&amp;rsquo;s load&lt;/strong>. When a load error occurs, FFR immediately registers the corresponding element, registers the remaining elements as 0 or false, and does not trigger an exception. Typically, the RDFFR instruction is used to read the FFR status. The RDFFR instruction ends iteration when the first element is false. If the first element is true, the RDFFR instruction continues iteration. The length of FFR is the same as the P vector. This value can be initialized using the SETFFR instruction. The following example uses LDFF1D to read data from memory, and FFR is updated accordingly:&lt;/p>
&lt;pre>&lt;code class="language-armasm">LDFF1D Z0.D, P0/Z, [Z1.D, #0] // Use the first-fault behavior to gather doublewords from the memory address generated by vector base address Z1 plus 0, loading into the active elements of Z0. Inactive elements do not read device memory or trigger a fault, and are set to zero in the destination vector. A successful load from valid memory sets the corresponding element in the FFR to true. The first-fault load sets the corresponding element and the remaining elements in the FFR to false or 0.
&lt;/code>&lt;/pre>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_Vector-partioning-for-software-managed-speculation-example.webp"
alt="Vector-partioning-for-software-managed-speculation-example-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Example of Vector Partitioning for Software-Managed Speculation&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>Extended floating point and horizontal reduction&lt;/li>
&lt;/ul>
&lt;p>In order to allow efficient reduction operations in vectors and meet different precision requirements, SVE enhances floating-point and horizontal reduction operations. These instructions may have a sequential (low to high) or tree-based (pairwise) floating-point reduction order, where the order of operations may lead to different rounding results. These operations require a trade-off between reproducibility and performance. For example:&lt;/p>
&lt;pre>&lt;code class="language-armasm">FADDA D0, P0/M, D1, Z2.D // Perform a floating-point addition strict-order reduction from the low to high elements of the source vector, accumulating the result into the SIMD&amp;amp;FP scalar register. This example instruction adds D1 to all active elements of Z2.D and stores the result into scalar register D0. Vector elements are processed in strict order from low to high, with scalar source D1 providing the initial value. Inactive elements in the source vector are ignored. FADDV performs a recursive pairwise reduction and stores the result into the scalar register.
&lt;/code>&lt;/pre>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_Extended_Floating-poing-and-horizontal-reductions-example.webp"
alt="Extended_Floating-poing-and-horizontal-reductions-example-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Extended Floating-point and Horizontal Reductions Example&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h2 id="3-new-features-of-sve2">
&lt;a href="#3-new-features-of-sve2" class="header-anchor">#&lt;/a>
3. New Features of SVE2
&lt;/h2>
&lt;p>This section introduces the features added by SVE2 to the Arm AArch64 architecture. To achieve scalable performance, SVE2 is built on SVE, allowing vectors to reach up to 2048 bits.&lt;/p>
&lt;p>In SVE2, many instructions that replicate existing instructions in Neon have been added, including:&lt;/p>
&lt;ul>
&lt;li>Converted Neon integer operations, for example, Signed Absolute Difference Accumulate (SAB) and Signed Halving Add (SHADD).&lt;/li>
&lt;li>Converted Neon extensions, narrowing and paired operations, for example, Unsigned Add Long - Bottom (UADDLB) and Unsigned Add Long - Top (UADDLT).&lt;/li>
&lt;/ul>
&lt;p>The order of element processing has changed. SVE2 processes interleaved even and odd elements, while Neon processes the low half and high half elements of narrow or wide operations. The diagram below illustrates the difference between Neon and SVE2 processing:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_transformed_neon_widen_narraow_pairwise_operations.webp"
alt="transformed_neon_widen_narraow_pairwise_operations-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Comparison of Transformed Neon Narrow or Wide Operations&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>Complex number operations, such as complex integer multiplication-accumulation with rotation (CMLA).&lt;/li>
&lt;li>Multi-precision arithmetic, used for large integer arithmetic and cryptography, for example, carry-in long addition - bottom (ADCLB), carry-in long addition - top (ADCLT) and SM4 encryption and decryption (SM4E).&lt;/li>
&lt;/ul>
&lt;p>For backward compatibility, the latest architecture requires Neon and VFP. Although SVE2 includes some features of SVE and Neon, SVE2 does not preclude the presence of Neon on the chip.&lt;/p>
&lt;p>SVE2 supports optimization for emerging applications beyond the HPC market, such as in machine learning (ML) (UDOT instructions), computer vision (TBL and TBX instructions), baseband networks (CADD and CMLA instructions), genomics (BDEP and BEXT instructions), and servers (MATCH and NMATCH instructions).&lt;/p>
&lt;p>SVE2 enhances the overall performance of general-purpose processors in handling large volumes of data, without the need for additional off-chip accelerators.&lt;/p>
&lt;h2 id="4-using-sve-programming">
&lt;a href="#4-using-sve-programming" class="header-anchor">#&lt;/a>
4. Using SVE programming
&lt;/h2>
&lt;p>This section introduces software tools and libraries that support SVE2 application development. This section also explains how to develop applications for targets that support SVE2, run the application on hardware that supports SVE2, and simulate the application on any Armv8-A hardware.&lt;/p>
&lt;h3 id="41-software-and-library-support">
&lt;a href="#41-software-and-library-support" class="header-anchor">#&lt;/a>
4.1 Software and Library Support
&lt;/h3>
&lt;p>To build SVE or SVE2 applications, you must choose a compiler that supports SVE and SVE2 features.&lt;/p>
&lt;ul>
&lt;li>GNU tools version 8.0+ supports SVE.&lt;/li>
&lt;li>&lt;a class="link" href="https://developer.arm.com/tools-and-software/server-and-hpc/compile/arm-compiler-for-linux" target="_blank" rel="noopener" >Arm Compiler for Linux
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
Version 18.0+ supports SVE, Version 20.0+ supports SVE and SVE2.&lt;/li>
&lt;li>Both GNU and Arm Compiler for Linux compilers support optimizing C/C++/Fortran code.&lt;/li>
&lt;li>LLVM (open-source Clang) version 5 and above includes support for SVE, and version 9 and above includes support for SVE2. To find out which SVE or SVE2 features are supported by each version of the LLVM tools, please refer to the &lt;a class="link" href="https://developer.arm.com/tools-and-software/open-source-software/developer-tools/llvm-toolchain/sve-support" target="_blank" rel="noopener" >LLVM toolchain SVE support page
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/li>
&lt;/ul>
&lt;p>&lt;a class="link" href="https://developer.arm.com/Tools%20and%20Software/Arm%20Performance%20Libraries" target="_blank" rel="noopener" >Arm Performance Libraries
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
are highly optimized for mathematical routines and can be linked to your applications. Arm Performance Libraries version 19.3+ supports SVE&amp;rsquo;s math library.&lt;/p>
&lt;p>Arm Compiler for Linux is part of Arm Allinea Studio, including Arm C/C++ Compiler, Arm Fortran Compiler, and Arm Performance Libraries.&lt;/p>
&lt;h3 id="42-how-to-program-using-sve2">
&lt;a href="#42-how-to-program-using-sve2" class="header-anchor">#&lt;/a>
4.2 How to Program Using SVE2
&lt;/h3>
&lt;p>There are several methods to write or generate SVE and SVE2 code. In this section, we will explore some of these methods.&lt;/p>
&lt;p>To write or generate SVE and SVE2 code, you can:&lt;/p>
&lt;ul>
&lt;li>Write SVE assembly code&lt;/li>
&lt;li>Programming with SVE intrinsics&lt;/li>
&lt;li>Automatic vectorization&lt;/li>
&lt;li>Use SVE optimization library&lt;/li>
&lt;/ul>
&lt;p>Let&amp;rsquo;s take a closer look at these four options.&lt;/p>
&lt;h4 id="421-write-sve-assembly-code">
&lt;a href="#421-write-sve-assembly-code" class="header-anchor">#&lt;/a>
4.2.1 Write SVE assembly code
&lt;/h4>
&lt;p>You can write SVE instructions as inline assembly in C/C++ code, or as a complete function in assembly source code. For example:&lt;/p>
&lt;pre>&lt;code class="language-armasm">```assembly
.globl subtract_arrays // -- Begin function
.p2align 2
.type subtract_arrays, @function
subtract_arrays: // @subtract_arrays
.cfi_startproc
// %bb.0:
orr w9, wzr, #0x400
mov x8, xzr
whilelo p0.s, xzr, x9
.LBB0_1: // =&amp;gt;This Inner Loop Header: Depth=1
ld1w { z0.s }, p0/z, [x1, x8, lsl #2]
ld1w { z1.s }, p0/z, [x2, x8, lsl #2]
sub z0.s, z0.s, z1.s
st1w { z0.s }, p0, [x0, x8, lsl #2]
incw x8
whilelo p0.s, x8, x9
b.mi .LBB0_1
// %bb.2:
ret
.Lfunc_end0:
.size subtract_arrays, .Lfunc_end0-subtract_arrays
.cfi_endproc
&lt;/code>&lt;/pre>
&lt;p>If you write functions that mix high-level language and assembly language, you must be familiar with the &lt;a class="link" href="https://developer.arm.com/documentation/ihi0036/latest/" target="_blank" rel="noopener" >Application Binary Interface (ABI)
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
standards updated for SVE. The &lt;a class="link" href="https://developer.arm.com/documentation/ihi0055/latest" target="_blank" rel="noopener" >Arm Architecture Procedure Call Standard (AAPCS)
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
specifies data types and register allocation, and is most relevant to assembly programming. AAPCS requires:&lt;/p>
&lt;ul>
&lt;li>&lt;code>Z0-Z7&lt;/code> and &lt;code>P0-P3&lt;/code> are used to pass scalable vector parameters and results.&lt;/li>
&lt;li>&lt;code>Z8-Z15&lt;/code> and &lt;code>P4-P15&lt;/code> are callee-saved.&lt;/li>
&lt;li>All other vector registers (&lt;code>Z16-Z31&lt;/code>) may be corrupted by the called function, and the calling function is responsible for backing up and restoring them when necessary.&lt;/li>
&lt;/ul>
&lt;h4 id="422-using-sve-instruction-functions-intrinsics">
&lt;a href="#422-using-sve-instruction-functions-intrinsics" class="header-anchor">#&lt;/a>
4.2.2 Using SVE Instruction Functions (Intrinsics)
&lt;/h4>
&lt;p>SVE intrinsic functions are functions supported by the compiler that can be replaced with corresponding instructions. Programmers can directly call instruction functions in high-level languages such as C and C++. The ACLE (Arm C Language Extensions) for SVE defines which SVE intrinsic functions are available, their parameters, and their functionality. A compiler that supports ACLE can replace intrinsics with mapped SVE instructions during compilation. To use ACLE intrinsics, you must include the header file &lt;code>arm_sve.h&lt;/code>, which contains a list of vector types and intrinsic functions (for SVE) that can be used in C/C++. Each data type describes the size and data type of the elements in the vector:&lt;/p>
&lt;ul>
&lt;li>&lt;code>svint8_t svuint8_t&lt;/code>&lt;/li>
&lt;li>&lt;code>svint16_t svuint16_t svfloat16_t&lt;/code>&lt;/li>
&lt;li>&lt;code>svint32_t svuint32_t svfloat32_t&lt;/code>&lt;/li>
&lt;li>&lt;code>svint64_t svuint64_t svfloat64_t&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>For example, &lt;code>svint64_t&lt;/code> represents a 64-bit signed integer vector, &lt;code>svfloat16_t&lt;/code> represents a half-precision floating-point vector.&lt;/p>
&lt;p>The following example C code has been manually optimized using SVE intrinsics:&lt;/p>
&lt;pre>&lt;code class="language-c">// intrinsic_example.c
#include &amp;lt;arm_sve.h&amp;gt;
svuint64_t uaddlb_array(svuint32_t Zs1, svuint32_t Zs2)
{
// widening add of even elements
svuint64_t result = svaddlb(Zs1, Zs2);
return result;
}
&lt;/code>&lt;/pre>
&lt;p>The source code that includes the &lt;code>arm_sve.h&lt;/code> header file can use SVE vector types, just like data types can be used for variable declarations and function parameters. To compile the code using the Arm C/C++ compiler and target the Armv8-A architecture that supports SVE, use:&lt;/p>
&lt;pre>&lt;code class="language-bash">armclang -O3 -S -march=armv8-a+sve2 -o intrinsic_example.s intrinsic_example.c
&lt;/code>&lt;/pre>
&lt;p>This command generates the following assembly code:&lt;/p>
&lt;pre>&lt;code class="language-armasm">// instrinsic_example.s
uaddlb_array: // @uaddlb_array
.cfi_startproc
// %bb.0:
uaddlb z0.d, z0.s, z1.s
ret
&lt;/code>&lt;/pre>
&lt;h4 id="423-automatic-vectorization">
&lt;a href="#423-automatic-vectorization" class="header-anchor">#&lt;/a>
4.2.3 Automatic Vectorization
&lt;/h4>
&lt;p>C/C++/Fortran compilers (for example, the native &lt;a class="link" href="https://developer.arm.com/tools-and-software/server-and-hpc/compile/arm-compiler-for-linux" target="_blank" rel="noopener" >Arm Compiler for Linux
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
for the Arm platform and the GNU compiler) support vectorization of C, C++, and Fortran loops using SVE or SVE2 instructions. To generate SVE or SVE2 code, choose the appropriate compiler options. For example, one option to enable SVE2 optimization using armclang is &lt;code>-march=armv8-a+sve2&lt;/code>. If you want to use the SVE version of the library, combine &lt;code>-march=armv8-a+sve2&lt;/code> with &lt;code>-armpl=sve&lt;/code>.&lt;/p>
&lt;h4 id="424-using-svesve2-to-optimize-libraries">
&lt;a href="#424-using-svesve2-to-optimize-libraries" class="header-anchor">#&lt;/a>
4.2.4 Using SVE/SVE2 to Optimize Libraries
&lt;/h4>
&lt;p>Use libraries highly optimized for SVE/SVE2, such as &lt;a class="link" href="https://developer.arm.com/Tools%20and%20Software/Arm%20Performance%20Libraries" target="_blank" rel="noopener" >Arm Performance Libraries
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
and Arm Compute Libraries. Arm Performance Libraries contain highly optimized implementations of mathematical functions optimized for BLAS, LAPACK, FFT, sparse linear algebra, and libamath. To be able to link any Arm Performance Libraries function, you must install Arm Allinea Studio and include armpl.h in your code. To build applications using Arm Compiler for Linux and Arm Performance Libraries, you must specify &lt;code>-armpl=&amp;lt;arg&amp;gt;&lt;/code> on the command line. If you are using GNU tools, you must include the Arm Performance Libraries installation path in the linker command line with &lt;code>-L&amp;lt;armpl_install_dir&amp;gt;/lib&lt;/code> and specify the GNU option equivalent to the Arm Compiler for Linux &lt;code>-armpl=&amp;lt;arg&amp;gt;&lt;/code> option, which is &lt;code>-larmpl_lp64&lt;/code>. For more information, please refer to the Arm Performance Libraries Getting Started Guide.&lt;/p>
&lt;h3 id="43-how-to-run-svesve2-programs">
&lt;a href="#43-how-to-run-svesve2-programs" class="header-anchor">#&lt;/a>
4.3 How to run SVE/SVE2 programs
&lt;/h3>
&lt;p>If you do not have access to SVE hardware, you can use models or simulators to run the code. You can choose from the following models and simulators:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>QEMU&lt;/strong>: Cross-compilation and native models, supporting modeling on Arm AArch64 platforms with SVE.&lt;/li>
&lt;li>&lt;strong>Fast Models&lt;/strong>: Cross-platform models that support modeling on Arm AArch64 platforms with SVE running on x86-based hosts. Architecture Envelope Model (AEM) with SVE2 support is only available to major partners.&lt;/li>
&lt;li>&lt;strong>Arm Instruction Emulator (ArmIE)&lt;/strong>: Runs directly on the Arm platform. Supports SVE and supports SVE2 from version 19.2+.&lt;/li>
&lt;/ul>
&lt;h2 id="5-acle-intrinsics">
&lt;a href="#5-acle-intrinsics" class="header-anchor">#&lt;/a>
5. ACLE Intrinsics
&lt;/h2>
&lt;h3 id="51-acle-introduction">
&lt;a href="#51-acle-introduction" class="header-anchor">#&lt;/a>
5.1 ACLE Introduction
&lt;/h3>
&lt;p>ACLE (Arm C Language Extensions) is used in C and C++ code to support Arm features through intrinsics and other characteristics.&lt;/p>
&lt;ul>
&lt;li>ACLE (ARM C Language Extensions) extends the C/C++ language with Arm-specific features.
&lt;ul>
&lt;li>Predefined macros: &lt;code>__ARM_ARCH_ISA_A64&lt;/code>, &lt;code>__ARM_BIG_ENDIAN&lt;/code>, etc.&lt;/li>
&lt;li>Internal functions: &lt;code>__clz(uint32_t x)&lt;/code>, &lt;code>__cls(uint32_t x)&lt;/code>, etc.&lt;/li>
&lt;li>Data types: SVE, NEON, and FP16 data types.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>ACLE support for SVE uses ACLE for variable-length vector (VLA) programming.
&lt;ul>
&lt;li>Almost every SVE instruction has a corresponding intrinsic function.&lt;/li>
&lt;li>Data type used to represent size-agnostic vectors used by SVE intrinsics.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Applicable scenarios for the following users:
&lt;ul>
&lt;li>Users who wish to manually adjust SVE code.&lt;/li>
&lt;li>Users who wish to adapt or manually optimize applications and libraries.&lt;/li>
&lt;li>Users who need low-level access to Arm targets.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="52-how-to-use-acle">
&lt;a href="#52-how-to-use-acle" class="header-anchor">#&lt;/a>
5.2 How to use ACLE
&lt;/h3>
&lt;ul>
&lt;li>Include header files
&lt;ul>
&lt;li>&lt;code>arm_acle.h&lt;/code>: Core ACLE&lt;/li>
&lt;li>&lt;code>arm_fp16.h&lt;/code>: Add FP16 data type.
&lt;ul>
&lt;li>The target platform must support FP16, i.e., &lt;code>march=armv8-a+fp16&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>arm_neon.h&lt;/code>: Add NEON Intrinsics and data types.
&lt;ul>
&lt;li>The target platform must support NEON, i.e., &lt;code>march=armv8-a+simd&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>arm_sve.h&lt;/code>: Add SVE Intrinsics and data types.
&lt;ul>
&lt;li>The target platform must support SVE, i.e., &lt;code>march=armv8-a+sve&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="53-sve-acle">
&lt;a href="#53-sve-acle" class="header-anchor">#&lt;/a>
5.3 SVE ACLE
&lt;/h3>
&lt;ul>
&lt;li>The first thing to do is to include the header files&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-c">#include &amp;lt;arm_sve.h&amp;gt;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>VLA data type
&lt;ul>
&lt;li>&lt;code>svfloat64_t&lt;/code>, &lt;code>svfloat16_t&lt;/code>, &lt;code>svuint32_t&lt;/code>, etc.&lt;/li>
&lt;li>Naming convention: &lt;code>sv&amp;lt;datatype&amp;gt;&amp;lt;datasize&amp;gt;_t&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Prediction
&lt;ul>
&lt;li>Merge: &lt;code>_m&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Reset: &lt;code>_z&lt;/code>&lt;/li>
&lt;li>Uncertain: &lt;code>_x&lt;/code>&lt;/li>
&lt;li>Data type of P register: &lt;code>svbool_t&lt;/code>&lt;/li>
&lt;li>Use generics for function overloading, for example, the function &lt;code>svadd&lt;/code> will automatically select the corresponding function based on the parameter type.&lt;/li>
&lt;li>Function naming convention: &lt;code>svbase[disambiguator][type0][type1]...[predication]&lt;/code>
&lt;ul>
&lt;li>base refers to basic operations, such as &lt;code>add&lt;/code>, &lt;code>mul&lt;/code>, &lt;code>sub&lt;/code>, etc.&lt;/li>
&lt;li>disambiguator is used to distinguish different variants of the same basic operation.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>typeN specifies the type of vector and P register.&lt;/li>
&lt;li>predication specifies the handling method for inactive elements.
&lt;ul>
&lt;li>For example: &lt;code>svfloat64_t svld1_f64&lt;/code>, &lt;code>svbool_t svwhilelt_b8&lt;/code>, &lt;code>svuint32_t svmla_u32_z&lt;/code>, &lt;code>svuint32_t svmla_u32_m&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="54-common-sve-intrinsics">
&lt;a href="#54-common-sve-intrinsics" class="header-anchor">#&lt;/a>
5.4 Common SVE Intrinsics
&lt;/h3>
&lt;ul>
&lt;li>Predicate
&lt;ul>
&lt;li>Predicate is a vector of type bool, used to control whether the corresponding position in the vector participates in the computation during the process.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>svbool_t pg = svwhilelt_b32(i, num)&lt;/code> generates a predicate for (i, i + 1, i + 2, &amp;hellip;, i + vl - 1) &amp;lt; num
&lt;ul>
&lt;li>&lt;code>svbool_t pg = svptrue_b32()&lt;/code> generates a predicate that is all true&lt;/li>
&lt;li>Among them, b32 corresponds to processing 32-bit data (int/float), in addition, there are also intrinsics corresponding to b8, b16, b64.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Memory data access
&lt;ul>
&lt;li>&lt;code>svld1(pg, *base)&lt;/code>: Load contiguous vector from address base.&lt;/li>
&lt;li>&lt;code>svst1(pg, *base, vec)&lt;/code>: Store the vector vec into the address base.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>svld1_gather_index(pg, *base, vec_index)&lt;/code>: Load the data corresponding to the vector index from the address base.&lt;/li>
&lt;li>&lt;code>svst1_scatter_index(pg, *base, vec_index, vec)&lt;/code>: Store data from vector vec to the positions corresponding to the vector indices.&lt;/li>
&lt;li>Basic calculation
&lt;ul>
&lt;li>&lt;code>svadd_z(pg, sv_vec1, sv_vec2)&lt;/code>&lt;/li>
&lt;li>&lt;code>svadd_m(pg, sv_vec1, sv_vec2)&lt;/code>&lt;/li>
&lt;li>&lt;code>svadd_x(pg, sv_vec1, sv_vec2)&lt;/code>&lt;/li>
&lt;li>&lt;code>svadd_x(pg, sv_vec1, x)&lt;/code>&lt;/li>
&lt;li>Among them, &lt;code>_z&lt;/code> indicates setting the position where pg is false to zero, &lt;code>_m&lt;/code> indicates retaining the original value, and &lt;code>_x&lt;/code> indicates uncertainty (any value is possible).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>The second operand can be scalar data.&lt;/li>
&lt;li>&lt;code>svmul&lt;/code>, &lt;code>svsub&lt;/code>, &lt;code>svsubr&lt;/code>, &lt;code>svdiv&lt;/code>, &lt;code>svdivr&lt;/code>: Among them, &lt;code>svsubr&lt;/code> swaps the position of the subtrahend and the minuend compared to &lt;code>svsub&lt;/code>.&lt;/li>
&lt;li>Others&lt;/li>
&lt;li>&lt;code>svdup_f64(double x)&lt;/code>: Generate a vector with all elements being x.
&lt;ul>
&lt;li>&lt;code>svcntd()&lt;/code>: Returns the vector length of 64-bit data: &lt;code>svcntb&lt;/code> corresponds to 8 bits, &lt;code>svcnth&lt;/code> corresponds to 16 bits, &lt;code>svcntw&lt;/code> corresponds to 32 bits.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="55-sve-structure-intrinsics">
&lt;a href="#55-sve-structure-intrinsics" class="header-anchor">#&lt;/a>
5.5 SVE Structure Intrinsics
&lt;/h3>
&lt;p>For corresponding structure data, SVE provides some special intrinsics, such as: &lt;code>svld3&lt;/code>, &lt;code>svget3&lt;/code>, &lt;code>svset3&lt;/code>, &lt;code>svst3&lt;/code>, etc. These intrinsics are used for processing structure data.&lt;/p>
&lt;p>For example, for the particle structure:&lt;/p>
&lt;pre>&lt;code class="language-c">typedef struct {
float x;
float y;
float z;
} Particle;
&lt;/code>&lt;/pre>
&lt;p>You can use &lt;code>svld3&lt;/code> to load all the data in the structure as a group of 3 vectors, and then use &lt;code>svget3&lt;/code> to extract a vector from the group of 3 vectors, where the value of index 0, 1, 2 corresponds to x, y, z respectively.&lt;/p>
&lt;pre>&lt;code class="language-c">Particle *ps;
float factor = 2.2;
// Initialization part omitted
for (int i = 0; i &amp;lt; num; i += svcntw()) {
svbool_t pg = svwhilelt_b32(i, num);
svfloat32x3_t sv_ps = svld3(pg, (float32_t *)&amp;amp;ps[i]);
svfloat32_t sv_ps_x = svget3(sv_ps, 0);
svfloat32_t sv_ps_y = svget3(sv_ps, 1);
// Perform calculation
sv_ps_x = svmul_x(pg, sv_ps_x, factor);
sv_ps_y = svmul_x(pg, sv_ps_y, factor);
// Save results
sv_ps = svset3(sv_ps, 0, sv_ps_x);
sv_ps = svset3(sv_ps, 1, sv_ps_y);
svst3(pg, (float32_t *)&amp;amp;ps[i], sv_ps);
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>svld3(pg, *base)&lt;/code>: Load all data in the structure as a group of 3 vectors; where base is the address of the 3-element structure array.&lt;/li>
&lt;li>&lt;code>svget3(tuple, index)&lt;/code>: Extract a vector from a group of 3 vectors; the value of index is 0, 1, or 2.&lt;/li>
&lt;li>&lt;code>svset3(tuple, index, vec)&lt;/code>: Set one vector in a group of 3 vectors; the value of index is 0, 1, or 2.&lt;/li>
&lt;li>&lt;code>svst3(pg, *base, vec)&lt;/code>: Store a group of 3 vectors into a structure; where base is the address of an array of structures with 3 elements.&lt;/li>
&lt;/ul>
&lt;h3 id="56-sve-condition-selection">
&lt;a href="#56-sve-condition-selection" class="header-anchor">#&lt;/a>
5.6 SVE Condition Selection
&lt;/h3>
&lt;p>SVE provides methods such as &lt;code>svcmplt&lt;/code>, &lt;code>svcompact&lt;/code>, &lt;code>svcntp_b32&lt;/code>, etc., which can select elements to retain in the vector based on conditions.&lt;/p>
&lt;p>For example, for non-vectorized code:&lt;/p>
&lt;pre>&lt;code class="language-c">for (int i = 0; i &amp;lt; num; i++) {
float tmp = provided[i];
if (tmp &amp;lt; mark) {
selected[count++] = tmp;
if (count &amp;gt;= maxSize) {
break;
}
}
}
&lt;/code>&lt;/pre>
&lt;p>The purpose of this code is to select elements from the provided array that are less than mark and store them in the selected array until the selected array is full.&lt;/p>
&lt;p>Rewrite with SVE Intrinsic:&lt;/p>
&lt;pre>&lt;code class="language-c">for (int i = 0; i &amp;lt; num; i += svcntw()) {
svbool_t pg = svwhilelt_b32(i, num);
svfloat32_t sv_tmp = svld1(pg, &amp;amp;provided[i]);
svbool_t pg_sel = svcmplt(pg, sv_tmp, mark);
sv_tmp = svcompact(pg_sel, sv_tmp);
svst1(pg, &amp;amp;selected[count], sv_tmp);
count += svcntp_b32(pg, pg_sel);
if (count &amp;gt;= maxSize) {
break;
}
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>svcmplt(pg, vec1, vec2)&lt;/code>: Compare the size of two vectors, returning a predicate indicating the positions in vec1 that are less than vec2.&lt;/li>
&lt;li>&lt;code>svcompact(pg, sv_tmp)&lt;/code>: Compress the vector, move the data with &lt;code>pg&lt;/code> as active to the lower positions of the vector in order, and set the remaining positions to zero.&lt;/li>
&lt;li>&lt;code>svcntp_b32(pg, pg2)&lt;/code>: Returns the number of active elements in pg2&lt;/li>
&lt;li>This code first loads the data from the provided array into sv_tmp, then uses &lt;code>svcmplt&lt;/code> to generate a predicate indicating the positions less than mark. Next, it uses &lt;code>svcompact&lt;/code> to compress sv_tmp, obtaining the data less than mark, and then stores it into the selected array using &lt;code>svst1&lt;/code>. Finally, it uses &lt;code>svcntp_b32&lt;/code> to count the number of active elements and update count.&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_compact.webp"
alt="compact-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>svcompact schematic diagram (256-bit vector)&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>Due to the compact operation, the selected array stores new data less than mark continuously from the count position, and the remaining positions are set to zero.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-08-13_svst1.webp"
alt="svst1-2024-08-13" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>svst1 schematic diagram (256-bit vector)&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="57-sve-vectorized-loop-interleaving">
&lt;a href="#57-sve-vectorized-loop-interleaving" class="header-anchor">#&lt;/a>
5.7 SVE Vectorized Loop Interleaving
&lt;/h3>
&lt;p>The vectorized loop interleaving implemented by SVE Intrinsic can greatly reduce the number of times vectors are read compared to compiler auto vectorization.&lt;/p>
&lt;p>For example, for non-vectorized code:&lt;/p>
&lt;pre>&lt;code class="language-c">for (int j = offset; j &amp;lt; outerLen - offset; j++) {
int m2index = (j - offset) * innerLen;
int m1index = m2index + innerLen;
int m0index = m1index + innerLen;
int p1index = m0index + innerLen;
int p2index = p1index + innerLen;
for (int i = 0; i &amp;lt; innerLen; i++) {
res[m0index + i] = m2factor * field[m2index + i] +
m1factor * field[m1index + i] +
m0factor * field[m0index + i] +
p1factor * field[p1index + i] +
p2factor * field[p2index + i];
}
}
&lt;/code>&lt;/pre>
&lt;p>After the compiler automatically vectorizes the code, each iteration requires reading data from five different vectors, resulting in low efficiency.&lt;/p>
&lt;p>Rewrite with SVE Intrinsic:&lt;/p>
&lt;pre>&lt;code class="language-c">for (int i = 0; i &amp;lt; innerLen; i += svcntd()) {
svbool_t pg = svwhilelt_b32(i, innerLen);
int dataIndex = i;
svfloat64_t jm2Field = svld1(pg, &amp;amp;field[dataIndex]);
dataIndex += innerLen;
svfloat64_t jm1Field = svld1(pg, &amp;amp;field[dataIndex]);
dataIndex += innerLen;
svfloat64_t jm0Field = svld1(pg, &amp;amp;field[dataIndex]);
dataIndex += innerLen;
svfloat64_t jp1Field = svld1(pg, &amp;amp;field[dataIndex]);
for (int j = offset; j &amp;lt; outerLen - offset; j += 1) {
svfloat64_t jp2Field = svld1(pg, &amp;amp;field[(j + offset) * innerLen + i]);
svfloat64_t svRes = svmul_x(pg, jm2Field, m2factor);
svRes = svmad_x(pg, jm1Field, m1factor, svRes);
svRes = svmad_x(pg, jm0Field, m0factor, svRes);
svRes = svmad_x(pg, jp1Field, p1factor, svRes);
svRes = svmad_x(pg, jp2Field, p2factor, svRes);
svst1(pg, &amp;amp;res[j * innerLen + 1], svRes);
jm2Field = jm1Field;
jm1Field = jm0Field;
jm0Field = jp1Field;
jp1Field = jp2Field;
}
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>svmad_x(pg, vec1, vec2, vec3)&lt;/code>: Calculates vec1 * vec2 + vec3, returns a vector.&lt;/li>
&lt;li>This code only needs to read one vector per iteration, greatly reducing the number of vector reads.&lt;/li>
&lt;/ul>
&lt;h2 id="references">
&lt;a href="#references" class="header-anchor">#&lt;/a>
References
&lt;/h2>
&lt;ol>
&lt;li>&lt;a class="link" href="https://developer.arm.com/-/media/Arm%20Developer%20Community/PDF/102340_0001_02_en_introduction-to-sve2.pdf?revision=b208e56b-6569-4ae2-b6f3-cd7d5d1ecac3" target="_blank" rel="noopener" >Introduction to SVE2
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;li>&lt;a class="link" href="https://www.stonybrook.edu/commcms/ookami/support/_docs/5%20-%20Advanced%20SVE.pdf" target="_blank" rel="noopener" >SVE Deep Dive
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;li>&lt;a class="link" href="https://arm-software.github.io/acle/main/acle.html" target="_blank" rel="noopener" >Arm C Language Extensions
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;/ol></description></item><item><title>Introduction to LLM Ecosystem: From Model Fine-tuning to Application Implementation</title><link>https://cuterwrite.top/en/p/llm-ecosystem/</link><pubDate>Fri, 05 Jul 2024 22:46:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/llm-ecosystem/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_119269220_p0_master1200.webp" alt="Featured image of post Introduction to LLM Ecosystem: From Model Fine-tuning to Application Implementation" />&lt;h1 id="llm-ecosystem-introduction-from-model-fine-tuning-to-application-implementation">
&lt;a href="#llm-ecosystem-introduction-from-model-fine-tuning-to-application-implementation" class="header-anchor">#&lt;/a>
LLM Ecosystem Introduction: From Model Fine-Tuning to Application Implementation
&lt;/h1>
&lt;h2 id="model-fine-tuning">
&lt;a href="#model-fine-tuning" class="header-anchor">#&lt;/a>
Model fine-tuning
&lt;/h2>
&lt;p>Pre-trained LLMs typically possess broad knowledge, but fine-tuning is essential for them to excel in specific tasks. Here are some commonly used LLM fine-tuning tools:&lt;/p>
&lt;h3 id="axolotl">
&lt;a href="#axolotl" class="header-anchor">#&lt;/a>
Axolotl
&lt;/h3>
&lt;a href="https://github.com/OpenAccess-AI-Collective/axolotl" target="_blank" class="card-github fetch-waiting no-styling"
repo="OpenAccess-AI-Collective/axolotl" id="repo-sWfSfdZrnoIBUsAV-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-sWfSfdZrnoIBUsAV-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">OpenAccess-AI-Collective&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">axolotl&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-sWfSfdZrnoIBUsAV-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-sWfSfdZrnoIBUsAV-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-sWfSfdZrnoIBUsAV-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-sWfSfdZrnoIBUsAV-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-sWfSfdZrnoIBUsAV-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-sWfSfdZrnoIBUsAV-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/OpenAccess-AI-Collective\/axolotl', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-sWfSfdZrnoIBUsAV-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-sWfSfdZrnoIBUsAV-language').innerText = data.language;
document.getElementById('repo-sWfSfdZrnoIBUsAV-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-sWfSfdZrnoIBUsAV-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-sWfSfdZrnoIBUsAV-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-sWfSfdZrnoIBUsAV-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-sWfSfdZrnoIBUsAV-license').classList.add = "no-license"
};
document.getElementById('repo-sWfSfdZrnoIBUsAV-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for OpenAccess-AI-Collective\/axolotl.")
}).catch(err => {
const c = document.getElementById('repo-sWfSfdZrnoIBUsAV-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for OpenAccess-AI-Collective\/axolotl.")
})
&lt;/script>
&lt;p>Axolotl is a tool designed to simplify the fine-tuning of various AI models, supporting multiple configurations and architectures.&lt;/p>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Train various Huggingface models, such as llama, pythia, falcon, mpt&lt;/li>
&lt;li>Supports fullfinetune, lora, qlora, relora, and gptq&lt;/li>
&lt;li>Customize configuration using simple yaml files or CLI rewrite functions&lt;/li>
&lt;li>Load different dataset formats, use custom formats, or built-in tokenized datasets&lt;/li>
&lt;li>Integrated with xformer, flash attention, rope scaling, and multi-packing&lt;/li>
&lt;li>Can work with a single GPU or multiple GPUs through FSDP or Deepspeed.&lt;/li>
&lt;li>Easily run locally or in the cloud using Docker&lt;/li>
&lt;li>Record the results and optional checkpoints to wandb or mlflow&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Quick Start:&lt;/strong>
Requirements: Python &amp;gt;=3.10 and Pytorch &amp;gt;=2.1.1&lt;/p>
&lt;pre>&lt;code class="language-bash">git clone https://github.com/OpenAccess-AI-Collective/axolotl
cd axolotl
pip3 install packaging ninja
pip3 install -e '.[flash-attn,deepspeed]'
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>Usage:&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash"># preprocess datasets - optional but recommended
CUDA_VISIBLE_DEVICES=&amp;quot;&amp;quot; python -m axolotl.cli.preprocess examples/openllama-3b/lora.yml
# finetune lora
accelerate launch -m axolotl.cli.train examples/openllama-3b/lora.yml
# inference
accelerate launch -m axolotl.cli.inference examples/openllama-3b/lora.yml \
--lora_model_dir=&amp;quot;./outputs/lora-out&amp;quot;
# gradio
accelerate launch -m axolotl.cli.inference examples/openllama-3b/lora.yml \
--lora_model_dir=&amp;quot;./outputs/lora-out&amp;quot; --gradio
# remote yaml files - the yaml config can be hosted on a public URL
# Note: the yaml config must directly link to the **raw** yaml
accelerate launch -m axolotl.cli.train https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/examples/openllama-3b/lora.yml
&lt;/code>&lt;/pre>
&lt;p>For more detailed information, please visit the &lt;a class="link" href="https://github.com/OpenAccess-AI-Collective/axolotl" target="_blank" rel="noopener" >Axolotl
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
project homepage.&lt;/p>
&lt;h3 id="llama-factory">
&lt;a href="#llama-factory" class="header-anchor">#&lt;/a>
Llama-Factory
&lt;/h3>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-07-07_llama-factory-logo.webp"
alt="Llama Factory Logo" width="80%" loading="lazy">
&lt;/figure>
&lt;a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" class="card-github fetch-waiting no-styling"
repo="hiyouga/LLaMA-Factory" id="repo-mUl70DsQDya40Ajd-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-mUl70DsQDya40Ajd-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">hiyouga&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">LLaMA-Factory&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-mUl70DsQDya40Ajd-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-mUl70DsQDya40Ajd-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-mUl70DsQDya40Ajd-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-mUl70DsQDya40Ajd-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-mUl70DsQDya40Ajd-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-mUl70DsQDya40Ajd-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/hiyouga\/LLaMA-Factory', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-mUl70DsQDya40Ajd-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-mUl70DsQDya40Ajd-language').innerText = data.language;
document.getElementById('repo-mUl70DsQDya40Ajd-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-mUl70DsQDya40Ajd-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-mUl70DsQDya40Ajd-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-mUl70DsQDya40Ajd-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-mUl70DsQDya40Ajd-license').classList.add = "no-license"
};
document.getElementById('repo-mUl70DsQDya40Ajd-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for hiyouga\/LLaMA-Factory.")
}).catch(err => {
const c = document.getElementById('repo-mUl70DsQDya40Ajd-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for hiyouga\/LLaMA-Factory.")
})
&lt;/script>
&lt;p>Llama-Factory is launched by Meta and is a framework focused on fine-tuning Llama models. It is built on top of the PyTorch ecosystem and provides efficient training and evaluation tools.&lt;/p>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Multiple models&lt;/strong>: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Yi, Gemma, Baichuan, ChatGLM, Phi, etc.&lt;/li>
&lt;li>&lt;strong>Integration Methods&lt;/strong>: (Incremental) Pre-training, (Multimodal) Instruction Supervised Fine-tuning, Reward Model Training, PPO Training, DPO Training, KTO Training, ORPO Training, etc.&lt;/li>
&lt;li>&lt;strong>Multiple Precisions&lt;/strong>: 16-bit full parameter fine-tuning, frozen fine-tuning, LoRA fine-tuning, and 2/3/4/5/6/8-bit QLoRA fine-tuning based on AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.&lt;/li>
&lt;li>&lt;strong>Advanced Algorithms&lt;/strong>: GaLore, BAdam, DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ, PiSSA, and Agent fine-tuning.&lt;/li>
&lt;li>&lt;strong>Practical Tips&lt;/strong>: FlashAttention-2, Unsloth, RoPE scaling, NEFTune, and rsLoRA.&lt;/li>
&lt;li>&lt;strong>Experiment Monitoring&lt;/strong>: LlamaBoard, TensorBoard, Wandb, MLflow, etc.&lt;/li>
&lt;li>&lt;strong>Rapid Reasoning&lt;/strong>: OpenAI-style API, browser interface, and command-line interface based on vLLM.&lt;/li>
&lt;/ul>
&lt;link href="https://web.sdk.qcloud.com/player/tcplayer/release/v4.2.1/tcplayer.min.css" rel="stylesheet">
&lt;script src="https://web.sdk.qcloud.com/player/tcplayer/release/v4.2.1/libs/hls.min.0.13.2m.js">&lt;/script>
&lt;script src="https://web.sdk.qcloud.com/player/tcplayer/release/v4.2.1/tcplayer.v4.2.1.min.js">&lt;/script>
&lt;style>
.tcplayer {
position: absolute;
width: 100%;
height: 100%;
left: 0;
top: 0;
border: 0;
}
&lt;/style>
&lt;div class="video-wrapper">
&lt;video
id="player-container-id"
preload="auto"
width="100%"
height="100%"
playsinline
webkit-playsinline>
&lt;/video>
&lt;/div>
&lt;script>
var tcplayer = TCPlayer("player-container-id", {
reportable: false,
poster: "",
});
tcplayer.src('https:\/\/cuterwrite-1302252842.file.myqcloud.com\/img\/2024-07-07_309488092-ec36a9dd-37f4-4f72-81bd-d76c6d0a6594_output.mp4');
&lt;/script>
&lt;p>&lt;strong>Performance Metrics&lt;/strong>&lt;/p>
&lt;p>Compared to the &lt;a class="link" href="https://github.com/THUDM/ChatGLM2-6B/tree/main/ptuning" target="_blank" rel="noopener" >P-Tuning
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
fine-tuning by ChatGLM official, the LoRA fine-tuning by LLaMA Factory provides a &lt;strong>3.7 times&lt;/strong> speedup and achieves higher Rouge scores in advertising copy generation tasks. Combined with 4-bit quantization technology, LLaMA Factory&amp;rsquo;s QLoRA fine-tuning further reduces GPU memory consumption.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-07-07_benchmark.webp"
alt="Performance" width="auto" loading="lazy">
&lt;/figure>
&lt;details>
&lt;summary>Variable Definition&lt;/summary>
&lt;ul>
&lt;li>&lt;strong>Training Speed&lt;/strong>: Number of samples processed per second during the training phase. (Batch size=4, truncation length=1024)&lt;/li>
&lt;li>&lt;strong>Rouge Score&lt;/strong>: Rouge-2 score on the validation set of the &lt;a class="link" href="https://aclanthology.org/D19-1321.pdf" target="_blank" rel="noopener" >advertising copy generation
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
task. (Batch size=4, truncation length=1024)&lt;/li>
&lt;li>&lt;strong>GPU Memory&lt;/strong>: Peak GPU memory for 4-bit quantization training. (Batch size=1, truncation length=1024)&lt;/li>
&lt;li>We use &lt;code>pre_seq_len=128&lt;/code> in the P-Tuning of ChatGLM, and &lt;code>lora_rank=32&lt;/code> in the LoRA fine-tuning of LLaMA Factory.&lt;/li>
&lt;/ul>
&lt;/details>
&lt;p>&lt;strong>Quick Start&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash">git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e &amp;quot;.[torch,metrics]&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Optional additional dependencies: torch, torch-npu, metrics, deepspeed, bitsandbytes, hqq, eetq, gptq, awq, aqlm, vllm, galore, badam, qwen, modelscope, quality&lt;/p>
&lt;blockquote class="alert-blockquote alert-tip">
&lt;p class="alert-heading">
&lt;svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16">
&lt;path d="M8 1.5c-2.363 0-4 1.69-4 3.75 0 .984.424 1.625.984 2.304l.214.253c.223.264.47.556.673.848.284.411.537.896.621 1.49a.75.75 0 0 1-1.484.211c-.04-.282-.163-.547-.37-.847a8.456 8.456 0 0 0-.542-.68c-.084-.1-.173-.205-.268-.32C3.201 7.75 2.5 6.766 2.5 5.25 2.5 2.31 4.863 0 8 0s5.5 2.31 5.5 5.25c0 1.516-.701 2.5-1.328 3.259-.095.115-.184.22-.268.319-.207.245-.383.453-.541.681-.208.3-.33.565-.37.847a.751.751 0 0 1-1.485-.212c.084-.593.337-1.078.621-1.489.203-.292.45-.584.673-.848.075-.088.147-.173.213-.253.561-.679.985-1.32.985-2.304 0-2.06-1.637-3.75-4-3.75ZM5.75 12h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM6 15.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 0 1.5h-2.5a.75.75 0 0 1-.75-.75Z">&lt;/path>
&lt;/svg>
&lt;span>Tip&lt;/span>
&lt;/p>
&lt;p>When encountering package conflicts, you can use pip install &amp;ndash;no-deps -e . to resolve them.&lt;/p>
&lt;/blockquote>
&lt;details>
&lt;summary>Windows User Guide&lt;/summary>
&lt;p>If you want to enable Quantized LoRA (QLoRA) on the Windows platform, you need to install the precompiled &lt;code>bitsandbytes&lt;/code> library, which supports CUDA 11.1 to 12.2. Please choose the appropriate &lt;a class="link" href="https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels" target="_blank" rel="noopener" >release version
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
according to your CUDA version.&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl
&lt;/code>&lt;/pre>
&lt;p>If you want to enable FlashAttention-2 on the Windows platform, you need to install the precompiled &lt;code>flash-attn&lt;/code> library, which supports CUDA 12.1 to 12.2. Please download and install the corresponding version according to your needs from &lt;a class="link" href="https://github.com/bdashore3/flash-attention/releases" target="_blank" rel="noopener" >flash-attention
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;/details>
&lt;details>
&lt;summary>Ascend NPU User Guide&lt;/summary>
&lt;p>When installing LLaMA Factory on Ascend NPU devices, you need to specify additional dependencies and use the command &lt;code>pip install -e &amp;quot;.[torch-npu,metrics]&amp;quot;&lt;/code> to install them. Additionally, you need to install the &lt;strong>&lt;a class="link" href="https://www.hiascend.com/developer/download/community/result?module=cann" target="_blank" rel="noopener" >Ascend CANN Toolkit and Kernels
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/strong>. Please refer to the &lt;a class="link" href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/80RC2alpha002/quickstart/quickstart/quickstart_18_0004.html" target="_blank" rel="noopener" >installation tutorial
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
or use the following command:&lt;/p>
&lt;pre>&lt;code class="language-bash"># Please replace URL with the URL corresponding to the CANN version and device model
# Install CANN Toolkit
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-toolkit_8.0.RC1.alpha001_linux-&amp;quot;$(uname -i)&amp;quot;.run
bash Ascend-cann-toolkit_8.0.RC1.alpha001_linux-&amp;quot;$(uname -i)&amp;quot;.run --install
# Install CANN Kernels
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run
bash Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run --install
# Set environment variables
source /usr/local/Ascend/ascend-toolkit/set_env.sh
&lt;/code>&lt;/pre>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Dependencies&lt;/th>
&lt;th style="text-align: left">Minimum&lt;/th>
&lt;th style="text-align: left">Recommended&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">CANN&lt;/td>
&lt;td style="text-align: left">8.0.RC1&lt;/td>
&lt;td style="text-align: left">8.0.RC1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">torch&lt;/td>
&lt;td style="text-align: left">2.1.0&lt;/td>
&lt;td style="text-align: left">2.1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">torch-npu&lt;/td>
&lt;td style="text-align: left">2.1.0&lt;/td>
&lt;td style="text-align: left">2.1.0.post3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">deepspeed&lt;/td>
&lt;td style="text-align: left">0.13.2&lt;/td>
&lt;td style="text-align: left">0.13.2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Please use &lt;code>ASCEND_RT_VISIBLE_DEVICES&lt;/code> instead of &lt;code>CUDA_VISIBLE_DEVICES&lt;/code> to specify the computation device.&lt;/p>
&lt;p>If you encounter a situation where reasoning cannot proceed normally, try setting &lt;code>do_sample: false&lt;/code>.&lt;/p>
&lt;p>Download pre-built Docker image: &lt;a class="link" href="http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html" target="_blank" rel="noopener" >32GB
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
| &lt;a class="link" href="http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html" target="_blank" rel="noopener" >64GB
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;/details>
&lt;p>The following three commands perform LoRA fine-tuning, inference, and merging on the Llama3-8B-Instruct model.&lt;/p>
&lt;pre>&lt;code class="language-bash">llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml
&lt;/code>&lt;/pre>
&lt;p>For more detailed information, please visit the &lt;a class="link" href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" rel="noopener" >Llama-Factory
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
project homepage.&lt;/p>
&lt;h3 id="firefly">
&lt;a href="#firefly" class="header-anchor">#&lt;/a>
Firefly
&lt;/h3>
&lt;a href="https://github.com/yangjianxin1/Firefly" target="_blank" class="card-github fetch-waiting no-styling"
repo="yangjianxin1/Firefly" id="repo-kbai5JGqxCdw8Qg2-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-kbai5JGqxCdw8Qg2-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">yangjianxin1&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">Firefly&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-kbai5JGqxCdw8Qg2-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-kbai5JGqxCdw8Qg2-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-kbai5JGqxCdw8Qg2-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-kbai5JGqxCdw8Qg2-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-kbai5JGqxCdw8Qg2-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-kbai5JGqxCdw8Qg2-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/yangjianxin1\/Firefly', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-kbai5JGqxCdw8Qg2-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-kbai5JGqxCdw8Qg2-language').innerText = data.language;
document.getElementById('repo-kbai5JGqxCdw8Qg2-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-kbai5JGqxCdw8Qg2-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-kbai5JGqxCdw8Qg2-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-kbai5JGqxCdw8Qg2-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-kbai5JGqxCdw8Qg2-license').classList.add = "no-license"
};
document.getElementById('repo-kbai5JGqxCdw8Qg2-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for yangjianxin1\/Firefly.")
}).catch(err => {
const c = document.getElementById('repo-kbai5JGqxCdw8Qg2-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for yangjianxin1\/Firefly.")
})
&lt;/script>
&lt;p>&lt;strong>Firefly&lt;/strong> is an open-source large model training project that supports pre-training, instruction fine-tuning, and DPO for mainstream large models, including but not limited to Qwen2, Yi-1.5, Llama3, Gemma, Qwen1.5, MiniCPM, Llama, InternLM, Baichuan, ChatGLM, Yi, Deepseek, Qwen, Orion, Ziya, Xverse, Mistral, Mixtral-8x7B, Zephyr, Vicuna, Bloom, etc.
This project supports &lt;strong>full parameter training, LoRA, QLoRA efficient training&lt;/strong>, and supports &lt;strong>pre-training, SFT, DPO&lt;/strong>. If your training resources are limited, we strongly recommend using QLoRA for instruction fine-tuning, as we have validated the effectiveness of this method on the Open LLM Leaderboard and achieved very good results.&lt;/p>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>📗 Supports pre-training, instruction fine-tuning, DPO, full parameter training, LoRA, QLoRA efficient training. Train different models through configuration files, allowing beginners to quickly get started with model training.&lt;/li>
&lt;li>📗 Supports using &lt;a class="link" href="https://github.com/yangjianxin1/unsloth" target="_blank" rel="noopener" >Unsloth
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
to accelerate training and save video memory.&lt;/li>
&lt;li>📗 Supports most mainstream open-source large models, such as Llama3, Gemma, MiniCPM, Llama, InternLM, Baichuan, ChatGLM, Yi, Deepseek, Qwen, Orion, Ziya, Xverse, Mistral, Mixtral-8x7B, Zephyr, Vicuna, Bloom, aligning with the templates of each official chat model during training.&lt;/li>
&lt;li>📗 Organize and open-source instruction fine-tuning datasets: firefly-train-1.1M, moss-003-sft-data, ultrachat, WizardLM_evol_instruct_V2_143k, school_math_0.25M.&lt;/li>
&lt;li>📗 Open source &lt;a class="link" href="https://huggingface.co/YeungNLP" target="_blank" rel="noopener" >Firefly series instruction fine-tuning model weights
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/li>
&lt;li>📗 Validated the effectiveness of the QLoRA training process on the Open LLM Leaderboard.&lt;/li>
&lt;/ul>
&lt;p>The README of the project contains detailed usage instructions, including how to install, how to train, how to fine-tune, and how to evaluate, etc. Please visit the &lt;a class="link" href="https://github.com/yangjianxin1/Firefly" target="_blank" rel="noopener" >Firefly
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
project homepage.&lt;/p>
&lt;h3 id="xtuner">
&lt;a href="#xtuner" class="header-anchor">#&lt;/a>
XTuner
&lt;/h3>
&lt;a href="https://github.com/InternLM/xtuner" target="_blank" class="card-github fetch-waiting no-styling"
repo="InternLM/xtuner" id="repo-uVsITipgIeprAJMK-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-uVsITipgIeprAJMK-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">InternLM&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">xtuner&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-uVsITipgIeprAJMK-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-uVsITipgIeprAJMK-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-uVsITipgIeprAJMK-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-uVsITipgIeprAJMK-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-uVsITipgIeprAJMK-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-uVsITipgIeprAJMK-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/InternLM\/xtuner', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-uVsITipgIeprAJMK-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-uVsITipgIeprAJMK-language').innerText = data.language;
document.getElementById('repo-uVsITipgIeprAJMK-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-uVsITipgIeprAJMK-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-uVsITipgIeprAJMK-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-uVsITipgIeprAJMK-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-uVsITipgIeprAJMK-license').classList.add = "no-license"
};
document.getElementById('repo-uVsITipgIeprAJMK-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for InternLM\/xtuner.")
}).catch(err => {
const c = document.getElementById('repo-uVsITipgIeprAJMK-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for InternLM\/xtuner.")
})
&lt;/script>
&lt;p>XTuner is an efficient, flexible, and versatile lightweight large model fine-tuning tool library.&lt;/p>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Efficient&lt;/strong>
&lt;ul>
&lt;li>Supports pre-training and lightweight fine-tuning of large language models (LLM) and multimodal image-text models (VLM). XTuner supports fine-tuning a 7B model with 8GB of video memory and also supports multi-node cross-device fine-tuning of larger scale models (70B+).&lt;/li>
&lt;li>Automatically distribute high-performance operators (such as FlashAttention, Triton kernels, etc.) to accelerate training throughput.&lt;/li>
&lt;li>Compatible with &lt;a class="link" href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener" >DeepSpeed
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
🚀, easily apply various ZeRO training optimization strategies.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Flexible&lt;/strong>
&lt;ul>
&lt;li>Supports multiple large language models, including but not limited to &lt;a class="link" href="https://huggingface.co/internlm" target="_blank" rel="noopener" >InternLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, &lt;a class="link" href="https://huggingface.co/mistralai" target="_blank" rel="noopener" >Mixtral-8x7B
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, &lt;a class="link" href="https://huggingface.co/meta-llama" target="_blank" rel="noopener" >Llama 2
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, &lt;a class="link" href="https://huggingface.co/THUDM" target="_blank" rel="noopener" >ChatGLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, &lt;a class="link" href="https://huggingface.co/Qwen" target="_blank" rel="noopener" >Qwen
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, &lt;a class="link" href="https://huggingface.co/baichuan-inc" target="_blank" rel="noopener" >Baichuan
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Supports pre-training and fine-tuning of the multimodal text-image model LLaVA. The model &lt;a class="link" href="https://huggingface.co/xtuner/llava-internlm2-20b" target="_blank" rel="noopener" >LLaVA-InternLM2-20B
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
trained with XTuner performs excellently.
&lt;ul>
&lt;li>Carefully designed data pipeline, compatible with any data format, both open-source data and custom data can be quickly adopted.&lt;/li>
&lt;li>Supports multiple fine-tuning algorithms such as &lt;a class="link" href="http://arxiv.org/abs/2305.14314" target="_blank" rel="noopener" >QLoRA
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, &lt;a class="link" href="http://arxiv.org/abs/2106.09685" target="_blank" rel="noopener" >LoRA
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, and full parameter fine-tuning, allowing users to make the optimal choice based on specific needs.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Omnipotent&lt;/strong>
&lt;ul>
&lt;li>Supports incremental pre-training, instruction fine-tuning, and Agent fine-tuning.&lt;/li>
&lt;li>Predefined numerous open-source dialogue templates, supporting conversation with open-source or trained models.&lt;/li>
&lt;li>The trained model can be seamlessly integrated with the deployment toolkit &lt;a class="link" href="https://github.com/InternLM/lmdeploy" target="_blank" rel="noopener" >LMDeploy
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, the large-scale evaluation toolkit &lt;a class="link" href="https://github.com/open-compass/opencompass" target="_blank" rel="noopener" >OpenCompass
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, and &lt;a class="link" href="https://github.com/open-compass/VLMEvalKit" target="_blank" rel="noopener" >VLMEvalKit
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Quick Start:&lt;/strong>
&lt;details>
&lt;summary>Installation&lt;/summary>
&lt;ul>
&lt;li>It is recommended to use conda to first build a Python-3.10 virtual environment&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash"> conda create --name xtuner-env python=3.10 -y
conda activate xtuner-env
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Install XTuner via pip:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-shell"> pip install -U xtuner
&lt;/code>&lt;/pre>
&lt;p>Can also integrate DeepSpeed installation:&lt;/p>
&lt;pre>&lt;code class="language-shell"> pip install -U 'xtuner[deepspeed]'
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Install XTuner from source:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-shell"> git clone https://github.com/InternLM/xtuner.git
cd xtuner
pip install -e '.[all]'
&lt;/code>&lt;/pre>
&lt;/details>
&lt;/p>
&lt;details>
&lt;summary>Fine-tuning&lt;/summary>
&lt;p>XTuner supports fine-tuning large language models. For dataset preprocessing guidelines, please refer to the &lt;a class="link" href="./docs/zh_cn/user_guides/dataset_prepare.md" >documentation
&lt;/a>
.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Step 0&lt;/strong>, prepare the configuration file. XTuner provides multiple out-of-the-box configuration files, which users can view with the following command:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-shell"> xtuner list-cfg
&lt;/code>&lt;/pre>
&lt;p>Or, if the provided configuration file does not meet the usage requirements, please export the provided configuration file and make the corresponding changes:&lt;/p>
&lt;pre>&lt;code class="language-shell"> xtuner copy-cfg ${CONFIG_NAME} ${SAVE_PATH}
vi ${SAVE_PATH}/${CONFIG_NAME}_copy.py
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;strong>Step 1&lt;/strong>, start fine-tuning.&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-shell"> xtuner train ${CONFIG_NAME_OR_PATH}
&lt;/code>&lt;/pre>
&lt;p>For example, we can use the QLoRA algorithm to fine-tune InternLM2.5-Chat-7B on the oasst1 dataset:&lt;/p>
&lt;pre>&lt;code class="language-shell"># Single card
xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2
# Multi-card
(DIST) NPROC_PER_NODE=${GPU_NUM} xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2
(SLURM) srun ${SRUN_ARGS} xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --launcher slurm --deepspeed deepspeed_zero2
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>
&lt;p>&lt;code>--deepspeed&lt;/code> indicates using &lt;a class="link" href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener" >DeepSpeed
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
🚀 to optimize the training process. XTuner has multiple built-in strategies, including ZeRO-1, ZeRO-2, ZeRO-3, etc. If the user wishes to disable this feature, please remove this parameter directly.&lt;/p>
&lt;ul>
&lt;li>For more examples, please refer to the &lt;a class="link" href="./docs/zh_cn/user_guides/finetune.md" >documentation
&lt;/a>
.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Step 2&lt;/strong>, convert the saved PTH model (if using DeepSpeed, it will be a folder) to a HuggingFace model:&lt;/p>
&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-shell"> xtuner convert pth_to_hf ${CONFIG_NAME_OR_PATH} ${PTH} ${SAVE_PATH}
&lt;/code>&lt;/pre>
&lt;/details>
&lt;p>For more detailed information, please visit the &lt;a class="link" href="https://github.com/InternLM/xtuner" target="_blank" rel="noopener" >XTuner
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
project homepage.&lt;/p>
&lt;h2 id="model-quantization">
&lt;a href="#model-quantization" class="header-anchor">#&lt;/a>
Model quantization
&lt;/h2>
&lt;p>LLM is usually large in volume and requires high computational resources. Model quantization techniques can compress model size, improve operational efficiency, and make it easier to deploy:&lt;/p>
&lt;h3 id="autogptq">
&lt;a href="#autogptq" class="header-anchor">#&lt;/a>
AutoGPTQ
&lt;/h3>
&lt;a href="https://github.com/PanQiWei/AutoGPTQ" target="_blank" class="card-github fetch-waiting no-styling"
repo="PanQiWei/AutoGPTQ" id="repo-4dNk8FLDDSayaw5s-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-4dNk8FLDDSayaw5s-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">PanQiWei&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">AutoGPTQ&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-4dNk8FLDDSayaw5s-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-4dNk8FLDDSayaw5s-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-4dNk8FLDDSayaw5s-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-4dNk8FLDDSayaw5s-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-4dNk8FLDDSayaw5s-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-4dNk8FLDDSayaw5s-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/PanQiWei\/AutoGPTQ', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-4dNk8FLDDSayaw5s-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-4dNk8FLDDSayaw5s-language').innerText = data.language;
document.getElementById('repo-4dNk8FLDDSayaw5s-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-4dNk8FLDDSayaw5s-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-4dNk8FLDDSayaw5s-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-4dNk8FLDDSayaw5s-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-4dNk8FLDDSayaw5s-license').classList.add = "no-license"
};
document.getElementById('repo-4dNk8FLDDSayaw5s-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for PanQiWei\/AutoGPTQ.")
}).catch(err => {
const c = document.getElementById('repo-4dNk8FLDDSayaw5s-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for PanQiWei\/AutoGPTQ.")
})
&lt;/script>
&lt;p>AutoGPTQ is a large language model quantization toolkit based on the GPTQ algorithm, simple to use and with a user-friendly interface.&lt;/p>
&lt;p>&lt;strong>Quick Installation&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>For CUDA 11.7:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>For CUDA 11.8:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>For RoCm 5.4.2:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/rocm542/
&lt;/code>&lt;/pre>
&lt;p>For more detailed information, please visit the project homepage of &lt;a class="link" href="https://github.com/AutoGPTQ/AutoGPTQ/" target="_blank" rel="noopener" >AutoGPTQ
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;h3 id="autoawq">
&lt;a href="#autoawq" class="header-anchor">#&lt;/a>
AutoAWQ
&lt;/h3>
&lt;a href="https://github.com/casper-hansen/AutoAWQ" target="_blank" class="card-github fetch-waiting no-styling"
repo="casper-hansen/AutoAWQ" id="repo-C6D48Dmmf83LBecs-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-C6D48Dmmf83LBecs-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">casper-hansen&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">AutoAWQ&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-C6D48Dmmf83LBecs-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-C6D48Dmmf83LBecs-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-C6D48Dmmf83LBecs-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-C6D48Dmmf83LBecs-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-C6D48Dmmf83LBecs-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-C6D48Dmmf83LBecs-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/casper-hansen\/AutoAWQ', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-C6D48Dmmf83LBecs-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-C6D48Dmmf83LBecs-language').innerText = data.language;
document.getElementById('repo-C6D48Dmmf83LBecs-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-C6D48Dmmf83LBecs-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-C6D48Dmmf83LBecs-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-C6D48Dmmf83LBecs-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-C6D48Dmmf83LBecs-license').classList.add = "no-license"
};
document.getElementById('repo-C6D48Dmmf83LBecs-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for casper-hansen\/AutoAWQ.")
}).catch(err => {
const c = document.getElementById('repo-C6D48Dmmf83LBecs-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for casper-hansen\/AutoAWQ.")
})
&lt;/script>
&lt;p>AutoAWQ is another automated model quantization tool that supports multiple quantization precisions and offers flexible configuration options, allowing adjustments based on different hardware platforms and performance requirements.&lt;/p>
&lt;p>AutoAWQ is an easy-to-use 4-bit quantization model package. Compared to FP16, AutoAWQ can increase model speed by 3 times and reduce memory requirements by 3 times. AutoAWQ implements the activation-aware weight quantization (AWQ) algorithm for quantizing LLMs. AutoAWQ was created and improved based on the original work &lt;a class="link" href="https://github.com/mit-han-lab/llm-awq" target="_blank" rel="noopener" >AWQ
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
from MIT.&lt;/p>
&lt;p>&lt;strong>Installation Method:&lt;/strong>&lt;/p>
&lt;p>Before installing, ensure that CUDA &amp;gt;= 12.1 is installed (Note: The following is just the quickest installation method)&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install autoawq
&lt;/code>&lt;/pre>
&lt;p>For more details and examples, please visit the project homepage of &lt;a class="link" href="https://github.com/casper-hansen/AutoAWQ" target="_blank" rel="noopener" >AutoAWQ
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;h3 id="neural-compressor">
&lt;a href="#neural-compressor" class="header-anchor">#&lt;/a>
Neural Compressor
&lt;/h3>
&lt;a href="https://github.com/intel/neural-compressor" target="_blank" class="card-github fetch-waiting no-styling"
repo="intel/neural-compressor" id="repo-hJifz0efiIgitERG-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-hJifz0efiIgitERG-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">intel&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">neural-compressor&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-hJifz0efiIgitERG-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-hJifz0efiIgitERG-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-hJifz0efiIgitERG-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-hJifz0efiIgitERG-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-hJifz0efiIgitERG-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-hJifz0efiIgitERG-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/intel\/neural-compressor', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-hJifz0efiIgitERG-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-hJifz0efiIgitERG-language').innerText = data.language;
document.getElementById('repo-hJifz0efiIgitERG-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-hJifz0efiIgitERG-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-hJifz0efiIgitERG-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-hJifz0efiIgitERG-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-hJifz0efiIgitERG-license').classList.add = "no-license"
};
document.getElementById('repo-hJifz0efiIgitERG-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for intel\/neural-compressor.")
}).catch(err => {
const c = document.getElementById('repo-hJifz0efiIgitERG-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for intel\/neural-compressor.")
})
&lt;/script>
&lt;p>Neural Compressor is a model compression toolkit developed by Intel, supporting popular model compression techniques on all major deep learning frameworks (TensorFlow, PyTorch, ONNX Runtime, and MXNet).&lt;/p>
&lt;p>&lt;strong>Installation Method:&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install &amp;quot;neural-compressor&amp;gt;=2.3&amp;quot; &amp;quot;transformers&amp;gt;=4.34.0&amp;quot; torch torchvision
&lt;/code>&lt;/pre>
&lt;p>For more detailed information and examples, please visit the project homepage of &lt;a class="link" href="https://github.com/intel/neural-compressor" target="_blank" rel="noopener" >Neural Compressor
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;h2 id="model-deployment">
&lt;a href="#model-deployment" class="header-anchor">#&lt;/a>
Model deployment
&lt;/h2>
&lt;p>Deploying a trained LLM to a production environment is crucial. Here are some commonly used LLM deployment tools:&lt;/p>
&lt;h3 id="vllm">
&lt;a href="#vllm" class="header-anchor">#&lt;/a>
vLLM
&lt;/h3>
&lt;a href="https://github.com/vllm-project/vllm" target="_blank" class="card-github fetch-waiting no-styling"
repo="vllm-project/vllm" id="repo-T25QZdLAifIsd6E6-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-T25QZdLAifIsd6E6-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">vllm-project&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">vllm&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-T25QZdLAifIsd6E6-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-T25QZdLAifIsd6E6-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-T25QZdLAifIsd6E6-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-T25QZdLAifIsd6E6-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-T25QZdLAifIsd6E6-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-T25QZdLAifIsd6E6-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/vllm-project\/vllm', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-T25QZdLAifIsd6E6-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-T25QZdLAifIsd6E6-language').innerText = data.language;
document.getElementById('repo-T25QZdLAifIsd6E6-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-T25QZdLAifIsd6E6-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-T25QZdLAifIsd6E6-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-T25QZdLAifIsd6E6-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-T25QZdLAifIsd6E6-license').classList.add = "no-license"
};
document.getElementById('repo-T25QZdLAifIsd6E6-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for vllm-project\/vllm.")
}).catch(err => {
const c = document.getElementById('repo-T25QZdLAifIsd6E6-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for vllm-project\/vllm.")
})
&lt;/script>
&lt;p>vLLM is a fast and easy-to-use LLM inference service library.&lt;/p>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Fast&lt;/strong>
&lt;ul>
&lt;li>SOTA service throughput&lt;/li>
&lt;li>Efficiently manage attention key-value memory using PagedAttention&lt;/li>
&lt;li>Continuously batch process received requests&lt;/li>
&lt;li>Use CUDA/HIP graphs for acceleration&lt;/li>
&lt;li>Quantization: Supports GPTQ, AWQ, SqueezeLLM, FP8 KV cache&lt;/li>
&lt;li>Optimized CUDA kernel&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Flexible&lt;/strong>
&lt;ul>
&lt;li>Seamless integration with popular Hugging Face models&lt;/li>
&lt;li>Provide high-throughput services using various decoding algorithms (including parallel sampling, beam search, etc.)&lt;/li>
&lt;li>Provide tensor parallel support for distributed inference&lt;/li>
&lt;li>Stream output&lt;/li>
&lt;li>Compatible with OpenAI&amp;rsquo;s application programming interface server&lt;/li>
&lt;li>Supports NVIDIA GPU, AMD GPU, Intel CPU, and GPU&lt;/li>
&lt;li>(Experimental) Support prefix caching&lt;/li>
&lt;li>(Experimental) Support for multiple languages&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Seamless Support&lt;/strong>
&lt;ul>
&lt;li>Transformer-based models, such as Llama&lt;/li>
&lt;li>MoE-based model, such as Mixtral&lt;/li>
&lt;li>Multimodal models, such as LLaVA&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Quick Installation:&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install vllm
&lt;/code>&lt;/pre>
&lt;p>For more detailed information, please refer to the &lt;a class="link" href="https://vllm.readthedocs.io/en/latest/" target="_blank" rel="noopener" >vLLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
official documentation.&lt;/p>
&lt;h3 id="sgl">
&lt;a href="#sgl" class="header-anchor">#&lt;/a>
SGL
&lt;/h3>
&lt;a href="https://github.com/sgl-project/sglang" target="_blank" class="card-github fetch-waiting no-styling"
repo="sgl-project/sglang" id="repo-fKc169WKGDk1gMJl-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-fKc169WKGDk1gMJl-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">sgl-project&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">sglang&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-fKc169WKGDk1gMJl-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-fKc169WKGDk1gMJl-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-fKc169WKGDk1gMJl-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-fKc169WKGDk1gMJl-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-fKc169WKGDk1gMJl-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-fKc169WKGDk1gMJl-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/sgl-project\/sglang', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-fKc169WKGDk1gMJl-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-fKc169WKGDk1gMJl-language').innerText = data.language;
document.getElementById('repo-fKc169WKGDk1gMJl-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-fKc169WKGDk1gMJl-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-fKc169WKGDk1gMJl-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-fKc169WKGDk1gMJl-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-fKc169WKGDk1gMJl-license').classList.add = "no-license"
};
document.getElementById('repo-fKc169WKGDk1gMJl-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for sgl-project\/sglang.")
}).catch(err => {
const c = document.getElementById('repo-fKc169WKGDk1gMJl-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for sgl-project\/sglang.")
})
&lt;/script>
&lt;p>SGLang is a structured generation language designed specifically for large language models (LLMs). By co-designing the front-end language and the runtime system, it makes your interactions with LLMs faster and more controllable.&lt;/p>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Flexible front-end language&lt;/strong>: Easily write LLM applications through chainable generation calls, advanced prompts, control flow, multiple modes, concurrency, and external interaction.&lt;/li>
&lt;li>&lt;strong>High-performance backend runtime&lt;/strong>: Features RadixAttention capability, which can accelerate complex LLM programs by reusing KV cache across multiple calls. It can also function as a standalone inference engine, implementing all common techniques (such as continuous batching and tensor parallelism).&lt;/li>
&lt;/ul>
&lt;p>For more detailed information, please visit the &lt;a class="link" href="https://github.com/sgl-project/" target="_blank" rel="noopener" >SGL
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
project homepage.&lt;/p>
&lt;h3 id="skypilot">
&lt;a href="#skypilot" class="header-anchor">#&lt;/a>
SkyPilot
&lt;/h3>
&lt;a href="https://github.com/skypilot-org/skypilot" target="_blank" class="card-github fetch-waiting no-styling"
repo="skypilot-org/skypilot" id="repo-HUsNN61GiZc0wYDa-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-HUsNN61GiZc0wYDa-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">skypilot-org&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">skypilot&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-HUsNN61GiZc0wYDa-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-HUsNN61GiZc0wYDa-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-HUsNN61GiZc0wYDa-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-HUsNN61GiZc0wYDa-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-HUsNN61GiZc0wYDa-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-HUsNN61GiZc0wYDa-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/skypilot-org\/skypilot', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-HUsNN61GiZc0wYDa-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-HUsNN61GiZc0wYDa-language').innerText = data.language;
document.getElementById('repo-HUsNN61GiZc0wYDa-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-HUsNN61GiZc0wYDa-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-HUsNN61GiZc0wYDa-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-HUsNN61GiZc0wYDa-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-HUsNN61GiZc0wYDa-license').classList.add = "no-license"
};
document.getElementById('repo-HUsNN61GiZc0wYDa-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for skypilot-org\/skypilot.")
}).catch(err => {
const c = document.getElementById('repo-HUsNN61GiZc0wYDa-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for skypilot-org\/skypilot.")
})
&lt;/script>
&lt;p>SkyPilot is a flexible cloud LLM deployment tool launched by UC Berkeley RISELab, supporting multiple cloud platforms and hardware accelerators. It can automatically select the optimal deployment plan and provide cost optimization features.&lt;/p>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Multi-cloud support:&lt;/strong> Supports various cloud platforms such as AWS, GCP, Azure, allowing users to choose the appropriate deployment environment.&lt;/li>
&lt;li>&lt;strong>Easy to expand&lt;/strong>: Queue and run multiple jobs, automatic management&lt;/li>
&lt;li>&lt;strong>Easy Access to Object Storage&lt;/strong>: Easily access object storage (S3, GCS, R2)&lt;/li>
&lt;/ul>
&lt;p>For more detailed information, please visit the &lt;a class="link" href="https://github.com/skypilot-org/skypilot" target="_blank" rel="noopener" >SkyPilot
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
project homepage.&lt;/p>
&lt;h3 id="tensorrt-llm">
&lt;a href="#tensorrt-llm" class="header-anchor">#&lt;/a>
TensorRT-LLM
&lt;/h3>
&lt;a href="https://github.com/NVIDIA/TensorRT-LLM" target="_blank" class="card-github fetch-waiting no-styling"
repo="NVIDIA/TensorRT-LLM" id="repo-Mk8qNDE480gUM3uZ-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-Mk8qNDE480gUM3uZ-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">NVIDIA&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">TensorRT-LLM&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-Mk8qNDE480gUM3uZ-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-Mk8qNDE480gUM3uZ-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-Mk8qNDE480gUM3uZ-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-Mk8qNDE480gUM3uZ-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-Mk8qNDE480gUM3uZ-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-Mk8qNDE480gUM3uZ-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/NVIDIA\/TensorRT-LLM', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-Mk8qNDE480gUM3uZ-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-Mk8qNDE480gUM3uZ-language').innerText = data.language;
document.getElementById('repo-Mk8qNDE480gUM3uZ-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-Mk8qNDE480gUM3uZ-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-Mk8qNDE480gUM3uZ-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-Mk8qNDE480gUM3uZ-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-Mk8qNDE480gUM3uZ-license').classList.add = "no-license"
};
document.getElementById('repo-Mk8qNDE480gUM3uZ-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for NVIDIA\/TensorRT-LLM.")
}).catch(err => {
const c = document.getElementById('repo-Mk8qNDE480gUM3uZ-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for NVIDIA\/TensorRT-LLM.")
})
&lt;/script>
&lt;p>TensorRT-LLM is a high-performance LLM inference engine launched by NVIDIA, capable of fully utilizing GPU accelerated computation and optimized for the Transformer model architecture, significantly improving inference speed.&lt;/p>
&lt;p>TensorRT-LLM provides users with an easy-to-use Python API for defining large language models (LLMs) and building TensorRT engines, which incorporate state-of-the-art optimization techniques for efficient inference execution on NVIDIA® graphics processors. TensorRT-LLM also includes components for creating Python and C++ runtimes that execute these TensorRT engines.&lt;/p>
&lt;p>For more details, please visit the &lt;a class="link" href="https://github.com/NVIDIA/TensorRT-LLM" target="_blank" rel="noopener" >TensorRT-LLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
project homepage.&lt;/p>
&lt;h3 id="openvino">
&lt;a href="#openvino" class="header-anchor">#&lt;/a>
OpenVino
&lt;/h3>
&lt;a href="https://github.com/openvinotoolkit/openvino" target="_blank" class="card-github fetch-waiting no-styling"
repo="openvinotoolkit/openvino" id="repo-44bD0niItiiq9xkg-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-44bD0niItiiq9xkg-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">openvinotoolkit&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">openvino&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-44bD0niItiiq9xkg-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-44bD0niItiiq9xkg-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-44bD0niItiiq9xkg-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-44bD0niItiiq9xkg-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-44bD0niItiiq9xkg-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-44bD0niItiiq9xkg-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/openvinotoolkit\/openvino', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-44bD0niItiiq9xkg-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-44bD0niItiiq9xkg-language').innerText = data.language;
document.getElementById('repo-44bD0niItiiq9xkg-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-44bD0niItiiq9xkg-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-44bD0niItiiq9xkg-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-44bD0niItiiq9xkg-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-44bD0niItiiq9xkg-license').classList.add = "no-license"
};
document.getElementById('repo-44bD0niItiiq9xkg-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for openvinotoolkit\/openvino.")
}).catch(err => {
const c = document.getElementById('repo-44bD0niItiiq9xkg-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for openvinotoolkit\/openvino.")
})
&lt;/script>
&lt;p>OpenVINO™ is an open-source toolkit for optimizing and deploying artificial intelligence inference.&lt;/p>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Inference Optimization&lt;/strong>: Enhance the performance of deep learning in computer vision, automatic speech recognition, generative AI, natural language processing using large and small language models, and many other common tasks.&lt;/li>
&lt;li>&lt;strong>Flexible Model Support&lt;/strong>: Models trained with popular frameworks such as TensorFlow, PyTorch, ONNX, Keras, and PaddlePaddle. Convert and deploy models without the need for the original framework.&lt;/li>
&lt;li>&lt;strong>Broad Platform Compatibility&lt;/strong>: Reduce resource requirements and efficiently deploy across a range of platforms from edge to cloud. OpenVINO™ supports inference on CPUs (x86, ARM), GPUs (integrated and discrete GPUs supporting OpenCL), and AI accelerators (Intel NPU).&lt;/li>
&lt;li>&lt;strong>Community and Ecosystem&lt;/strong>: Join an active community contributing to improving deep learning performance in various fields.&lt;/li>
&lt;/ul>
&lt;p>For more information, please visit the project homepage of &lt;a class="link" href="https://github.com/openvinotoolkit/openvino" target="_blank" rel="noopener" >OpenVino
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;h3 id="tgi">
&lt;a href="#tgi" class="header-anchor">#&lt;/a>
TGI
&lt;/h3>
&lt;a href="https://github.com/huggingface/text-generation-inference" target="_blank" class="card-github fetch-waiting no-styling"
repo="huggingface/text-generation-inference" id="repo-nSvNE65Qx9ruMVNT-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-nSvNE65Qx9ruMVNT-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">huggingface&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">text-generation-inference&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-nSvNE65Qx9ruMVNT-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-nSvNE65Qx9ruMVNT-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-nSvNE65Qx9ruMVNT-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-nSvNE65Qx9ruMVNT-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-nSvNE65Qx9ruMVNT-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-nSvNE65Qx9ruMVNT-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/huggingface\/text-generation-inference', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-nSvNE65Qx9ruMVNT-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-nSvNE65Qx9ruMVNT-language').innerText = data.language;
document.getElementById('repo-nSvNE65Qx9ruMVNT-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-nSvNE65Qx9ruMVNT-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-nSvNE65Qx9ruMVNT-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-nSvNE65Qx9ruMVNT-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-nSvNE65Qx9ruMVNT-license').classList.add = "no-license"
};
document.getElementById('repo-nSvNE65Qx9ruMVNT-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for huggingface\/text-generation-inference.")
}).catch(err => {
const c = document.getElementById('repo-nSvNE65Qx9ruMVNT-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for huggingface\/text-generation-inference.")
})
&lt;/script>
&lt;p>Text Generation Inference (TGI) is a toolkit for deploying and serving large language models (LLMs). TGI enables high-performance text generation for the most popular open-source LLMs, including Llama, Falcon, StarCoder, BLOOM, GPT-NeoX, and others.&lt;/p>
&lt;p>TGI has implemented many features, and detailed information can be found on the project homepage of &lt;a class="link" href="https://github.com/huggingface/text-generation-inference" target="_blank" rel="noopener" >TGI
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;h2 id="local-run">
&lt;a href="#local-run" class="header-anchor">#&lt;/a>
Local run
&lt;/h2>
&lt;p>Thanks to model compression and optimization techniques, we can also run LLM on personal devices:&lt;/p>
&lt;h3 id="mlx">
&lt;a href="#mlx" class="header-anchor">#&lt;/a>
MLX
&lt;/h3>
&lt;a href="https://github.com/ml-explore/mlx" target="_blank" class="card-github fetch-waiting no-styling"
repo="ml-explore/mlx" id="repo-cKaF3fCjkUTWdRI3-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-cKaF3fCjkUTWdRI3-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">ml-explore&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">mlx&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-cKaF3fCjkUTWdRI3-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-cKaF3fCjkUTWdRI3-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-cKaF3fCjkUTWdRI3-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-cKaF3fCjkUTWdRI3-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-cKaF3fCjkUTWdRI3-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-cKaF3fCjkUTWdRI3-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/ml-explore\/mlx', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-cKaF3fCjkUTWdRI3-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-cKaF3fCjkUTWdRI3-language').innerText = data.language;
document.getElementById('repo-cKaF3fCjkUTWdRI3-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-cKaF3fCjkUTWdRI3-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-cKaF3fCjkUTWdRI3-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-cKaF3fCjkUTWdRI3-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-cKaF3fCjkUTWdRI3-license').classList.add = "no-license"
};
document.getElementById('repo-cKaF3fCjkUTWdRI3-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for ml-explore\/mlx.")
}).catch(err => {
const c = document.getElementById('repo-cKaF3fCjkUTWdRI3-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for ml-explore\/mlx.")
})
&lt;/script>
&lt;p>MLX is a framework specifically designed to support running LLM on Apple devices, fully utilizing Metal to accelerate computation, and providing easy-to-use APIs to facilitate developers in integrating LLM into iOS applications.&lt;/p>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Similar Application Programming Interface&lt;/strong>: MLX&amp;rsquo;s Python API is very similar to NumPy. MLX also has fully functional C++, C, and Swift APIs, which are very similar to the Python API. MLX has higher-level packages like &lt;code>mlx.nn&lt;/code> and &lt;code>mlx.optimizers&lt;/code>, whose APIs are very close to PyTorch, simplifying the construction of more complex models.&lt;/li>
&lt;li>&lt;strong>Composable Function Transformations&lt;/strong>: MLX supports composable function transformations for automatic differentiation, automatic vectorization, and computation graph optimization.&lt;/li>
&lt;li>&lt;strong>Lazy Evaluation&lt;/strong>: In MLX, computations only materialize arrays when needed.&lt;/li>
&lt;li>&lt;strong>Dynamic Graph Construction&lt;/strong>: In MLX, the computational graph is dynamically constructed. Changing the shape of function parameters does not slow down the compilation speed, and debugging is simple and intuitive.&lt;/li>
&lt;li>&lt;strong>Multi-device&lt;/strong>: Operations can run on any supported device (currently CPU and GPU).&lt;/li>
&lt;li>&lt;strong>Unified Memory&lt;/strong>: The unified memory model is a significant difference between MLX and other frameworks. Arrays in MLX reside in shared memory. Operations on MLX arrays can be performed on any supported device type without the need to transfer data.&lt;/li>
&lt;/ul>
&lt;p>MLX is designed by machine learning researchers for machine learning researchers. The framework aims to be user-friendly while still efficiently training and deploying models. The design concept of the framework itself is also very simple. Our goal is to enable researchers to easily expand and improve MLX, thus quickly exploring new ideas. For more details, please visit the project homepage of &lt;a class="link" href="https://github.com/ml-explore/mlx" target="_blank" rel="noopener" >MLX
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;h3 id="llamacpp">
&lt;a href="#llamacpp" class="header-anchor">#&lt;/a>
Llama.cpp
&lt;/h3>
&lt;p>Llama.cpp is a Llama model inference engine implemented in C++, capable of running efficiently on CPUs, and supports multiple operating systems and hardware platforms, allowing developers to run LLM on resource-constrained devices.&lt;/p>
&lt;a href="https://github.com/ggerganov/llama.cpp" target="_blank" class="card-github fetch-waiting no-styling"
repo="ggerganov/llama.cpp" id="repo-uUnVgkedX7rILYus-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-uUnVgkedX7rILYus-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">ggerganov&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">llama.cpp&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-uUnVgkedX7rILYus-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-uUnVgkedX7rILYus-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-uUnVgkedX7rILYus-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-uUnVgkedX7rILYus-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-uUnVgkedX7rILYus-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-uUnVgkedX7rILYus-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/ggerganov\/llama.cpp', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-uUnVgkedX7rILYus-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-uUnVgkedX7rILYus-language').innerText = data.language;
document.getElementById('repo-uUnVgkedX7rILYus-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-uUnVgkedX7rILYus-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-uUnVgkedX7rILYus-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-uUnVgkedX7rILYus-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-uUnVgkedX7rILYus-license').classList.add = "no-license"
};
document.getElementById('repo-uUnVgkedX7rILYus-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for ggerganov\/llama.cpp.")
}).catch(err => {
const c = document.getElementById('repo-uUnVgkedX7rILYus-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for ggerganov\/llama.cpp.")
})
&lt;/script>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>CPU Inference:&lt;/strong> Optimized for CPU platforms, allowing LLM to run on devices without a GPU.&lt;/li>
&lt;li>&lt;strong>Cross-platform support:&lt;/strong> Supports multiple operating systems such as Linux, macOS, Windows, making it convenient for users to use on different platforms.&lt;/li>
&lt;li>&lt;strong>Lightweight Deployment:&lt;/strong> The compiled binary files are small, making it convenient for users to deploy and use.&lt;/li>
&lt;/ul>
&lt;p>For more detailed information, please visit the project homepage of &lt;a class="link" href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener" >Llama.cpp
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;h3 id="ollama">
&lt;a href="#ollama" class="header-anchor">#&lt;/a>
Ollama
&lt;/h3>
&lt;a href="https://github.com/ollama/ollama" target="_blank" class="card-github fetch-waiting no-styling"
repo="ollama/ollama" id="repo-v7DkUBW1iQjguKwD-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-v7DkUBW1iQjguKwD-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">ollama&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">ollama&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-v7DkUBW1iQjguKwD-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-v7DkUBW1iQjguKwD-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-v7DkUBW1iQjguKwD-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-v7DkUBW1iQjguKwD-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-v7DkUBW1iQjguKwD-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-v7DkUBW1iQjguKwD-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/ollama\/ollama', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-v7DkUBW1iQjguKwD-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-v7DkUBW1iQjguKwD-language').innerText = data.language;
document.getElementById('repo-v7DkUBW1iQjguKwD-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-v7DkUBW1iQjguKwD-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-v7DkUBW1iQjguKwD-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-v7DkUBW1iQjguKwD-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-v7DkUBW1iQjguKwD-license').classList.add = "no-license"
};
document.getElementById('repo-v7DkUBW1iQjguKwD-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for ollama\/ollama.")
}).catch(err => {
const c = document.getElementById('repo-v7DkUBW1iQjguKwD-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for ollama\/ollama.")
})
&lt;/script>
&lt;p>In the article &lt;a class="link" href="https://cuterwrite.top/en/p/ollama/" >【Ollama: From Beginner to Advanced】
&lt;/a>
, it is introduced that Ollama is a tool for building large language model applications. It provides a simple and easy-to-use command line interface and server, allowing you to easily download, run, and manage various open-source LLMs. Unlike traditional LLMs that require complex configuration and powerful hardware, Ollama allows you to conveniently experience the powerful features of LLMs as easily as using a mobile app.&lt;/p>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Simple and Easy to Use&lt;/strong>: Ollama provides a simple and easy-to-use command line tool for users to download, run, and manage LLM.&lt;/li>
&lt;li>&lt;strong>Multiple models&lt;/strong>: Ollama supports various open-source LLMs, including Qwen2, Llama3, Mistral, etc.&lt;/li>
&lt;li>&lt;strong>Compatible with OpenAI Interface&lt;/strong>: Ollama supports the OpenAI API interface, making it easy to switch existing applications to Ollama.&lt;/li>
&lt;/ul>
&lt;p>For more details, please visit the project homepage of &lt;a class="link" href="https://github.com/ollama/ollama" target="_blank" rel="noopener" >Ollama
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;h2 id="agent-and-rag-framework">
&lt;a href="#agent-and-rag-framework" class="header-anchor">#&lt;/a>
Agent and RAG framework
&lt;/h2>
&lt;p>Combining LLM with external data and tools can build more powerful applications. Here are some commonly used Agent and RAG frameworks:&lt;/p>
&lt;h3 id="llamaindex">
&lt;a href="#llamaindex" class="header-anchor">#&lt;/a>
LlamaIndex
&lt;/h3>
&lt;a href="https://github.com/run-llama/llama_index" target="_blank" class="card-github fetch-waiting no-styling"
repo="run-llama/llama_index" id="repo-TpinpT2G8XRuE492-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-TpinpT2G8XRuE492-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">run-llama&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">llama_index&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-TpinpT2G8XRuE492-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-TpinpT2G8XRuE492-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-TpinpT2G8XRuE492-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-TpinpT2G8XRuE492-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-TpinpT2G8XRuE492-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-TpinpT2G8XRuE492-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/run-llama\/llama_index', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-TpinpT2G8XRuE492-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-TpinpT2G8XRuE492-language').innerText = data.language;
document.getElementById('repo-TpinpT2G8XRuE492-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-TpinpT2G8XRuE492-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-TpinpT2G8XRuE492-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-TpinpT2G8XRuE492-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-TpinpT2G8XRuE492-license').classList.add = "no-license"
};
document.getElementById('repo-TpinpT2G8XRuE492-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for run-llama\/llama_index.")
}).catch(err => {
const c = document.getElementById('repo-TpinpT2G8XRuE492-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for run-llama\/llama_index.")
})
&lt;/script>
&lt;p>LlamaIndex (GPT Index) is a data framework for LLM applications. Building applications with LlamaIndex typically requires using LlamaIndex core and a selected set of integrations (or plugins). There are two ways to build applications with LlamaIndex in Python:&lt;/p>
&lt;ul>
&lt;li>Launcher: &lt;code>llama-index&lt;/code> ( &lt;a class="link" href="https://pypi.org/project/llama-index/%29" target="_blank" rel="noopener" >https://pypi.org/project/llama-index/)
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
. Python starter package, includes core LlamaIndex and some integrations.&lt;/li>
&lt;li>Customization: &lt;code>llama-index-core&lt;/code> ( &lt;a class="link" href="https://pypi.org/project/llama-index-core/%29" target="_blank" rel="noopener" >https://pypi.org/project/llama-index-core/)
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
. Install the core LlamaIndex and add the necessary LlamaIndex integration packages for your application on LlamaHub. There are currently over 300 LlamaIndex integration packages that can seamlessly collaborate with the core, allowing you to build using your preferred LLM, embeddings, and vector storage databases.&lt;/li>
&lt;/ul>
&lt;p>The LlamaIndex Python library is named as such, so import statements containing &lt;code>core&lt;/code> mean that the core package is being used. Conversely, those statements without &lt;code>core&lt;/code> mean that the integration package is being used.&lt;/p>
&lt;pre>&lt;code class="language-python"># typical pattern
from llama_index.core.xxx import ClassABC # core submodule xxx
from llama_index.xxx.yyy import (
SubclassABC,
) # integration yyy for submodule xxx
# concrete example
from llama_index.core.llms import LLM
from llama_index.llms.openai import OpenAI
&lt;/code>&lt;/pre>
&lt;h3 id="crewai">
&lt;a href="#crewai" class="header-anchor">#&lt;/a>
CrewAI
&lt;/h3>
&lt;a href="https://github.com/joaomdmoura/crewAI" target="_blank" class="card-github fetch-waiting no-styling"
repo="joaomdmoura/crewAI" id="repo-8XMjSIWXxMRSoTP0-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-8XMjSIWXxMRSoTP0-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">joaomdmoura&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">crewAI&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-8XMjSIWXxMRSoTP0-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-8XMjSIWXxMRSoTP0-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-8XMjSIWXxMRSoTP0-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-8XMjSIWXxMRSoTP0-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-8XMjSIWXxMRSoTP0-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-8XMjSIWXxMRSoTP0-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/joaomdmoura\/crewAI', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-8XMjSIWXxMRSoTP0-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-8XMjSIWXxMRSoTP0-language').innerText = data.language;
document.getElementById('repo-8XMjSIWXxMRSoTP0-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-8XMjSIWXxMRSoTP0-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-8XMjSIWXxMRSoTP0-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-8XMjSIWXxMRSoTP0-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-8XMjSIWXxMRSoTP0-license').classList.add = "no-license"
};
document.getElementById('repo-8XMjSIWXxMRSoTP0-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for joaomdmoura\/crewAI.")
}).catch(err => {
const c = document.getElementById('repo-8XMjSIWXxMRSoTP0-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for joaomdmoura\/crewAI.")
})
&lt;/script>
&lt;p>CrewAI is a framework for building AI Agents that can integrate LLM with other tools and APIs to accomplish more complex tasks, such as automating web operations, generating code, and more.&lt;/p>
&lt;p>&lt;strong>Main Features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Role-Based Agent Design&lt;/strong>: You can customize agents using specific roles, goals, and tools.&lt;/li>
&lt;li>&lt;strong>Delegation between Autonomous Agents&lt;/strong>: Agents can autonomously delegate tasks to other agents and query information from each other, thereby improving problem-solving efficiency.&lt;/li>
&lt;li>&lt;strong>Flexible task management&lt;/strong>: Customizable tools can be used to define tasks and dynamically assign tasks to agents.&lt;/li>
&lt;li>&lt;strong>Process-Driven&lt;/strong>: The system is process-centered, currently supporting sequential task execution and hierarchical processes. In the future, it will also support more complex processes, such as negotiation and autonomous processes.&lt;/li>
&lt;li>&lt;strong>Save output as file&lt;/strong>: Allows saving the output of a single task as a file for later use.&lt;/li>
&lt;li>&lt;strong>Parse output to Pydantic or Json&lt;/strong>: It is possible to parse the output of a single task into a Pydantic model or Json format for easy subsequent processing and analysis.&lt;/li>
&lt;li>&lt;strong>Support for Open Source Models&lt;/strong>: You can use OpenAI or other open source models to run your agent team. For more information on configuring agent and model connections, including how to connect to a locally running model, see &lt;a class="link" href="https://docs.crewai.com/how-to/LLM-Connections/" target="_blank" rel="noopener" >Connecting crewAI to Large Language Models
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/li>
&lt;/ul>
&lt;p>For more detailed information, please visit the project homepage of &lt;a class="link" href="https://github.com/joaomdmoura/crewAI" target="_blank" rel="noopener" >CrewAI
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;h3 id="opendevin">
&lt;a href="#opendevin" class="header-anchor">#&lt;/a>
OpenDevin
&lt;/h3>
&lt;a href="https://github.com/opendevin/opendevin" target="_blank" class="card-github fetch-waiting no-styling"
repo="opendevin/opendevin" id="repo-GbZhOUFcnsgJevz9-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-GbZhOUFcnsgJevz9-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">opendevin&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">opendevin&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-GbZhOUFcnsgJevz9-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-GbZhOUFcnsgJevz9-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-GbZhOUFcnsgJevz9-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-GbZhOUFcnsgJevz9-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-GbZhOUFcnsgJevz9-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-GbZhOUFcnsgJevz9-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/opendevin\/opendevin', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-GbZhOUFcnsgJevz9-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-GbZhOUFcnsgJevz9-language').innerText = data.language;
document.getElementById('repo-GbZhOUFcnsgJevz9-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-GbZhOUFcnsgJevz9-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-GbZhOUFcnsgJevz9-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-GbZhOUFcnsgJevz9-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-GbZhOUFcnsgJevz9-license').classList.add = "no-license"
};
document.getElementById('repo-GbZhOUFcnsgJevz9-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for opendevin\/opendevin.")
}).catch(err => {
const c = document.getElementById('repo-GbZhOUFcnsgJevz9-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for opendevin\/opendevin.")
})
&lt;/script>
&lt;p>OpenDevin is an autonomous software engineer platform powered by artificial intelligence and LLMs.&lt;/p>
&lt;p>OpenDevin agents collaborate with human developers to write code, fix bugs, and release features.&lt;/p>
&lt;p>For more information, please visit the project homepage of &lt;a class="link" href="https://github.com/opendevin/opendevin" target="_blank" rel="noopener" >OpenDevin
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;h2 id="model-evaluation">
&lt;a href="#model-evaluation" class="header-anchor">#&lt;/a>
Model evaluation
&lt;/h2>
&lt;p>In order to select a suitable LLM and evaluate its performance, we need to conduct model evaluation:&lt;/p>
&lt;h3 id="lmsys">
&lt;a href="#lmsys" class="header-anchor">#&lt;/a>
LMSys
&lt;/h3>
&lt;p>LMSys Org is an open research organization founded by students and faculty from the University of California, Berkeley, in collaboration with the University of California, San Diego, and Carnegie Mellon University.&lt;/p>
&lt;p>The goal is to make large models accessible to everyone by jointly developing open models, datasets, systems, and evaluation tools. Train large language models and make their applications widely available, while also developing distributed systems to accelerate their training and inference process.&lt;/p>
&lt;p>Currently, the LMSys Chatbot Area is one of the most recognized large model rankings, acknowledged by many companies and research institutions.&lt;/p>
&lt;p>Leaderboard address: &lt;a class="link" href="https://arena.lmsys.org/" target="_blank" rel="noopener" >https://arena.lmsys.org/
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h3 id="opencompass">
&lt;a href="#opencompass" class="header-anchor">#&lt;/a>
OpenCompass
&lt;/h3>
&lt;p>OpenCompass is an LLM evaluation platform that supports various models (Llama3, Mistral, InternLM2, GPT-4, LLaMa2, Qwen, GLM, Claude, etc.) on over 100 datasets.&lt;/p>
&lt;a href="https://github.com/open-compass/opencompass" target="_blank" class="card-github fetch-waiting no-styling"
repo="open-compass/opencompass" id="repo-92eJCAb4a3Mdxdbv-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-92eJCAb4a3Mdxdbv-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">open-compass&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">opencompass&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-92eJCAb4a3Mdxdbv-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-92eJCAb4a3Mdxdbv-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-92eJCAb4a3Mdxdbv-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-92eJCAb4a3Mdxdbv-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-92eJCAb4a3Mdxdbv-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-92eJCAb4a3Mdxdbv-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/open-compass\/opencompass', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-92eJCAb4a3Mdxdbv-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-92eJCAb4a3Mdxdbv-language').innerText = data.language;
document.getElementById('repo-92eJCAb4a3Mdxdbv-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-92eJCAb4a3Mdxdbv-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-92eJCAb4a3Mdxdbv-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-92eJCAb4a3Mdxdbv-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-92eJCAb4a3Mdxdbv-license').classList.add = "no-license"
};
document.getElementById('repo-92eJCAb4a3Mdxdbv-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for open-compass\/opencompass.")
}).catch(err => {
const c = document.getElementById('repo-92eJCAb4a3Mdxdbv-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for open-compass\/opencompass.")
})
&lt;/script>
&lt;h3 id="open-llm-leaderboard">
&lt;a href="#open-llm-leaderboard" class="header-anchor">#&lt;/a>
Open LLM Leaderboard
&lt;/h3>
&lt;p>Open LLM Leaderboard is a continuously updated LLM ranking list that ranks different models based on multiple evaluation metrics, making it convenient for developers to understand the latest model performance and development trends.&lt;/p>
&lt;p>Leaderboard address: &lt;a class="link" href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard" target="_blank" rel="noopener" >https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h2 id="summary">
&lt;a href="#summary" class="header-anchor">#&lt;/a>
Summary
&lt;/h2>
&lt;p>The LLM ecosystem is thriving, covering all aspects from model training to application implementation. With continuous technological advancements, it is believed that LLM will play a more important role in more fields, bringing us a more intelligent application experience.&lt;/p></description></item><item><title>RDMA: Memory Window</title><link>https://cuterwrite.top/en/p/rdma-memory-window/</link><pubDate>Wed, 26 Jun 2024 23:55:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/rdma-memory-window/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116373724_p0_master1200.webp" alt="Featured image of post RDMA: Memory Window" />&lt;h1 id="memory-window-of-rdma">
&lt;a href="#memory-window-of-rdma" class="header-anchor">#&lt;/a>
Memory Window of RDMA
&lt;/h1>
&lt;p>&lt;strong>This article welcomes non-commercial reprints, please indicate the source when reprinting.&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>Statement: For collection only, for easier reading.&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/353590347">&lt;cite>Zhihu Column: 14. RDMA Memory Window&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>&lt;strong>This article is the 14th in the &amp;ldquo;RDMA Talk&amp;rdquo; column. Welcome to repost, please indicate the source when reposting.&lt;/strong>&lt;/p>
&lt;p>In the article &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-mr/" >【RDMA Memory Region】
&lt;/a>
, we introduced Memory Region, which is a special memory area registered by the user: on one hand, its contents will not be swapped to the hard disk, and on the other hand, the RDMA network card records its address translation relationship, allowing the hardware to find the corresponding physical address after obtaining the virtual address specified by the user in the WR.&lt;/p>
&lt;p>In this article, we will explain the concept of Memory Window, which is a more flexible memory management unit based on Memory Region. Besides the concept of MW, this article will also provide a more detailed introduction to some memory-related concepts in the RDMA field, such as L_Key/R_Key, etc. It is recommended to read this article in conjunction with &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-mr/" >【RDMA Memory Region】
&lt;/a>
for better understanding, and it is suggested that readers review it first.&lt;/p>
&lt;h2 id="what-is-memory-window">
&lt;a href="#what-is-memory-window" class="header-anchor">#&lt;/a>
What is Memory Window
&lt;/h2>
&lt;p>Memory Window, abbreviated as MW, can be translated into Chinese as 内存窗口. It is an RDMA resource requested by the user to allow a remote node to access the local memory area. Each MW is bound (referred to as bind) to an already registered MR, but compared to MR, it can provide more flexible permission control. MW can be roughly understood as a subset of MR, and many MWs can be divided from one MR, each MW can set its own permissions. The relationship between MW and MR is shown in the following diagram:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_1.webp"
alt="2024-06-28_12_1" width="30%" loading="lazy">&lt;figcaption>
&lt;h4>The relationship between MR and MW&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h2 id="memory-access-permission-control">
&lt;a href="#memory-access-permission-control" class="header-anchor">#&lt;/a>
Memory access permission control
&lt;/h2>
&lt;p>To explain why MW is designed, let&amp;rsquo;s first discuss the access control involved in both MR and MW.&lt;/p>
&lt;h3 id="mrmw-permissions-configuration">
&lt;a href="#mrmw-permissions-configuration" class="header-anchor">#&lt;/a>
MR/MW permissions configuration
&lt;/h3>
&lt;p>The permissions here refer to the local/remote node, for the read/write permissions of the local memory, they form four combinations:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">&lt;/th>
&lt;th style="text-align: left">Local End&lt;/th>
&lt;th style="text-align: left">Remote End&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">Read&lt;/td>
&lt;td style="text-align: left">Local Read&lt;/td>
&lt;td style="text-align: left">Remote Read&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Write&lt;/td>
&lt;td style="text-align: left">Local Write&lt;/td>
&lt;td style="text-align: left">Remote Write&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Apart from these four types of permissions, there are also Atomic permissions, etc., which are not within the scope of this article.&lt;/p>
&lt;p>Among the four types of permissions in the table, the lowest is Local Read, which is a permission that users must grant to MR/MW because if a piece of memory is inaccessible to local users, it loses its meaning. Additionally, there is a restriction: if an MR needs to be configured with Remote Write or the not-yet-introduced Remote Atomic permissions, it must also be configured with Local Write permissions. Under this constraint, each MR or MW can configure permissions as needed. For example, if an MR we registered needs to allow remote nodes to write data but not read, we enable the Remote Write permission and disable the Remote Read permission. In this way, when the HCA (network card) receives a WRITE request initiated by the peer for a certain address within the range of this MR, it can allow it; however, when the HCA receives a READ operation from the peer on this MR, it will reject the request and return an error message to the peer.&lt;/p>
&lt;h3 id="memory-key">
&lt;a href="#memory-key" class="header-anchor">#&lt;/a>
Memory Key
&lt;/h3>
&lt;p>The above access permission configuration cannot prevent malicious users from accessing local or remote memory. For example, if a node grants Remote Write permission to a memory region, wouldn&amp;rsquo;t any remote node (process) be able to write to this region as long as it provides the correct address information? Therefore, the IB specification designed the Memory Key, which can be simply understood as a key mechanism for accessing MR. Only with the correct key can one open the door to MR/MW.&lt;/p>
&lt;p>Key is a string of numbers, consisting of two parts: a 24-bit Index and an 8-bit Key:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_2.webp"
alt="2024-06-28_12_2" width="60%" loading="lazy">&lt;figcaption>
&lt;h4>Composition of L_Key/R_Key&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>Among them, Index is used by HCA for quick indexing to local virtual-to-physical address translation tables and other MR-related information, while Key is used to verify the legality of the entire field to prevent unauthorized users from arbitrarily passing the Index.&lt;/p>
&lt;p>Memory Key is divided into two types according to their usage, Local Key and Remote Key:&lt;/p>
&lt;h4 id="l_key">
&lt;a href="#l_key" class="header-anchor">#&lt;/a>
L_Key
&lt;/h4>
&lt;p>Local Key, associated with an MR, is used for HCA to access local memory. When a process on the local side attempts to use memory of an already registered MR, the HCA will verify the L_Key it passes. It uses the index in the L_Key to look up the address translation table, translates the virtual address into a physical address, and then accesses the memory.&lt;/p>
&lt;p>In the article &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-shared-receive-queue/" >【RDMA Shared Receive Queue】
&lt;/a>
, we described sge, which consists of a starting address, length, and key. When users fill out a WR, if they need the HCA to access the local memory, they need to describe the memory block through a linked list of sge (sgl). Here, the key in the sge is filled with L_Key, which are key1 and key3 in the diagram below, representing the L_Key of MR1 and MR2, respectively. Without L_Key, any local user process could direct the hardware to access the contents of other locally registered MRs, and the hardware would find it difficult to efficiently translate virtual addresses to physical addresses.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_3.webp"
alt="2024-06-28_12_3" width="50%" loading="lazy">&lt;figcaption>
&lt;h4>The function of L_Key&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h4 id="r_key">
&lt;a href="#r_key" class="header-anchor">#&lt;/a>
R_Key
&lt;/h4>
&lt;p>Remote Key, associated with an MR or MW, is used for a remote node to access local memory. When a remote node attempts to access local memory, on one hand, the local HCA will verify whether the R_Key is valid, and on the other hand, it will use the index in the R_Key to check the address translation table, translating the virtual address into a physical address and then accessing the memory.&lt;/p>
&lt;p>For any RDMA operation (i.e., Write/Read/Atomic), the user must carry the remote memory region&amp;rsquo;s R_Key in the WR.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_4.webp"
alt="2024-06-28_12_4" width="70%" loading="lazy">&lt;figcaption>
&lt;h4>The Function of R_Key&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>The IB specification ensures that MR can be accessed correctly and safely according to the user&amp;rsquo;s expectations through the two mechanisms mentioned above. We use a metaphor to summarize the content related to MR/MW permission control:&lt;/p>
&lt;p>A equipped their room (MR) with two keys (Memory Key), one for personal use (L_Key), and the other key (R_Key) was sent to B (can be via any communication method). B can open the door when A is not home (the local CPU does not perceive the remote node&amp;rsquo;s RDMA operations on local memory) using the key (R_Key). After opening the door, B might only be able to view the room&amp;rsquo;s arrangement through glass (A only granted remote read permission for this MR), or enter the room and find it completely dark, unable to see anything, but can place items in the room (A only granted remote write permission for this MR), and of course, it is also possible that there&amp;rsquo;s no glass and the lights are on (remote read and write permissions were granted simultaneously).&lt;/p>
&lt;h2 id="why-have-mw">
&lt;a href="#why-have-mw" class="header-anchor">#&lt;/a>
Why have MW
&lt;/h2>
&lt;p>In short, the purpose of designing MW is to control remote memory access permissions more flexibly.&lt;/p>
&lt;p>In the article &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-mr/" >【RDMA 之 Memory Region】
&lt;/a>
, we introduced the process of user registering MR, which requires transitioning from user mode to kernel mode, calling the function provided by the kernel to pin the memory (to prevent paging), and then creating a virtual-physical address mapping table and issuing it to the hardware.&lt;/p>
&lt;p>Because MR is managed by the kernel, if a user wants to modify the information of an existing MR, for example, if I want to revoke the remote write permission of a certain MR, leaving only the remote read permission; or if I want to invalidate an R_Key that was previously authorized to a remote node, the user needs to use the Reregister MR interface to make modifications. This interface is equivalent to first Deregister MR and then Register MR. &lt;strong>The above process requires transitioning to kernel mode to complete, and this process is time-consuming.&lt;/strong>&lt;/p>
&lt;p>Unlike MR, which requires permission modification through the control path, &lt;strong>MW can be dynamically bound to an already registered MR through the data path (i.e., directly issuing WR to the hardware from user space) after creation, and simultaneously set or change its access permissions. This process is much faster than re-registering MR&lt;/strong>.&lt;/p>
&lt;p>In order for a piece of memory to be capable of RDMA WRITE/READ operations by a remote node, we have two methods: registering an MR and registering an MW and then binding it to an already registered MR. Both will generate an R_Key to provide to the remote node. The first method has simpler preparation steps but is less flexible, and once registered, modifications are relatively troublesome. The second method involves additional steps of registering an MW and binding the MW to an MR compared to the first method, but it allows for convenient and quick control over remote access permissions.&lt;/p>
&lt;h2 id="the-relationship-between-mw-and-mr-permissions">
&lt;a href="#the-relationship-between-mw-and-mr-permissions" class="header-anchor">#&lt;/a>
The relationship between MW and MR permissions
&lt;/h2>
&lt;p>Perhaps some readers might think, when configuring their permissions during MR application, and when MW is bound to MR, their permissions are also configured, what is the relationship between these two permissions? The IB specification has a dedicated section on this in 10.6.7.2.2:&lt;/p>
&lt;blockquote>
&lt;p>When binding a Memory Window, a Consumer can request any combination of remote access rights for the Window. However, if the associated Region does not have local write access enabled and the Consumer requests remote write or remote atomic access for the Window, the Channel Interface must return an error either at bind time or access time.&lt;/p>
&lt;/blockquote>
&lt;p>In summary, &lt;strong>if you want to configure remote write or remote atomic operation (Atomic) permissions for MW, then the MR it is bound to must have local write permissions. In other cases, the permissions of the two do not interfere with each other&lt;/strong>: remote users using MW must follow the permission configuration of MW; remote users using MR must follow the permission configuration of MR.&lt;/p>
&lt;h2 id="user-interface">
&lt;a href="#user-interface" class="header-anchor">#&lt;/a>
User Interface
&lt;/h2>
&lt;p>As usual, when it comes to user interfaces, we classify them according to control paths and data paths:&lt;/p>
&lt;h3 id="control-path">
&lt;a href="#control-path" class="header-anchor">#&lt;/a>
Control path
&lt;/h3>
&lt;p>MW supports addition, deletion, and search, but cannot be directly modified:&lt;/p>
&lt;h4 id="create---allocate-mw">
&lt;a href="#create---allocate-mw" class="header-anchor">#&lt;/a>
Create - Allocate MW
&lt;/h4>
&lt;p>Apply for MW, mainly to create the software structure related to MW and prepare the hardware. The user needs to specify the type of MW introduced later in the text. This interface will generate a handle for the Memory Window, which the user can use to refer to this MW in the future.&lt;/p>
&lt;p>Note that at this time MW is not bound to MR and is in a state that cannot be accessed remotely.&lt;/p>
&lt;h4 id="delete---deallocate-mw">
&lt;a href="#delete---deallocate-mw" class="header-anchor">#&lt;/a>
Delete - Deallocate MW
&lt;/h4>
&lt;p>Unregister MW. It&amp;rsquo;s easy to understand, just destroy the related resources.&lt;/p>
&lt;h4 id="query---query-mw">
&lt;a href="#query---query-mw" class="header-anchor">#&lt;/a>
Query - Query MW
&lt;/h4>
&lt;p>Query MW information, including R_Key and its status, MW type, and PD, etc.&lt;/p>
&lt;p>It needs to be emphasized again that although this Verbs is described in the IB specification, the related API has not been implemented in the RDMA software stack. There are quite a few Verbs interfaces in similar situations. The RDMA software stack is based on practicality, and interfaces without user demand are generally not implemented.&lt;/p>
&lt;h3 id="data-path">
&lt;a href="#data-path" class="header-anchor">#&lt;/a>
Data path
&lt;/h3>
&lt;p>MW has a unique set of interfaces in the data path, divided into Bind and Invalidate categories:&lt;/p>
&lt;h4 id="bind">
&lt;a href="#bind" class="header-anchor">#&lt;/a>
Bind
&lt;/h4>
&lt;p>Bind(ing) means &amp;ldquo;binding,&amp;rdquo; which refers to associating an MW with a specified range of an already registered MR and configuring certain read and write permissions. The result of binding will generate an R_key, which the user can pass to a remote node for remote access. Note that an MW can be bound multiple times, and multiple MWs can be bound to a single MR. If an MR still has bound MWs, then this MR cannot be deregistered.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_5.webp"
alt="2024-06-28_12_5" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>Bind&amp;#39;s Software and Hardware Interaction&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>There are two ways to bind: one is to call the Post Send interface to issue a Bind MW WR, and the other is to call the Bind MW interface.&lt;/p>
&lt;ul>
&lt;li>Post Send Bind MW WR&lt;/li>
&lt;/ul>
&lt;p>In the previous text, we discussed that compared to MR, the biggest advantage of MW is the ability to quickly configure permissions from the data path. Post Send Bind MW WR operation refers to the user issuing a WR to the SQ through the post send interface (such as ibv_post_send()), where the operation type of this WR (such as SEND/RDMA WRITE/RDMA READ) is specified as BIND MW. Additionally, the WR carries information about the permissions and the range of the MR to be bound. Unlike other WRs, after issuing a Bind MW WR, the hardware does not send any packets but instead binds the MW to the specified MR.&lt;/p>
&lt;p>This method is only applicable to Type 2 MW introduced later.&lt;/p>
&lt;ul>
&lt;li>Bind MW&lt;/li>
&lt;/ul>
&lt;p>Although this is an independent interface, it is actually an additional layer encapsulated outside Post Send Bind MW WR. The user provides the relevant information for MW binding, including permissions and the information of the MR to be bound. The driver is responsible for assembling and issuing the WR to the hardware. After the interface succeeds, the newly generated R_Key will be returned to the user.&lt;/p>
&lt;p>This method is only applicable to Type 1 MW introduced later.&lt;/p>
&lt;p>The relationship between the above two operations is as follows:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_6.webp"
alt="2024-06-28_12_6" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>The relationship between two types of Bind operations&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h4 id="invalidation">
&lt;a href="#invalidation" class="header-anchor">#&lt;/a>
Invalidation
&lt;/h4>
&lt;p>Invalidate means invalidation, referring to the operation where a user sends a WR with an Invalidate opcode to the hardware to invalidate an R_Key.&lt;/p>
&lt;p>&lt;strong>It is important to emphasize that the object of the Invalidate operation is the R_Key, not the MW itself. The effect after Invalidate is that the remote user can no longer use this R_Key to access the corresponding MW, but the MW resource still exists, and new R_Keys can still be generated for remote use in the future.&lt;/strong>&lt;/p>
&lt;p>The Invalidate operation can only be used for Type 2 MW introduced below.&lt;/p>
&lt;p>According to the different initiators of the Invalidate operation, it can be further divided into two types:&lt;/p>
&lt;ul>
&lt;li>Local Invalidate&lt;/li>
&lt;/ul>
&lt;p>Invalid local operation. If a higher-level user wants to revoke the R_Key permissions of a certain remote user without reclaiming MW resources, they can issue a Local Invalidate operation to the SQ. After the hardware receives it, it will modify the configuration of the corresponding MR. After successful execution, if the remote user holding this R_Key attempts to perform RDMA operations on the MW, the local hardware will reject it and return an error.&lt;/p>
&lt;p>Because it is a local operation, the hardware will not send a message to the link after receiving this WR.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_7.webp"
alt="2024-06-28_12_7" width="60%" loading="lazy">&lt;figcaption>
&lt;h4>Software and Hardware Interaction of Local Invalidate Operation&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>Remote Invalidate&lt;/li>
&lt;/ul>
&lt;p>Remote invalid operation. When a remote user no longer uses an R_Key, they can proactively send a message to allow the local end to reclaim this R_Key. The remote user issues a WR with this operation code to the SQ, and once the hardware receives it, it will assemble a message and send it to the local end. After the local hardware receives the remote&amp;rsquo;s Remote Invalidate operation, it will set the corresponding R_Key to an unusable state. Just like Local Invalidate, thereafter the remote end will not be able to use this R_Key to perform RDMA operations on the corresponding MW.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_8.webp"
alt="2024-06-28_12_8" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>Remote Invalidate operation&amp;#39;s software and hardware interaction&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h2 id="type-of-mw">
&lt;a href="#type-of-mw" class="header-anchor">#&lt;/a>
Type of MW
&lt;/h2>
&lt;p>According to different implementations and application scenarios, the IB specification classifies MW:&lt;/p>
&lt;h3 id="type-1">
&lt;a href="#type-1" class="header-anchor">#&lt;/a>
Type 1
&lt;/h3>
&lt;p>Type 1 MW is associated with a PD and a QP, and it is not bound to a QP, so it will not affect the destruction of a QP under the same PD.&lt;/p>
&lt;p>The key field of the R_Key for Type 1 MW is controlled by the driver and hardware. Here, &amp;ldquo;controlled&amp;rdquo; means that the key is allocated by the driver and hardware, not by the upper-level user. This is also the reason mentioned earlier that Type 1 MW cannot perform the Invalidate operation. If a user of Type 1 MW wants to invalidate an R_Key, they can bind this MW again through the Bind MW interface. The hardware or driver will automatically allocate a new key field for the R_Key, and the original R_Key will become invalid.&lt;/p>
&lt;p>In addition, if a user temporarily wants to unbind an MW from any MR but still wants to retain the related resources instead of destroying this MW, they can achieve this by calling the Bind MW interface and setting the MW length to 0.&lt;/p>
&lt;p>The IB specification allows multiple Type 1 MWs to be bound to the same MR, and their ranges can overlap.&lt;/p>
&lt;h3 id="type-2">
&lt;a href="#type-2" class="header-anchor">#&lt;/a>
Type 2
&lt;/h3>
&lt;p>Type 2 MW grants users greater freedom, with the key field segment of the R_Key controlled by the user, allowing them to allocate it as they wish. As mentioned earlier, users perform binding through the Post Send Bind MW WR operation, and this process does not return an R_Key. Users must remember the index from the Allocate MW operation and combine it with their chosen 8-bit key to form the R_Key and send it to the peer.&lt;/p>
&lt;p>The user can invalidate an R_Key through the Invalidate operation introduced earlier. If you want to assign a new R_Key to the MW, you must first invalidate the previous R_Key through the Invalidate operation.&lt;/p>
&lt;p>Unlike Type 1, Type 2&amp;rsquo;s MW does not support 0-length binding.&lt;/p>
&lt;p>The IB specification also allows multiple Type 2s to be bound to the same MR, and the ranges can overlap.&lt;/p>
&lt;p>In addition, based on different binding relationships, Type 2 can be further divided into two implementation methods, with their differences lying solely in the binding relationship with QP.&lt;/p>
&lt;h4 id="type-2a">
&lt;a href="#type-2a" class="header-anchor">#&lt;/a>
Type 2A
&lt;/h4>
&lt;p>Associated with a QP through QPN, meaning that when remote access occurs within this MW range, in addition to the R_Key, the correct QPN must also be specified. If a QP has a bound Type 2A MW, then this QP cannot be destroyed.&lt;/p>
&lt;h4 id="type-2b">
&lt;a href="#type-2b" class="header-anchor">#&lt;/a>
Type 2B
&lt;/h4>
&lt;p>By associating a QP with QPN and PD, there is an additional PD verification compared to Type 2A. When the remote end accesses the memory of the MW through RDMA operations, besides the QPN needing to be correct, the PD specified for the local QP must also be the same as the PD bound to this MW. Additionally, unlike Type 2A, a QP can be destroyed even if there is still a Type 2B MW binding relationship.&lt;/p>
&lt;p>The introduction in the original IB specification is relatively scattered, so let&amp;rsquo;s briefly summarize the similarities and differences of several MWs:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">&lt;/th>
&lt;th style="text-align: left">Type 1&lt;/th>
&lt;th style="text-align: left">Type 2A&lt;/th>
&lt;th style="text-align: left">Type 2B&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">Correlation&lt;/td>
&lt;td style="text-align: left">PD&lt;/td>
&lt;td style="text-align: left">QP&lt;/td>
&lt;td style="text-align: left">PD + QP&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">R_Key&amp;rsquo;s key field ownership&lt;/td>
&lt;td style="text-align: left">Driver + Hardware&lt;/td>
&lt;td style="text-align: left">User&lt;/td>
&lt;td style="text-align: left">User&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Binding Method&lt;/td>
&lt;td style="text-align: left">Bind MW After binding, the previous R_Key automatically becomes invalid&lt;/td>
&lt;td style="text-align: left">Post Send Bind MWWR Before binding, the previous R_Key needs to be invalidated&lt;/td>
&lt;td style="text-align: left">Post Send Bind MWWR Before binding, the previous R_Key needs to be invalidated&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Is zero length supported&lt;/td>
&lt;td style="text-align: left">Yes&lt;/td>
&lt;td style="text-align: left">No&lt;/td>
&lt;td style="text-align: left">No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Supports Invalidate&lt;/td>
&lt;td style="text-align: left">No&lt;/td>
&lt;td style="text-align: left">Yes&lt;/td>
&lt;td style="text-align: left">Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Can the associated QP be destroyed&lt;/td>
&lt;td style="text-align: left">-&lt;/td>
&lt;td style="text-align: left">No&lt;/td>
&lt;td style="text-align: left">Yes&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>In addition, the IB specification also provides the following descriptions for the above types: HCA must implement Type 1 MW, and can optionally choose to implement either Type 2A or 2B. Type 1 and Type 2 MW can be simultaneously associated with the same MR. Since I have not encountered many applications using MW, I cannot clearly explain in which scenarios each type of MW should be used. If readers have insights on this topic, they are welcome to share and discuss.&lt;/p>
&lt;p>Alright, MW will be discussed up to here, and this concludes the introduction of common resources in RDMA technology.&lt;/p>
&lt;p>Given that devices generally supporting RDMA are quite expensive, in the next article I will introduce how to conduct some programming experiments through software-simulated devices—namely Soft-RoCE.&lt;/p>
&lt;h2 id="ib-specification-related-chapters">
&lt;a href="#ib-specification-related-chapters" class="header-anchor">#&lt;/a>
IB specification related chapters
&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>3.5.3 Memory Keys Introduction&lt;/p>
&lt;/li>
&lt;li>
&lt;p>9.4.1.1 Invalidate Operation&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.6.7 Permission Management&lt;/p>
&lt;/li>
&lt;li>
&lt;p>11.2.10.9~12 Related Verbs Introduction&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="reference-document">
&lt;a href="#reference-document" class="header-anchor">#&lt;/a>
Reference document
&lt;/h2>
&lt;p>[1] IB Specification Vol 1-Release-1.4&lt;/p>
&lt;p>[2] Linux Kernel Networking - Implementation and Theory. Chapter 13&lt;/p></description></item><item><title>RDMA: Shared Receive Queue</title><link>https://cuterwrite.top/en/p/rdma-shared-receive-queue/</link><pubDate>Wed, 26 Jun 2024 23:34:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/rdma-shared-receive-queue/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116373922_p0_master1200.webp" alt="Featured image of post RDMA: Shared Receive Queue" />&lt;h1 id="shared-receive-queue-in-rdma">
&lt;a href="#shared-receive-queue-in-rdma" class="header-anchor">#&lt;/a>
Shared Receive Queue in RDMA
&lt;/h1>
&lt;p>&lt;strong>This article welcomes non-commercial reprints, please indicate the source when reprinting.&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>Statement: For collection only, for easy reading.&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/279904125">&lt;cite>Zhihu Column: 11. RDMA Shared Receive Queue&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>We briefly introduced the concept of SRQ in &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-element/" >【3. Basic Elements of RDMA】
&lt;/a>
. This article will take you through more details about SRQ.&lt;/p>
&lt;h2 id="basic-concepts">
&lt;a href="#basic-concepts" class="header-anchor">#&lt;/a>
Basic Concepts
&lt;/h2>
&lt;h3 id="what-is-srq">
&lt;a href="#what-is-srq" class="header-anchor">#&lt;/a>
What is SRQ?
&lt;/h3>
&lt;p>The full name is Shared Receive Queue, literally translated as a shared receive queue. We know that the basic unit of RDMA communication is QP, and each QP consists of a send queue SQ and a receive queue RQ.&lt;/p>
&lt;p>SRQ is designed by the IB protocol to save resources for the receiver. We can share an RQ with all associated QPs, and this shared RQ is called an SRQ. When a QP associated with it wants to post a receive WQE, it is filled into this SRQ. Then, whenever the hardware receives data, it stores the data in the specified location based on the content of the next WQE in the SRQ.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_11_1.webp"
alt="2024-06-28_11_1" width="80%" loading="lazy">
&lt;/figure>
&lt;h3 id="why-use-srq">
&lt;a href="#why-use-srq" class="header-anchor">#&lt;/a>
Why use SRQ
&lt;/h3>
&lt;p>Under normal circumstances, the number of tasks we issue to SQ is far greater than the number of tasks issued to RQ. Why is that? Please first recall which types of operations use SQ and which use RQ.&lt;/p>
&lt;p>SEND/WRITE/READ all require the communication initiator to issue a WR to the SQ, and only the RECV operation paired with SEND requires the communication responder to issue a WR to the RQ (the Write operation with immediate value will also consume Receive WR, which we haven&amp;rsquo;t discussed yet). As we know, the SEND-RECV pair of operations is usually used for transmitting control information, while WRITE and READ are the main operations for performing large amounts of remote memory read and write operations, so naturally, the usage rate of SQ is much higher than that of RQ.&lt;/p>
&lt;p>Each queue is an entity, occupying memory and on-chip storage space of the network card. In commercial scenarios, the number of QPs can reach hundreds of thousands or even higher, which places high demands on memory capacity. Memory is bought with hard-earned money, and SRQ is a mechanism designed by the IB protocol to save user memory.&lt;/p>
&lt;p>Let&amp;rsquo;s take a look at the official explanation in the agreement for why SRQ is used (Section 10.2.9.1):&lt;/p>
&lt;blockquote>
&lt;p>Without SRQ, an RC, UC or UD Consumer must post the number of receive WRs necessary to handle incoming receives on a given QP. If the Consumer cannot predict the incoming rate on a given QP, because, for example, the connection has a bursty nature, the Consumer must either: post a sufficient number of RQ WRs to handle the highest incoming rate for each connection, or, for RC, let message flow control cause the remote sender to back off until local Consumer posts more WRs.&lt;/p>
&lt;p>• Posting sufficient WRs on each QP to hold the possible incoming rate, wastes WQEs, and the associated Data Segments, when the Receive Queue is inactive. Furthermore, the HCA doesn’t provide a way of reclaiming these WQEs for use on other connections.&lt;/p>
&lt;p>• Letting the RC message flow control cause the remote sender to back off can add unnecessary latencies, especially if the local Consumer is unaware that the RQ is starving.&lt;/p>
&lt;/blockquote>
&lt;p>In simple terms, without SRQ, because the receiver of RC/UC/UD does not know how much data the other end will send and when it will arrive, it must prepare for the worst-case scenario, preparing for the possibility of receiving a large amount of data suddenly, which means issuing a sufficient number of receive WQEs to the RQ. Additionally, the RC service type can use flow control mechanisms to exert backpressure on the sender, essentially telling the other end &amp;ldquo;I don&amp;rsquo;t have enough RQ WQEs here,&amp;rdquo; so the sender will temporarily slow down or stop sending data.&lt;/p>
&lt;p>However, as we mentioned earlier, the first method, being prepared for the worst-case scenario, often results in a large number of RQ WQEs being idle and unused, which is a significant waste of memory. Although the second method doesn&amp;rsquo;t require issuing as many RQ WQEs, flow control comes at a cost, which is the increased communication latency.&lt;/p>
&lt;p>And SRQ solves the above problem by allowing many QPs to share receive WQEs (as well as memory space for storing data). When any QP receives a message, the hardware will take a WQE from the SRQ, store the received data according to its content, and then the hardware will return the completion information of the receive task to the corresponding upper-level user through the Completion Queue.&lt;/p>
&lt;p>Let&amp;rsquo;s take a look at how much memory can be saved by using SRQ compared to using a standard RQ&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>:&lt;/p>
&lt;p>Assume that there are N pairs of QP on the node receiving data, and each QP may receive a consecutive M messages at random times (each message consumes a WQE from an RQ),&lt;/p>
&lt;ul>
&lt;li>If SRQ is not used, the user needs to issue N * M RQ WQEs in total.&lt;/li>
&lt;li>If using SRQ, the user only needs to issue K * M RQ WQEs, where K is much smaller than N.&lt;/li>
&lt;/ul>
&lt;p>This K can be configured by the user according to the business needs. If there is a large amount of concurrent reception, then set K to a larger value; otherwise, setting K to a single digit is sufficient to handle general situations.&lt;/p>
&lt;p>We have saved a total of (N - K) * M RQ WQEs, and RQ WQEs themselves are not very large, approximately a few KB in size, which doesn&amp;rsquo;t seem to take up much memory. However, as mentioned earlier, what is actually saved is the &lt;strong>memory space used to store data&lt;/strong>, which is a significant amount of memory. We will use a diagram to illustrate:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_11_2.webp"
alt="2024-06-28_11_2" width="80%" loading="lazy">
&lt;/figure>
&lt;p>In the above diagram, there are two RQ WQEs in the SRQ. Let&amp;rsquo;s take a look at the contents of an RQ WQE, which are composed of several SGEs (Scatter/Gather Elements). Each SGE consists of a memory address, length, and key. With a starting address and length, an SGE can point to a contiguous memory region, and multiple SGEs can represent multiple discrete contiguous memory blocks. We refer to multiple SGEs as an SGL (Scatter/Gather List). SGEs are ubiquitous in the IB software protocol stack (and indeed very common throughout Linux), allowing very large memory regions to be represented with minimal space. IB users use SGEs to specify send and receive areas.&lt;/p>
&lt;p>You can simply estimate the size of the memory region each sge can point to. The length is a 32-bit unsigned integer, which can represent 4GB of space. Assuming an RQ WQE can hold a maximum of 256 sge, then an RQ WQE would be a total of 1TB. Of course, in reality, it cannot be that large, this is just to intuitively inform the reader of the potential memory space an RQ WQE might occupy.&lt;/p>
&lt;h3 id="srqc">
&lt;a href="#srqc" class="header-anchor">#&lt;/a>
SRQC
&lt;/h3>
&lt;p>That is SRQ Context. Like QPC, SRQC is used to inform the hardware about attributes related to SRQ, including depth, WQE size, and other information, which will not be elaborated on in this article.&lt;/p>
&lt;h3 id="srqn">
&lt;a href="#srqn" class="header-anchor">#&lt;/a>
SRQN
&lt;/h3>
&lt;p>That is SRQ Number. Like QP, there may be multiple SRQs in each node. To identify and distinguish these SRQs, each SRQ has a serial number, called SRQN.&lt;/p>
&lt;h3 id="pd-of-srq">
&lt;a href="#pd-of-srq" class="header-anchor">#&lt;/a>
PD of SRQ
&lt;/h3>
&lt;p>In &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-protection-domain/" >【7. RDMA Protection Domain】
&lt;/a>
, we introduced the concept of Protection Domain, which is used to isolate different RDMA resources. Each SRQ must specify its own PD, which can be the same as the PD of its associated QP, or it can be different; SRQs can also use the same PD.&lt;/p>
&lt;p>If a packet is received while using SRQ, it will only be properly received if the MR and SRQ being accessed are under the same PD; otherwise, an immediate error will occur.&lt;/p>
&lt;h2 id="asynchronous-event">
&lt;a href="#asynchronous-event" class="header-anchor">#&lt;/a>
Asynchronous event
&lt;/h2>
&lt;p>In the article &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-completion-queue/" >【10. RDMA Completion Queue】
&lt;/a>
, we introduced that the IB protocol classifies error types into immediate errors, completion errors, and asynchronous errors based on the method of error reporting. Among them, asynchronous errors are similar to interrupts/events, so we sometimes refer to them as asynchronous events. Each HCA registers an event handling function specifically for handling asynchronous events. Upon receiving an asynchronous event, the driver performs the necessary processing and further reports it to the user.&lt;/p>
&lt;p>There is a special asynchronous event regarding SRQ, used to promptly notify upper-level users of the SRQ status, namely the SRQ Limit Reached event.&lt;/p>
&lt;h3 id="srq-limit">
&lt;a href="#srq-limit" class="header-anchor">#&lt;/a>
SRQ Limit
&lt;/h3>
&lt;p>SRQ can set a watermark/threshold, when the number of remaining WQEs in the queue is less than the watermark, this SRQ will report an asynchronous event. It reminds the user &amp;ldquo;The WQEs in the queue are about to run out, please issue more WQEs to prevent having no place to receive new data.&amp;rdquo; This watermark/threshold is called the SRQ Limit, and the reported event is called SRQ Limit Reached.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_11_3.webp"
alt="2024-06-28_11_3" width="30%" loading="lazy">
&lt;/figure>
&lt;p>Because the SRQ is shared by multiple QPs, if the depth is relatively small, it is very likely that the WQE inside will suddenly run out. Therefore, the protocol is designed with this mechanism to ensure that users can promptly intervene in situations where the WQE is insufficient.&lt;/p>
&lt;p>After reporting an asynchronous event, the value of SRQ Limit will be reset to 0 by the hardware (presumably to prevent continuously reporting asynchronous events to the upper layer). Of course, users can choose not to use this mechanism by simply setting the value of SRQ Limit to 0.&lt;/p>
&lt;h2 id="user-interface">
&lt;a href="#user-interface" class="header-anchor">#&lt;/a>
User interface
&lt;/h2>
&lt;h3 id="control-surface">
&lt;a href="#control-surface" class="header-anchor">#&lt;/a>
Control surface
&lt;/h3>
&lt;p>Still the old four types—&amp;ldquo;Add, Delete, Modify, Query&amp;rdquo;:&lt;/p>
&lt;ul>
&lt;li>Create SRQ&lt;/li>
&lt;/ul>
&lt;p>When creating an SRQ, similar to a QP, all software and hardware resources related to the SRQ are allocated. For example, the driver will request an SRQN, allocate space for the SRQC, and fill in the configuration. When creating an SRQ, you must also specify the depth of each SRQ (how many WQEs it can store) and the maximum number of sges per WQE.&lt;/p>
&lt;ul>
&lt;li>Destroy SRQ&lt;/li>
&lt;/ul>
&lt;p>Destroy all related software and hardware resources of SRQ.&lt;/p>
&lt;ul>
&lt;li>Modify SRQ&lt;/li>
&lt;/ul>
&lt;p>In addition to attributes such as SRQ depth, the value of SRQ Limit is also set through this interface. Because the value of the watermark is cleared every time an SRQ Limit Reached event occurs, the user needs to call Modify SRQ to reset the watermark each time.&lt;/p>
&lt;ul>
&lt;li>Query SRQ&lt;/li>
&lt;/ul>
&lt;p>It is usually used to query the configuration of the waterline.&lt;/p>
&lt;h3 id="data-surface">
&lt;a href="#data-surface" class="header-anchor">#&lt;/a>
Data surface
&lt;/h3>
&lt;h3 id="post-srq-receive">
&lt;a href="#post-srq-receive" class="header-anchor">#&lt;/a>
Post SRQ Receive
&lt;/h3>
&lt;p>Just like Post Receive, it issues a receive WQE to the SRQ, which contains information about the memory block used as the receive buffer. It is important to note that the subject is SRQ and has nothing to do with QP. Currently, the user is not concerned with which QP this SRQ is associated with.&lt;/p>
&lt;h2 id="the-difference-between-srq-and-rq">
&lt;a href="#the-difference-between-srq-and-rq" class="header-anchor">#&lt;/a>
The difference between SRQ and RQ
&lt;/h2>
&lt;p>In terms of functionality, both SRQ and RQ are used to store received task requests, but due to the shared nature of SRQ, there are some differences between it and RQ.&lt;/p>
&lt;h3 id="state-machine">
&lt;a href="#state-machine" class="header-anchor">#&lt;/a>
State machine
&lt;/h3>
&lt;p>We introduced in &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-queue-pair/" >【9. RDMA Queue Pair】
&lt;/a>
that QP has a complex state machine, and the sending and receiving capabilities of QP vary in different states. However, SRQ only has two states: non-error and error.&lt;/p>
&lt;p>Regardless of the state, users can issue WQEs to the SRQ. However, in an error state, the associated QP cannot receive data from this SRQ. Additionally, in an error state, users cannot query or modify the attributes of the SRQ.&lt;/p>
&lt;p>When a QP is in an error state, it can be returned to the RESET state through Modify QP, but for SRQ, it can only exit the error state by destroying it.&lt;/p>
&lt;h3 id="receiving-process">
&lt;a href="#receiving-process" class="header-anchor">#&lt;/a>
Receiving process
&lt;/h3>
&lt;p>For a QP, RQ and SRQ cannot be used simultaneously, one must be chosen. If a WQE is issued to the RQ of a QP that is already associated with SRQ, an immediate error will be returned.&lt;/p>
&lt;p>Let&amp;rsquo;s compare the reception processes of SRQ and RQ. The content of this section is a key point of this article, and I believe that after reading it, readers will have a more complete understanding of the SRQ mechanism.&lt;/p>
&lt;h3 id="rqs-receiving-process">
&lt;a href="#rqs-receiving-process" class="header-anchor">#&lt;/a>
RQ&amp;rsquo;s receiving process
&lt;/h3>
&lt;p>First, let&amp;rsquo;s revisit the receiving process of a regular RQ (for the complete process on the sender&amp;rsquo;s side, please read &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-op/" >【4. RDMA Operation Types】
&lt;/a>
):&lt;/p>
&lt;ol start="0">
&lt;li>
&lt;p>Create QP.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Through the Post Recv interface, the user submits receive WQE to the RQ of QP2 and QP3, respectively. The WQE contains information about which memory region to place the received data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The hardware receives the data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Hardware discovery is sent to QP3, then WQE1 is taken from QP3&amp;rsquo;s RQ, and the received data is placed in the memory area specified by WQE1.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>After the hardware completes data storage, it generates a CQE to CQ3 associated with RQ of QP3, reporting task completion information.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The user retrieves WC (CQE) from CQ3, and then takes data from the specified memory area.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The hardware receives the data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The hardware discovery is sent to QP2, then WQE1 is extracted from the RQ of QP2, and the received data is placed in the memory area specified by WQE1.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>After the hardware completes data storage, it generates a CQE for CQ2 associated with RQ of QP2, reporting task completion information.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The user retrieves WC (CQE) from CQ2, and then takes data from the specified memory area.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_11_4.webp"
alt="2024-06-28_11_4" width="90%" loading="lazy">
&lt;/figure>
&lt;h3 id="srqs-reception-process">
&lt;a href="#srqs-reception-process" class="header-anchor">#&lt;/a>
SRQ&amp;rsquo;s reception process
&lt;/h3>
&lt;p>And the SRQ receiving process has some differences:&lt;/p>
&lt;ol start="0">
&lt;li>
&lt;p>Create SRQ1, and create QP2 and QP3, both associated with SRQ1.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Through the Post SRQ Recv interface, the user issues two receive WQEs to SRQ1, containing information about which memory region to place the received data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Hardware receives data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The hardware discovery is sent to QP3, extracting the first WQE from SRQ1 (now it is WQE1), and storing the received data according to the content of the WQE.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>Each WQE in the SRQ is &amp;ldquo;ownerless&amp;rdquo;, not associated with any QP. The hardware sequentially takes out the WQE according to the queue order and places the data inside.&lt;/p>
&lt;/blockquote>
&lt;ol start="4">
&lt;li>
&lt;p>The hardware discovers that the CQ associated with QP3&amp;rsquo;s RQ is CQ3, so it generates a CQE in it.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The user retrieves the CQE from CQ3 and takes data from the specified memory area.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>Attentive readers may ask, when a user issues a WR, each WR specifies some memory regions for storing data in the future. However, an SRQ is a pool where each WQE points to several different memory regions. After the user receives a WC in the CQ corresponding to a certain QP, how do they know where the received data has been stored?&lt;/p>
&lt;p>There is actually wr_id information in the WC, informing the user of which WR (WQE) designated memory area the data is placed in. Since the WR is issued by the user, the user naturally knows its specific location.&lt;/p>
&lt;/blockquote>
&lt;ol start="6">
&lt;li>
&lt;p>Hardware received data&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The hardware discovery is sent to QP2, and the first WQE is taken from SRQ1 (now it is WQE2), and the received data is stored according to the content of the WQE.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The hardware discovers that the CQ associated with QP2&amp;rsquo;s RQ is CQ2, so a CQE is generated in it.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The user takes out the CQE from CQ2 and retrieves data from the specified memory area.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_11_5.webp"
alt="2024-06-28_11_5" width="90%" loading="lazy">
&lt;/figure>
&lt;h2 id="summary">
&lt;a href="#summary" class="header-anchor">#&lt;/a>
Summary
&lt;/h2>
&lt;p>This text first introduces the basic concept of SRQ, followed by its design purpose, related mechanisms, and user interface. Finally, it compares the SRQ receiving process with RQ. In actual business, the usage rate of SRQ is quite high, and it is hoped that readers can gain a deep understanding.&lt;/p>
&lt;p>Let&amp;rsquo;s stop here, thank you for reading. In the next article, I will introduce the Memory Window.&lt;/p>
&lt;h2 id="relevant-sections-of-the-agreement">
&lt;a href="#relevant-sections-of-the-agreement" class="header-anchor">#&lt;/a>
Relevant sections of the agreement
&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>10.2.9 The design concept of SRQ and related operations&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.2.3 PD of SRQ and QP&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.8.2 The relationship between QP associated with SRQ and QP not using SRQ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.8.5 SRQ related returns WC&lt;/p>
&lt;/li>
&lt;li>
&lt;p>11.5.2.4 Asynchronous Events&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="other-references">
&lt;a href="#other-references" class="header-anchor">#&lt;/a>
Other references
&lt;/h2>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>Linux Kernel Networking - Implementation and Theory. Chapter 13. Shared Receive Queue&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>RDMA: Completion Queue</title><link>https://cuterwrite.top/en/p/rdma-completion-queue/</link><pubDate>Wed, 26 Jun 2024 23:11:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/rdma-completion-queue/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116903369_p0_master1200.webp" alt="Featured image of post RDMA: Completion Queue" />&lt;h1 id="rdmas-completion-queue">
&lt;a href="#rdmas-completion-queue" class="header-anchor">#&lt;/a>
RDMA&amp;rsquo;s Completion Queue
&lt;/h1>
&lt;p>&lt;strong>This article welcomes non-commercial reproduction, please indicate the source.&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>Statement: For collection only, for easy reading&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/259650980">&lt;cite>Zhihu Column: 10. RDMA Completion Queue&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>We have briefly introduced CQ in previous articles, and this article will delve deeper into some of its details. Before reading this article, readers can first review this article: &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-element/" >【“3. RDMA Basic Elements”】
&lt;/a>
.&lt;/p>
&lt;h2 id="basic-concepts">
&lt;a href="#basic-concepts" class="header-anchor">#&lt;/a>
Basic Concepts
&lt;/h2>
&lt;p>Let&amp;rsquo;s first review the function of CQ. CQ stands for Completion Queue, and its function is opposite to that of WQ (SQ and RQ). The hardware uses CQE/WC in the CQ to inform the software about the completion status of a certain WQE/WR. A reminder to readers: for upper-layer users, WC is generally used, while for drivers, it is generally referred to as CQE. This article does not distinguish between the two.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_1.webp"
alt="2024-06-27_10_1" width="80%" loading="lazy">
&lt;/figure>
&lt;p>CQE can be regarded as a &amp;ldquo;report&amp;rdquo; that specifies the execution status of a certain task, including:&lt;/p>
&lt;ul>
&lt;li>Which task specified by which WQE of which QP was completed this time (QP Number and WR ID)&lt;/li>
&lt;li>What operation was performed in this task (Opcode operation type)&lt;/li>
&lt;li>This task executed successfully/failed, the reason for failure is XXX (Status and error code)&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>Whenever the hardware completes processing a WQE, a CQE is generated and placed in the CQ queue. If a CQE corresponding to a WQE is not generated, then this WQE will always be considered as not yet processed. What does this mean?&lt;/p>
&lt;ul>
&lt;li>Operations involving fetching data from memory (SEND and WRITE)&lt;/li>
&lt;/ul>
&lt;p>Before generating a CQE, the hardware may not have sent the message yet, may be in the process of sending the message, or the peer may have received the correct message. Since the memory region is allocated before sending, the upper-level software must consider this memory region still in use before receiving the corresponding CQE and cannot release all related memory resources.&lt;/p>
&lt;ul>
&lt;li>Operations involving storing data in memory (RECV and READ)&lt;/li>
&lt;/ul>
&lt;p>Before the CQE is generated, it is possible that the hardware has not started writing data, it is possible that only half of the data has been written, or it is possible that a data verification error has occurred. Therefore, before the upper-layer software receives the CQE, the contents of the memory area used to store the received data are unreliable.&lt;/p>
&lt;p>In summary, the user must obtain the CQE and confirm its content before considering the message sending and receiving task complete.&lt;/p>
&lt;h3 id="when-was-it-generated">
&lt;a href="#when-was-it-generated" class="header-anchor">#&lt;/a>
When was it generated?
&lt;/h3>
&lt;p>We will explain separately according to the service type (this article only discusses RC and UD) and the operation type, because the timing and meaning of generating CQE are different in different situations. Readers are advised to review the 4th article &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-op/" >&amp;ldquo;4. Basic RDMA Operations&amp;rdquo;
&lt;/a>
and the 5th article &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-service-types/" >&amp;ldquo;5. Basic RDMA Service Types&amp;rdquo;
&lt;/a>
.&lt;/p>
&lt;ul>
&lt;li>Reliable Service Type (RC)&lt;/li>
&lt;/ul>
&lt;p>The previous article mentioned that &lt;strong>reliability means that the sender is concerned that the message sent can be accurately received by the receiver&lt;/strong>, which is ensured through mechanisms such as ACK, checksum, and retransmission.&lt;/p>
&lt;ul>
&lt;li>SEND&lt;/li>
&lt;/ul>
&lt;p>SEND operation requires hardware to fetch data from memory, then assemble it into packets to send to the other end through a physical link. For SEND, the Client side generates a CQE indicating &lt;strong>the other end has received the data accurately&lt;/strong>, after the other end&amp;rsquo;s hardware receives and verifies the data, it will reply with an ACK packet to the sender. Only after the sender receives this ACK will a CQE be generated, thus informing the user that the task has been successfully executed. As shown in the figure, the left Client side generates the CQE for this task at the position marked by the red dot.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_2.webp"
alt="2024-06-27_10_2" width="50%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>RECV&lt;/li>
&lt;/ul>
&lt;p>The RECV operation requires the hardware to place the received data into the memory area specified in the user&amp;rsquo;s WQE. After completing the checksum and data storage actions, the hardware will generate a CQE, as shown on the right side of the above figure on the server side.&lt;/p>
&lt;ul>
&lt;li>WRITE&lt;/li>
&lt;/ul>
&lt;p>For the Client side, WRITE operation and SEND operation are the same, the hardware will fetch data from memory and wait for the peer to reply with an ACK before generating a CQE. The difference is that because WRITE is an RDMA operation, the peer CPU is not aware of it, and naturally the user is not aware of it either, so the diagram above becomes like this:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_3.webp"
alt="2024-06-27_10_3" width="50%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>READ&lt;/li>
&lt;/ul>
&lt;p>READ and RECV are somewhat similar. After the Client initiates a READ operation, the other side will reply with the data we want to read. Then, after verifying that there are no issues, the data will be placed in the specified location in the WQE. After completing the above actions, a CQE will be generated on our side. READ is also an RDMA operation, which is not perceived by the other side&amp;rsquo;s user, and naturally, no CQE is generated. In this situation, the diagram becomes like this:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_4.webp"
alt="2024-06-27_10_4" width="50%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>Unreliable Service Type (UD)&lt;/li>
&lt;/ul>
&lt;p>Because unreliable service types lack retransmission and acknowledgment mechanisms, generating a CQE indicates that the hardware &lt;strong>has already sent out the data specified by the corresponding WQE&lt;/strong>. It was previously mentioned that UD only supports SEND-RECV operations and does not support RDMA operations. Therefore, for both ends of the UD service, the timing for CQE generation is as shown in the figure below:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_5.webp"
alt="2024-06-27_10_5" width="50%" loading="lazy">
&lt;/figure>
&lt;h3 id="the-correspondence-between-wq-and-cq">
&lt;a href="#the-correspondence-between-wq-and-cq" class="header-anchor">#&lt;/a>
The correspondence between WQ and CQ
&lt;/h3>
&lt;p>&lt;strong>Each WQ must be associated with a CQ, and each CQ can be associated with multiple SQs and RQs.&lt;/strong>&lt;/p>
&lt;p>The so-called &amp;ldquo;association&amp;rdquo; here refers to the fact that all CQEs corresponding to a WQ&amp;rsquo;s WQEs will be placed by the hardware into the bound CQ. It&amp;rsquo;s important to note that the SQ and RQ belonging to the same QP can each be associated with different CQs. As shown in the diagram below, both the SQ and RQ of QP1 are associated with CQ1, while the RQ of QP2 is associated with CQ1 and the SQ is associated with CQ2.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_6.webp"
alt="2024-06-27_10_6" width="auto" loading="lazy">
&lt;/figure>
&lt;p>Because each WQ must be associated with a CQ, the user needs to create the CQ in advance before creating the QP, and then specify which CQ will be used by the SQ and RQ respectively.&lt;/p>
&lt;p>&lt;strong>The WQEs in the same WQ correspond to CQEs that are ordered&lt;/strong>&lt;/p>
&lt;p>The hardware retrieves WQEs from a certain WQ (SQ or RQ) and processes them in a &amp;ldquo;First In, First Out&amp;rdquo; FIFO order, and when placing CQEs in the CQ associated with WRs, it also follows the order in which these WQEs were placed in the WQ. Simply put, whoever is placed in the queue first is completed first. This process is shown in the diagram below:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_7.webp"
alt="2024-06-27_10_7" width="auto" loading="lazy">
&lt;/figure>
&lt;p>It should be noted that the use of SRQ and the RQ in RD service type are both non-order-preserving, which will not be discussed in this article.&lt;/p>
&lt;p>&lt;strong>The WQEs in different WQs are not ordered with respect to their corresponding CQEs.&lt;/strong>&lt;/p>
&lt;p>In the previous text, we mentioned that a CQ might be shared by multiple WQs. In this case, the order of generation for the CQEs corresponding to these WQEs cannot be guaranteed. As shown in the figure below (the WQE number indicates the order of issuance, i.e., 1 is issued first, and 6 is issued last):&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_8.webp"
alt="2024-06-27_10_8" width="auto" loading="lazy">
&lt;/figure>
&lt;p>The above description actually also includes the situation of &amp;ldquo;WQE in SQ and RQ of the same QP, their corresponding CQE is not ordered.&amp;rdquo; This is actually quite easy to understand. SQ and RQ, one is responsible for actively initiating tasks, and the other for passively receiving tasks. They can be considered as channels in two different directions and naturally should not affect each other. Suppose the user first issues a Receive WQE and then a Send WQE for the same QP. It can&amp;rsquo;t be that if the peer doesn&amp;rsquo;t send a message to the local end, the local end cannot send a message to the peer, right?&lt;/p>
&lt;p>In this case, since the order in which CQEs are generated is not related to the order in which WQEs are obtained, how do the upper-level application and driver know which WQE the received CQE is associated with? It&amp;rsquo;s actually quite simple, &lt;strong>the CQE indicates the number of the WQE it corresponds to&lt;/strong>.&lt;/p>
&lt;p>Additionally, it should be noted that even when multiple WQs share a single CQ, &amp;ldquo;WQEs in the same WQ have their corresponding CQEs ordered&amp;rdquo; is always guaranteed. This means that the CQEs corresponding to WQE 1, 3, and 4 belonging to WQ1 in the above diagram are generated in sequence, and the same applies to WQE 2, 5, and 6 belonging to WQ2.&lt;/p>
&lt;h3 id="cqc">
&lt;a href="#cqc" class="header-anchor">#&lt;/a>
CQC
&lt;/h3>
&lt;p>Just like QP, CQ is merely a queue memory space for storing CQEs. Apart from knowing the starting address, the hardware is essentially unaware of this area. Therefore, it is necessary to agree on a format with the software in advance, and then the driver will allocate memory and fill in the basic information of the CQ in this memory according to the format for the hardware to read. This memory is the CQC. The CQC contains information such as the capacity size of the CQ, the sequence number of the currently processed CQE, and so on. So by slightly modifying the QPC diagram, you can represent the relationship between CQC and CQ:&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_9.webp"
alt="2024-06-27_10_9" width="auto" loading="lazy">
&lt;/figure>
&lt;h3 id="cqn">
&lt;a href="#cqn" class="header-anchor">#&lt;/a>
CQN
&lt;/h3>
&lt;p>CQ Number is the CQ&amp;rsquo;s identifier, used to distinguish different CQs. CQ does not have special reserved numbers like QP0 and QP1, which will not be further elaborated in this article.&lt;/p>
&lt;h2 id="complete-error">
&lt;a href="#complete-error" class="header-anchor">#&lt;/a>
Complete error
&lt;/h2>
&lt;p>There are three types of errors in the IB protocol: immediate error, Completion Error, and Asynchronous Errors.&lt;/p>
&lt;p>Immediate error refers to &amp;ldquo;immediately stop the current operation and return an error to the upper-level user&amp;rdquo;; completion error refers to &amp;ldquo;return the error information to the upper-level user via CQE&amp;rdquo;; whereas asynchronous error refers to &amp;ldquo;report to the upper-level user through an interrupt event.&amp;rdquo; It might still be a bit abstract, so let&amp;rsquo;s give an example to illustrate under what circumstances these two types of errors might occur:&lt;/p>
&lt;ul>
&lt;li>The user passed an illegal opcode when sending a Post Send, for example, trying to use RDMA WRITE operation during UD.&lt;/li>
&lt;/ul>
&lt;p>Result: Immediate error generated (some manufacturers may generate a completion error in this situation)&lt;/p>
&lt;p>Generally, in this situation, the driver will directly exit the post send process and return an error code to the upper-level user. Note that at this point, the WQE has not yet been issued to the hardware before returning.&lt;/p>
&lt;ul>
&lt;li>The user issued a WQE with the operation type SEND, but did not receive an ACK from the other party for a long time.&lt;/li>
&lt;/ul>
&lt;p>Result: Generation completed with error&lt;/p>
&lt;p>Because the WQE has already reached the hardware, the hardware will generate the corresponding CQE, which contains error details of the timeout unresponse.&lt;/p>
&lt;ul>
&lt;li>Multiple WQEs were issued in user mode, so the hardware generated multiple CQEs, but the software did not retrieve the CQEs from the CQ, causing the CQ to overflow.
Result: Generate asynchronous error&lt;/li>
&lt;/ul>
&lt;p>Because the software has not fetched the CQE, it naturally will not obtain information from the CQE. At this time, the IB framework will call the event handler function registered by the software to notify the user to handle the current error.&lt;/p>
&lt;p>From this, it can be seen that they are all ways for the lower layer to report errors to the upper layer users, only the timing of their occurrence is different. In the IB protocol, it is specified which method should be used to report errors in different situations. For example, in the diagram below, for modifying illegal parameters during the Modify QP process, an immediate error should be returned.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_10.webp"
alt="2024-06-27_10_10" width="auto" loading="lazy">
&lt;/figure>
&lt;p>The focus of this text is on CQ, so after introducing the error types, we will take a closer look at completion errors. Completion errors are reported by the hardware through filling error codes in the CQE. A communication process requires the participation of a requester and a responder, and the specific error causes are divided into local and remote. Let&amp;rsquo;s first take a look at the stage at which error detection is performed (the figure below is a redrawn version of Figure 118 in the IB protocol):&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_11.webp"
alt="2024-06-27_10_11" width="auto" loading="lazy">
&lt;/figure>
&lt;p>There are two error detection points for the Requester:&lt;/p>
&lt;ol>
&lt;li>Local error detection&lt;/li>
&lt;/ol>
&lt;p>Check the WQE in the SQ, if an error is detected, directly generate a CQE from the local error checking module to the CQ, and no data will be sent to the responder; if there is no error, send the data to the peer.&lt;/p>
&lt;ol start="2">
&lt;li>Remote Error Detection&lt;/li>
&lt;/ol>
&lt;p>Detect whether the response side&amp;rsquo;s ACK is abnormal. ACK/NAK is generated by the peer&amp;rsquo;s local error detection module after detection, and it contains whether there is an error on the response side and the specific type of error. Regardless of whether there is an issue with the remote error detection result, a CQE will be generated in the CQ.&lt;/p>
&lt;p>Responder&amp;rsquo;s error detection point is only one:&lt;/p>
&lt;ol>
&lt;li>Local error detection&lt;/li>
&lt;/ol>
&lt;p>In fact, what is detected is whether there is an issue with the peer message, which is also referred to as &amp;ldquo;local&amp;rdquo; error detection in the IB protocol. If an error is detected, it will be reflected in the ACK/NAK message sent back to the peer and will generate a CQE locally.&lt;/p>
&lt;p>It should be noted that the generation of ACK and remote error detection mentioned above is only applicable to connection-oriented service types. Connectionless service types, such as UD type, do not care whether the peer receives it, and the receiver will not generate an ACK. Therefore, a CQE will definitely be generated after the local error detection of the Requester, regardless of whether there is a remote error.&lt;/p>
&lt;p>Then we will briefly introduce several common completion errors:&lt;/p>
&lt;ul>
&lt;li>RC service type SQ completion error&lt;/li>
&lt;li>Local Protection Error
&lt;ul>
&lt;li>Local protection domain error. The data memory address specified in the local WQE is invalid for the MR, meaning the user is attempting to use data from an unregistered memory region.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Remote Access Error
&lt;ul>
&lt;li>Remote permission error. The local end does not have permission to read/write the specified remote memory address.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Transport Retry Counter Exceeded Error
&lt;ul>
&lt;li>Retransmission limit exceeded error. The peer has not responded with the correct ACK, causing multiple retransmissions from this end, exceeding the preset number of times.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>RC service type RQ completion error&lt;/li>
&lt;li>Local Access Error
&lt;ul>
&lt;li>Local access error. Indicates that the peer attempted to write to a memory area it does not have permission to write to.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Local Length Error
&lt;ul>
&lt;li>Local length error. The local RQ does not have enough space to receive the data sent by the peer.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>For a complete list of error types, please refer to Section 10.10.3 of the IB protocol.&lt;/p>
&lt;h2 id="user-interface">
&lt;a href="#user-interface" class="header-anchor">#&lt;/a>
User interface
&lt;/h2>
&lt;p>Like QP, we still introduce the interface provided by the IB protocol to the upper layer regarding CQ from the communication preparation phase (control plane) and the communication execution phase (data plane).&lt;/p>
&lt;h3 id="control-surface">
&lt;a href="#control-surface" class="header-anchor">#&lt;/a>
Control surface
&lt;/h3>
&lt;p>Just like QP, there are still the four types of &amp;ldquo;add, delete, modify, and query,&amp;rdquo; but perhaps because for CQ, the upper-layer users are resource users rather than managers, they can only read data from CQ and cannot write data. Therefore, the configurable parameter open to users is only the &amp;ldquo;CQ specification.&amp;rdquo;&lt;/p>
&lt;ul>
&lt;li>Create CQ&lt;/li>
&lt;/ul>
&lt;p>When creating, the user must specify the size of the CQ, i.e., how many CQEs it can store. Additionally, the user can provide a pointer to a callback function that is triggered after a CQE is generated (this will be discussed later). The kernel-mode driver will configure other related parameters and fill them into the CQC, as agreed with the hardware, to inform the hardware.&lt;/p>
&lt;ul>
&lt;li>Destroy CQ&lt;/li>
&lt;/ul>
&lt;p>Release a CQ hardware and software resource, including CQ itself and CQC, and naturally, CQN will also become invalid.&lt;/p>
&lt;ul>
&lt;li>Resize CQ&lt;/li>
&lt;/ul>
&lt;p>The name here is slightly different because CQ only allows users to modify the size of the specifications, so Resize is used instead of Modify.&lt;/p>
&lt;ul>
&lt;li>Query CQ&lt;/li>
&lt;/ul>
&lt;p>Query the current specifications of CQ, as well as the callback function pointer used for notifications.&lt;/p>
&lt;blockquote>
&lt;p>By comparing RDMA specifications and software protocol stacks, it can be found that many verbs interfaces are not implemented according to the specifications. Therefore, if readers find discrepancies between the software API and the protocol, there is no need to be puzzled, as RDMA technology itself is still evolving, and the software framework is in an active state of updates. If you are more concerned with programming implementation, please refer to the API documentation of the software protocol stack; if you are more concerned with academic research, please refer to the RDMA specifications.&lt;/p>
&lt;/blockquote>
&lt;h3 id="data-surface">
&lt;a href="#data-surface" class="header-anchor">#&lt;/a>
Data surface
&lt;/h3>
&lt;p>CQE is the medium through which hardware conveys information to software. Although the software knows under what circumstances a CQE will be generated, it does not know exactly when the hardware will place the CQE into the CQ. In the fields of communication and computing, this mode where the receiver does not know when the sender will send is called &amp;ldquo;asynchronous&amp;rdquo;. Let&amp;rsquo;s first take an example of a network card and then explain how a user can obtain a CQE (WC) through the data plane interface.&lt;/p>
&lt;p>After the network card receives a data packet, how to let the CPU know about this and process the packet, there are two common modes:&lt;/p>
&lt;ul>
&lt;li>Interrupt mode&lt;/li>
&lt;/ul>
&lt;p>When the amount of data is small, or when there are frequent sporadic data exchanges, it is suitable to use the interrupt mode—meaning the CPU is usually doing other tasks, and when the network card receives a data packet, it will report an interrupt to interrupt the current task of the CPU, and the CPU will switch to handle the data packet (such as parsing the various layers of the TCP/IP protocol stack). After processing the data, the CPU jumps back to the task before the interrupt to continue execution.&lt;/p>
&lt;p>Each interrupt requires saving the context, which means saving the current values of various registers, local variables, etc., to the stack, and then restoring the context (popping from the stack) upon return. This itself incurs overhead. If the business load is heavy and the network card is constantly receiving packets, the CPU will continuously receive interrupts, and the CPU will be busy with interrupt switching, causing other tasks to not be scheduled.&lt;/p>
&lt;ul>
&lt;li>Polling mode&lt;/li>
&lt;/ul>
&lt;p>So in addition to interrupt mode, the network card also has a polling mode, where received packets are first placed in the buffer, and the CPU periodically checks whether the network card has received data. If there is data, it takes the data from the buffer for processing; if not, it continues to handle other tasks.&lt;/p>
&lt;p>By comparing interrupt modes, we can find that although the polling mode requires the CPU to check at intervals, which brings some overhead, using polling mode when the business is busy can greatly reduce the number of context switches for interrupts, thereby reducing the CPU&amp;rsquo;s burden.&lt;/p>
&lt;p>The current network cards generally use a combination of interrupt and polling, which dynamically switches based on business load.&lt;/p>
&lt;p>In the RDMA protocol, a CQE is equivalent to a data packet received by the network card, and the RDMA hardware passes it to the CPU for processing. The RDMA framework defines two types of interfaces for the upper layer, namely poll and notify, corresponding to polling and interrupt modes.&lt;/p>
&lt;h3 id="poll-completion-queue">
&lt;a href="#poll-completion-queue" class="header-anchor">#&lt;/a>
Poll completion queue
&lt;/h3>
&lt;p>Very straightforward, poll means polling. After the user calls this interface, the CPU will periodically check if there are fresh CQEs in the CQ. If there are, it will extract this CQE (note that once extracted, the CQE is &amp;ldquo;consumed&amp;rdquo;), parse the information within, and return it to the upper-level user.&lt;/p>
&lt;h3 id="solicitud-de-notificación-de-finalización">
&lt;a href="#solicitud-de-notificaci%c3%b3n-de-finalizaci%c3%b3n" class="header-anchor">#&lt;/a>
Solicitud de notificación de finalización
&lt;/h3>
&lt;p>Literally translated, it is a request completion notification. After the user calls this interface, it is equivalent to registering an interrupt with the system. This way, when the hardware places a CQE into the CQ, it will immediately trigger an interrupt to the CPU. The CPU will then stop its current work to retrieve the CQE, process it, and return it to the user.&lt;/p>
&lt;p>Similarly, which of these two interfaces to use depends on the user&amp;rsquo;s requirements for real-time performance and the actual busyness of the business.&lt;/p>
&lt;p>Thank you for reading, that concludes the introduction to CQ. In the next article, I plan to discuss SRQ in detail.&lt;/p>
&lt;h2 id="relevant-sections-of-the-agreement">
&lt;a href="#relevant-sections-of-the-agreement" class="header-anchor">#&lt;/a>
Relevant sections of the agreement
&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>9.9 CQ Error Detection and Recovery&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.2.6 The relationship between CQ and WQ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.10 Error Types and Their Handling&lt;/p>
&lt;/li>
&lt;li>
&lt;p>11.2.8 CQ Related Control Plane Interface&lt;/p>
&lt;/li>
&lt;li>
&lt;p>11.4.2 CQ related data surface interface&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="other-references">
&lt;a href="#other-references" class="header-anchor">#&lt;/a>
Other references
&lt;/h2>
&lt;p>[1] Linux Kernel Networking - Implement and Theory. Chapter 13. Completion Queue&lt;/p></description></item><item><title>RDMA: Queue Pair</title><link>https://cuterwrite.top/en/p/rdma-queue-pair/</link><pubDate>Tue, 25 Jun 2024 02:21:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/rdma-queue-pair/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116342820_p0_master1200.webp" alt="Featured image of post RDMA: Queue Pair" />&lt;h1 id="queue-pair-of-rdma">
&lt;a href="#queue-pair-of-rdma" class="header-anchor">#&lt;/a>
Queue Pair of RDMA
&lt;/h1>
&lt;p>&lt;strong>This article welcomes non-commercial reposting, please indicate the source.&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>Statement: For collection only, for convenient reading&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/195757767">&lt;cite>Zhihu Column: 9. Basic RDMA Service Types&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;h2 id="queue-pair">
&lt;a href="#queue-pair" class="header-anchor">#&lt;/a>
Queue Pair
&lt;/h2>
&lt;p>We have previously provided a brief introduction to the concept of QP in the article &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-element/" >&amp;ldquo;3. Basic Elements of RDMA&amp;rdquo;
&lt;/a>
. This article will delve deeper into some details about QP.&lt;/p>
&lt;h2 id="review-of-basic-concepts">
&lt;a href="#review-of-basic-concepts" class="header-anchor">#&lt;/a>
Review of Basic Concepts
&lt;/h2>
&lt;p>First, let&amp;rsquo;s briefly review the basic knowledge about QP:&lt;/p>
&lt;p>According to the description in the IB protocol, QP is a virtual interface between hardware and software. QP is a queue structure that sequentially stores tasks (WQE) issued by software to hardware. The WQE contains information such as where to retrieve data, how long the data is, and to which destination it should be sent.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-26_9_1.webp"
alt="2024-06-26_9_1" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Concept of QP&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>Each QP is independent and isolated from each other through PD, so a QP can be regarded as a resource exclusively used by a certain user, and a user can also use multiple QPs simultaneously.&lt;/p>
&lt;p>QP has many types of services, including RC, UD, RD, and UC, etc. All source QPs and destination QPs must be of the same type to interact with each other.&lt;/p>
&lt;p>Although the IB protocol refers to QP as a &amp;ldquo;virtual interface,&amp;rdquo; it is tangible:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>On the hardware side, a QP is a storage space containing several WQEs. The IB network card reads the contents of the WQEs from this space and accesses the memory to store or retrieve data according to the user&amp;rsquo;s expectations. As for whether this storage space is memory space or on-chip storage space of the IB network card, the IB protocol does not impose restrictions, and each manufacturer has its own implementation.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>In software, QP is a data structure maintained by the driver of the IB network card, which contains the address pointer of the QP and some related software attributes.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="qpc">
&lt;a href="#qpc" class="header-anchor">#&lt;/a>
QPC
&lt;/h3>
&lt;p>In the article &lt;a class="link" href="https://cuterwrite.top/en/p/rdma-service-types/" >&amp;ldquo;5. RDMA Basic Service Types&amp;rdquo;
&lt;/a>
, we mentioned that QPC stands for Queue Pair Context, which is used to store properties related to QP. The driver does store the software properties of QP, so if we can store QP properties in software, why do we still use QPC?&lt;/p>
&lt;p>This is because &lt;strong>QPC is mainly for hardware viewing and is also used to synchronize QP information between software and hardware.&lt;/strong>&lt;/p>
&lt;p>We have mentioned that the entity of a QP on hardware is merely a segment of storage space, and the hardware knows nothing beyond the starting address and size of this space, not even the service type of this QP. There is also a lot of other important information, such as a QP containing several WQEs. How does the hardware know how many there are and which one it should currently process?&lt;/p>
&lt;p>All of the above information can be structured into a data structure by the software, and memory space can be allocated for it. However, the software only sees virtual addresses, and these memory spaces are physically discrete; the hardware does not know where this data is stored. Therefore, the software needs to pre-allocate a large contiguous space through the operating system, namely QPC, to present this information to the hardware. The network card and its accompanying driver program have pre-agreed on what content is included in the QPC, how much space each content occupies, and in what order they are stored. This way, the driver and hardware can read and write the status and other information of the QP through this QPC space.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-26_9_2_QPC.webp"
alt="2024-06-26_9_2" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>The concept of QPC&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>As shown in the figure above, the hardware actually only needs to know the address 0x12350000 of the QPC, because it can parse the contents of the QPC to determine the position of the QP, the QP sequence number, the QP size, and other information. Consequently, it can locate the QP and determine which WQE to process. Different manufacturers may have some variations in implementation, but the general principle is like this.&lt;/p>
&lt;p>There are many Context concepts in the IB software stack, in addition to QPC, there are also Device Context, SRQC, CQC, EQC (Event Queue Context), etc. Their functions are similar to QPC, all used to record and synchronize the related attributes of certain resources.&lt;/p>
&lt;h3 id="qp-number">
&lt;a href="#qp-number" class="header-anchor">#&lt;/a>
QP Number
&lt;/h3>
&lt;p>Referred to as QPN, which is the number of each QP. The IB protocol specifies using $2^{24}$ bits to represent QPN, meaning each node can simultaneously use up to $2^{24}$ QPs, which is already a very large number and almost impossible to exhaust. Each node maintains its own set of QPNs independently, meaning QPs with the same number can exist on different nodes.&lt;/p>
&lt;p>The concept of QPN itself is very simple, but there are two special reserved numbers that require extra attention:&lt;/p>
&lt;h4 id="qp0">
&lt;a href="#qp0" class="header-anchor">#&lt;/a>
QP0
&lt;/h4>
&lt;p>QP with ID 0 is used for the Subnet Management Interface (SMI), which is used to manage all nodes in the subnet. To be honest, I haven&amp;rsquo;t figured out the purpose of this interface yet, so let&amp;rsquo;s put it aside for now.&lt;/p>
&lt;h4 id="qp1">
&lt;a href="#qp1" class="header-anchor">#&lt;/a>
QP1
&lt;/h4>
&lt;p>QP numbered 1 is used for the General Service Interface (GSI), which is a set of management services, the most well-known of which is CM (Communication Management). It is a method used to exchange necessary information before formally establishing a connection between the communication nodes. Its details will be elaborated in a later article.&lt;/p>
&lt;p>This is the reason why QP0 and QP1 did not appear in the diagram about QP in our previous article. All other QPs besides these two are regular QPs. When a user creates a QP, the driver or hardware will assign a QPN to this new QP, and generally, QPNs are assigned sequentially like 2, 3, 4. After a QP is destroyed, its QPN will be reclaimed and allocated to other newly created QPs at an appropriate time.&lt;/p>
&lt;h2 id="user-interface">
&lt;a href="#user-interface" class="header-anchor">#&lt;/a>
User interface
&lt;/h2>
&lt;p>We classify and introduce user interfaces from the control plane and data plane perspectives. The control plane refers to the user&amp;rsquo;s configuration of a certain resource, which is generally done before the actual data transmission; whereas the data plane naturally involves operations during the actual data transmission process.&lt;/p>
&lt;h3 id="control-surface">
&lt;a href="#control-surface" class="header-anchor">#&lt;/a>
Control surface
&lt;/h3>
&lt;p>Readers who have encountered algorithms should all understand that the nodes of a linked list involve four operations: &amp;ldquo;add, delete, modify, and search.&amp;rdquo; The nodes of a linked list are a memory area and a type of software resource.&lt;/p>
&lt;p>&amp;ldquo;Increase&amp;rdquo; means requesting a piece of memory from the operating system to store data. The system will allocate a space in memory and mark it as &amp;ldquo;in use by process XX,&amp;rdquo; and other unauthorized processes will not be able to overwrite or even read this memory space.&lt;/p>
&lt;p>&amp;ldquo;Delete&amp;rdquo; means notifying the operating system that I am no longer using this space, and it can be marked as &amp;ldquo;unused&amp;rdquo; and made available for other processes to use.&lt;/p>
&lt;p>&amp;ldquo;Modify&amp;rdquo; means to write, i.e., to change the contents of this memory area.&lt;/p>
&lt;p>&amp;ldquo;Query&amp;rdquo; means read, that is, to obtain the content of this memory area.&lt;/p>
&lt;p>QP, as one of the most important resources in RDMA technology, is no different from a linked list in its lifecycle:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Operation&lt;/th>
&lt;th style="text-align: left">Linked List Node&lt;/th>
&lt;th style="text-align: left">QP&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">Increase&lt;/td>
&lt;td style="text-align: left">struct ListNode *node = malloc(sizeof(struct ListNode *));&lt;/td>
&lt;td style="text-align: left">Create QP&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Delete&lt;/td>
&lt;td style="text-align: left">free(node);&lt;/td>
&lt;td style="text-align: left">Destroy QP&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Modify&lt;/td>
&lt;td style="text-align: left">node-&amp;gt;val = xxx;&lt;/td>
&lt;td style="text-align: left">Modify QP&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Check&lt;/td>
&lt;td style="text-align: left">xxx = node-&amp;gt;val;&lt;/td>
&lt;td style="text-align: left">Query QP&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>These four operations are actually the Verbs (RDMA&amp;rsquo;s API for upper-layer applications) that provide several interfaces to upper-layer users on the control plane:&lt;/p>
&lt;h4 id="create-qp">
&lt;a href="#create-qp" class="header-anchor">#&lt;/a>
Create QP
&lt;/h4>
&lt;p>Create a QP&amp;rsquo;s hardware and software resources, including the QP itself and the QPC. When the user creates it, they will input a series of initialization attributes, including the service type of the QP, the number of WQEs that can be stored, and other information.&lt;/p>
&lt;h4 id="destroy-qp">
&lt;a href="#destroy-qp" class="header-anchor">#&lt;/a>
Destroy QP
&lt;/h4>
&lt;p>Release all software and hardware resources of a QP, including the QP itself and the QPC. After destroying the QP, the user will no longer be able to index this QP through QPN.&lt;/p>
&lt;h4 id="modify-qp">
&lt;a href="#modify-qp" class="header-anchor">#&lt;/a>
Modify QP
&lt;/h4>
&lt;p>Modify certain attributes of a QP, such as the state of the QP, the MTU of the path, etc. This modification process includes both the modification of software data structures and the modification of the QPC.&lt;/p>
&lt;h4 id="query-qp">
&lt;a href="#query-qp" class="header-anchor">#&lt;/a>
Query QP
&lt;/h4>
&lt;p>Query the current status and some attributes of a QP. The data queried comes from the driver and the content of the QPC.&lt;/p>
&lt;p>These four operations all have corresponding Verbs interfaces, similar to &lt;code>ibv_create_qp()&lt;/code> form, which we can directly call when writing the APP. More details about the upper-level API will be introduced later.&lt;/p>
&lt;h2 id="data-surface">
&lt;a href="#data-surface" class="header-anchor">#&lt;/a>
Data surface
&lt;/h2>
&lt;p>In terms of data, a QP actually has only two interfaces to the upper layer, used to fill in send and receive requests in the QP. &lt;strong>Here, &amp;ldquo;send&amp;rdquo; and &amp;ldquo;receive&amp;rdquo; do not refer to sending and receiving data, but rather the &amp;ldquo;initiator&amp;rdquo; (Requestor) and &amp;ldquo;responder&amp;rdquo; (Responser) in a communication process.&lt;/strong>&lt;/p>
&lt;p>In behavior, the software fills a WQE (called WR at the application layer) into the QP, requesting the hardware to perform an action. Therefore, both behaviors are called &amp;ldquo;Post XXX Request,&amp;rdquo; meaning issuing an XXX request.&lt;/p>
&lt;h3 id="send-request">
&lt;a href="#send-request" class="header-anchor">#&lt;/a>
Send Request
&lt;/h3>
&lt;p>To emphasize again, Post Send itself does not mean that the operation type of this WQE is Send, but indicates that this WQE belongs to the initiator of the communication. The WQE/WR filled into the QP in this process can be a Send operation, RDMA Write operation, or RDMA Read operation, etc.&lt;/p>
&lt;p>The user needs to prepare the data buffer, destination address, and other information in advance, then call the interface to pass the WR to the driver, and the driver will fill the WQE into the QP.&lt;/p>
&lt;h3 id="post-receive-request">
&lt;a href="#post-receive-request" class="header-anchor">#&lt;/a>
Post Receive Request
&lt;/h3>
&lt;p>The usage scenarios for Post Recv are relatively fewer, generally only executed on the receiving end of the Send-Recv operation. The receiving end needs to prepare the buffer for receiving data in advance and inform the hardware of the buffer address and other information in the form of a WQE.&lt;/p>
&lt;h2 id="qp-state-machine">
&lt;a href="#qp-state-machine" class="header-anchor">#&lt;/a>
QP state machine
&lt;/h2>
&lt;p>Speaking of the state of QP, we have to bring out the following image (taken from section 10.3.1 of the IB protocol):&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-26_9_3.webp"
alt="2024-06-26_9_3" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>QP State Machine&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>The so-called state machine describes the different states of an object and the conditions that trigger transitions between states. Designing a state machine for an object can make the lifecycle of this object very clear, and in implementation, it will also make the logic more coherent.&lt;/p>
&lt;p>For QP, the IB specification also designs several states for it. The functions of a QP in different states vary. For example, only after entering the Ready to Send state can the QP perform Post Send data operations. State transitions between normal states (in green) are actively triggered by the user through the Modify QP user interface introduced above; whereas error states (in red) often automatically transition after an error occurs. When a QP is in an error state, it cannot perform normal operations and needs to be reconfigured to a normal state by the upper layer through Modify QP.&lt;/p>
&lt;p>In the above diagram, we only focus on the part of QP. EE (End-to-End Context) is a concept specifically used for RD service types, which we will not cover for now. We enter this state diagram through the Create QP interface and exit this state diagram through the Destroy QP interface.&lt;/p>
&lt;p>QP has the following states, we will only introduce some important points:&lt;/p>
&lt;h3 id="rst-reset">
&lt;a href="#rst-reset" class="header-anchor">#&lt;/a>
RST (Reset)
&lt;/h3>
&lt;p>Reset state. When a QP is created through Create QP, it is in this state. The related resources have already been allocated, but this QP cannot do anything at the moment. It cannot receive WQEs issued by the user, nor can it receive messages from a QP on the peer end.&lt;/p>
&lt;h3 id="initinitialized">
&lt;a href="#initinitialized" class="header-anchor">#&lt;/a>
INIT（Initialized）
&lt;/h3>
&lt;p>Initialized state. In this state, the user can issue Receive WR to this QP via Post Receive, but the received messages will not be processed and will be silently discarded; if the user issues a Post Send WR, an error will occur.&lt;/p>
&lt;h3 id="rtrready-to-receive">
&lt;a href="#rtrready-to-receive" class="header-anchor">#&lt;/a>
RTR（Ready to Receive）
&lt;/h3>
&lt;p>Ready to receive status. Based on the INIT state, RQ can function normally, meaning it can move data to the specified memory location according to the instructions in the received message&amp;rsquo;s WQE. In this state, SQ still cannot function.&lt;/p>
&lt;h3 id="rts-ready-to-send">
&lt;a href="#rts-ready-to-send" class="header-anchor">#&lt;/a>
RTS (Ready to Send)
&lt;/h3>
&lt;p>Ready to send status. Based on RTR, SQ can work normally, meaning the user can perform Post Send, and the hardware will also send the data according to the content of SQ. Before entering this state, QP must have already established a connection with the peer.&lt;/p>
&lt;h3 id="sqd-send-queue-drain">
&lt;a href="#sqd-send-queue-drain" class="header-anchor">#&lt;/a>
SQD (Send Queue Drain)
&lt;/h3>
&lt;p>SQ emptying state. As the name suggests, this state will process all the existing unprocessed WQEs in the SQ queue. At this time, the user can still submit new WQEs, but these WQEs will be processed only after all the old WQEs have been processed.&lt;/p>
&lt;h3 id="sqer-send-queue-error">
&lt;a href="#sqer-send-queue-error" class="header-anchor">#&lt;/a>
SQEr (Send Queue Error)
&lt;/h3>
&lt;p>SQ error state. When a Send WR encounters a completion error (i.e., an error reported to the driver by the hardware through CQE), it causes the QP to enter this state.&lt;/p>
&lt;h3 id="err-error">
&lt;a href="#err-error" class="header-anchor">#&lt;/a>
ERR (Error)
&lt;/h3>
&lt;p>Error state. If an error occurs in other states, they may enter this state. In the Error state, the QP will stop processing WQE, and any WQE that is halfway processed will also stop. The upper layer needs to switch the QP back to the initial RST state after fixing the error.&lt;/p>
&lt;h2 id="summary">
&lt;a href="#summary" class="header-anchor">#&lt;/a>
Summary
&lt;/h2>
&lt;p>This article first reviews some important basic concepts of QP, then explains QPC, QPN, and other concepts closely related to QP, and finally introduces the interfaces commonly used by users to operate QP and the QP state machine. I believe that after reading this article, readers will have a deeper understanding of QP.&lt;/p>
&lt;p>In fact, as a core concept of RDMA, there is a lot of content regarding QP, and this article cannot cover everything. I will gradually complete the related content in future articles. For example, the concept of QKey will be explained in detail in subsequent articles dedicated to various Keys.&lt;/p>
&lt;p>Alright, this is the end of the article. Thank you for reading. A preview of the next article will provide a detailed explanation of CQ.&lt;/p>
&lt;h2 id="relevant-sections-of-the-agreement">
&lt;a href="#relevant-sections-of-the-agreement" class="header-anchor">#&lt;/a>
Relevant sections of the agreement
&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>3.5.1 10.2.4 Basic Concepts of QP&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.3 QP State Machine&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.2.5 Software interfaces related to QP&lt;/p>
&lt;/li>
&lt;li>
&lt;p>11.4 Post Send Post Recv&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>Implementing Hugo Progressive Web App Based on Workbox</title><link>https://cuterwrite.top/en/p/hugo-pwa/</link><pubDate>Tue, 18 Jun 2024 22:28:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/hugo-pwa/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116406967_p0_master1200.webp" alt="Featured image of post Implementing Hugo Progressive Web App Based on Workbox" />&lt;h1 id="implement-hugo-pwa-based-on-workbox">
&lt;a href="#implement-hugo-pwa-based-on-workbox" class="header-anchor">#&lt;/a>
Implement Hugo PWA based on Workbox
&lt;/h1>
&lt;p>Recently added PWA functionality to a blog built on &lt;a class="link" href="https://gohugo.io/" target="_blank" rel="noopener" >Hugo
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, significantly improving loading speed and user experience, even enabling offline access. As for how to achieve this, you need to understand &lt;strong>Progressive Web Apps (PWA)&lt;/strong>.&lt;/p>
&lt;h2 id="what-is-pwa">
&lt;a href="#what-is-pwa" class="header-anchor">#&lt;/a>
What is PWA?
&lt;/h2>
&lt;p>Progressive Web Apps (PWA) leverage modern Web APIs and traditional progressive enhancement strategies to create cross-platform web applications. These applications are ubiquitous, feature-rich, and provide users with an experience comparable to native apps.&lt;/p>
&lt;p>&lt;strong>Advantages of PWA:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>⚡️ &lt;strong>Faster loading speed&lt;/strong>: PWA can cache important resources and load quickly even in poor network conditions.&lt;/li>
&lt;li>✈️ &lt;strong>Offline Access&lt;/strong>: PWA can cache content, allowing users to access content even when offline.&lt;/li>
&lt;li>🔔 &lt;strong>Push Notifications&lt;/strong>: Like native applications, PWAs can send push notifications to users to increase user engagement.&lt;/li>
&lt;li>📱 &lt;strong>Install to Home Screen&lt;/strong>: Users can add your application to the desktop of their computer or phone and browse your web application like a native app.&lt;/li>
&lt;/ul>
&lt;p>The implementation principle of PWA is &lt;strong>Service Worker&lt;/strong>. &lt;strong>Service Worker is a special JavaScript resource that runs independently in the browser background, acting as a proxy between the web browser and the web server. It can intercept and handle network requests, cache resources, and push notifications&lt;/strong>.&lt;/p>
&lt;p>Mainstream front-end frameworks Vue, React, and Angular all provide corresponding PWA plugins. As for static site generators like Hugo, we can implement PWA functionality by manually adding &lt;a class="link" href="https://developer.chrome.com/docs/workbox" target="_blank" rel="noopener" >Workbox
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
.&lt;/p>
&lt;h2 id="workbox">
&lt;a href="#workbox" class="header-anchor">#&lt;/a>
Workbox
&lt;/h2>
&lt;p>&lt;a class="link" href="https://developer.chrome.com/docs/workbox" target="_blank" rel="noopener" >Workbox
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
is a set of modules developed by the Google Chrome team, designed to simplify common Service Worker routing and caching operations. Each module is optimized for a specific aspect of Service Worker development. The goal of Workbox is to simplify the use of Service Workers as much as possible while providing the flexibility to meet the needs of complex applications when necessary.&lt;/p>
&lt;p>If there is no Workbox, we need to manually write a Service Worker to listen to fetch events, cache resources, and implement offline access and other functions. Workbox provides a set of tools that can help us automatically generate a Service Worker and comes with some commonly used caching strategies, allowing us to focus more on business logic.&lt;/p>
&lt;h2 id="configure-pwa">
&lt;a href="#configure-pwa" class="header-anchor">#&lt;/a>
Configure PWA
&lt;/h2>
&lt;p>In the previous section, we learned about the concept and advantages of PWA, and how Workbox simplifies the development of Service Workers. Next, we will step by step configure PWA functionality for the Hugo blog.&lt;/p>
&lt;h3 id="register-service-worker">
&lt;a href="#register-service-worker" class="header-anchor">#&lt;/a>
Register Service Worker
&lt;/h3>
&lt;p>First, we need to register the Service Worker on the page. Add the following code snippet to your Hugo theme&amp;rsquo;s &lt;code>layouts/partials/footer/custom.html&lt;/code> file (other themes may need adjustments based on the file structure):&lt;/p>
&lt;pre>&lt;code class="language-javascript">&amp;lt;script&amp;gt;
// Check that service workers are registered
if ('serviceWorker' in navigator) {
// Use the window load event to keep the page load performant
window.addEventListener('load', () =&amp;gt; {
navigator.serviceWorker.register('/sw.js').then(reg =&amp;gt; {
console.log('Service worker registered with scope: ', reg.scope);
}, err =&amp;gt; {
console.log('Service worker registration failed: ', err);
});
});
}
&amp;lt;/script&amp;gt;
&lt;/code>&lt;/pre>
&lt;blockquote class="alert-blockquote alert-note">
&lt;p class="alert-heading">
&lt;svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16">
&lt;path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z">&lt;/path>
&lt;/svg>
&lt;span>Note&lt;/span>
&lt;/p>
&lt;p>Note: Before registering the Service Worker, you need to first create the &lt;code>sw.js&lt;/code> file, which we will complete in the next section.&lt;/p>
&lt;/blockquote>
&lt;p>After completing the registration, you can view the registration status of the Service Worker in the developer tools (F12) of the browser under &lt;strong>&amp;ldquo;Application&amp;rdquo; -&amp;gt; &amp;ldquo;Service Workers&amp;rdquo;&lt;/strong> panel.&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-19_service-worker.webp"
alt="Service Worker" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>Service Worker&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="import-workbox">
&lt;a href="#import-workbox" class="header-anchor">#&lt;/a>
Import Workbox
&lt;/h3>
&lt;p>In the &lt;code>static&lt;/code> folder of your Hugo site root directory, create the &lt;code>sw.js&lt;/code> file. Then, add the following code in the &lt;code>sw.js&lt;/code> file to import Workbox using CDN:&lt;/p>
&lt;pre>&lt;code class="language-javascript">importScripts('https://cdn.bootcdn.net/ajax/libs/workbox-sw/7.1.0/workbox-sw.js');
&lt;/code>&lt;/pre>
&lt;h3 id="cache-strategy">
&lt;a href="#cache-strategy" class="header-anchor">#&lt;/a>
Cache strategy
&lt;/h3>
&lt;p>Workbox provides some common caching strategies, such as &lt;code>CacheFirst&lt;/code>, &lt;code>NetworkFirst&lt;/code>, &lt;code>StaleWhileRevalidate&lt;/code>, etc. Here we introduce some common strategies first.&lt;/p>
&lt;h4 id="cacheonly-cache-only">
&lt;a href="#cacheonly-cache-only" class="header-anchor">#&lt;/a>
CacheOnly Cache only
&lt;/h4>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-19_cache-6850d07d742bf_1440.webp"
alt="CacheOnly" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>CacheOnly&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>Forced response from cache.&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp(regex),
new workbox.strategies.CacheOnly()
);
&lt;/code>&lt;/pre>
&lt;h4 id="networkonly-network-only">
&lt;a href="#networkonly-network-only" class="header-anchor">#&lt;/a>
NetworkOnly Network Only
&lt;/h4>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-19_network-48f46158a5575_1440.webp"
alt="NetworkOnly" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>NetworkOnly&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>This caching strategy forces all requests to retrieve the latest data from the network, completely bypassing the cache.&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp(regex),
new workbox.strategies.NetworkOnly()
);
&lt;/code>&lt;/pre>
&lt;h4 id="cachefirst-cache-priority">
&lt;a href="#cachefirst-cache-priority" class="header-anchor">#&lt;/a>
CacheFirst Cache Priority
&lt;/h4>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-19_cache-falling-to-networ-f4c1aa5570621_1440.webp"
alt="CacheFirst" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>CacheFirst&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>This caching strategy prioritizes speed, first attempting to retrieve the response from the cache to display content to the user as quickly as possible. If the required data is not in the cache, it will then make a request to the network to obtain the data.&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp(regex),
new workbox.strategies.CacheFirst()
);
&lt;/code>&lt;/pre>
&lt;h4 id="networkfirst-优先网络">
&lt;a href="#networkfirst-%e4%bc%98%e5%85%88%e7%bd%91%e7%bb%9c" class="header-anchor">#&lt;/a>
NetworkFirst 优先网络
&lt;/h4>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-19_network-falling-to-cache-39d267a044b35_1440.webp"
alt="NetworkFirst" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>NetworkFirst&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>This caching strategy prioritizes using the latest data, so it will first attempt to fetch the response from the network. If the network request fails, such as when the user is offline or the network connection is unstable, it will fall back to using cached data to ensure that the user can still access the content.&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp(regex),
new workbox.strategies.NetworkFirst()
);
&lt;/code>&lt;/pre>
&lt;h4 id="stalewhilerevalidate-reads-the-cache-while-initiating-a-network-request">
&lt;a href="#stalewhilerevalidate-reads-the-cache-while-initiating-a-network-request" class="header-anchor">#&lt;/a>
StaleWhileRevalidate reads the cache while initiating a network request
&lt;/h4>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-19_cache-network-873b1ec5f25cc_1440.webp"
alt="StaleWhileRevalidate" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>StaleWhileRevalidate&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>This caching strategy prioritizes returning cached content (if available). Even if the cached content is valid, it will initiate a network request in the background to obtain the latest data, ensuring that the user ultimately sees the most up-to-date content. Although this strategy ensures that the cache is regularly updated for the user, it also means that every request generates network traffic, which can be a waste of bandwidth even if the data hasn&amp;rsquo;t changed.&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp(regex),
new workbox.strategies.StaleWhileRevalidate()
);
&lt;/code>&lt;/pre>
&lt;h4 id="strategy-configuration">
&lt;a href="#strategy-configuration" class="header-anchor">#&lt;/a>
Strategy Configuration
&lt;/h4>
&lt;p>Workbox not only provides the aforementioned strategies but also allows customization through configuration options such as cacheName, plugins, and expiration. You can customize routing behavior by defining the plugins you want to use. For example, you can configure the cache name, cache expiration, and cacheable response status codes as follows:&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp(regex),
new workbox.strategies.CacheFirst({
cacheName: 'my-cache',
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: 60,
maxAgeSeconds: 30 * 24 * 60 * 60, // 30 Days
}),
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
&lt;/code>&lt;/pre>
&lt;h3 id="site-configuration">
&lt;a href="#site-configuration" class="header-anchor">#&lt;/a>
Site Configuration
&lt;/h3>
&lt;h4 id="global-configuration">
&lt;a href="#global-configuration" class="header-anchor">#&lt;/a>
Global configuration
&lt;/h4>
&lt;p>The following is the global cache configuration:&lt;/p>
&lt;pre>&lt;code class="language-javascript">// Cache version number
let cacheVersion = '-240619';
// Maximum number of entries
const maxEntries = 100;
&lt;/code>&lt;/pre>
&lt;h4 id="twitto-configuration">
&lt;a href="#twitto-configuration" class="header-anchor">#&lt;/a>
Twitto Configuration
&lt;/h4>
&lt;p>In order to ensure that users can view comments even when offline, the Twitto Comments API uses a &lt;code>NetworkFirst&lt;/code> caching strategy. This means the browser will first attempt to fetch the latest data from the network, and if the network is unavailable, it will use the data from the cache.&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('^https://comment\.cuterwrite\.top'),
new workbox.strategies.NetworkFirst({
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
&lt;/code>&lt;/pre>
&lt;h4 id="rss-and-sitemap-configuration">
&lt;a href="#rss-and-sitemap-configuration" class="header-anchor">#&lt;/a>
RSS and Sitemap Configuration
&lt;/h4>
&lt;p>In order to ensure that users always obtain the latest RSS and Sitemap data, these pages are configured to use only the network strategy (&lt;code>NetworkOnly&lt;/code>) without caching.&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('^https://cuterwrite\.top/(index|sitemap)\.xml'),
new workbox.strategies.NetworkOnly()
);
&lt;/code>&lt;/pre>
&lt;h4 id="html-configuration">
&lt;a href="#html-configuration" class="header-anchor">#&lt;/a>
HTML Configuration
&lt;/h4>
&lt;p>In order to ensure that users can quickly load pages while also obtaining the latest content, the website uses the &lt;code>StaleWhileRevalidate&lt;/code> caching strategy for HTML pages. This means the browser will prioritize displaying the page from the cache while simultaneously making a request to the server in the background to fetch the latest version, which will be used on the next request.&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('.*\.html'),
new workbox.strategies.StaleWhileRevalidate({
cacheName: 'html-cache' + cacheVersion,
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
&lt;/code>&lt;/pre>
&lt;h4 id="google-fonts-configuration">
&lt;a href="#google-fonts-configuration" class="header-anchor">#&lt;/a>
Google Fonts Configuration
&lt;/h4>
&lt;p>In order to ensure the font files are updated while also utilizing caching to speed up page loading, the website uses a &lt;code>CacheFirst&lt;/code> caching strategy for Google Fonts resources and sets a long cache expiration time.&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('.*\.(?:woff|woff2|ttf|otf|eot)'),
new workbox.strategies.StaleWhileRevalidate({
cacheName: 'google-fonts' + cacheVersion,
plugins: [
// Use expiration plugin to control the number and time of cache entries
new workbox.expiration.ExpirationPlugin({
// Maximum number of cache entries
maxEntries: maxEntries,
// Maximum cache time 30 days
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
// Use cacheableResponse plugin to cache requests with status code 0
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
&lt;/code>&lt;/pre>
&lt;h4 id="cdn-configuration">
&lt;a href="#cdn-configuration" class="header-anchor">#&lt;/a>
CDN Configuration
&lt;/h4>
&lt;p>In order to maximize the use of cache to speed up page loading, the website adopts a &lt;code>CacheFirst&lt;/code> caching strategy for resources from common CDNs and sets a long cache expiration time.&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('^https://(?:cdn\.bootcdn\.net|unpkg\.com|cdn\.jsdelivr\.net)'),
new workbox.strategies.CacheFirst({
cacheName: 'cdn' + cacheVersion,
fetchOptions: {
mode: 'cors',
credentials: 'omit',
},
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
],
})
);
&lt;/code>&lt;/pre>
&lt;h4 id="umani-website-statistics-configuration">
&lt;a href="#umani-website-statistics-configuration" class="header-anchor">#&lt;/a>
Umani website statistics configuration
&lt;/h4>
&lt;p>In order to ensure the accuracy of website statistics, the website adopts the &lt;code>NetworkOnly&lt;/code> strategy for Umani website statistics requests and uses the &lt;code>BackgroundSyncPlugin&lt;/code> to ensure that data is eventually sent successfully even when the network is offline.&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('^https://analytics\.cuterwrite\.top/uma'),
new workbox.strategies.NetworkOnly({
plugins: [
// Use background sync plugin to implement background synchronization
new workbox.backgroundSync.BackgroundSyncPlugin('Optical_Collect', {
maxRetentionTime: 12 * 60,
}),
],
})
);
&lt;/code>&lt;/pre>
&lt;h4 id="image-configuration">
&lt;a href="#image-configuration" class="header-anchor">#&lt;/a>
Image Configuration
&lt;/h4>
&lt;p>In order to speed up image loading and reduce the number of network requests, the website uses a &lt;code>CacheFirst&lt;/code> caching strategy for image resources and sets a long cache expiration time.&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('^(https://cuterwrite-1302252842\.file\.myqcloud\.com|https://s2\.loli\.net)'),
new workbox.strategies.CacheFirst({
cacheName: 'image-cache' + cacheVersion,
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
&lt;/code>&lt;/pre>
&lt;h4 id="suffix-match-configuration">
&lt;a href="#suffix-match-configuration" class="header-anchor">#&lt;/a>
Suffix match configuration
&lt;/h4>
&lt;p>In order to balance loading speed and content updates, the website uses the &lt;code>StaleWhileRevalidate&lt;/code> caching strategy for static files (such as images, CSS, and JavaScript files) that are not matched by the domain name.&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('.*\.(?:png|jpg|jpeg|svg|gif|webp|ico)'),
new workbox.strategies.StaleWhileRevalidate()
);
workbox.routing.registerRoute(
new RegExp('.*\.(css|js)'),
new workbox.strategies.StaleWhileRevalidate()
);
&lt;/code>&lt;/pre>
&lt;h4 id="default-behavior-configuration">
&lt;a href="#default-behavior-configuration" class="header-anchor">#&lt;/a>
Default behavior configuration
&lt;/h4>
&lt;p>In order to handle requests that are not matched by any custom routing rules, the website is configured with a default caching behavior, using the &lt;code>NetworkFirst&lt;/code> strategy and setting a network timeout to balance resource retrieval speed and offline availability.&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.setDefaultHandler(
// Prefer using cache, if cache is not available then use network request
new workbox.strategies.NetworkFirst({
networkTimeoutSeconds: 3,
})
);
&lt;/code>&lt;/pre>
&lt;h3 id="full-configuration">
&lt;a href="#full-configuration" class="header-anchor">#&lt;/a>
Full configuration
&lt;/h3>
&lt;details>
&lt;summary>sw.js&lt;/summary>
&lt;pre>&lt;code class="language-javascript">importScripts('https://cdn.bootcdn.net/ajax/libs/workbox-sw/7.1.0/workbox-sw.js');
// Cache version number
let cacheVersion = '-240619';
// Maximum number of entries
const maxEntries = 100;
if (workbox) {
console.log(`Yay! Workbox is loaded 🎉`);
// Comment cache
workbox.routing.registerRoute(
new RegExp('^https://comment\.cuterwrite\.top'),
new workbox.strategies.NetworkFirst({
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
// Do not cache rss and sitemap
workbox.routing.registerRoute(
new RegExp('^https://cuterwrite\.top/(index|sitemap)\.xml'),
new workbox.strategies.NetworkOnly()
);
// Cache HTML
workbox.routing.registerRoute(
new RegExp('.*\.html'),
new workbox.strategies.StaleWhileRevalidate({
cacheName: 'html-cache' + cacheVersion,
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
// Cache Google Fonts
workbox.routing.registerRoute(
new RegExp('.*\.(?:woff|woff2|ttf|otf|eot)'),
new workbox.strategies.StaleWhileRevalidate({
cacheName: 'google-fonts' + cacheVersion,
plugins: [
// Use expiration plugin to control cache entry number and time
new workbox.expiration.ExpirationPlugin({
// Maximum number of cache entries
maxEntries: maxEntries,
// Maximum cache time 30 days
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
// Use cacheableResponse plugin to cache requests with status code 0
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
// Cache public libraries like bootcdn, unpkg, jsdelivr using regex
workbox.routing.registerRoute(
new RegExp('^https://(?:cdn\.bootcdn\.net|unpkg\.com|cdn\.jsdelivr\.net)'),
new workbox.strategies.CacheFirst({
cacheName: 'cdn' + cacheVersion,
fetchOptions: {
mode: 'cors',
credentials: 'omit',
},
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
],
})
);
// Self-built UMA statistics script: https://analytics.cuterwrite.top/uma
workbox.routing.registerRoute(
new RegExp('^https://analytics\.cuterwrite\.top/uma'),
new workbox.strategies.NetworkOnly({
plugins: [
// Use background sync plugin for background synchronization
new workbox.backgroundSync.BackgroundSyncPlugin('Optical_Collect', {
maxRetentionTime: 12 * 60,
}),
],
})
);
// Cache bucket images https://cuterwrite-1302252842.file.myqcloud.com/
workbox.routing.registerRoute(
new RegExp('^(https://cuterwrite-1302252842\.file\.myqcloud\.com|https://s2\.loli\.net)'),
new workbox.strategies.CacheFirst({
cacheName: 'image-cache' + cacheVersion,
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
// Suffix matching for other static files not matched by domain
workbox.routing.registerRoute(
new RegExp('.*\.(?:png|jpg|jpeg|svg|gif|webp|ico)'),
new workbox.strategies.StaleWhileRevalidate()
);
workbox.routing.registerRoute(
new RegExp('.*\.(css|js)'),
new workbox.strategies.StaleWhileRevalidate()
);
// Default match for remaining requests
workbox.routing.setDefaultHandler(
// Prefer cache, if cache is not available, use network request
new workbox.strategies.NetworkFirst({
networkTimeoutSeconds: 3,
})
);
} else {
console.log(`Boo! Workbox didn't load 😬`);
}
&lt;/code>&lt;/pre>
&lt;/details>
&lt;h3 id="manifestjson">
&lt;a href="#manifestjson" class="header-anchor">#&lt;/a>
manifest.json
&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>Create manifest.json file&lt;/strong>&lt;/li>
&lt;/ol>
&lt;p>Create a &lt;code>manifest.json&lt;/code> file in the &lt;code>static&lt;/code> folder at the root directory of your Hugo blog, which contains metadata about your blog, such as name, icon, and display options.&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;name&amp;quot;: &amp;quot;Your Blog Name&amp;quot;,
&amp;quot;short_name&amp;quot;: &amp;quot;Blog Short Name&amp;quot;,
&amp;quot;start_url&amp;quot;: &amp;quot;/&amp;quot;,
&amp;quot;display&amp;quot;: &amp;quot;standalone&amp;quot;,
&amp;quot;background_color&amp;quot;: &amp;quot;#ffffff&amp;quot;,
&amp;quot;theme_color&amp;quot;: &amp;quot;#000000&amp;quot;,
&amp;quot;icons&amp;quot;: [{
&amp;quot;src&amp;quot;: &amp;quot;/icon-192x192.png&amp;quot;,
&amp;quot;sizes&amp;quot;: &amp;quot;192x192&amp;quot;,
&amp;quot;type&amp;quot;: &amp;quot;image/png&amp;quot;
},
{
&amp;quot;src&amp;quot;: &amp;quot;/icon-512x512.png&amp;quot;,
&amp;quot;sizes&amp;quot;: &amp;quot;512x512&amp;quot;,
&amp;quot;type&amp;quot;: &amp;quot;image/png&amp;quot;
}
]
}
&lt;/code>&lt;/pre>
&lt;blockquote class="alert-blockquote alert-note">
&lt;p class="alert-heading">
&lt;svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16">
&lt;path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z">&lt;/path>
&lt;/svg>
&lt;span>Note&lt;/span>
&lt;/p>
&lt;p>Note: Replace icon-192x192.png and icon-512x512.png with your own icon filenames. And make sure to place these two icon files in the &lt;code>static&lt;/code> folder of your Hugo blog. If you want to modify the theme color and background color, you can modify the theme_color and background_color fields.&lt;/p>
&lt;/blockquote>
&lt;ol start="2">
&lt;li>&lt;strong>Link manifest.json file&lt;/strong>&lt;/li>
&lt;/ol>
&lt;p>In your Hugo blog&amp;rsquo;s &lt;code>layouts/partials/head/custom.html&lt;/code> file, add the following code to link the &lt;code>manifest.json&lt;/code> file to your website:&lt;/p>
&lt;pre>&lt;code class="language-html">&amp;lt;link rel=&amp;quot;manifest&amp;quot; href=&amp;quot;/manifest.json&amp;quot;&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>After completing the above steps, your Hugo blog will have PWA functionality, allowing users to access your site as if it were a native application.&lt;/p>
&lt;h2 id="references">
&lt;a href="#references" class="header-anchor">#&lt;/a>
References
&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;a class="link" href="https://web.dev/articles/offline-cookbook?hl=zh-cn" target="_blank" rel="noopener" >Offline Cookbook
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a class="link" href="https://developers.google.com/web/tools/workbox" target="_blank" rel="noopener" >Workbox
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a class="link" href="https://github.com/GoogleChrome/workbox" target="_blank" rel="noopener" >Workbox Github
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>Ollama: From Beginner to Advanced</title><link>https://cuterwrite.top/en/p/ollama/</link><pubDate>Sat, 15 Jun 2024 23:10:00 +0000</pubDate><guid>https://cuterwrite.top/en/p/ollama/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116903387_p0_master1200.webp" alt="Featured image of post Ollama: From Beginner to Advanced" />&lt;p>In recent years, large language models (LLM) have become a cornerstone of the artificial intelligence field due to their powerful text generation and understanding capabilities. Commercial LLMs are often expensive and have closed-source code, limiting the exploration space for researchers and developers. Fortunately, the open-source community offers excellent alternatives like Ollama, allowing everyone to easily experience the charm of LLMs and combine them with HPC and IDE plugins to create more powerful personal assistants.&lt;/p>
&lt;h2 id="what-is-ollama">
&lt;a href="#what-is-ollama" class="header-anchor">#&lt;/a>
What is Ollama?
&lt;/h2>
&lt;p>Ollama is a tool for building large language model applications. It offers a simple and easy-to-use command line interface and server, allowing you to easily download, run, and manage various open-source LLMs. Unlike traditional LLMs that require complex configurations and powerful hardware, Ollama allows you to conveniently experience the powerful features of LLMs as if you were using a mobile app.&lt;/p>
&lt;h2 id="advantages-of-ollama">
&lt;a href="#advantages-of-ollama" class="header-anchor">#&lt;/a>
Advantages of Ollama
&lt;/h2>
&lt;p>Ollama has the following significant advantages:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Open source and free&lt;/strong>: Ollama and its supported models are completely open source and free, allowing anyone to use, modify, and distribute them freely.&lt;/li>
&lt;li>&lt;strong>Simple and Easy to Use&lt;/strong>: No need for complex configuration and installation processes, Ollama can be started and run with just a few commands.&lt;/li>
&lt;li>&lt;strong>Rich Model&lt;/strong>: Ollama supports many popular open-source LLMs such as Llama 3, Mistral, Qwen2, and provides one-click download and switching features.&lt;/li>
&lt;li>&lt;strong>Low resource consumption&lt;/strong>: Compared to commercial LLMs, Ollama has lower hardware requirements and can run smoothly even on ordinary laptops.&lt;/li>
&lt;li>&lt;strong>Community Activity&lt;/strong>: Ollama has a large and active community where users can easily get help, share experiences, and participate in model development.&lt;/li>
&lt;/ul>
&lt;h2 id="how-to-use-ollama">
&lt;a href="#how-to-use-ollama" class="header-anchor">#&lt;/a>
How to use Ollama?
&lt;/h2>
&lt;p>Using Ollama is very simple, just follow these steps:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Install Ollama&lt;/strong>: Download and install the latest version from the &lt;a class="link" href="https://ollama.com/" target="_blank" rel="noopener" >Ollama official website
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
according to your operating system.&lt;/li>
&lt;li>&lt;strong>Start Ollama&lt;/strong>: Open the terminal or command line and enter the &lt;code>ollama serve&lt;/code> command to start the Ollama server.&lt;/li>
&lt;li>&lt;strong>Download Model&lt;/strong>: Find the desired model in the &lt;a class="link" href="https://ollama.com/library" target="_blank" rel="noopener" >model library
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, then use the &lt;code>ollama pull&lt;/code> command to download it, for example, &lt;code>ollama pull llama3:70b&lt;/code>.&lt;/li>
&lt;li>&lt;strong>Run the model&lt;/strong>: Use the &lt;code>ollama run&lt;/code> command to start the model, for example &lt;code>ollama run llama3:70b&lt;/code>.&lt;/li>
&lt;li>&lt;strong>Start chatting&lt;/strong>: Enter your question or command in the terminal, and Ollama will generate a corresponding response based on the model.&lt;/li>
&lt;/ol>
&lt;h3 id="install-ollama">
&lt;a href="#install-ollama" class="header-anchor">#&lt;/a>
Install Ollama
&lt;/h3>
&lt;h4 id="macos">
&lt;a href="#macos" class="header-anchor">#&lt;/a>
macOS
&lt;/h4>
&lt;p>&lt;a class="link" href="https://ollama.com/download/Ollama-darwin.zip" target="_blank" rel="noopener" >Download Ollama for macOS
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h4 id="windows">
&lt;a href="#windows" class="header-anchor">#&lt;/a>
Windows
&lt;/h4>
&lt;p>&lt;a class="link" href="https://ollama.com/download/OllamaSetup.exe" target="_blank" rel="noopener" >Download Ollama for Windows
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h4 id="linux">
&lt;a href="#linux" class="header-anchor">#&lt;/a>
Linux
&lt;/h4>
&lt;pre>&lt;code class="language-bash">curl -fsSL https://ollama.com/install.sh | sh
&lt;/code>&lt;/pre>
&lt;h4 id="docker">
&lt;a href="#docker" class="header-anchor">#&lt;/a>
Docker
&lt;/h4>
&lt;h5 id="cpu-version">
&lt;a href="#cpu-version" class="header-anchor">#&lt;/a>
CPU version
&lt;/h5>
&lt;pre>&lt;code class="language-bash">docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
&lt;/code>&lt;/pre>
&lt;h5 id="gpu-version">
&lt;a href="#gpu-version" class="header-anchor">#&lt;/a>
GPU version
&lt;/h5>
&lt;ol>
&lt;li>Install &lt;a class="link" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation" target="_blank" rel="noopener" >Nvidia container toolkit
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;li>Run Ollama in a Docker container&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
&lt;/code>&lt;/pre>
&lt;h3 id="launch-ollama">
&lt;a href="#launch-ollama" class="header-anchor">#&lt;/a>
Launch Ollama
&lt;/h3>
&lt;pre>&lt;code class="language-bash">ollama serve
&lt;/code>&lt;/pre>
&lt;p>Output the following information indicating that the Ollama server has successfully started (V100 machine):&lt;/p>
&lt;pre>&lt;code class="language-bash">$ ollama serve
### Omitted log output ###
Listening on [::]:11434 (version 0.1.42)
&lt;/code>&lt;/pre>
&lt;h3 id="download-model">
&lt;a href="#download-model" class="header-anchor">#&lt;/a>
Download model
&lt;/h3>
&lt;pre>&lt;code class="language-bash">ollama pull qwen2:72b
&lt;/code>&lt;/pre>
&lt;h3 id="run-model">
&lt;a href="#run-model" class="header-anchor">#&lt;/a>
Run model
&lt;/h3>
&lt;pre>&lt;code class="language-bash">ollama run qwen2:72b
&lt;/code>&lt;/pre>
&lt;p>For example, after running the following command:&lt;/p>
&lt;pre>&lt;code class="language-bash">You are trained on data up to October 2023.
&lt;/code>&lt;/pre>
&lt;h4 id="run-model-in-docker-container">
&lt;a href="#run-model-in-docker-container" class="header-anchor">#&lt;/a>
Run model in Docker container
&lt;/h4>
&lt;pre>&lt;code class="language-bash">docker exec -it ollama ollama run qwen2:72b
&lt;/code>&lt;/pre>
&lt;h3 id="configure-ollama">
&lt;a href="#configure-ollama" class="header-anchor">#&lt;/a>
Configure Ollama
&lt;/h3>
&lt;p>Ollama provides a variety of environment variables for configuration:&lt;/p>
&lt;ul>
&lt;li>&lt;code>OLLAMA_DEBUG&lt;/code>: Whether to enable debug mode, default is &lt;code>false&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_FLASH_ATTENTION&lt;/code>: Whether to flash attention, default is &lt;code>true&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_HOST&lt;/code>: Host address of the Ollama server, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_KEEP_ALIVE&lt;/code>: Time to keep the connection alive, default is &lt;code>5m&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_LLM_LIBRARY&lt;/code>: LLM library, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_LOADED_MODELS&lt;/code>: Maximum number of loaded models, default is &lt;code>1&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_QUEUE&lt;/code>: Maximum number of queues, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_VRAM&lt;/code>: Maximum virtual memory, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_MODELS&lt;/code>: Model directory, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_NOHISTORY&lt;/code>: Whether to save history, defaults to &lt;code>false&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_NOPRUNE&lt;/code>: Whether to enable pruning, default is &lt;code>false&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_NUM_PARALLEL&lt;/code>: Number of parallels, default is &lt;code>1&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_ORIGINS&lt;/code>: Allowed origins, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_RUNNERS_DIR&lt;/code>: Runner directory, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_SCHED_SPREAD&lt;/code>: Scheduling distribution, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_TMPDIR&lt;/code>: Temporary file directory, defaults to empty. Here is the optimized list in the desired format:&lt;/li>
&lt;li>&lt;code>OLLAMA_DEBUG&lt;/code>: Whether to enable debug mode, default is &lt;code>false&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_FLASH_ATTENTION&lt;/code>: Whether to flash attention, default is &lt;code>true&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_HOST&lt;/code>: Host address of the Ollama server, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_KEEP_ALIVE&lt;/code>: Duration to keep the connection alive, default is &lt;code>5m&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_LLM_LIBRARY&lt;/code>: LLM library, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_LOADED_MODELS&lt;/code>: Maximum number of loaded models, default is &lt;code>1&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_QUEUE&lt;/code>: Maximum queue number, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_VRAM&lt;/code>: Maximum virtual memory, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_MODELS&lt;/code>: Model directory, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_NOHISTORY&lt;/code>: Whether to save history, default is &lt;code>false&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_NOPRUNE&lt;/code>: Whether to enable pruning, default is &lt;code>false&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_NUM_PARALLEL&lt;/code>: Number of parallels, default is &lt;code>1&lt;/code>.&lt;/li>
&lt;li>&lt;code>OLLAMA_ORIGINS&lt;/code>: Allowed origins, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_RUNNERS_DIR&lt;/code>: Runner directory, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_SCHED_SPREAD&lt;/code>: Scheduling distribution, default is empty.&lt;/li>
&lt;li>&lt;code>OLLAMA_TMPDIR&lt;/code>: Temporary file directory, default is empty.&lt;/li>
&lt;/ul>
&lt;h2 id="advanced-usage-deploying-ollama-on-an-hpc-cluster">
&lt;a href="#advanced-usage-deploying-ollama-on-an-hpc-cluster" class="header-anchor">#&lt;/a>
Advanced Usage: Deploying Ollama on an HPC Cluster
&lt;/h2>
&lt;p>For large models or situations requiring higher performance, the powerful computing power of an HPC cluster can be used to run Ollama. By combining with Slurm for task management and using port mapping to expose the service locally, remote access and use can be conveniently achieved:&lt;/p>
&lt;ol>
&lt;li>Configure the Ollama environment on the login node: Install Ollama and download the required models.&lt;/li>
&lt;li>&lt;strong>Write a slurm script&lt;/strong>: Specify resource requirements (CPU, memory, GPU, etc.), and use the &lt;code>ollama serve&lt;/code> command to start the model service and bind it to a specific port.&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">#!/bin/bash
#SBATCH --job-name=ollama
#SBATCH -N 1
#SBATCH -p GPU
#SBATCH --gres=gpu:1
module load CUDA
ollama serve
&lt;/code>&lt;/pre>
&lt;ol start="3">
&lt;li>&lt;strong>Submit slurm task&lt;/strong>: Use the &lt;code>sbatch&lt;/code> command to submit the script, Slurm will allocate the task to compute nodes for execution.&lt;/li>
&lt;li>&lt;strong>Local Port Mapping&lt;/strong>: Use the ssh -L command to map the compute node&amp;rsquo;s port to the local machine, for example:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">ssh -t -t username@login node ip -L 11434:localhost:11434 -i login node private key ssh compute node IP -L 11434:127.0.0.1:11434
&lt;/code>&lt;/pre>
&lt;ol start="5">
&lt;li>&lt;strong>Local Access&lt;/strong>: Access http://localhost:11434 in a browser or application to use the Ollama service.&lt;/li>
&lt;/ol>
&lt;blockquote class="alert-blockquote alert-important">
&lt;p class="alert-heading">
&lt;svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16">
&lt;path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v9.5A1.75 1.75 0 0 1 14.25 13H8.06l-2.573 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25Zm7 2.25v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 9a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z">&lt;/path>
&lt;/svg>
&lt;span>Important&lt;/span>
&lt;/p>
&lt;p>Note: Since the compute node is not connected to the internet, you need to use &lt;code>ollama pull&lt;/code> on the login node in advance to download the required model. Additionally, you need to set &lt;code>OLLAMA_ORIGINS&lt;/code> to &lt;code>*&lt;/code> and &lt;code>OLLAMA_HOST&lt;/code> to &lt;code>0.0.0.0&lt;/code> to allow all sources to access the service.&lt;/p>
&lt;/blockquote>
&lt;h2 id="advanced-usage-local-code-completion-assistant">
&lt;a href="#advanced-usage-local-code-completion-assistant" class="header-anchor">#&lt;/a>
Advanced Usage: Local Code Completion Assistant
&lt;/h2>
&lt;p>Ollama can not only be used for chatting and text creation but also for creating a powerful code completion assistant by combining code generation models and IDE plugins. For example, using the Codeqwen 7B model and the VS Code plugin &lt;a class="link" href="https://continue.dev/" target="_blank" rel="noopener" >Continue
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
, efficient and convenient code completion functionality can be achieved.&lt;/p>
&lt;p>First, let me introduce Continue:
&lt;blockquote>
&lt;p>&lt;a class="link" href="https://continue.dev/" target="_blank" rel="noopener" >Continue
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
allows you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs. All of this can run entirely on your own laptop or have Ollama deployed on a server to remotely power code completion and chat experiences based on your needs.&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Continue&lt;/span>&lt;cite>&lt;/cite>&lt;/span>&lt;/blockquote>&lt;/p>
&lt;p>Before starting, you need to install the following tools:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://docs.continue.dev/quickstart" target="_blank" rel="noopener" >Continue
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
: &lt;a class="link" href="https://marketplace.visualstudio.com/items?itemName=Continue.continue" target="_blank" rel="noopener" >VS Code Version
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
or &lt;a class="link" href="https://plugins.jetbrains.com/plugin/22707-continue" target="_blank" rel="noopener" >JetBrains Version
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;li>&lt;a class="link" href="https://ollama.com/" target="_blank" rel="noopener" >Ollama
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;/ul>
&lt;p>Next, using VS Code as an example, we will introduce how to use Ollama + Continue to implement code completion functionality:&lt;/p>
&lt;h3 id="codestral-22b-model">
&lt;a href="#codestral-22b-model" class="header-anchor">#&lt;/a>
Codestral 22B Model
&lt;/h3>
&lt;p>Codestral is capable of both code auto-completion and chat functionality. However, given that it has 22 billion parameters and lacks a production license, it requires a significant amount of video memory and is limited to research and testing use, so it may not be suitable for everyday local applications.&lt;/p>
&lt;h4 id="download-and-run-the-codestral-model">
&lt;a href="#download-and-run-the-codestral-model" class="header-anchor">#&lt;/a>
Download and run the Codestral model
&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run codestral
&lt;/code>&lt;/pre>
&lt;h4 id="configure-configjson">
&lt;a href="#configure-configjson" class="header-anchor">#&lt;/a>
Configure config.json
&lt;/h4>
&lt;ul>
&lt;li>In the VS Code sidebar, click the Continue plugin icon, then click the &amp;ldquo;gear&amp;rdquo; icon at the bottom right of the panel to open the &lt;code>config.json&lt;/code> file. Then copy the following configuration into the &lt;code>config.json&lt;/code> file:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-json">{
&amp;quot;models&amp;quot;: [
{
&amp;quot;title&amp;quot;: &amp;quot;Codestral&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;codestral&amp;quot;
}
],
&amp;quot;tabAutocompleteModel&amp;quot;: {
&amp;quot;title&amp;quot;: &amp;quot;Codestral&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;codestral&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="deepseek-coder-67b-model--llama-3-8b-model">
&lt;a href="#deepseek-coder-67b-model--llama-3-8b-model" class="header-anchor">#&lt;/a>
DeepSeek Coder 6.7B model + Llama 3 8B model
&lt;/h3>
&lt;p>Depending on the machine&amp;rsquo;s VRAM size, you can utilize Ollama&amp;rsquo;s ability to run multiple models simultaneously and handle multiple concurrent requests, using &lt;code>DeepSeek Coder 6.7B&lt;/code> for auto-completion, and &lt;code>Llama 3 8B&lt;/code> for chatting. If your machine cannot run both at the same time, you can try them separately to decide whether you prefer the local auto-completion or the local chat experience.&lt;/p>
&lt;h4 id="download-and-run-the-deepseek-coder-model">
&lt;a href="#download-and-run-the-deepseek-coder-model" class="header-anchor">#&lt;/a>
Download and run the DeepSeek Coder model
&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run deepseek-coder:6.7b-base
&lt;/code>&lt;/pre>
&lt;h4 id="download-and-run-the-llama-3-model">
&lt;a href="#download-and-run-the-llama-3-model" class="header-anchor">#&lt;/a>
Download and run the Llama 3 model
&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run llama3:8b
&lt;/code>&lt;/pre>
&lt;h4 id="configure-configjson-1">
&lt;a href="#configure-configjson-1" class="header-anchor">#&lt;/a>
Configure config.json
&lt;/h4>
&lt;pre>&lt;code class="language-json">{
&amp;quot;models&amp;quot;: [
{
&amp;quot;title&amp;quot;: &amp;quot;Llama 3 8B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;llama3:8b&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
],
&amp;quot;tabAutocompleteModel&amp;quot;: {
&amp;quot;title&amp;quot;: &amp;quot;DeepSeek Coder 6.7B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;deepseek-coder:6.7b-base&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="codeqwen-7b-model--qwen2-7b-model">
&lt;a href="#codeqwen-7b-model--qwen2-7b-model" class="header-anchor">#&lt;/a>
Codeqwen 7B model + Qwen2 7B model
&lt;/h3>
&lt;p>The Codeqwen 7B model is a model specifically designed for code completion, while the Qwen2 7B model is a general-purpose chat model. These two models can be well combined to achieve both code completion and chat functions.&lt;/p>
&lt;h4 id="download-and-run-the-codeqwen-model">
&lt;a href="#download-and-run-the-codeqwen-model" class="header-anchor">#&lt;/a>
Download and run the Codeqwen model
&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run codeqwen
&lt;/code>&lt;/pre>
&lt;h4 id="download-and-run-the-qwen2-model">
&lt;a href="#download-and-run-the-qwen2-model" class="header-anchor">#&lt;/a>
Download and run the Qwen2 model
&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run qwen2:7b
&lt;/code>&lt;/pre>
&lt;h4 id="configure-configjson-2">
&lt;a href="#configure-configjson-2" class="header-anchor">#&lt;/a>
Configure config.json
&lt;/h4>
&lt;pre>&lt;code class="language-json">{
&amp;quot;models&amp;quot;: [
{
&amp;quot;title&amp;quot;: &amp;quot;Codeqwen 7B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;codeqwen&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
],
&amp;quot;tabAutocompleteModel&amp;quot;: {
&amp;quot;title&amp;quot;: &amp;quot;Qwen2 7B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;qwen2:7b&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="optimize-chat-using-rag-vector-retrieval">
&lt;a href="#optimize-chat-using-rag-vector-retrieval" class="header-anchor">#&lt;/a>
Optimize chat using RAG vector retrieval
&lt;/h3>
&lt;p>Continue has a built-in &lt;a class="link" href="https://docs.continue.dev/customization/context-providers#codebase-retrieval" target="_blank" rel="noopener" >@codebase
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
context provider that can automatically retrieve the most relevant code snippets from the codebase. If you have set up a chat model (such as Codestral, Llama 3), then with the help of Ollama and &lt;a class="link" href="https://blog.lancedb.com/lancedb-x-continue/" target="_blank" rel="noopener" >LanceDB
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&amp;rsquo;s vectorization technology, you can achieve more efficient code retrieval and chat experience.&lt;/p>
&lt;p>Here, we use the &lt;code>nomic-embed-text&lt;/code> model as the vector retrieval model:&lt;/p>
&lt;h4 id="download-and-run-the-nomic-embed-text-model">
&lt;a href="#download-and-run-the-nomic-embed-text-model" class="header-anchor">#&lt;/a>
Download and run the Nomic Embed Text model
&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run nomic-embed-text
&lt;/code>&lt;/pre>
&lt;h4 id="configure-configjson-3">
&lt;a href="#configure-configjson-3" class="header-anchor">#&lt;/a>
Configure config.json
&lt;/h4>
&lt;ul>
&lt;li>Add the following content to the file:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-json">{
&amp;quot;embeddingsProvider&amp;quot;: {
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;nomic-embed-text&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="code-completion-effect">
&lt;a href="#code-completion-effect" class="header-anchor">#&lt;/a>
Code completion effect
&lt;/h3>
&lt;ul>
&lt;li>&lt;code>Ctrl + I&lt;/code>: Generate code snippet based on instructions.&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_1-2024-06-17.webp"
alt="codeqwen_1-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_2-2024-06-17.webp"
alt="codeqwen_2-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_3-2024-06-17.webp"
alt="codeqwen_3-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>Cursor hover auto-complete code&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_4-2024-06-17.webp"
alt="codeqwen_4-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;h3 id="chat-with-ollama">
&lt;a href="#chat-with-ollama" class="header-anchor">#&lt;/a>
Chat with Ollama
&lt;/h3>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_5-2024-06-17.webp"
alt="codeqwen_5-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;h3 id="code-auto-comment">
&lt;a href="#code-auto-comment" class="header-anchor">#&lt;/a>
Code auto-comment
&lt;/h3>
&lt;ul>
&lt;li>Select code to open the right-click menu&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_6-2024-06-17.webp"
alt="codeqwen_6-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_7-2024-06-17.webp"
alt="codeqwen_7-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;h2 id="summary">
&lt;a href="#summary" class="header-anchor">#&lt;/a>
Summary
&lt;/h2>
&lt;p>Ollama has opened the door to the world of open-source LLM, allowing everyone to easily experience the powerful features of LLM and customize applications according to their own needs. Whether for research, development, or daily use, Ollama can provide you with a platform to explore the limitless possibilities of LLM. With the continuous development of Ollama, it is believed that it will bring us more surprises and promote the application and development of LLM technology in various fields.&lt;/p></description></item></channel></rss>