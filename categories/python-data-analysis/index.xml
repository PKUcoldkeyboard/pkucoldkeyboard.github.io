<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Python-数据分析 on Cuterwrite's Blog</title><link>https://cuterwrite.top/categories/python-data-analysis/</link><description>Recent content in Python-数据分析 on Cuterwrite's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>cuterwrite</copyright><lastBuildDate>Tue, 11 Jul 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://cuterwrite.top/categories/python-data-analysis/index.xml" rel="self" type="application/rss+xml"/><item><title>SVD 与 NMF：矩阵分解的两种方法</title><link>https://cuterwrite.top/p/matrix-factorization/</link><pubDate>Tue, 11 Jul 2023 00:00:00 +0000</pubDate><guid>https://cuterwrite.top/p/matrix-factorization/</guid><description>&lt;img src="https://cloud.cuterwrite.fun/blog/65cf6588fa725014c7cd617ccbeb997f27742e49.jpg@1256w_1880h_!web-article-pic.webp" alt="Featured image of post SVD 与 NMF：矩阵分解的两种方法" />&lt;h1 id="svd-与-nmf矩阵分解的两种方法">
&lt;a href="#svd-%e4%b8%8e-nmf%e7%9f%a9%e9%98%b5%e5%88%86%e8%a7%a3%e7%9a%84%e4%b8%a4%e7%a7%8d%e6%96%b9%e6%b3%95" class="header-anchor">#&lt;/a>
SVD 与 NMF：矩阵分解的两种方法
&lt;/h1>
&lt;p>在数据科学中，矩阵分解技术是一种强大的工具，可以用于各种应用，如推荐系统、图像处理和自然语言处理。在这篇文章中，我们将深入探讨两种流行的矩阵分解技术：奇异值分解（SVD）和非负矩阵分解（NMF）。我们将详细解析它们的理论基础，以及如何在实际问题中应用它们。&lt;/p>
&lt;h2 id="一奇异值分解svd">
&lt;a href="#%e4%b8%80%e5%a5%87%e5%bc%82%e5%80%bc%e5%88%86%e8%a7%a3svd" class="header-anchor">#&lt;/a>
一、奇异值分解（SVD）
&lt;/h2>
&lt;p>奇异值分解是一种在线性代数中常用的矩阵分解方法。对于给定的 $m\times n$ 矩阵 A，我们可以将其分解为三个矩阵的乘积：&lt;/p>
&lt;p>$$
A = U\Sigma V^T
$$&lt;/p>
&lt;p>这里，$U$ 是一个 $m\times m$ 的正交矩阵，$V$ 是一个 $n\times n$ 的正交矩阵，$\Sigma$ 是一个 $m\times n$ 的对角矩阵。对角线上的元素称为奇异值，它们是 $A^T A$ 的特征值的平方根。它们是按降序排列的，代表了原始矩阵中的“能量”或信息量。。我们可以将奇异值分解看作是一种特征值分解，其中 $U$ 和 $V$ 是特征向量，$\Sigma$ 是特征值的对角矩阵。&lt;/p>
&lt;p>计算 SVD 的基本步骤如下：&lt;/p>
&lt;ol>
&lt;li>&lt;!-- raw HTML omitted -->构造矩阵 A 的 Gram 矩阵&lt;!-- raw HTML omitted -->：对于给定的 $m\times n$ 矩阵 A，我们可以构造一个 $n\times n$ 的矩阵 $A^T A$，称为 A 的 Gram 矩阵。Gram 矩阵是一个对称半正定矩阵，因此它的特征值都是非负的。&lt;/li>
&lt;li>&lt;!-- raw HTML omitted -->计算 Gram 矩阵的特征值和特征向量&lt;!-- raw HTML omitted -->：我们可以使用任何标准的特征值分解算法来计算 Gram 矩阵的特征值和特征向量。这些特征值就是 A 的奇异值的平方，特征向量则构成了右奇异向量和左奇异向量。我们将特征值按降序排列，将特征向量按相同的顺序排列。&lt;/li>
&lt;li>&lt;!-- raw HTML omitted -->构造奇异值矩阵 $\Sigma$ &lt;!-- raw HTML omitted -->：我将特征值的平方根按照从大到小的顺序排列在对角线上，构成 $m\times n $ 的对角矩阵 $\Sigma$ 。&lt;/li>
&lt;li>&lt;!-- raw HTML omitted -->构造左奇异向量矩阵 $U$ 和右奇异向量矩阵 $V$ &lt;!-- raw HTML omitted -->：将对应于特征值的特征向量按照特征值的顺序排列，构成 $n\times n$ 的矩阵 $V$ 和 $m\times m$ 的矩阵 $U$ 。这些特征向量是标准化的，即它们的长度为 1，并且互相正交。&lt;/li>
&lt;/ol>
&lt;p>这样，我们就得到了 A 的奇异值分解。在 Python 中，我们可以使用 NumPy 的&lt;code>np.linalg.svd&lt;/code> 函数来计算 SVD，这个函数会自动执行上述步骤，并返回 $ U, \Sigma, V^T $ 。如下所示：&lt;/p>
&lt;pre>&lt;code class="language-python">import numpy as np
U, S, Vt = np.linalg.svd(A)
&lt;/code>&lt;/pre>
&lt;p>需要注意的是，虽然理论上 SVD 总是存在的，但在实际计算中可能会遇到数值稳定性的问题。此外，对于非常大的矩阵，计算 SVD 可能会非常耗时。在这些情况下，我们可能需要使用一些更高效的算法或者近似方法，如随机 SVD。&lt;/p>
&lt;p>SVD 的一个重要应用是在推荐系统中进行矩阵补全。在推荐系统中，我们通常有一个用户-商品评分矩阵，但这个矩阵通常是非常稀疏的，因为大多数用户只评价了少数商品。SVD 可以用于预测用户对未评价商品的评分，从而提供个性化的推荐。&lt;/p>
&lt;h2 id="二非负矩阵分解nmf">
&lt;a href="#%e4%ba%8c%e9%9d%9e%e8%b4%9f%e7%9f%a9%e9%98%b5%e5%88%86%e8%a7%a3nmf" class="header-anchor">#&lt;/a>
二、非负矩阵分解（NMF）
&lt;/h2>
&lt;p>由于维度的复杂性和维度诅咒，直接处理高维数据需要大量的计算资源。非负矩阵分解（NMF）作为一种降维技术被提出，在图像处理中得到了重要的应用。通过采用 NMF，非负的高维矩阵可以被分解成两个非负的低维矩阵，其中一个包括列向量，可以被视为数据空间中的基向量，另一个则包含缩放这些基向量的系数行。此外，NMF 也可用于文本数据处理。我们可以检查系数矩阵中的每一列，并确定具有最大系数的行号，其中行号表示原始矩阵中各列的聚类 ID。这种聚类特性意味着 NMF 可以用于数据聚类。&lt;/p>
&lt;p>NMF 对矩阵的元素有一个额外的非负约束。对于给定的 $K\times N$ 非负矩阵 $M\in R^{K\times N}$ ，我们可以找到两个非负矩阵 $W$ 和 $H$ ，使得 $M\approx WH$ 。其中 $W\in R^{K\times r}$ 和 $H\in R^{r\times N}$ 是两个非负矩阵，即 $W\geq 0$ 和 $H\geq 0$ 。矩阵 $W$ 代表捕捉数据特征的基向量，而矩阵 $H$ 是表示每个基向量对重建原始数据的贡献的权重。NMF 中的非负约束允许学习整体数据的部分表征，而这种约束在 SVD 中是不允许的。为了找到 $M\approx WH$的近似解，定义基于欧氏距离的成本函数来量化近似的质量，即:&lt;/p>
&lt;p>$$
Q=\Vert M-WH\Vert^2_F=\sum_{i,j}(M_{ij}-(WH)_{ij})^2
$$&lt;/p>
&lt;p>由于成本函数 $Q$ 在 $W$ 和 $H$ 中都是非凸的，所以在求解 $Q$ 的最小值过程中找到全局最小值是不现实的。一些数值优化技术，如梯度下降和共轭梯度，可以被用来寻找局部最小值。然而，梯度下降的收敛速度很慢，共轭梯度的实现很复杂。此外，基于梯度的方法对步长的参数设置很敏感，这对现实应用来说并不方便。为此，可以利用 $W$ 和 $H$ 的 multiplicative update rules，作为收敛速度和实现复杂性之间的折中方案，具体如下:&lt;/p>
&lt;p>$$
H_{aj} \leftarrow H_{aj} \frac{W^T M_{aj}}{W^T W H_{aj}}, W_{ia} \leftarrow W_{ia} \frac{M H^T_{ia}}{W H H^T_{ia}}
$$&lt;/p>
&lt;p>其中，矩阵 $W$ 和 $H$ 可以被随机初始化，然后通过迭代更新来优化 $Q$ 。这些更新规则可以保证 $Q$ 在每次迭代中都会减少，因此可以保证收敛到局部最小值。&lt;/p>
&lt;p>在 Python 中，我们可以使用&lt;code>sklearn.decomposition.NMF&lt;/code> 类来计算 NMF。如下所示：&lt;/p>
&lt;pre>&lt;code class="language-python">from sklearn.decomposition import NMF
model = NMF(n_components=2, init='random', random_state=0)
W = model.fit_transform(X)
H = model.components_
&lt;/code>&lt;/pre>
&lt;h2 id="三svd-与-nmf-的比较">
&lt;a href="#%e4%b8%89svd-%e4%b8%8e-nmf-%e7%9a%84%e6%af%94%e8%be%83" class="header-anchor">#&lt;/a>
三、SVD 与 NMF 的比较
&lt;/h2>
&lt;p>虽然 SVD 和 NMF 都是矩阵分解技术，但它们有一些重要的区别。&lt;/p>
&lt;ol>
&lt;li>&lt;!-- raw HTML omitted -->数据类型和约束&lt;!-- raw HTML omitted -->：SVD 可以应用于任何矩阵，而 NMF 只能应用于非负矩阵。其次，SVD 提供了一种最优的低秩近似，而 NMF 则没有这种保证。然而，NMF 的非负约束使得它的分解更具解释性，这在许多应用中是非常有用的。&lt;/li>
&lt;li>&lt;!-- raw HTML omitted -->分解的解释性&lt;!-- raw HTML omitted -->：虽然 SVD 和 NMF 都可以将原始矩阵分解为一些基本的“构成元素”，但 NMF 的分解通常更具解释性。这是因为 NMF 的非负约束使得分解的结果更容易解释和理解。在许多应用中，如主题模型或社区发现，NMF 的分解可以直接解释为数据的一些基本模式或特征。&lt;/li>
&lt;li>&lt;!-- raw HTML omitted -->优化和稳定性&lt;!-- raw HTML omitted -->：SVD 的优化问题有闭式解，这意味着我们可以直接计算出最优解。而 NMF 的优化问题通常需要使用迭代方法来求解，如梯度下降或坐标下降。这使得 NMF 的计算过程可能更复杂，而且可能需要更多的时间。而且 SVD 的结果是唯一的（除了奇异向量的符号），而 NMF 的结果可能依赖于初始化和优化算法。这意味着对同一个数据集，NMF 可能会给出不同的结果。&lt;/li>
&lt;li>&lt;!-- raw HTML omitted -->近似质量&lt;!-- raw HTML omitted -->：SVD 提供了一种最优的低秩近似，即它可以找到最接近原始矩阵的低秩矩阵。而 NMF 则没有这种保证，它的近似质量可能会比 SVD 差。然而，NMF 的非负约束使得它的近似可能更符合实际的需求，尤其是在那些原始数据是非负的情况下。&lt;/li>
&lt;li>&lt;!-- raw HTML omitted -->计算复杂性&lt;!-- raw HTML omitted -->：SVD 和 NMF 的计算复杂性也有所不同。对于一个 $m\times n$ 的矩阵，SVD 的计算复杂性大约为 $\mathbf{O}(\min {m^2n, mn^2})$ ，而 NMF 的计算复杂性则取决于迭代次数和所选的优化算法。在实践中，NMF 通常比 SVD 更慢，但也有一些高效的 NMF 算法可以缩短计算时间。&lt;/li>
&lt;/ol>
&lt;p>总的来说，SVD 和 NMF 各有优势，选择使用哪一种技术取决于具体的应用和需求。&lt;/p>
&lt;h2 id="四实战图像压缩">
&lt;a href="#%e5%9b%9b%e5%ae%9e%e6%88%98%e5%9b%be%e5%83%8f%e5%8e%8b%e7%bc%a9" class="header-anchor">#&lt;/a>
四、实战：图像压缩
&lt;/h2>
&lt;p>让我们通过一个实战演示来看看如何使用 SVD 和 NMF 进行图像压缩。我们将使用 Python 的 NumPy 和 scikit-learn 库来执行这些任务。&lt;/p>
&lt;p>首先，我们需要导入必要的库，并加载一张图像：&lt;/p>
&lt;pre>&lt;code class="language-python">import numpy as np
from sklearn.decomposition import NMF
from PIL import Image
# 加载图像
img = Image.open('image.jpg')
width, height = img.size
img = np.array(img)
r, g, b = img[:, :, 0], img[:, :, 1], img[:, :, 2]
img_matrix = []
img_matrix.extend([r.flatten(), g.flatten(), b.flatten()])
M = np.array(img_matrix).T
&lt;/code>&lt;/pre>
&lt;p>然后，我们可以使用 NumPy 的&lt;code>np.linalg.svd&lt;/code> 函数来进行 SVD，得到 $U, S, V^T$ ：&lt;/p>
&lt;pre>&lt;code class="language-python"># 执行 SVD
U, s, Vt = np.linalg.svd(M, full_matrices=False)
&lt;/code>&lt;/pre>
&lt;p>我们可以选择前 $r$ 个奇异值和对应的 $U$ 和 $V^T$ 的列来进行低秩近似：&lt;/p>
&lt;pre>&lt;code class="language-python">d = 16
U_d = U[:, :d]
s_d = s[:d]
Vt_d = Vt[:d, :]
M_d = U_d @ np.diag(s_d) @ Vt_d
r, g, b = M_d[:, 0], M_d[:, 1], M_d[:, 2]
img = np.dstack((r, g, b)).reshape(height, width, 3)
img[img &amp;lt; 0] = 0
img[img &amp;gt; 255] = 255
img = Image.fromarray(np.uint8(img), mode='RGB')
img.show()
&lt;/code>&lt;/pre>
&lt;p>同样，我们也可以使用 scikit-learn 的 NMF 类来进行 NMF：&lt;/p>
&lt;pre>&lt;code class="language-python"># 执行 NMF
model = NMF(n_components=16, init='random', random_state=0)
W = model.fit_transform(M)
H = model.components_
M_d = W @ H
r, g, b = M_d[:, 0], M_d[:, 1], M_d[:, 2]
img = np.dstack((r, g, b)).reshape(height, width, 3)
img[img &amp;lt; 0] = 0
img[img &amp;gt; 255] = 255
img = Image.fromarray(np.uint8(img), mode='RGB')
img.show()
&lt;/code>&lt;/pre>
&lt;p>最后，我们可以比较原始图像和重构图像的差异，以及 SVD 和 NMF 的压缩效果。这种压缩方法的优点是可以大大减少存储和传输图像所需的数据量，而且如果选择的秩 r 足够大，压缩后的图像的质量也可以接受。&lt;/p>
&lt;h2 id="五结论">
&lt;a href="#%e4%ba%94%e7%bb%93%e8%ae%ba" class="header-anchor">#&lt;/a>
五、结论
&lt;/h2>
&lt;p>总的来说，SVD 和 NMF 都是强大的矩阵分解技术，它们在许多数据科学应用中都有广泛的用途。虽然 SVD 提供了一种最优的低秩近似，但 NMF 的非负约束使得它的分解更具解释性。在选择使用哪一种技术时，我们需要考虑我们的具体需求，以及我们的数据是否满足这些技术的要求。&lt;/p>
&lt;h2 id="参考资料">
&lt;a href="#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99" class="header-anchor">#&lt;/a>
参考资料
&lt;/h2>
&lt;p>[1] Lee, Daniel, and H. Sebastian Seung. &amp;ldquo;Unsupervised learning by convex and conic coding. &amp;quot; Advances in neural information processing systems 9 (1996).&lt;/p>
&lt;p>[2] Lee, Daniel D., and H. Sebastian Seung. &amp;ldquo;Learning the parts of objects by non-negative matrix factorization.&amp;rdquo; Nature 401.6755 (1999): 788-791.&lt;/p>
&lt;p>[3] Lee, Daniel, and H. Sebastian Seung. &amp;ldquo;Algorithms for non-negative matrix factorization. &amp;quot; Advances in neural information processing systems 13 (2000).&lt;/p></description></item><item><title>机器学习重要术语词汇表</title><link>https://cuterwrite.top/p/machine-learning-terms/</link><pubDate>Thu, 15 Jul 2021 00:00:00 +0000</pubDate><guid>https://cuterwrite.top/p/machine-learning-terms/</guid><description>&lt;img src="https://cloud.cuterwrite.fun/blog/image.2lqvb7dnlg80.webp" alt="Featured image of post 机器学习重要术语词汇表" />&lt;h1 id="机器学习重要术语词汇表">
&lt;a href="#%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e9%87%8d%e8%a6%81%e6%9c%af%e8%af%ad%e8%af%8d%e6%b1%87%e8%a1%a8" class="header-anchor">#&lt;/a>
机器学习重要术语词汇表
&lt;/h1>
&lt;h2 id="精确度-accuracy">
&lt;a href="#%e7%b2%be%e7%a1%ae%e5%ba%a6-accuracy" class="header-anchor">#&lt;/a>
精确度 Accuracy
&lt;/h2>
&lt;p>在分类中，准确性是正确分类的项数目除以测试集内的项总数。 范围从 0（最不准确）到 1（最准确）。 准确性是模型性能的评估指标之一。 将其与 Precision、Recall 和 F-score 结合考虑。&lt;/p>
&lt;h2 id="曲线下面积-auc">
&lt;a href="#%e6%9b%b2%e7%ba%bf%e4%b8%8b%e9%9d%a2%e7%a7%af-auc" class="header-anchor">#&lt;/a>
曲线下面积 (AUC)
&lt;/h2>
&lt;p>二元分类的一项评估指标，即曲线下面积值，它绘制真阳性率（y 轴）与误报率（x 轴）进行对照。 范围从 0.5（最差）到 1（最佳）。 也称为 ROC 曲线下面积。&lt;/p>
&lt;h2 id="二元分类">
&lt;a href="#%e4%ba%8c%e5%85%83%e5%88%86%e7%b1%bb" class="header-anchor">#&lt;/a>
二元分类
&lt;/h2>
&lt;p>一个分类任务，其中标签仅为两个类中的一个。&lt;/p>
&lt;h2 id="校准">
&lt;a href="#%e6%a0%a1%e5%87%86" class="header-anchor">#&lt;/a>
校准
&lt;/h2>
&lt;p>校准是将原始分数映射到类成员身份的过程，用于二元和多类分类。&lt;/p>
&lt;h2 id="分类">
&lt;a href="#%e5%88%86%e7%b1%bb" class="header-anchor">#&lt;/a>
分类
&lt;/h2>
&lt;p>当使用这些数据来预测某一类别，有监督学习任务被称为“分类”。 二分类指的是仅预测两个类别（例如，将图像划分为“猫”或“狗”图片）。 多分类指的是预测多个类别（例如，当将图像划分为特定品种狗的图片）。&lt;/p>
&lt;h2 id="决定系数">
&lt;a href="#%e5%86%b3%e5%ae%9a%e7%b3%bb%e6%95%b0" class="header-anchor">#&lt;/a>
决定系数
&lt;/h2>
&lt;p>回归中的一项评估指标，表明数据与模型的匹配程度。 范围从 0 到 1。 值 0 表示数据是随机的，否则就无法与模型相匹配。 1 表示模型与数据完全匹配。 这通常称 r 平方值。&lt;/p>
&lt;h2 id="特征工程">
&lt;a href="#%e7%89%b9%e5%be%81%e5%b7%a5%e7%a8%8b" class="header-anchor">#&lt;/a>
特征工程
&lt;/h2>
&lt;p>特征工程是涉及定义一组特征和开发软件以从可用现象数据中生成特征向量（即特征提取）的过程。&lt;/p>
&lt;h2 id="f-score">
&lt;a href="#f-score" class="header-anchor">#&lt;/a>
F-score
&lt;/h2>
&lt;p>分类的一项评估指标，用于平衡 Precision 和 Recall&lt;/p>
&lt;h2 id="超参数">
&lt;a href="#%e8%b6%85%e5%8f%82%e6%95%b0" class="header-anchor">#&lt;/a>
超参数
&lt;/h2>
&lt;p>机器学习算法的参数。 示例包括在决策林中学习的树的数量，或者梯度下降算法中的步长。 在对模型进行定型之前，先设置超参数 的值，并控制查找预测函数参数的过程，例如，决策树中的比较点或线性回归模型中的权重。&lt;/p>
&lt;h2 id="label">
&lt;a href="#label" class="header-anchor">#&lt;/a>
Label
&lt;/h2>
&lt;p>使用机器学习模型进行预测的元素。 例如，狗的品种或将来的股票价格。&lt;/p>
&lt;h2 id="对数损失">
&lt;a href="#%e5%af%b9%e6%95%b0%e6%8d%9f%e5%a4%b1" class="header-anchor">#&lt;/a>
对数损失
&lt;/h2>
&lt;p>在分类中，描述分类器准确性的评估指标。 对数损失越小，分类器越准确。&lt;/p>
&lt;h2 id="损失函数">
&lt;a href="#%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0" class="header-anchor">#&lt;/a>
损失函数
&lt;/h2>
&lt;p>损失函数是指训练标签值与模型所做预测之间的差异。 通过最小化损失函数来估算模型参数。&lt;/p>
&lt;p>可以为不同的训练程序配置不同的损失函数。&lt;/p>
&lt;h2 id="平均绝对误差-mae">
&lt;a href="#%e5%b9%b3%e5%9d%87%e7%bb%9d%e5%af%b9%e8%af%af%e5%b7%ae-mae" class="header-anchor">#&lt;/a>
平均绝对误差 (MAE)
&lt;/h2>
&lt;p>回归中的一项评估指标，即所有模型误差的平均值，其中模型误差是预测&lt;a class="link" href="https://docs.microsoft.com/zh-cn/dotnet/machine-learning/resources/glossary?WT.mc_id=academic-15963-cxa#label" target="_blank" rel="noopener" >标签
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
值和正确标签值之间的差距。&lt;/p>
&lt;h2 id="多类分类">
&lt;a href="#%e5%a4%9a%e7%b1%bb%e5%88%86%e7%b1%bb" class="header-anchor">#&lt;/a>
多类分类
&lt;/h2>
&lt;p>一种分类任务，其中标签为三个或三个以上类中的一个。&lt;/p>
&lt;h2 id="n-元语法">
&lt;a href="#n-%e5%85%83%e8%af%ad%e6%b3%95" class="header-anchor">#&lt;/a>
N 元语法
&lt;/h2>
&lt;p>文本数据的特征提取方案：N 个单词的任何序列都将转变为特征值&lt;/p>
&lt;h2 id="标准化">
&lt;a href="#%e6%a0%87%e5%87%86%e5%8c%96" class="header-anchor">#&lt;/a>
标准化
&lt;/h2>
&lt;p>标准化是将浮点数据缩放到 0 到 1 之间的值的过程。&lt;/p>
&lt;h2 id="管道">
&lt;a href="#%e7%ae%a1%e9%81%93" class="header-anchor">#&lt;/a>
管道
&lt;/h2>
&lt;p>要将模型与数据集相匹配所需的所有操作。 管道由数据导入、转换、特征化和学习步骤组成。 对管道进行定型后，它会转变为模型。&lt;/p>
&lt;h2 id="precision">
&lt;a href="#precision" class="header-anchor">#&lt;/a>
Precision
&lt;/h2>
&lt;p>在分类中，Precision 是正确预测为属于该类的项目的数量，除以预测为属于该类的项目的总数。&lt;/p>
&lt;h2 id="recall">
&lt;a href="#recall" class="header-anchor">#&lt;/a>
Recall
&lt;/h2>
&lt;p>在分类中，Recall 是正确预测为属于该类的项目的数量，除以实际属于该类的项目的总数。&lt;/p>
&lt;h2 id="正则化">
&lt;a href="#%e6%ad%a3%e5%88%99%e5%8c%96" class="header-anchor">#&lt;/a>
正则化
&lt;/h2>
&lt;p>正则化会对过于复杂的线性模型进行惩罚。 正则化有两种类型：&lt;/p>
&lt;ul>
&lt;li>L1 正则化将无意义特征的权重归零。 进行这种正则化之后，所保存模型的大小可能会变小。&lt;/li>
&lt;li>L2 正则化将无意义特征的权重范围最小化。 这是一种更通用的过程，并且对离群值不太敏感。&lt;/li>
&lt;/ul>
&lt;h2 id="回归">
&lt;a href="#%e5%9b%9e%e5%bd%92" class="header-anchor">#&lt;/a>
回归
&lt;/h2>
&lt;p>有监督学习任务，其中输出是一个实际值，例如，双精度值。 示例包括预测股票价格。&lt;/p>
&lt;h2 id="相对绝对误差">
&lt;a href="#%e7%9b%b8%e5%af%b9%e7%bb%9d%e5%af%b9%e8%af%af%e5%b7%ae" class="header-anchor">#&lt;/a>
相对绝对误差
&lt;/h2>
&lt;p>回归中的一项评估指标，即所有绝对误差总和除以正确标签值和所有正确标签值的平均值之间的差值总和。&lt;/p>
&lt;h2 id="相对平方误差">
&lt;a href="#%e7%9b%b8%e5%af%b9%e5%b9%b3%e6%96%b9%e8%af%af%e5%b7%ae" class="header-anchor">#&lt;/a>
相对平方误差
&lt;/h2>
&lt;p>回归中的一项评估指标，即所有绝对平方误差总和除以正确标签值和所有正确标签值的平均值之间的平方差值总和。&lt;/p>
&lt;h2 id="均方误差根-rmse">
&lt;a href="#%e5%9d%87%e6%96%b9%e8%af%af%e5%b7%ae%e6%a0%b9-rmse" class="header-anchor">#&lt;/a>
均方误差根 (RMSE)
&lt;/h2>
&lt;p>误差平方平均值的平方根。&lt;/p></description></item></channel></rss>