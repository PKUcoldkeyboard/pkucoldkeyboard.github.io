<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>高性能计算 on Cuterwrite's Blog</title><link>https://cuterwrite.top/categories/hpc/</link><description>Recent content in 高性能计算 on Cuterwrite's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>cuterwrite</copyright><lastBuildDate>Wed, 26 Jun 2024 23:11:00 +0000</lastBuildDate><atom:link href="https://cuterwrite.top/categories/hpc/index.xml" rel="self" type="application/rss+xml"/><item><title>RDMA 之 Completion Queue</title><link>https://cuterwrite.top/p/rdma-completion-queue/</link><pubDate>Wed, 26 Jun 2024 23:11:00 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-completion-queue/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116903369_p0_master1200.webp" alt="Featured image of post RDMA 之 Completion Queue" />&lt;h1 id="rdma-之-completion-queue">RDMA 之 Completion Queue&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/259650980">&lt;cite>知乎专栏：10. RDMA 之 Completion Queue&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>我们曾经在前面的文章中简单介绍过 CQ，本文将更深入的讲解关于它的一些细节。阅读本文前，读者可以先温习一下这篇文章： &lt;a class="link" href="https://cuterwrite.top/p/rdma-element/" >“3. RDMA 基本元素”
&lt;/a>
。&lt;/p>
&lt;h2 id="基本概念">基本概念&lt;/h2>
&lt;p>我们先回顾下 CQ 的作用。CQ 意为完成队列，它的作用和 WQ（SQ 和 RQ）相反，硬件通过 CQ 中的 CQE/WC 来告诉软件某个 WQE/WR 的完成情况。再次提醒读者，对于上层用户来说一般用 WC，对于驱动程序来说，一般称为 CQE，本文不对两者进行区分。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_1.webp"
alt="2024-06-27_10_1" width="80%" loading="lazy">
&lt;/figure>
&lt;p>CQE 可以看作一份“报告”，其中写明了某个任务的执行情况，其中包括：&lt;/p>
&lt;ul>
&lt;li>本次完成了哪个 QP 的哪一个 WQE 指定的任务（QP Number 和 WR ID）&lt;/li>
&lt;li>本次任务执行了什么操作（Opcode 操作类型）&lt;/li>
&lt;li>本次任务执行成功/失败，失败原因是 XXX（Status 状态和错误码）&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>每当硬件处理完一个 WQE 之后，都会产生一个 CQE 放在 CQ 队列中。如果一个 WQE 对应的 CQE 没有产生，那么这个 WQE 就会一直被认为还未处理完，这意味着什么呢？&lt;/p>
&lt;ul>
&lt;li>涉及从内存中取数据的操作（SEND 和 WRITE）&lt;/li>
&lt;/ul>
&lt;p>在产生 CQE 之前，硬件可能还未发送消息，可能正在发送消息，可能对端有接收到正确的消息。由于内存区域是在发送前申请好的，所以上层软件收到对应的 CQE 之前，其必须认为这片内存区域仍在使用中，不能将所有相关的内存资源进行释放。&lt;/p>
&lt;ul>
&lt;li>涉及向内存中存放数据的操作（RECV 和 READ）&lt;/li>
&lt;/ul>
&lt;p>在产生 CQE 之前，有可能硬件还没有开始写入数据，有可能数据才写了一半，也有可能数据校验出错。所以上层软件在获得 CQE 之前，这段用于存放接收数据的内存区域中的内容是不可信的。&lt;/p>
&lt;p>总之，用户必须获取到 CQE 并确认其内容之后才能认为消息收发任务已经完成。&lt;/p>
&lt;h3 id="何时产生">何时产生&lt;/h3>
&lt;p>我们将按照服务类型（本篇只讲 RC 和 UD）和操作类型来分别说明，因为不同的情况产生 CQE 的时机和含义都不同，建议读者回顾第 4 篇和第 5 篇： &lt;a class="link" href="https://cuterwrite.top/p/rdma-op/" >“4. RDMA 基本操作”
&lt;/a>
、&lt;a class="link" href="https://cuterwrite.top/p/rdma-service-types/" >“5. RDMA 基本服务类型”
&lt;/a>
。&lt;/p>
&lt;ul>
&lt;li>可靠服务类型（RC）&lt;/li>
&lt;/ul>
&lt;p>前面的文章说过，&lt;strong>可靠意味着本端关心发出的消息能够被对端准确的接收&lt;/strong>，这是通过 ACK、校验和重传等机制保证的。&lt;/p>
&lt;ul>
&lt;li>SEND&lt;/li>
&lt;/ul>
&lt;p>SEND 操作需要硬件从内存中获取数据，然后组装成数据包通过物理链路发送到对端。对 SEND 来说，Client 端产生 CQE 表示&lt;strong>对端已准确无误的收到数据&lt;/strong>，对端硬件收到数据并校验之后，会回复 ACK 包给发送方。发送方收到这 ACK 之后才会产生 CQE，从而告诉用户这个任务成功执行了。如图所示，左侧 Client 端在红点的位置产生了本次任务的 CQE。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_2.webp"
alt="2024-06-27_10_2" width="50%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>RECV&lt;/li>
&lt;/ul>
&lt;p>RECV 操作需要硬件将收到的数据放到用户 WQE 中指定的内存区域，完成校验和数据存放动作后，硬件就会产生 CQE。如上图右侧 Server 端所示。&lt;/p>
&lt;ul>
&lt;li>WRITE&lt;/li>
&lt;/ul>
&lt;p>对于 Client 端来说，WRITE 操作和 SEND 操作是一样的，硬件会从内存中取出数据，并等待对端回复 ACK 后，才会产生 CQE。差别在于，因为 WRITE 是 RDMA 操作，对端 CPU 不感知，自然用户也不感知，所以上面的图变成了这样：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_3.webp"
alt="2024-06-27_10_3" width="50%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>READ&lt;/li>
&lt;/ul>
&lt;p>READ 和 RECV 有点像，Client 端发起 READ 操作后，对端会回复我们想读取的数据，然后本端校验没问题后，会把数据放到 WQE 中指定的位置。完成上述动作后，本端会产生 CQE。READ 同样是 RDMA 操作，对端用户不感知，自然也没有 CQE 产生。这种情况上图变成了这样：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_4.webp"
alt="2024-06-27_10_4" width="50%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>不可靠服务类型（UD）&lt;/li>
&lt;/ul>
&lt;p>因为不可靠的服务类型没有重传和确认机制，所以产生 CQE 表示硬件&lt;strong>已经将对应 WQE 指定的数据发送出去了&lt;/strong>。以前说过 UD 只支持 SEND-RECV 操作，不支持 RDMA 操作。所以对于 UD 服务的两端，CQE 产生时机如下图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_5.webp"
alt="2024-06-27_10_5" width="50%" loading="lazy">
&lt;/figure>
&lt;h3 id="wq-和-cq-的对应关系">WQ 和 CQ 的对应关系&lt;/h3>
&lt;p>&lt;strong>每个 WQ 都必须关联一个 CQ，而每个 CQ 可以关联多个 SQ 和 RQ。&lt;/strong>&lt;/p>
&lt;p>这里的所谓“关联”，指的是一个 WQ 的所有 WQE 对应的 CQE，都会被硬件放到绑定的 CQ 中，需要注意同属于一个 QP 的 SQ 和 RQ 可以各自关联不同的 CQ。如下图所示，QP1 的 SQ 和 RQ 都关联了 CQ1，QP2 的 RQ 关联到了 CQ1、SQ 关联到了 CQ2。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_6.webp"
alt="2024-06-27_10_6" width="auto" loading="lazy">
&lt;/figure>
&lt;p>因为每个 WQ 必须关联一个 CQ，所以用户创建 QP 前需要提前创建好 CQ，然后分别指定 SQ 和 RQ 将会使用的 CQ。&lt;/p>
&lt;p>&lt;strong>同一个 WQ 中的 WQE，其对应的 CQE 间是保序的&lt;/strong>&lt;/p>
&lt;p>硬件是按照“先进先出”的 FIFO 顺序从某一个 WQ（SQ 或者 RQ）中取出 WQE 并进行处理的，而向 WR 关联的 CQ 中存放 CQE 时，也是遵从这些 WQE 被放到 WQ 中的顺序的。简单来说，就是谁先被放到队列里，谁就先被完成。该过程如下图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_7.webp"
alt="2024-06-27_10_7" width="auto" loading="lazy">
&lt;/figure>
&lt;p>需要注意的是，使用 SRQ 的情况以及 RD 服务类型的 RQ 这两种情况是不保序的，本文中不展开讨论。&lt;/p>
&lt;p>&lt;strong>不同 WQ 中的 WQE，其对应的 CQE 间是不保序的&lt;/strong>&lt;/p>
&lt;p>前文中我们说过，一个 CQ 可能会被多个 WQ 共享。这种情况下，是不能保证这些 WQE 对应的 CQE 的产生顺序的。如下图所示（WQE 编号表示下发的次序，即 1 最先被下发，6 最后被下发）：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_8.webp"
alt="2024-06-27_10_8" width="auto" loading="lazy">
&lt;/figure>
&lt;p>上面的描述其实还包含了“同一个 QP 的 SQ 和 RQ 中的 WQE，其对应的 CQE 间是不保序的”的情况，这一点其实比较容易理解，SQ 和 RQ，一个负责主动发起的任务，一个负责被动接收的任务，它们本来就可以是认为是两条不同方向的通道，自然不应该相互影响。假设用户对同一个 QP 先下发了一个 Receive WQE，又下发一个 Send WQE，总不能对端不给本端发送消息，本端就不能发送消息给对端了吧？&lt;/p>
&lt;p>既然这种情况下 CQE 产生的顺序和获取 WQE 的顺序是不相关的，那么上层应用和驱动是如何知道收到的 CQE 关联的是哪个 WQE 呢？其实很简单，&lt;strong>CQE 中指明它所对应的 WQE 的编号&lt;/strong>就可以了。&lt;/p>
&lt;p>另外需要注意的是，即使在多个 WQ 共用一个 CQ 的情况下，“同一个 WQ 中的 WQE，其对应的 CQE 间是保序的”这一点也是一定能够保证的，即上图中的属于 WQ1 的 WQE 1、3、4 对应的 CQE 一定是按照顺序产生的，对于属于 WQ2 的 WQE 2、5、6 也是如此。&lt;/p>
&lt;h3 id="cqc">CQC&lt;/h3>
&lt;p>同 QP 一样，CQ 只是一段存放 CQE 的队列内存空间。硬件除了知道首地址以外，对于这片区域可以说是一无所知。所以需要提前跟软件约定好格式，然后驱动将申请内存，并按照格式把 CQ 的基本信息填写到这片内存中供硬件读取，这片内存就是 CQC。CQC 中包含了 CQ 的容量大小，当前处理的 CQE 的序号等等信息。所以把 QPC 的图稍微修改一下，就能表示出 CQC 和 CQ 的关系：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_9.webp"
alt="2024-06-27_10_9" width="auto" loading="lazy">
&lt;/figure>
&lt;h3 id="cqn">CQN&lt;/h3>
&lt;p>CQ Number，就是 CQ 的编号，用来区别不同的 CQ。CQ 没有像 QP0 和 QP1 一样的特殊保留编号，本文中不再赘述了。&lt;/p>
&lt;h2 id="完成错误">完成错误&lt;/h2>
&lt;p>IB 协议中有三种错误类型，立即错误（immediate error）、完成错误（Completion Error）以及异步错误（Asynchronous Errors)。&lt;/p>
&lt;p>立即错误的是“立即停止当前操作，并返回错误给上层用户”；完成错误指的是“通过 CQE 将错误信息返回给上层用户”；而异步错误指的是“通过中断事件的方式上报给上层用户”。可能还是有点抽象，我们来举个例子说明这两种错误都会在什么情况下产生：&lt;/p>
&lt;ul>
&lt;li>用户在 Post Send 时传入了非法的操作码，比如想在 UD 的时候使用 RDMA WRITE 操作。&lt;/li>
&lt;/ul>
&lt;p>结果：产生立即错误（有的厂商在这种情况会产生完成错误）&lt;/p>
&lt;p>一般这种情况下，驱动程序会直接退出 post send 流程，并返回错误码给上层用户。注意此时 WQE 还没有下发到硬件就返回了。&lt;/p>
&lt;ul>
&lt;li>用户下发了一个 WQE，操作类型为 SEND，但是长时间没有受到对方的 ACK。&lt;/li>
&lt;/ul>
&lt;p>结果：产生完成错误&lt;/p>
&lt;p>因为 WQE 已经到达了硬件，所以硬件会产生对应的 CQE，CQE 中包含超时未响应的错误详情。&lt;/p>
&lt;ul>
&lt;li>用户态下发了多个 WQE，所以硬件会产生多个 CQE，但是软件一直没有从 CQ 中取走 CQE，导致 CQ 溢出。
结果：产生异步错误&lt;/li>
&lt;/ul>
&lt;p>因为软件一直没取 CQE，所以自然不会从 CQE 中得到信息。此时 IB 框架会调用软件注册的事件处理函数，来通知用户处理当前的错误。&lt;/p>
&lt;p>由此可见，它们都是底层向上层用户报告错误的方式，只是产生的时机不一样而已。IB 协议中对不同情况的错误应该以哪种方式上报做了规定，比如下图中，对于 Modify QP 过程中修改非法的参数，应该返回立即错误。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_10.webp"
alt="2024-06-27_10_10" width="auto" loading="lazy">
&lt;/figure>
&lt;p>本文的重点在于 CQ，所以介绍完错误类型之后，我们着重来看一下完成错误。完成错误是硬件通过在 CQE 中填写错误码来实现上报的，一次通信过程需要发起端（Requester）和响应端（Responder）参与，具体的错误原因也分为本端和对端。我们先来看一下错误检测是在什么阶段进行的（下图对 IB 协议中 Figure 118 进行了重画）：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_11.webp"
alt="2024-06-27_10_11" width="auto" loading="lazy">
&lt;/figure>
&lt;p>Requester 的错误检测点有两个：&lt;/p>
&lt;ol>
&lt;li>本地错误检测&lt;/li>
&lt;/ol>
&lt;p>即对 SQ 中的 WQE 进行检查，如果检测到错误，就从本地错误检查模块直接产生 CQE 到 CQ，不会发送数据到响应端了；如果没有错误，则发送数据到对端。&lt;/p>
&lt;ol start="2">
&lt;li>远端错误检测&lt;/li>
&lt;/ol>
&lt;p>即检测响应端的 ACK 是否异常，ACK/NAK 是由对端的本地错误检测模块检测后产生的，里面包含了响应端是否有错误，以及具体的错误类型。无论远端错误检测的结果是否有问题，都会产生 CQE 到 CQ 中。&lt;/p>
&lt;p>Responder 的错误检测点只有一个：&lt;/p>
&lt;ol>
&lt;li>本地错误检测&lt;/li>
&lt;/ol>
&lt;p>实际上检测的是对端报文是否有问题，IB 协议也将其称为“本地”错误检测。如果检测到错误，则会体现在 ACK/NAK 报文中回复给对端，以及在本地产生一个 CQE。&lt;/p>
&lt;p>需要注意的是，上述的产生 ACK 和远端错误检测只对面向连接的服务类型有效，无连接的服务类型。比如 UD 类型并不关心对端是否收到，接收端也不会产生 ACK，所以在 Requester 的本地错误检测之后就一定会产生 CQE，无论是否有远端错误。&lt;/p>
&lt;p>然后我们简单介绍下几种常见的完成错误：&lt;/p>
&lt;ul>
&lt;li>RC 服务类型的 SQ 完成错误&lt;/li>
&lt;li>Local Protection Error
&lt;ul>
&lt;li>本地保护域错误。本地 WQE 中指定的数据内存地址的 MR 不合法，即用户试图使用一片未注册的内存中的数据。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Remote Access Error
&lt;ul>
&lt;li>远端权限错误。本端没有权限读/写指定的对端内存地址。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Transport Retry Counter Exceeded Error
&lt;ul>
&lt;li>重传超次错误。对端一直未回复正确的 ACK，导致本端多次重传，超过了预设的次数。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>RC 服务类型的 RQ 完成错误&lt;/li>
&lt;li>Local Access Error
&lt;ul>
&lt;li>本地访问错误。说明对端试图写入其没有权限写入的内存区域。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Local Length Error
&lt;ul>
&lt;li>本地长度错误。本地 RQ 没有足够的空间来接收对端发送的数据。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>完整的完成错误类型列表请参考 IB 协议的 10.10.3 节。&lt;/p>
&lt;h2 id="用户接口">用户接口&lt;/h2>
&lt;p>同 QP 一样，我们依然从通信准备阶段（控制面）和通信进行阶段（数据面）来介绍 IB 协议对上层提供的关于 CQ 的接口。&lt;/p>
&lt;h3 id="控制面">控制面&lt;/h3>
&lt;p>同 QP 一样，还是“增删改查”四种，但是可能因为对于 CQ 来说，上层用户是资源使用者而不是管理者，只能从 CQ 中读数据而不能写数据，所以对用户开放的可配的参数就只有“CQ 规格”一种。&lt;/p>
&lt;ul>
&lt;li>创建——Create CQ&lt;/li>
&lt;/ul>
&lt;p>创建的时候用户必须指定 CQ 的规格，即能够储存多少个 CQE，另外用户还可以填写一个 CQE 产生后的回调函数指针（下文会涉及）。内核态驱动会将其他相关的参数配置好，填写到跟硬件约定好的 CQC 中告知硬件。&lt;/p>
&lt;ul>
&lt;li>销毁——Destroy CQ&lt;/li>
&lt;/ul>
&lt;p>释放一个 CQ 软硬件资源，包含 CQ 本身及 CQC，另外 CQN 自然也将失效。&lt;/p>
&lt;ul>
&lt;li>修改——Resize CQ&lt;/li>
&lt;/ul>
&lt;p>这里名字稍微有点区别，因为 CQ 只允许用户修改规格大小，所以就用的 Resize 而不是 Modify。&lt;/p>
&lt;ul>
&lt;li>查询——Query CQ&lt;/li>
&lt;/ul>
&lt;p>查询 CQ 的当前规格，以及用于通知的回调函数指针。&lt;/p>
&lt;blockquote>
&lt;p>通过对比 RDMA 规范和软件协议栈，可以发现很多 verbs 接口并不是按照规范实现的。所以读者如果发现软件 API 和协议有差异时也无须感到疑惑，RDMA 技术本身一直还在演进，软件框架也处于活跃更新的状态。如果更关心编程实现，那么请以软件协议栈的 API 文档为准；如果更关心学术上的研究，那么请以 RDMA 规范为准。&lt;/p>
&lt;/blockquote>
&lt;h3 id="数据面">数据面&lt;/h3>
&lt;p>CQE 是硬件将信息传递给软件的媒介，虽然软件知道在什么情况下会产生 CQE，但是软件并不知道具体什么时候硬件会把 CQE 放到 CQ 中。在通信和计算机领域，我们把这种接收方不知道发送方什么时候发送的模式称为“异步”。我们先来举一个网卡的例子，再来说明用户如何通过数据面接口获取 CQE（WC）。&lt;/p>
&lt;p>网卡收到数据包后如何让 CPU 知道这件事，并进行数据包处理，有两种常见的模式：&lt;/p>
&lt;ul>
&lt;li>中断模式&lt;/li>
&lt;/ul>
&lt;p>当数据量较少，或者说偶发的数据交换较多时，适合采用中断模式——即 CPU 平常在做其他事情，当网卡收到数据包时，会上报中断打断 CPU 当前的任务，CPU 转而来处理数据包（比如 TCP/IP 协议栈的各层解析）。处理完数据之后，CPU 跳回到中断前的任务继续执行。&lt;/p>
&lt;p>每次中断都需要保护现场，也就是把当前各个寄存器的值、局部变量的值等等保存到栈中，回来之后再恢复现场（出栈），这本身是有开销的。如果业务负载较重，网卡一直都在接收数据包，那么 CPU 就会一直收到中断，CPU 将一直忙于中断切换，导致其他任务得不到调度。&lt;/p>
&lt;ul>
&lt;li>轮询模式&lt;/li>
&lt;/ul>
&lt;p>所以除了中断模式之外，网卡还有一种轮询模式，即收到数据包后都先放到缓冲区里，CPU 每隔一段时间会去检查网卡是否受到数据。如果有数据，就把缓冲区里的数据一波带走进行处理，没有的话就接着处理别的任务。&lt;/p>
&lt;p>通过对比中断模式我们可以发现，轮询模式虽然每隔一段时间需要 CPU 检查一次，带来了一定的开销，但是当业务繁忙的时候采用轮询模式能够极大的减少中断上下文的切换次数，反而减轻了 CPU 的负担。&lt;/p>
&lt;p>现在的网卡，一般都是中断+轮询的方式，也就是根据业务负载动态切换。&lt;/p>
&lt;p>在 RDMA 协议中，CQE 就相当于是网卡收到的数据包，RDMA 硬件把它传递给 CPU 去处理。RDMA 框架定义了两种对上层的接口，分别是 poll 和 notify，对应着轮询和中断模式。&lt;/p>
&lt;h3 id="poll-completion-queue">Poll completion queue&lt;/h3>
&lt;p>很直白，poll 就是轮询的意思。用户调用这个接口之后，CPU 就会定期去检查 CQ 里面是否有新鲜的 CQE，如果有的话，就取出这个 CQE（注意取出之后 CQE 就被“消耗”掉了），解析其中的信息并返回给上层用户。&lt;/p>
&lt;h3 id="request-completion-notification">Request completion notification&lt;/h3>
&lt;p>直译过来是请求完成通知，用户调用这个接口之后，相当于向系统注册了一个中断。这样当硬件将 CQE 放到 CQ 中后，会立即触发一个中断给 CPU，CPU 进而就会停止手上的工作取出 CQE，处理后返回给用户。&lt;/p>
&lt;p>同样的，这两种接口使用哪种，取决于用户对于实时性的要求，以及实际业务的繁忙程度。&lt;/p>
&lt;p>感谢阅读，CQ 就介绍到这里，下篇打算详细讲讲 SRQ。&lt;/p>
&lt;h2 id="协议相关章节">协议相关章节&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>9.9 CQ 错误检测和恢复&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.2.6 CQ 和 WQ 的关系&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.10 错误类型及其处理&lt;/p>
&lt;/li>
&lt;li>
&lt;p>11.2.8 CQ 相关控制面接口&lt;/p>
&lt;/li>
&lt;li>
&lt;p>11.4.2 CQ 相关数据面接口&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="其他参考资料">其他参考资料&lt;/h2>
&lt;p>[1] Linux Kernel Networking - Implement and Theory. Chapter 13. Completion Queue&lt;/p></description></item><item><title>RDMA 之 Queue Pair</title><link>https://cuterwrite.top/p/rdma-queue-pair/</link><pubDate>Tue, 25 Jun 2024 02:21:00 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-queue-pair/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116342820_p0_master1200.webp" alt="Featured image of post RDMA 之 Queue Pair" />&lt;h1 id="rdma-之-queue-pair">RDMA 之 Queue Pair&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/195757767">&lt;cite>知乎专栏：9. RDMA 基本服务类型&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;h2 id="queue-pair">Queue Pair&lt;/h2>
&lt;p>我们曾经在 &lt;a class="link" href="https://cuterwrite.top/p/rdma-element/" >“3. RDMA 基本元素”
&lt;/a>
一文中简单的介绍了 QP 的概念，本文将更深入的讲解一些关于 QP 的细节。&lt;/p>
&lt;h2 id="基本概念回顾">基本概念回顾&lt;/h2>
&lt;p>首先我们来简单回顾下关于 QP 的基础知识：&lt;/p>
&lt;p>根据 IB 协议中的描述，QP 是硬件和软件之间的一个虚拟接口。QP 是队列结构，按顺序存储着软件给硬件下发的任务（WQE），WQE 中包含从哪里取出多长的数据，并且发送给哪个目的地等等信息。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-26_9_1.webp"
alt="2024-06-26_9_1" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>QP 的概念&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>每个 QP 间都是独立的，彼此通过 PD 隔离，因此一个 QP 可以被视为某个用户独占的一种资源，一个用户也可以同时使用多个 QP。&lt;/p>
&lt;p>QP 有很多种服务类型，包括 RC、UD、RD 和 UC 等，所有的源 QP 和目的 QP 必须为同一种类型才能进行数据交互。&lt;/p>
&lt;p>虽然 IB 协议将 QP 称为“虚拟接口”，但是它是有实体的：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>硬件上，QP 是一段包含着若干个 WQE 的存储空间，IB 网卡会从这段空间中读取 WQE 的内容，并按照用户的期望去内存中存取数据。至于这个存储空间是内存空间还是 IB 网卡的片内存储空间，IB 协议并未做出限制，每个厂商有各自的实现&lt;/p>
&lt;/li>
&lt;li>
&lt;p>软件上，QP 是一个由 IB 网卡的驱动程序所维护的数据结构，其中包含 QP 的地址指针以及一些相关的软件属性。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="qpc">QPC&lt;/h3>
&lt;p>&lt;a class="link" href="https://cuterwrite.top/p/rdma-service-types/" >“5. RDMA 基本服务类型”
&lt;/a>
一文中，我们曾经提到过 QPC 全称是 Queue Pair Context，用于存储 QP 相关属性。驱动程序里面是有储存 QP 的软件属性的，既然我们可以在软件里储存 QP 的属性，为什么还要用使用 QPC 呢？&lt;/p>
&lt;p>这是因为&lt;strong>QPC 主要是给硬件看的，也会用来在软硬件之间同步 QP 的信息。&lt;/strong>&lt;/p>
&lt;p>我们说过 QP 在硬件上的实体只是一段存储空间而已，硬件除了知道这段空间的起始地址和大小之外一无所知，甚至连这个 QP 服务类型都不知道。还有很多其他的重要信息，比如某个 QP 中包含了若干个 WQE，硬件怎么知道有多少个，当前应该处理第几个呢？&lt;/p>
&lt;p>所有上述的这些信息，软件是可以设计一定的数据结构并为其申请内存空间的，但是软件看到的都是虚拟地址，这些内存空间在物理上是离散的，硬件并不知道这些数据存放到了哪里。所以就需要软件通过操作系统提前申请好一大片连续的空间，即 QPC 来承载这些信息给硬件看。网卡及其配套的驱动程序提前约定好了 QPC 中都有哪些内容，这些内容分别占据多少空间，按照什么顺序存放。这样驱动和硬件就可以通过通过 QPC 这段空间来读写 QP 的状态等等信息。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-26_9_2_QPC.webp"
alt="2024-06-26_9_2" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>QPC 的概念&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>如上图所示，硬件其实只需要知道 QPC 的地址 0x12350000 就可以了，因为它可以解析 QPC 的内容，从而得知 QP 的位置，QP 序号，QP 大小等等信息。进而就能找到 QP，知道应该取第几个 WQE 去处理。不同的厂商可能实现有些差异，但是大致的原理就是这样。&lt;/p>
&lt;p>IB 软件栈中还有很多 Context 的概念，除了 QPC 之外，还有 Device Context，SRQC，CQC，EQC（Event Queue Context，事件队列上下文）等，它们的作用与 QPC 类似，都是用来在记录和同步某种资源的相关属性。&lt;/p>
&lt;h3 id="qp-number">QP Number&lt;/h3>
&lt;p>简称为 QPN，就是每个 QP 的编号。IB 协议中规定用 $2^{24}$ 个 bit 来表示 QPN，即每个节点最大可以同时使用 $2^{24}$ 个 QP，这已经是一个很大的数量了，几乎不可能用完。每个节点都各自维护着 QPN 的集合，相互之间是独立的，即不同的节点上可以存在编号相同的 QP。&lt;/p>
&lt;p>QPN 的概念本身非常简单，但是有两个特殊的保留编号需要额外注意一下：&lt;/p>
&lt;h4 id="qp0">QP0&lt;/h4>
&lt;p>编号为 0 的 QP 用于子网管理接口 SMI（Subnet Management Interface），用于管理子网中的全部节点，说实话我也还没搞清楚这个接口的作用，暂且按下不表。&lt;/p>
&lt;h4 id="qp1">QP1&lt;/h4>
&lt;p>编号为 1 的 QP 用于通用服务接口 GSI（General Service Interface），GSI 是一组管理服务，其中最出名的就是 CM（Communication Management），是一种在通信双方节点正式建立连接之前用来交换必须信息的一种方式。其细节将在后面的文章中专门展开介绍。&lt;/p>
&lt;p>这也就是我们之前的文章画的关于 QP 的图中，没有出现过 QP0 和 QP1 的原因了。这两个 QP 之外的其他 QP 就都是普通 QP 了。用户在创建 QP 的时候，驱动或者硬件会给这个新 QP 分配一个 QPN，一般的 QPN 都是 2、3、4 这样按顺序分配的。当 QP 被销毁之后，它的 QPN 也会被重新回收，并在合适的时候分配给其他新创建的 QP。&lt;/p>
&lt;h2 id="用户接口">用户接口&lt;/h2>
&lt;p>我们从控制层面和数据层面来分类介绍用户接口，控制面即用户对某种资源进行某种设置，一般都是在正式收发数据之前进行；而数据面自然就是真正的数据收发过程中进行的操作。&lt;/p>
&lt;h3 id="控制面">控制面&lt;/h3>
&lt;p>接触过算法的读者应该都了解，链表的节点涉及到“增、删、改、查”四个操作，链表的节点是一片内存区域，是一种软件资源。&lt;/p>
&lt;p>“增”即向操作系统申请一片内存用来存放数据，系统将在内存中划分一块空间，并将其标记为“已被进程 XX 使用”，其他没有权限的进程将无法覆盖甚至读取这片内存空间。&lt;/p>
&lt;p>“删”即通知操作系统，这片空间我不使用了，可以标记成“未使用”并给其它进程使用了。&lt;/p>
&lt;p>“改”就是写，即修改这片内存区域的内容。&lt;/p>
&lt;p>&amp;ldquo;查&amp;quot;就是读，即获取这片内存区域的内容。&lt;/p>
&lt;p>QP 作为 RDMA 技术中最重要的一种资源，在生命周期上与链表并无二致：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>操作&lt;/th>
&lt;th>链表节点&lt;/th>
&lt;th>QP&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>增&lt;/td>
&lt;td>struct ListNode *node = malloc(sizeof(struct ListNode *));&lt;/td>
&lt;td>Create QP&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>删&lt;/td>
&lt;td>free(node);&lt;/td>
&lt;td>Destroy QP&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>改&lt;/td>
&lt;td>node-&amp;gt;val = xxx;&lt;/td>
&lt;td>Modify QP&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>查&lt;/td>
&lt;td>xxx = node-&amp;gt;val;&lt;/td>
&lt;td>Query QP&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>这四种操作，其实就是 Verbs（RDMA 对上层应用的 API）在控制面上对上层用户提供给用户的几个接口：&lt;/p>
&lt;h4 id="create-qp">Create QP&lt;/h4>
&lt;p>创建一个 QP 的软硬件资源，包含 QP 本身以及 QPC。用户创建时会写传入一系列的初始化属性，包含该 QP 的服务类型，可以储存的 WQE 数量等信息&lt;/p>
&lt;h4 id="destroy-qp">Destroy QP&lt;/h4>
&lt;p>释放一个 QP 的全部软硬件资源，包含 QP 本身及 QPC。销毁 QP 后，用户将无法通过 QPN 索引到这个 QP。&lt;/p>
&lt;h4 id="modify-qp">Modify QP&lt;/h4>
&lt;p>修改一个 QP 的某些属性，比如 QP 的状态，路径的 MTU 等等。这个修改过程既包括软件数据结构的修改，也包括对 QPC 的修改。&lt;/p>
&lt;h4 id="query-qp">Query QP&lt;/h4>
&lt;p>查询一个 QP 当前的状态和一些属性，查询到的数据来源于驱动以及 QPC 的内容。&lt;/p>
&lt;p>这四种操作都有配套的 Verbs 接口，类似于 &lt;code>ibv_create_qp()&lt;/code> 这种形式，我们编写 APP 时直接调用就可以了。更多关于对上层的 API 的细节，我们将在后面专门进行介绍。&lt;/p>
&lt;h2 id="数据面">数据面&lt;/h2>
&lt;p>数据面上，一个 QP 对上层的接口其实只有两种，分别用于向 QP 中填写发送和接收请求。&lt;strong>这里的“发送”和“接收”并不是指的发送和接收数据，而是指的是一次通信过程的“发起方”（Requestor）和“接收方”（Responser）&lt;/strong>。&lt;/p>
&lt;p>在行为上都是软件向 QP 中填写一个 WQE（对应用层来说叫 WR），请求硬件执行一个动作。所以这两种行为都叫做“Post XXX Request”的形式，即下发 XXX 请求。&lt;/p>
&lt;h3 id="post-send-request">Post Send Request&lt;/h3>
&lt;p>再强调一下，Post Send 本身不是指这个 WQE 的操作类型是 Send，而是表示这个 WQE 属于通信发起方。这个流程中填写到 QP 中的 WQE/WR 可以是 Send 操作，RDMA Write 操作以及 RDMA Read 操作等。&lt;/p>
&lt;p>用户需要提前准备好数据缓冲区、目的地址等信息，然后调用接口将 WR 传给驱动，驱动再把 WQE 填写到 QP 中。&lt;/p>
&lt;h3 id="post-receive-request">Post Receive Request&lt;/h3>
&lt;p>Post Recv 的使用场景就相对比较少了，一般只在 Send-Recv 操作的接收端执行，接收端需要提前准备好接收数据的缓冲区，并将缓冲区地址等信息以 WQE 的形式告知硬件。&lt;/p>
&lt;h2 id="qp-状态机">QP 状态机&lt;/h2>
&lt;p>说到 QP 的状态，就不得不祭出下面这张图（取自 IB 协议 10.3.1 节）：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-26_9_3.webp"
alt="2024-06-26_9_3" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>QP 状态机&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>所谓状态机，就是描述一个对象的不同状态，以及触发状态间跳转的条件。为一个对象设计状态机可以使这个对象的生命周期变得非常明确，实现上也会使得逻辑更加清晰。&lt;/p>
&lt;p>对于 QP 来说，IB 规范也为其设计了几种状态，处于不同状态的 QP 的功能是有差异的，比如只有进入到 Ready to Send 状态之后，QP 才能够进行 Post Send 数据操作。正常状态（绿色的）之间的状态转换都是由用户通过上文介绍的 Modify QP 的用户接口来主动触发的；而错误状态（红色的）往往是出错之后自动跳转的，当一个 QP 处于错误状态之后就无法执行正常的业务了，就需要上层通过 Modify QP 将其重新配置到正常状态上。&lt;/p>
&lt;p>上图中我们只关注 QP 的部分，EE（End-to-End Context）是专门给 RD 服务类型使用的一个概念，我们暂不涉及。我们通过 Create QP 接口来进入这个状态图，通过 Destroy QP 接口来离开这个状态图。&lt;/p>
&lt;p>QP 有以下几种状态，我们仅介绍一下比较重要的点：&lt;/p>
&lt;h3 id="rstreset">RST（Reset）&lt;/h3>
&lt;p>复位状态。当一个 QP 通过 Create QP 创建好之后就处于这个状态，相关的资源都已经申请好了，但是这个 QP 目前什么都做不了，其无法接收用户下发的 WQE，也无法接受对端某个 QP 的消息。&lt;/p>
&lt;h3 id="initinitialized">INIT（Initialized）&lt;/h3>
&lt;p>已初始化状态。这个状态下，用户可以通过 Post Receive 给这个 QP 下发 Receive WR，但是接收到的消息并不会被处理，会被静默丢弃；如果用户下发了一个 Post Send 的 WR，则会报错。&lt;/p>
&lt;h3 id="rtrready-to-receive">RTR（Ready to Receive）&lt;/h3>
&lt;p>准备接收状态。在 INIT 状态的基础上，RQ 可以正常工作，即对于接收到的消息，可以按照其中 WQE 的指示搬移数据到指定内存位置。此状态下 SQ 仍然不能工作。&lt;/p>
&lt;h3 id="rtsready-to-send">RTS（Ready to Send）&lt;/h3>
&lt;p>准备发送状态。在 RTR 基础上，SQ 可以正常工作，即用户可以进行 Post Send，并且硬件也会根据 SQ 的内容将数据发送出去。进入该状态前，QP 必须已于对端建立好链接。&lt;/p>
&lt;h3 id="sqdsend-queue-drain">SQD（Send Queue Drain）&lt;/h3>
&lt;p>SQ 排空状态。顾名思义，该状态会将 SQ 队列中现存的未处理的 WQE 全部处理掉，这个时候用户还可以下发新的 WQE 下来，但是这些 WQE 要等到旧的 WQE 全处理之后才会被处理。&lt;/p>
&lt;h3 id="sqersend-queue-error">SQEr（Send Queue Error）&lt;/h3>
&lt;p>SQ 错误状态。当某个 Send WR 发生完成错误（即硬件通过 CQE 告知驱动发生的错误）时，会导致 QP 进入此状态。&lt;/p>
&lt;h3 id="errerror">ERR（Error）&lt;/h3>
&lt;p>即错误状态。其他状态如果发生了错误，都可能进入该状态。Error 状态时，QP 会停止处理 WQE，已经处理到一半的 WQE 也会停止。上层需要在修复错误后再将 QP 重新切换到 RST 的初始状态。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>本文先回顾了 QP 的一些重要基本概念，然后讲解了 QPC、QPN 等 QP 强相关的概念，最后介绍了用户操作 QP 常用的接口以及 QP 状态机，相信本文过后读者一定对 QP 有了更深的了解。&lt;/p>
&lt;p>其实作为 RDMA 的核心概念，QP 的内容很多，本文难以全部囊括。我将在后面的文章中逐渐把相关的内容补全，比如 QKey 的概念将在后续专门介绍各种 Key 的文章中讲解。&lt;/p>
&lt;p>好了，本文就到这了，感谢阅读。预告下一篇文章将详细讲解 CQ。&lt;/p>
&lt;h2 id="协议相关章节">协议相关章节&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>3.5.1 10.2.4 QP 的基本概念&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.3 QP 状态机&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.2.5 QP 相关的软件接口&lt;/p>
&lt;/li>
&lt;li>
&lt;p>11.4 Post Send Post Recv&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>RDMA 之 Address Handle</title><link>https://cuterwrite.top/p/rdma-address-handle/</link><pubDate>Sat, 15 Jun 2024 01:00:00 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-address-handle/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116373922_p9_master1200.webp" alt="Featured image of post RDMA 之 Address Handle" />&lt;h1 id="rdma-之-address-handle">RDMA 之 Address Handle&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/163552044">&lt;cite>知乎专栏：8. RDMA 之 Address Handle&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>前面已经介绍过，RDMA 通信的基本单元是 QP。我们来思考一个问题，假设 A 节点的某个 QP 要跟 B 节点的某个 QP 交换信息，除了要知道 B 节点的 QP 序号——QPN 之外，还需要什么信息？要知道，QPN 是每个节点独立维护的序号，不是整个网络中唯一的。比如 A 的 QP 3 要跟 B 的 QP 5 通信，网络中可不止一个 QP5，可能有很多个节点都有自己的 QP 5。所以我们自然可以想到，还需要找到让每个节点都有一个独立的标识。&lt;/p>
&lt;p>在传统 TCP-IP 协议栈中，使用了家喻户晓的 IP 地址来标识网络层的每个节点。而 IB 协议中的这个标识被称为&lt;strong>GID（Global Identifier，全局 ID）&lt;/strong>，是一个 128 bits 的序列。关于 GID 本篇不展开讨论，将在后面介绍。&lt;/p>
&lt;h2 id="ah-是什么">AH 是什么&lt;/h2>
&lt;p>AH 全称为 Address Handle，没有想到特别合适的中文翻译，就先直译为“地址句柄”吧。这里的地址，指的是一组用于找到某个远端节点的信息的集合，在 IB 协议中，地址指的是 GID、端口号等等信息；而所谓句柄，我们可以理解为一个指向某个对象的指针。&lt;/p>
&lt;p>大家是否还记得 IB 协议中有四种基本服务类型——RC、UD、RD 和 UC，其中最常用的是 RC 和 UD。RC 的特点是两个节点的 QP 之间会建立可靠的连接，一旦建立连接关系便不容易改变，对端的信息是创建 QP 的时候储存在 QP Context 中的；&lt;/p>
&lt;p>而对于 UD 来说，QP 间没有连接关系，用户想发给谁，就在 WQE 中填好对端的地址信息就可以了。&lt;strong>用户不是直接把对端的地址信息填到 WQE 中的，而是提前准备了一个“地址薄”，每次通过一个索引来指定对端节点的地址信息，而这个索引就是 AH。&lt;/strong>&lt;/p>
&lt;p>AH 的概念大致可以用下图表示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_8_1.webp"
alt="2024-06-16_8_1" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Address Handle 功能示意图&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>对于每一个目的节点，本端都会创建一个对应的 AH，而同一个 AH 可以被多个 QP 共同使用。&lt;/p>
&lt;h2 id="ah-的作用">AH 的作用&lt;/h2>
&lt;p>每次进行 UD 服务类型的通信之前，用户都需要先通过 IB 框架提供的接口，来&lt;strong>为每一个可能的对端节点创建一个 AH&lt;/strong>，然后这些 AH 会被驱动放到一个“安全”的区域，并返回一个索引（指针/句柄）给用户。用户真正下发 WR（Work Request）时，就把这个索引传递进来就可以了。&lt;/p>
&lt;p>上述过程如下图所示，A 节点收到用户的这样一个任务——使用本端的 QP4 与 B 节点（通过 AH 指定）的 QP3 进行数据交换：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_8_2.webp"
alt="2024-06-16_8_2" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>UD 服务类型使用 AH 指定对端节点&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>IB 协议中并没有对为什么使用 AH 做出解释，我认为定义 AH 的概念的原因有以下三种：&lt;/p>
&lt;ol>
&lt;li>保证目的地址可用，提高效率&lt;/li>
&lt;/ol>
&lt;p>因为 UD 无连接的特点，用户可以在用户态直接通过 WR 来指定目的地。而如果让用户随意填写地址信息，然后硬件就根据这些信息进行组包的话，是会带来问题的。比如有这样一种场景：用户通过 WR 告诉硬件请给 GID 为 X，MAC 地址为 Y 的节点的端口 Z 发送数据。然而 X，Y，Z 可能不是一个合法的组合，或者 GID 为 X 的节点压根都不存在于网络中，而硬件是无力校验这些内容的，只能乖乖的组包、发送数据，这个目的地无效的数据包就白白发送出去了。&lt;/p>
&lt;p>而提前准备好地址信息，则可以避免上述情况。用户在创建 AH 时会陷入内核态，如果用户传递的参数有效，内核会把这些目的节点信息储存起来，生成一个指针返回给用户；如果用户传递的参数无效，AH 将创建失败。这一过程可以保证地址信息是有效的。用户通过指针就可以快速指定目的节点，加快数据交互流程。&lt;/p>
&lt;p>可能有人会问，既然内核是可信的，为什么不能在发送数据时陷入内核态去校验用户传递的地址信息呢？请别忘了 RDMA 技术的一大优势在哪里——数据流程可以直接从用户空间到硬件，完全绕过内核，这样可以避免系统调用和拷贝的开销。如果每次发送都要检验地址合法性的话，必然会降低通信速率。&lt;/p>
&lt;ol start="2">
&lt;li>向用户隐藏底层地址细节&lt;/li>
&lt;/ol>
&lt;p>用户创建 AH 时，只需要传递 gid、端口号、静态速率等信息，而其他通信所需的地址信息（主要是 MAC 地址）是内核驱动通过查询系统邻居表等方式解析到的，底层没有必要暴露这些额外的信息给用户层。&lt;/p>
&lt;ol start="3">
&lt;li>可以使用 PD 对目的地址进行管理&lt;/li>
&lt;/ol>
&lt;p>前文我们介绍保护域时曾经提过，除了 QP、MR 之外，AH 也由 PD 来进行资源划分。当定义了 AH 这个软件实体之后，我们就可以对所有的 QP 可达的目的地进行相互隔离和管理。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_8_3.webp"
alt="2024-06-16_8_3" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>使用 PD 隔离 AH&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>​比如上图中，AH1~3 只能被同一个 PD 下的 QP3 和 QP9 使用，而 AH4 只能被 QP5 使用。&lt;/p>
&lt;h2 id="协议相关章节">协议相关章节&lt;/h2>
&lt;p>协议中关于 AH 的篇幅并不多，甚至没有独立介绍其概念的章节：&lt;/p>
&lt;p>[1] 9.8.3 UD 服务类型中的目的地址由哪些部分组成：包括 AH、 QPN 和 Q_key&lt;/p>
&lt;p>[2] 10.2.2.2 目的地址的相关注意事项&lt;/p>
&lt;p>[3] 11.2.2.1 AH 相关的 Verbs 接口&lt;/p>
&lt;p>AH 就介绍到这里，感谢阅读。下一篇打算向大家描述更多关于 QP 的细节。&lt;/p></description></item><item><title>RDMA 之 Protection Domain</title><link>https://cuterwrite.top/p/rdma-protection-domain/</link><pubDate>Thu, 18 Apr 2024 21:42:00 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-protection-domain/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/d31a474af07682028ca085f871bc5d07195413-2024-04-19.webp" alt="Featured image of post RDMA 之 Protection Domain" />&lt;h1 id="rdma-之-protection-domain">RDMA 之 Protection Domain&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/159493100">&lt;cite>知乎专栏：7. RDMA 之 Protection Domain&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>前文我们简单介绍了 RDMA 中最常见的一些资源，包括各种 Queue，以及 MR 的概念等等。MR 用于控制和管理 HCA 对于本端和远端内存的访问权限，确保 HCA 只有拿到正确 Key 之后才能读写用户已经注册了的内存区域。为了更好的保障安全性，IB 协议又提出了 Protection Domain（PD）的概念，用于保证 RDMA 资源间的相互隔离，本文就介绍一下 PD 的概念。&lt;/p>
&lt;h2 id="pd-是什么">PD 是什么&lt;/h2>
&lt;p>PD 全称是 Protection Domain，意为&amp;quot;保护域&amp;quot;。域的概念我们经常见到，从数学上的“实数域”、“复数域”，到地理上的“空域”、“海域”等等，表示一个空间/范围。在 RDMA 中，PD 像是一个容纳了各种资源（QP、MR 等）的“容器”，将这些资源纳入自己的保护范围内，避免他们被未经授权的访问。一个节点中可以定义多个保护域，各个 PD 所容纳的资源彼此隔离，无法一起使用。&lt;/p>
&lt;p>概念还是有些抽象，下面我们来看一下 PD 有什么作用，具体解决了什么问题。&lt;/p>
&lt;h2 id="pd-的作用">PD 的作用&lt;/h2>
&lt;p>一个用户可能创建多个 QP 和多个 MR，每个 QP 可能和不同的远端 QP 建立了连接，比如下图这样（灰色箭头表示 QP 间的连接关系）：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/7_1-2024-04-19.webp"
alt="7_1-2024-04-19" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>图 1：没有 PD 概念时的 RDMA 资源&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>由于 MR 和 QP 之间并没有绑定关系，这就意味着一旦某个远端的 QP 与本端的一个 QP 建立了连接，具备了通信的条件，那么理论上远端节点只要知道 VA 和 R_key（甚至可以靠不断的猜测直到得到一对有效的值），就可以访问本端节点某个 MR 的内容。&lt;/p>
&lt;p>其实一般情况下，MR 的虚拟地址 VA 和秘钥 R_Key 是很难猜到的，已经可以保证一定的安全性了。但是为了更好的保护内存中的数据，把各种资源的权限做进一步的隔离和划分，我们在又在每个节点中定义了 PD，如下图所示&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/7_2-2024-04-19.webp"
alt="7_2-2024-04-19" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>图 2：加入 PD 概念时的 RDMA 资源&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>图中 Node 0 上有两个 PD，将 3 个 QP 和 2 个 MR 分为了两组，此外 Node 1 和 Node 2 中各有一个 PD 包含了所有 QP 和 MR。Node 0 上的两个 PD 中的资源不可以一起使用，也就是说 QP3 和 QP9 不能访问 MR1 的数据，QP6 也不可以访问 MR0 的数据。如果我们在数据收发时，指定硬件使用 QP3 和 MR1，那么硬件校验他们不属于同一个 PD 后，会返回错误。&lt;/p>
&lt;p>对于远端节点来说，Node1 只能通过 QP8 相连的 QP3 来访问 Node0 的内存，但是因为 Node 0 的 QP3 被“圈”到了 PD0 这个保护域中，所以 Node 1 的 QP8 也只能访问 MR0 对应的内存，&lt;strong>无论如何都无法访问 MR1 中的数据&lt;/strong>，这是从两个方面限制的：&lt;/p>
&lt;ol>
&lt;li>Node 1 的 QP8 只跟 Node 0 的 QP3 有连接关系，无法通过 Node 0 的 QP6 进行内存访问。&lt;/li>
&lt;li>Node 0 的 MR1 和 QP3 属于不同的 PD，就算 Node 1 的 QP8 拿到了 MR1 的 VA 和 R_key，硬件也会因为 PD 不同而拒绝提供服务。&lt;/li>
&lt;/ol>
&lt;p>所以就如本文一开始所说的，PD 就像是一个容器，将一些 RDMA 资源保护起来，彼此隔离，以提高安全性。其实 RDMA 中不止有 QP、MR 这些资源，后文即将介绍的 Address Handle，Memory Window 等也是由 PD 进行隔离保护的。&lt;/p>
&lt;h2 id="如何使用-pd">如何使用 PD&lt;/h2>
&lt;p>还是看上面的图，我们注意到 Node 0 为了隔离资源，存在两个 PD；而 Node 1 和 Node 2 只有一个 PD 包含了所有资源。&lt;/p>
&lt;p>我之所以这样画，是为了说明一个节点上划分多少个 PD 完全是由用户决定的，&lt;strong>如果想提高安全性，那么对每个连接到远端节点的 QP 和供远端访问的 MR 都应该尽量通过划分 PD 做到隔离；如果不追求更高的安全性，那么创建一个 PD，囊括所有的资源也是可以的&lt;/strong>。&lt;/p>
&lt;p>IB 协议中规定：&lt;strong>每个节点都至少要有一个 PD，每个 QP 都必须属于一个 PD，每个 MR 也必须属于一个 PD&lt;/strong>。&lt;/p>
&lt;p>那么 PD 的包含关系在软件上是如何体现的呢？它本身是有一个软件实体的（结构体），记录了这个保护域的一些信息。用户在创建 QP 和 MR 等资源之前，必须先通过 IB 框架的接口创建一个 PD，拿到它的指针/句柄。接下来在创建 QP 和 MR 的时候，需要传入这个 PD 的指针/句柄，PD 信息就会包含在 QP 和 MR 中。硬件收发包时，会对 QP 和 MR 的 PD 进行校验。更多的软件协议栈的内容，我会在后面的文章中介绍。&lt;/p>
&lt;p>另外需要强调的是，&lt;strong>PD 是本地概念，仅存在于节点内部&lt;/strong>，对其他节点是不可见的；而 MR 是对本端和对端都可见的。&lt;/p>
&lt;p>为了方便大家查阅和学习，以后我会列出文章涉及的协议章节，前面的内容有时间的时候我也会补充一下。&lt;/p>
&lt;h2 id="pd-相关协议章节">PD 相关协议章节&lt;/h2>
&lt;ul>
&lt;li>3.5.5 PD 的基本概念和作用&lt;/li>
&lt;li>10.2.3 介绍了 PD 和其他一些 RDMA 资源的关系，以及 PD 相关的软件接口。&lt;/li>
&lt;li>10.6.3.5 再次强调 PD 和 MR 及 QP 的关系。&lt;/li>
&lt;li>11.2.1.5 详细介绍 PD 的 Verbs 接口，包括作用、入参、出参和返回值等。&lt;/li>
&lt;/ul>
&lt;p>好了，关于 PD 的介绍就到这里。下文我会介绍用于 UD 服务类型的 Address Handle 的概念。&lt;/p></description></item><item><title>RDMA 之 Memory Region</title><link>https://cuterwrite.top/p/rdma-mr/</link><pubDate>Wed, 03 Apr 2024 16:17:00 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-mr/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/8fa232626b76940fddc8cc52a49c49e9195413-2024-04-04.webp" alt="Featured image of post RDMA 之 Memory Region" />&lt;h1 id="rdma-之-memory-region">RDMA 之 Memory Region&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/156975042">&lt;cite>知乎专栏：6. RDMA 之 Memory Region&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>我们假设一种场景，同时也顺便温习一下 RDMA WRITE 操作的流程：&lt;/p>
&lt;p>如下图所示，A 节点想要通过 IB 协议向 B 节点的内存中写入一段数据，上层应用给本节点的 RDMA 网卡下发了一个 WQE，WQE 中包含了源内存地址、目的内存地址、数据长度和秘钥等信息，然后硬件会从内存中取出数据，组包发送到对端网卡。B 节点的网卡收到数据后，解析到其中的目的内存地址，把数据写入到本节点的内存中。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/6_1-2024-04-04.webp"
alt="6_1-2024-04-04" width="auto" loading="lazy">
&lt;/figure>
&lt;p>那么问题来了，APP 提供的地址都是虚拟地址（Virtual Address，下文称 VA），经过 MMU 的转换才能得到真实的物理地址（Physical Address，下文称 PA），我们的&lt;strong>RDMA 网卡是如何得到 PA 从而去内存中拿到数据的呢&lt;/strong>？就算网卡知道上哪去取数据，&lt;strong>如果用户恶意指定了一个非法的 VA，那网卡岂不是有可能被“指使”去读写关键内存&lt;/strong>？&lt;/p>
&lt;p>为了解决上面的问题，IB 协议提出了 MR 的概念。&lt;/p>
&lt;h2 id="mr-是什么">MR 是什么&lt;/h2>
&lt;p>MR 全称为 Memory Region，指的是由 RDMA 软件层在内存中规划出的一片区域，用于存放收发的数据。IB 协议中，用户在申请完用于存放数据的内存区域之后，都需要通过调用 IB 框架提供的 API 注册 MR，才能让 RDMA 网卡访问这片内存区域。由下图可以看到，MR 就是一片特殊的内存而已：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/6_2-2024-04-04.webp"
alt="6_2-2024-04-04" width="auto" loading="lazy">
&lt;/figure>
&lt;p>在对 IB 协议进行相关描述时，我们通常称 RDMA 硬件为&lt;strong>HCA（Host Channel Adapter， 宿主通道适配器）&lt;/strong>，IB 协议中对其的定义是“处理器和 I/O 单元中能够产生和消耗数据包的 IB 设备”，为了与协议保持一致，我们在包括本文及之后的文章中都称硬件部分为 HCA。&lt;/p>
&lt;h2 id="为什么要注册-mr">为什么要注册 MR&lt;/h2>
&lt;p>下面我们来看一下 MR 是如何解决本文开篇提出的两个问题的：&lt;/p>
&lt;h3 id="1-注册-mr-以实现虚拟地址与物理地址转换">1. 注册 MR 以实现虚拟地址与物理地址转换&lt;/h3>
&lt;p>我们都知道 APP 只能看到虚拟地址，而且会在 WQE 中直接把 VA 传递给 HCA（既包括本端的源 VA，也包括对端的目的 VA）。现在的 CPU 都有 MMU 和页表这一“利器”来进行 VA 和 PA 之间的转换，而 HCA 要么直接连接到总线上，要么通过 IOMMU/SMMU 做地址转换后连接到总线上，它是“看不懂”APP 提供的 VA 所对应的真实物理内存地址的。&lt;/p>
&lt;p>所以注册 MR 的过程中，硬件会在内存中创建并填写一个 VA to PA 的映射表，这样需要的时候就能通过查表把 VA 转换成 PA 了。我们还是提供一个具体的例子来讲一下这个过程：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/6_3-2024-04-04.webp"
alt="6_3-2024-04-04" width="auto" loading="lazy">
&lt;/figure>
&lt;p>现在假设左边的节点向右边的节点发起了 RDMA WRITE 操作，即直接向右节点的内存区域中写入数据。假设图中两端都已经完成了注册 MR 的动作，MR 即对应图中的“数据 Buffer”，同时也创建好了 VA-&amp;gt;PA 的映射表。&lt;/p>
&lt;ul>
&lt;li>首先本端 APP 会下发一个 WQE 给 HCA，告知 HCA，用于存放待发送数据的本地 Buffer 的虚拟地址，以及即将写入的对端数据 Buffer 的虚拟地址。&lt;/li>
&lt;li>本端 HCA 查询 VA-&amp;gt;PA 映射表，得知待发数据的物理地址，然后从内存中拿到数据，组装数据包并发送出去。&lt;/li>
&lt;li>对端 HCA 收到了数据包，从中解析出了目的 VA。&lt;/li>
&lt;li>对端 HCA 通过存储在本地内存中的 VA-&amp;gt;PA 映射表，查到真实的物理地址，核对权限无误后，将数据存放到内存中。&lt;/li>
&lt;/ul>
&lt;p>再次强调一下，对于右侧节点来说，&lt;strong>无论是地址转换还是写入内存，完全不用其 CPU 的参与&lt;/strong>。&lt;/p>
&lt;h3 id="2-mr-可以控制-hca-访问内存的权限">2. MR 可以控制 HCA 访问内存的权限&lt;/h3>
&lt;p>因为 HCA 访问的内存地址来自于用户，如果用户传入了一个非法的地址（比如系统内存或者其他进程使用的内存），HCA 对其进行读写可能造成信息泄露或者内存覆盖。所以我们需要一种机制来确保 HCA 只能访问已被授权的、安全的内存地址。IB 协议中，APP 在为数据交互做准备的阶段，需要执行注册 MR 的动作。&lt;/p>
&lt;p>而用户注册 MR 的动作会产生两把钥匙——L_KEY（Local Key）和 R_KEY（Remote Key），说是钥匙，它们的实体其实就是一串序列而已。它们将分别用于保障对于本端和远端内存区域的访问权限。下面两张图分别是描述 L_Key 和 R_Key 的作用的示意图：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/6_4-2024-04-04.webp"
alt="6_4-2024-04-04" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>L_Key&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/6_5-2024-04-04.webp"
alt="6_5-2024-04-04" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>R_Key&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>这里大家可能会有疑问，本端是如何知道对端节点的可用 VA 和对应的 R_Key 的？其实两端的节点在真正的 RDMA 通信之前，都会通过某些方式先建立一条链路（可能是 Socket 连接，也可能是 CM 连接）并通过这条链路交换一些 RDMA 通信所必须的信息（VA，Key，QPN 等），我们称这一过程叫做“建链”和“握手”。我将在后面的文章中详细介绍。&lt;/p>
&lt;p>除了上面两个点之外，注册 MR 还有个重要的作用：&lt;/p>
&lt;h3 id="3-mr-可以避免换页">3. MR 可以避免换页&lt;/h3>
&lt;p>因为物理内存是有限的，所以操作系统通过换页机制来暂时把某个进程不用的内存内容保存到硬盘中。当该进程需要使用时，再通过缺页中断把硬盘中的内容搬移回内存，这一过程几乎必然导致 VA-PA 的映射关系发生改变。&lt;/p>
&lt;p>由于 HCA 经常会绕过 CPU 对用户提供的 VA 所指向的物理内存区域进行读写，如果前后的 VA-PA 映射关系发生改变，那么我们在前文提到的 VA-&amp;gt;PA 映射表将失去意义，HCA 将无法找到正确的物理地址。&lt;/p>
&lt;p>为了防止换页所导致的 VA-PA 映射关系发生改变，注册 MR 时会 &amp;ldquo;Pin&amp;rdquo; 住这块内存（亦称“锁页”），即锁定 VA-PA 的映射关系。也就是说，MR 这块内存区域会长期存在于物理内存中不被换页，直到完成通信之后，用户主动注销这片 MR。&lt;/p>
&lt;p>好了，至此我们介绍完了 MR 的概念和作用，下一篇文章我将给大家介绍一下 PD（Protection Domain，保护域）的概念。&lt;/p>
&lt;h2 id="代码示例">代码示例&lt;/h2>
&lt;p>下面是一个简单的 RDMA 程序，展示了如何注册 MR：&lt;/p>
&lt;pre>&lt;code class="language-c">#include &amp;lt;infiniband/verbs.h&amp;gt;
int main() {
// 省略初始化过程...
struct ibv_mr *mr;
mr = ibv_reg_mr(pd, buf, 1024, IBV_ACCESS_LOCAL_WRITE |
IBV_ACCESS_REMOTE_WRITE);
// 获取 L_Key 和 R_Key
uint32_t lkey = mr-&amp;gt;lkey;
uint32_t rkey = mr-&amp;gt;rkey;
// 省略其它代码...
}
&lt;/code>&lt;/pre></description></item><item><title>RDMA 基本服务类型</title><link>https://cuterwrite.top/p/rdma-service-types/</link><pubDate>Sun, 25 Feb 2024 22:04:01 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-service-types/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/f71da3ec40dd74648e15471d47ba3b84195413_crop-2024-02-26.webp" alt="Featured image of post RDMA 基本服务类型" />&lt;h1 id="rdma-基本服务类型">RDMA 基本服务类型&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/144099636">&lt;cite>知乎专栏：5. RDMA 基本服务类型&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>我们在 &lt;a class="link" href="https://cuterwrite.top/p/rdma-element/" >“3. RDMA 基本元素”
&lt;/a>
一文中提到过，&lt;strong>RDMA 的基本通信单元是 QP&lt;/strong>，而基于 QP 的通信模型有很多种，我们在 RDMA 领域称其为“服务类型”。IB 协议中通过“可靠”和“连接”两个维度来描述一种服务类型。&lt;/p>
&lt;h2 id="可靠">可靠&lt;/h2>
&lt;p>通信中的可靠性指的是通过一些机制保证发出去的数据包都能够被正常接收。IB 协议中是这样描述可靠服务的：&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Reliable Service&lt;/strong> provides a guarantee that messages are delivered from a requester to a responder at most once, in order and without corruption.&lt;/p>
&lt;/blockquote>
&lt;p>即“可靠服务在发送和接受者之间保证了信息最多只会传递一次，并且能够保证其按照发送顺序完整的被接收”。&lt;/p>
&lt;p>IB 通过以下三个机制来保证可靠性：&lt;/p>
&lt;h2 id="应答机制">应答机制&lt;/h2>
&lt;p>假设 A 给 B 发了一个数据包，A 怎样才能知道 B 收到了呢，自然是 B 回复一个“我收到了”消息给 A。在通信领域我们一般称这个回复为应答包或者 ACK（Acknowledge）。在 IB 协议的可靠服务类型中，使用了应答机制来保证数据包被对方收到。IB 的可靠服务类型中，接收方不是每一个包都必须回复，也可以一次回复多个包的 ACK，以后我们再展开讨论。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/ib_ack-2024-02-26.webp"
alt="ib_ack-2024-02-26" width="auto" loading="lazy">
&lt;/figure>
&lt;h2 id="数据校验机制">数据校验机制&lt;/h2>
&lt;p>这个比较好理解，发端会对 Header 和 Payload（有效载荷，也就是真正要收发的数据）通过一定的算法得到一个校验值放到数据包的末尾。对端收到数据包后，也会用相同的算法计算出校验值，然后与数据包中的校验值比对，如果不一致，说明数据中包含错误（一般是链路问题导致的），那么接收端就会丢弃这个数据包。IB 协议使用的 CRC 校验，本文对 CRC 不做展开介绍。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/crc-2024-02-26.png"
alt="crc-2024-02-26" width="auto" loading="lazy">
&lt;/figure>
&lt;h2 id="保序机制">保序机制&lt;/h2>
&lt;p>保序指的是，保证先被发送到物理链路上的数据包一定要先于后发送的数据包被接收方收到。有一些业务对数据包的先后顺序是有严格要求的，比如语音或者视频。IB 协议中有 PSN（Packet Sequence Number，包序号）的概念，即每个包都有一个递增的编号。PSN 可以用来检测是否丢包，比如收端收到了 1，但是在没收到 2 的情况下就收到了 3，那么其就会认为传输过程中发生了错误，之后会回复一个 NAK 给发端，让其重发丢失的包。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/psn-2024-02-26.webp"
alt="psn-2024-02-26" width="auto" loading="lazy">
&lt;/figure>
&lt;p>不可靠服务，没有上述这些机制来保证数据包被正确的接收，属于“发出去就行，我不关心有没有被收到”的服务类型。&lt;/p>
&lt;h2 id="连接与数据报">连接与数据报&lt;/h2>
&lt;p>&lt;strong>连接（Connection）&lt;/strong> 在这里指的是一个抽象的逻辑概念，需要区别于物理连接，熟悉 Socket 的读者一定对这个其不陌生。连接是一条通信的“管道”，一旦管道建立好了，管道这端发出的数据一定会沿着这条管道到达另一端。&lt;/p>
&lt;p>对于“连接”或者说“面向连接”的定义有很多种，有的侧重于保证消息顺序，有的侧重于消息的传递路径唯一，有的强调需要软硬件开销来维护连接，有的还和可靠性的概念有交集。本专栏既然是介绍 RDMA 技术，那么我们就看一下 IB 协议 3.2.2 节中对其的描述：&lt;/p>
&lt;blockquote>
&lt;p>IBA supports both connection oriented and datagram service. For connected service, each QP is associated with exactly one remote consumer. In this case the QP context is configured with the identity of the remote consumer’s queue pair. &amp;hellip; During the communication establishment process, this and other information is exchanged between the two nodes.&lt;/p>
&lt;/blockquote>
&lt;p>即“IBA 支持基于连接和数据报的服务。对于基于连接的服务来说，每个 QP 都和另一个远端节点相关联。在这种情况下，QP Context 中包含有远端节点的 QP 信息。在建立通信的过程中，两个节点会交换包括稍后用于通信的 QP 在内的对端信息&amp;quot;。&lt;/p>
&lt;p>上面这端描述中的 Context 一般被翻译成上下文，QP Context（简称 QPC）可以简单理解为是记录一个 QP 相关信息的表格。我们知道 QP 是两个队列，除了这两个队列之外，我们还需要把关于 QP 的信息记录到一张表里面，这些信息可能包括队列的深度，队列的编号等等，后面我们会展开讲。&lt;/p>
&lt;p>可能还是有点抽象，我们用图说话：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/QPC-2024-02-26.webp"
alt="QPC-2024-02-26" width="auto" loading="lazy">
&lt;/figure>
&lt;p>A、B 和 A、C 节点的网卡在物理上是连接在一起的，A 上面的 QP2 和 B 上面的 QP7、A 上面的 QP4 和 B 上面的 QP2 建立了逻辑上的连接，或者说“绑定到了一起”。&lt;strong>在连接服务类型中的每个 QP，都和唯一的另一个 QP 建立了连接，也就是说 QP 下发的每个 WQE 的目的地都是唯一的&lt;/strong>。拿上图来说，对于 A 的 QP2 下发的每个 WQE，硬件都可以通过 QPC 得知其目的为 B 的 QP7，就会把组装好的数据包发送给 B，然后 B 会根据 QP7 下发的 RQ WQE 来存放数据；同理，对于 A 的 QP4 下发的每个 WQE，A 的硬件都知道应该把数据发给 Node C 的 QP2。&lt;/p>
&lt;p>“连接”是如何维护的呢？其实就是在 QPC 里面的一个记录而已。如果 A 的 QP2 想断开与 B 的 QP7 的“连接”然后与其他 QP 相“连接”，只需要修改 QPC 就可以了。两个节点在建立连接的过程中，会交换稍后用于数据交互的 QP Number，然后分别记录在 QPC 中。&lt;/p>
&lt;p>&lt;strong>数据报（Datagram）&lt;/strong> 与连接相反，发端和收端间不需要“建立管道”的步骤，只要发端到收端物理上是可以到达的，那么我就可能从任何路径发给任意的收端节点。IB 协议对其的定义是这样的：&lt;/p>
&lt;blockquote>
&lt;p>For datagram service, a QP is not tied to a single remote consumer, but rather information in the WQE identifies the destination. A communication setup process similar to the connection setup process needs to occur with each destination to exchange that information.&lt;/p>
&lt;p>即“对于数据报服务来说，QP 不会跟一个唯一的远端节点绑定，而是通过 WQE 来指定目的节点。和连接类型的服务一样，建立通信的过程也需要两端交换对端信息，但是数据报服务对于每个目的节点都需要执行一次这个交换过程。”&lt;/p>
&lt;/blockquote>
&lt;p>我们举个例子：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/Datagram-2024-02-26.webp"
alt="Datagram-2024-02-26" width="auto" loading="lazy">
&lt;/figure>
&lt;p>在数据报类型的 QP 的 Context 中，不包含对端信息，即每个 QP 不跟另一个 QP 绑定。&lt;strong>QP 下发给硬件的每个 WQE 都可能指向不同的目的地&lt;/strong>。比如节点 A 的 QP2 下发的第一个 WQE，指示给节点 C 的 QP3 发数据；而下一个 WQE，可以指示硬件发给节点 B 的 QP7。&lt;/p>
&lt;p>与连接服务类型一样，本端 QP 可以和哪个对端 QP 发送数据，是在准备阶段提前通过某些方式相互告知的。这也是上文“数据报服务对于每个目的节点都需要执行一次这个交换过程”的含义。&lt;/p>
&lt;h2 id="服务类型">服务类型&lt;/h2>
&lt;p>上面介绍的两个维度两两组合就形成了 IB 的四种基本服务类型：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>可靠(Reliable)&lt;/th>
&lt;th>不可靠(Unreliable)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>连接(Connection)&lt;/td>
&lt;td>RC（Reliable Connection）&lt;/td>
&lt;td>UC（Unreliable Connection）&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>数据报(Datagram)&lt;/td>
&lt;td>RD（Reliable Datagram）&lt;/td>
&lt;td>UD（Unreliable Datagram）&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>RC 和 UD 是应用最多也是最基础的两种服务类型，我们可以将他们分别类比成 TCP/IP 协议栈传输层的 TCP 和 UDP。&lt;/p>
&lt;p>RC 用于对数据完整性和可靠性要求较高的场景，跟 TCP 一样，因为需要各种机制来保证可靠，所以开销自然会大一些。另外由于 RC 服务类型和每个节点间需要各自维护一个 QP，假设有 N 个节点要相互通信，那么至少需要 &lt;strong>N * (N - 1)&lt;/strong> 个 QP，而 QP 和 QPC 本身是需要占用网卡资源或者内存的，当节点数很多时，存储资源消耗将会非常大。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/RC_Connect-2024-02-26.webp"
alt="RC_Connect-2024-02-26" width="auto" loading="lazy">
&lt;/figure>
&lt;p>UD 硬件开销小并且节省存储资源，比如 N 个节点需要相互通信，只需要创建 &lt;strong>N&lt;/strong> 个 QP 就可以了，但是可靠性跟 UDP 一样没法保证。用户如果想基于 UD 服务类型实现可靠性，那么需要自己基于 IB 传输层实现应用层的可靠传输机制。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/UD_Connect-2024-02-26.webp"
alt="UD_Connect-2024-02-26" width="auto" loading="lazy">
&lt;/figure>
&lt;p>除此之外，还有 RD 和 UC 类型，以及 XRC（Extended Reliable Connection），SRD（Scalable Reliable Datagram）等更复杂的服务类型，我们将在协议解析部分对其进行详细的描述。&lt;/p>
&lt;p>更多关于 QP 类型选择的信息可以参考 RDMAmojo 上的&lt;a class="link" href="https://www.rdmamojo.com/2013/06/01/which-queue-pair-type-to-use/" target="_blank" rel="noopener" >Which Queue Pair type to use?
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
这篇文章，感谢 &lt;a class="link" href="https://www.zhihu.com/people/fc04fe143ad43b66fabb7050dadef923" target="_blank" rel="noopener" >@sinkinben
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
同学在评论区指路。&lt;/p>
&lt;h2 id="代码示例">代码示例&lt;/h2>
&lt;p>在 RDMA 编程中，我们可以通过 &lt;code>ibv_create_qp&lt;/code> 函数来创建 QP，其中的 &lt;code>struct ibv_qp_init_attr&lt;/code> 结构体中的 &lt;code>qp_type&lt;/code> 字段就是用来指定 QP 的服务类型的。下面是一个简单的示例代码：&lt;/p>
&lt;pre>&lt;code class="language-c">struct ibv_qp_init_attr qp_init_attr;
qp_init_attr.qp_type = IBV_QPT_RC; // RC 类型
qp_init_attr.sq_sig_all = 1; // 1 表示 SQ 中的每个 WQE 都需要对应的接收一个 CQE
qp_init_attr.send_cq = cq; // 发送 CQ
qp_init_attr.recv_cq = cq; // 接收 CQ
qp_init_attr.cap.max_send_wr = 1024; // SQ 的深度
struct ibv_qp *qp = ibv_create_qp(pd, &amp;amp;qp_init_attr);
&lt;/code>&lt;/pre></description></item><item><title>RDMA 操作类型</title><link>https://cuterwrite.top/p/rdma-op/</link><pubDate>Sat, 24 Feb 2024 03:09:01 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-op/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/bcb5351691a864a6827138cf4c2e0642195413_crop-2024-02-25.webp" alt="Featured image of post RDMA 操作类型" />&lt;h1 id="rdma-操作类型">RDMA 操作类型&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/142175657">&lt;cite>知乎专栏：4. RDMA 操作类型&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>前面几篇涉及 RDMA 的通信流程时一直在讲 SEND-RECV，然而它其实称不上是“RDMA”，只是一种加入了 0 拷贝和协议栈卸载的传统收发模型的“升级版”，这种操作类型没有完全发挥 RDMA 技术全部实力，常用于两端交换控制信息等场景。当涉及大量数据的收发时，更多使用的是两种 RDMA 独有的操作：WRITE 和 READ。&lt;/p>
&lt;p>我们先来复习下双端操作——SEND 和 RECV，然后再对比介绍单端操作——WRITE 和 READ。&lt;/p>
&lt;h2 id="send--recv">SEND &amp;amp; RECV&lt;/h2>
&lt;p>SEND 和 RECV 是两种不同的操作类型，但是因为如果一端进行 SEND 操作，对端必须进行 RECV 操作，所以通常都把他们放到一起描述。&lt;/p>
&lt;p>为什么称之为“双端操作”？因为&lt;strong>完成一次通信过程需要两端 CPU 的参与&lt;/strong>，并且收端需要提前显式的下发 WQE。下图是一次 SEND-RECV 操作的过程示意图。原图来自于[1]，我做了一些修改。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/rdma-op-1-2024-02-25.webp"
alt="rdma-op-1-2024-02-25" width="auto" loading="lazy">
&lt;/figure>
&lt;p>上一篇我们讲过，上层应用通过 WQE（WR）来给硬件下任务。在 SEND-RECV 操作中，不止发送端需要下发 WQE，接收端也需要下发 WQE 来告诉硬件收到的数据需要放到哪个地址。发送端并不知道发送的数据会放到哪里，每次发送数据，接收端都要提前准备好接收 Buffer，而接收端 CPU 自然会感知这一过程。&lt;/p>
&lt;p>为了下文对比 SEND/RECV 与 WRITE/READ 的异同，我们将上一篇的 SEND-RECV 流程中补充内存读写这一环节，即下图中的步骤④——发送端硬件根据 WQE 从内存中取出数据封装成可在链路上传输数据包和步骤⑦——接收端硬件将数据包解析后根据 WQE 将数据放到指定内存区域，其他步骤不再赘述。另外再次强调一下，收发端的步骤未必是图中这个顺序，比如步骤⑧⑪⑫和步骤⑨⑩的先后顺序就是不一定的。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/rdma-op-2-2024-02-25.webp"
alt="rdma-op-2-2024-02-25" width="auto" loading="lazy">
&lt;/figure>
&lt;p>下面将介绍 WRITE 操作，对比之后相信大家可以理解的更好。&lt;/p>
&lt;h2 id="write">WRITE&lt;/h2>
&lt;p>WRITE 全称是 RDMA WRITE 操作，是本端主动写入远端内存的行为，除了准备阶段，远端 CPU 不需要参与，也不感知何时有数据写入、数据在何时接收完毕。所以这是一种单端操作。&lt;/p>
&lt;p>通过下图我们对比一下 WRITE 和 SEND-RECV 操作的差异，本端在准备阶段通过数据交互，获取了对端某一片可用的内存的&lt;strong>地址&lt;/strong>和“&lt;strong>钥匙&lt;/strong>” ，相当于获得了这片远端内存的读写权限。拿到权限之后，本端就可以像访问自己的内存一样&lt;strong>直接对这一远端内存区域进行读写&lt;/strong>，这也是 RDMA——远程直接地址访问的内涵所在。&lt;/p>
&lt;p>WRITE/READ 操作中的目的地址和钥匙是如何获取的呢？通常可以通过我们刚刚讲过的 SEND-RECV 操作来完成，因为拿到钥匙这个过程总归是要由远端内存的控制者——CPU 允许的。虽然准备工作还比较复杂， 但是一旦完成准备工作，RDMA 就可以发挥其优势，对大量数据进行读写。一旦远端的 CPU 把内存授权给本端使用，它便不再会参与数据收发的过程，这就解放了远端 CPU，也降低了通信的时延。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/rdma-op-3-2024-02-25.webp"
alt="rdma-op-3-2024-02-25" width="auto" loading="lazy">
&lt;/figure>
&lt;p>需要注意的是，本端是通过&lt;strong>虚拟地址&lt;/strong>来读写远端内存的，上层应用可以非常方便的对其进行操作。实际的虚拟地址—物理地址的转换是由 RDMA 网卡完成的。具体是如何转换的，将在后面的文章介绍。&lt;/p>
&lt;p>忽略准备阶段 key 和 addr 的获取过程，下面我们描述一次 WRITE 操作的流程，此后我们不再将本端和对端称为“发送”和“接收”端，而是改为“请求”和“响应”端，这样对于描述 WRITE 和 READ 操作都更恰当一些，也不容易产生歧义。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/rdma-op-4-2024-02-25.webp"
alt="rdma-op-4-2024-02-25" width="auto" loading="lazy">
&lt;/figure>
&lt;ol>
&lt;li>请求端 APP 以 WQE（WR）的形式下发一次 WRITE 任务。&lt;/li>
&lt;li>请求端硬件从 SQ 中取出 WQE，解析信息。&lt;/li>
&lt;li>请求端网卡根据 WQE 中的虚拟地址，转换得到物理地址，然后从内存中拿到待发送数据，组装数据包。&lt;/li>
&lt;li>请求端网卡将数据包通过物理链路发送给响应端网卡。&lt;/li>
&lt;li>响应端收到数据包，解析目的虚拟地址，转换成本地物理地址，解析数据，将数据放置到指定内存区域。&lt;/li>
&lt;li>响应端回复 ACK 报文给请求端。&lt;/li>
&lt;li>请求端网卡收到 ACK 后，生成 CQE，放置到 CQ 中。&lt;/li>
&lt;li>请求端 APP 取得任务完成信息。&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>注：严谨地说，第 6 步回复 ACK 之时，RDMA 网卡只能保证数据包中的 Payload 已经被”暂存“了下来，但不能保证一定已经把数据放到目的内存里面了。不过这一点不影响我们对整理流程的理解，感谢@nekomii 同学的提醒。&lt;/p>
&lt;p>IB Spec. 9.7.5.1.6 ACKNOWLEDGE MESSAGE SCHEDULING 原文：”For SEND or RDMA WRITE requests, an ACK may be scheduled before data is actually written into the responder’s memory. The ACK simply indicates that the data has successfully reached the fault domain of the responding node. That is, the data has been received by the channel adapter and the channel adapter will write that data to the memory system of the responding node, or the responding application will at least be informed of the failure.“&lt;/p>
&lt;/blockquote>
&lt;h2 id="read">READ&lt;/h2>
&lt;p>顾名思义，READ 跟 WRITE 是相反的过程，是本端主动读取远端内存的行为。同 WRITE 一样，远端 CPU 不需要参与，也不感知数据在内存中被读取的过程。&lt;/p>
&lt;p>获取 key 和虚拟地址的流程也跟 WRITE 没有区别，需要注意的是 &lt;strong>&amp;ldquo;读”这个动作所请求的数据&lt;/strong>，是在对端回复的报文中携带的。&lt;/p>
&lt;p>下面描述一次 READ 操作的流程，注意跟 WRITE 只是方向和步骤顺序的差别。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/rdma5-2024-02-25.webp"
alt="rdma5-2024-02-25" width="auto" loading="lazy">
&lt;/figure>
&lt;ol>
&lt;li>请求端 APP 以 WQE 的形式下发一次 READ 任务。&lt;/li>
&lt;li>请求端网卡从 SQ 中取出 WQE，解析信息。&lt;/li>
&lt;li>请求端网卡将 READ 请求包通过物理链路发送给响应端网卡。&lt;/li>
&lt;li>响应端收到数据包，解析目的虚拟地址，转换成本地物理地址，解析数据，从指定内存区域取出数据。&lt;/li>
&lt;li>响应端硬件将数据组装成回复数据包发送到物理链路。&lt;/li>
&lt;li>请求端硬件收到数据包，解析提取出数据后放到 READ WQE 指定的内存区域中。&lt;/li>
&lt;li>请求端网卡生成 CQE，放置到 CQ 中。&lt;/li>
&lt;li>请求端 APP 取得任务完成信息。&lt;/li>
&lt;/ol>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>我们忽略各种细节进行抽象，RDMA WRITE 和 READ 操作就是在利用网卡完成下面左图的内存拷贝操作而已，只不过复制的过程是由 RDMA 网卡通过网络链路完成的；而本地内存拷贝则如下面右图所示由 CPU 通过总线完成的：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/rdma-op-6-2024-02-25.webp"
alt="rdma-op-6-2024-02-25" width="auto" loading="lazy">
&lt;/figure>
&lt;p>RDMA 标准定义上述几种操作的时候使用的单词是非常贴切的，“收”和“发”是需要有对端主动参与的语义 ，而‘读“和”写“更像是本端对一个没有主动性的对端进行操作的语义。&lt;/p>
&lt;p>通过对比 SEND/RECV 和 WRITE/READ 操作，我们可以发现传输数据时不需要响应端 CPU 参与的 WRITE/READ 有更大的优势，缺点就是请求端需要在准备阶段获得响应端的一段内存的读写权限。但是实际数据传输时，这个准备阶段的功率和时间损耗都是可以忽略不计的，所以 RDMA WRITE/READ 才是大量传输数据时所应用的操作类型，SEND/RECV 通常只是用来传输一些控制信息。&lt;/p>
&lt;p>除了本文介绍的几种操作之外，还有 ATOMIC 等更复杂一些的操作类型，将在后面的协议解读部分详细分析。本篇就到这里，下一篇将介绍 RDMA 基本服务类型。&lt;/p>
&lt;h2 id="代码示例">代码示例&lt;/h2>
&lt;p>本文中的操作类型都是通过 WQE 来下发的，下面是一个简单的例子，展示了如何使用 libibverbs 来创建一个 QP，然后通过 WQE 来下发一个 WRITE 操作。&lt;/p>
&lt;pre>&lt;code class="language-c">#include &amp;lt;infiniband/verbs.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
int main() {
struct ibv_device **dev_list = ibv_get_device_list(NULL);
struct ibv_context *ctx = ibv_open_device(dev_list[0]);
struct ibv_pd *pd = ibv_alloc_pd(ctx);
struct ibv_cq *cq = ibv_create_cq(ctx, 10, NULL, NULL, 0);
struct ibv_qp *qp;
struct ibv_qp_init_attr qp_init_attr = {
.send_cq = cq,
.recv_cq = cq,
.qp_type = IBV_QPT_RC,
};
qp = ibv_create_qp(pd, &amp;amp;qp_init_attr);
struct ibv_mr *mr;
char *buf = malloc(1024);
mr = ibv_reg_mr(pd, buf, 1024, IBV_ACCESS_LOCAL_WRITE | IBV_ACCESS_REMOTE_WRITE);
struct ibv_sge sge = {
.addr = (uintptr_t)buf,
.length = 1024,
.lkey = mr-&amp;gt;lkey,
};
struct ibv_send_wr wr = {
.wr_id = 1,
.sg_list = &amp;amp;sge,
.num_sge = 1,
.opcode = IBV_WR_RDMA_WRITE,
.send_flags = IBV_SEND_SIGNALED,
};
struct ibv_send_wr *bad_wr;
ibv_post_send(qp, &amp;amp;wr, &amp;amp;bad_wr);
return 0;
}
&lt;/code>&lt;/pre>
&lt;h2 id="参考资料">参考资料&lt;/h2>
&lt;p>[1] part1-OFA_Training_Sept_2016.pdf&lt;/p></description></item><item><title>搭建玄铁 900 系列工具链与 xuantie-qemu 环境</title><link>https://cuterwrite.top/p/thead-tools/</link><pubDate>Tue, 20 Feb 2024 01:51:00 +0000</pubDate><guid>https://cuterwrite.top/p/thead-tools/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/59aa9fecb7e1a3a2b2c88811e6360647195413.jpg@1256w_774h_!web-article-pic-2024-02-20.webp" alt="Featured image of post 搭建玄铁 900 系列工具链与 xuantie-qemu 环境" />&lt;h1 id="搭建玄铁-900-系列工具链与-xuantie-qemu-环境">搭建玄铁 900 系列工具链与 xuantie-qemu 环境&lt;/h1>
&lt;h2 id="一搭建平台">一、搭建平台&lt;/h2>
&lt;ul>
&lt;li>Linux 发行版：CentOS Linux release 7.6.1810 (Core)&lt;/li>
&lt;li>内核版本：3.10.0-957.el7.x86_64&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">$ cat /etc/centos-release
CentOS Linux release 7.6.1810 (Core)
$ uname -r
3.10.0-957.el7.x86_64
&lt;/code>&lt;/pre>
&lt;h2 id="二搭建玄铁-900-系列工具链环境">二、搭建玄铁 900 系列工具链环境&lt;/h2>
&lt;h3 id="1-下载玄铁-900-系列工具链">1. 下载玄铁 900 系列工具链&lt;/h3>
&lt;p>首先，我们需要下载适用于 RISC-V 架构的 Xuantie GNU 工具链。前往&lt;a class="link" href="https://www.xrvm.cn/" target="_blank" rel="noopener" >玄铁官网
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
获取最新版本的预编译包，并根据你的操作系统进行安装。在 Linux 系统中，通常解压后通过添加 &lt;code>bin&lt;/code> 路径到 &lt;code>$PATH&lt;/code> 环境变量即可。&lt;/p>
&lt;p>工具链安装包由于执行平台和目标程序平台的不同分为不同的版本，如 Xuantie-&lt;em>-elf-&lt;/em>-x86_64-V*-.tar.gz 是 64 位 linux 平台的 riscv 裸程序工具链套件。具体分类如下：&lt;/p>
&lt;ul>
&lt;li>根据执行平台
&lt;ul>
&lt;li>x86_64：64 位 linux 平台&lt;/li>
&lt;li>i386：32 位 linux 平台&lt;/li>
&lt;li>mingw：Windows Mingw 平台&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>根据目标程序平台
&lt;ul>
&lt;li>elf：裸程序编译套件&lt;/li>
&lt;li>linux：linux 应用程序编译套件&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>这里我们下载最新的版本为 2.8.1 的适用于 64 位 linux 平台的 linux 应用程序编译套件，即 Xuantie-900-gcc-linux-5.10.4-glibc-x86_64 。&lt;/p>
&lt;pre>&lt;code class="language-bash">wget https://occ-oss-prod.oss-cn-hangzhou.aliyuncs.com/resource//1705395627867/Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.1-20240115.tar.gz
tar -xzvf Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.1-20240115.tar.gz
sudo mv Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.1-20240115 /opt
export PATH=/opt/Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.1-20240115/bin:$PATH
&lt;/code>&lt;/pre>
&lt;h3 id="2-验证工具链安装">2. 验证工具链安装&lt;/h3>
&lt;pre>&lt;code class="language-bash">$ riscv64-unknown-linux-gnu-gcc -v
Using built-in specs.
COLLECT_GCC=riscv64-unknown-linux-gnu-gcc
COLLECT_LTO_WRAPPER=/opt/Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0/bin/../libexec/gcc/riscv64-unknown-linux-gnu/10.4.0/lto-wrapper
Target: riscv64-unknown-linux-gnu
Configured with: /mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/./source/riscv/riscv-gcc/configure --target=riscv64-unknown-linux-gnu --with-gmp=/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/build-gcc-riscv64-unknown-linux-gnu/build-Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0/lib-for-gcc-x86_64-linux --with-mpfr=/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/build-gcc-riscv64-unknown-linux-gnu/build-Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0/lib-for-gcc-x86_64-linux --with-mpc=/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/build-gcc-riscv64-unknown-linux-gnu/build-Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0/lib-for-gcc-x86_64-linux --with-libexpat-prefix=/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/build-gcc-riscv64-unknown-linux-gnu/build-Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0/lib-for-gcc-x86_64-linux --with-libmpfr-prefix=/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/build-gcc-riscv64-unknown-linux-gnu/build-Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0/lib-for-gcc-x86_64-linux --with-pkgversion='Xuantie-900 linux-5.10.4 glibc gcc Toolchain V2.8.0 B-20231018' CXXFLAGS='-g -O2 -DTHEAD_VERSION_NUMBER=2.8.0 ' --prefix=/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/build-gcc-riscv64-unknown-linux-gnu/Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0 --with-sysroot=/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/build-gcc-riscv64-unknown-linux-gnu/Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.8.0/sysroot --with-system-zlib --enable-shared --enable-tls --enable-languages=c,c++,fortran --disable-libmudflap --disable-libssp --disable-libquadmath --enable-libsanitizer --disable-nls --disable-bootstrap --src=https://cuterwrite.top/mnt/ssd/jenkins_iotsw/slave/workspace/Toolchain/build-gnu-riscv_4/./source/riscv/riscv-gcc --enable-multilib --with-abi=lp64d --with-arch=rv64gc_zfh_xtheadc 'CFLAGS_FOR_TARGET=-O2 -mcmodel=medany' 'CXXFLAGS_FOR_TARGET=-O2 -mcmodel=medany'
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 10.4.0 (Xuantie-900 linux-5.10.4 glibc gcc Toolchain V2.8.0 B-20231018)
&lt;/code>&lt;/pre>
&lt;p>可以看到输出了 gcc 的版本信息，说明工具链安装成功。&lt;/p>
&lt;h2 id="三搭建-xuantie-qemu-环境">三、搭建 xuantie-qemu 环境&lt;/h2>
&lt;h3 id="1-前提条件">1. 前提条件&lt;/h3>
&lt;p>在安装 xuantie-qemu 之前，需要确保系统含有以下工具或库。&lt;/p>
&lt;ul>
&lt;li>gcc 编译器&lt;/li>
&lt;li>automake&lt;/li>
&lt;li>autoconf&lt;/li>
&lt;li>libtool&lt;/li>
&lt;li>glib2 库&lt;/li>
&lt;li>其它&amp;hellip;..&lt;/li>
&lt;/ul>
&lt;p>通过以下命令安装上述工具或库。&lt;/p>
&lt;pre>&lt;code class="language-bash">sudo yum update -y
sudo yum install -y autoconf automake libtool make gcc gcc-c++ gawk bison flex texinfo gperf patchutils bc \
zlib-devel mpfr-devel gmp-devel curl-devel expat-devel git \
glib2-devel libfdt-devel pixman-devel ncurses-devel ncurses-compat-libs
&lt;/code>&lt;/pre>
&lt;p>如果是 Ubuntu/Dedian 系统，可以使用以下命令安装。&lt;/p>
&lt;pre>&lt;code class="language-bash">sudo apt-get update
sudo apt-get install -y autoconf automake autotools-dev curl libmpc-dev libmpfr-dev libgmp-dev \
gawk build-essential bison flex texinfo gperf libtool patchutils bc \
zlib1g-dev libexpat-dev git \
libglib2.0-dev libfdt-dev libpixman-1-dev \
libncurses5-dev libncursesw5-dev
&lt;/code>&lt;/pre>
&lt;h3 id="2-下载并安装-xuantie-qemu">2. 下载并安装 xuantie-qemu&lt;/h3>
&lt;p>访问 &lt;a class="link" href="https://github.com/T-head-Semi/qemu.git" target="_blank" rel="noopener" >Xuantie QEMU 官方仓库
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
，获取适用于玄铁 900 系列芯片的 xuantie-qemu 源代码，然后按照常规步骤编译安装：&lt;/p>
&lt;pre>&lt;code class="language-bash">git clone https://github.com/T-head-Semi/qemu.git
git checkout xuantie-qemu-6.1.0
&lt;/code>&lt;/pre>
&lt;h3 id="3-编译安装-xuantie-qemu">3. 编译安装 xuantie-qemu&lt;/h3>
&lt;pre>&lt;code class="language-bash">cd qemu
mkdir build
cd build
../configure --target-list=riscv64-softmmu,riscv64-linux-user --prefix=/opt/qemu/6.1.0-xuantie
make -j $(nproc)
sudo make install
export PATH=/opt/qemu/6.1.0-xuantie/bin:$PATH
&lt;/code>&lt;/pre>
&lt;h3 id="4-验证-xuantie-qemu-安装">4. 验证 xuantie-qemu 安装&lt;/h3>
&lt;p>安装完毕后如果执行如下命令后能够查看到 qemu 的具体版本，则说明安装成功&lt;/p>
&lt;pre>&lt;code class="language-bash">$ qemu-riscv64 --version
qemu-riscv64 version 6.0.94 (v6.1.0-12-g03813c9)
Copyright (c) 2003-2021 Fabrice Bellard and the QEMU Project developers
&lt;/code>&lt;/pre>
&lt;p>编写一段 C 语言程序，如下所示：&lt;/p>
&lt;pre>&lt;code class="language-c">#include &amp;lt;stdio.h&amp;gt;
int main() {
printf(&amp;quot;Hello RISC-V \n&amp;quot;);
return 0;
}
&lt;/code>&lt;/pre>
&lt;p>使用 Xuantie 900 系列工具链编译该程序，并使用用户模式的 xuantie-qemu 运行程序。&lt;/p>
&lt;pre>&lt;code class="language-bash">$ riscv64-unknown-linux-gnu-gcc -static -o hello hello.c
$ qemu-riscv64 ./hello
Hello RISC-V
&lt;/code>&lt;/pre>
&lt;p>再写一段 RVV 向量化的 C 语言程序，如下所示：&lt;/p>
&lt;details>
&lt;summary>&lt;strong>RVV 向量化 C 语言程序&lt;/strong>&lt;/summary>
&lt;pre>&lt;code class="language-c">#include &amp;lt;riscv_vector.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#define N 15
float vsum(float* v, int n) {
vfloat32m1_t vs, vv, vtmp;
float s = 0.0;
int i;
int vlmax;
vlmax = vsetvlmax_e32m1();
printf(&amp;quot;vlmax:%d\n&amp;quot;, vlmax);
vs = vfmv_v_f_f32m1(0.0, vlmax);
vtmp = vfmv_v_f_f32m1(0.0, vlmax);
for (i = 0; i &amp;lt; n - vlmax; i += vlmax) {
vv = vle32_v_f32m1(&amp;amp;v[i], vlmax);
vtmp = vfadd_vv_f32m1(vtmp, vv, vlmax);
}
vs = vfredusum_vs_f32m1_f32m1(vs, vtmp, vs, vlmax);
s = vfmv_f_s_f32m1_f32(vs);
for (; i &amp;lt; n; i++) {
s += v[i];
}
return s;
}
float vsum1(float* v, int n) {
vfloat32m1_t vs, vv;
float s;
int i;
int vl, vlmax;
vlmax = vsetvlmax_e32m1();
vs = vfmv_v_f_f32m1(0.0, vlmax);
for (i = 0; n &amp;gt; 0; i += vl, n -= vl) {
vl = vsetvl_e32m1(n);
printf(&amp;quot;vl:%d\n&amp;quot;, vl);
vv = vle32_v_f32m1(&amp;amp;v[i], vl);
vs = vfredusum_vs_f32m1_f32m1(vs, vv, vs, vl);
}
s = vfmv_f_s_f32m1_f32(vs);
return s;
}
float vsum2(float* v, int n) {
vfloat32m2_t vv;
vfloat32m1_t vs;
float s;
int i;
int vl, vlmax;
vlmax = vsetvlmax_e32m1();
vs = vfmv_v_f_f32m1(0.0, vlmax);
for (i = 0; n &amp;gt; 0; i += vl, n -= vl) {
vl = vsetvl_e32m2(n);
printf(&amp;quot;vl:%d\n&amp;quot;, vl);
vv = vle32_v_f32m2(&amp;amp;v[i], vl);
vs = vfredusum_vs_f32m2_f32m1(vs, vv, vs, vl);
}
s = vfmv_f_s_f32m1_f32(vs);
return s;
}
int main() {
int i;
float v[N], sum = 0.0;
printf(&amp;quot;Hello RISC-V!\n&amp;quot;);
for (i = 0; i &amp;lt; N; i++) {
v[i] = i;
}
sum = vsum(v, N);
printf(&amp;quot;%f\n&amp;quot;, sum);
return 0;
}
&lt;/code>&lt;/pre>
&lt;/details>
&lt;p>编译并运行该程序（这时需要指定 &lt;code>-cpu&lt;/code> ，否则会报非法指定的异常，即 Illegal instruction (core dumped)）：&lt;/p>
&lt;pre>&lt;code class="language-bash">$ riscv64-unknown-linux-gnu-gcc -static -O3 -march=rv64imafdcv0p7_zfh_xtheadc -o test_vec test_vec.c
$ qemu-riscv64 -cpu c920 ./test_vec
Hello RISC-V!
vlmax:4
105.000000
&lt;/code>&lt;/pre>
&lt;h2 id="四在-qemu-上运行-risc-v-64-位-linux-系统">四、在 QEMU 上运行 RISC-V 64 位 Linux 系统&lt;/h2>
&lt;h3 id="1-制作内核">1. 制作内核&lt;/h3>
&lt;h4 id="11-下载内核源码">1.1 下载内核源码&lt;/h4>
&lt;pre>&lt;code class="language-bash">$ wget https://mirrors.edge.kernel.org/pub/linux/kernel/v5.x/linux-5.10.42.tar.gz
$ tar -xzvf linux-5.10.42.tar.gz
&lt;/code>&lt;/pre>
&lt;p>下载后进入内核源码目录&lt;/p>
&lt;pre>&lt;code class="language-bash">$ cd linux-5.10.42
&lt;/code>&lt;/pre>
&lt;h4 id="12-配置和编译内核">1.2 配置和编译内核&lt;/h4>
&lt;pre>&lt;code class="language-bash">$ make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- defconfig
$ make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- -j $(nproc)
...
AR drivers/built-in.a
GEN .version
CHK include/generated/compile.h
LD vmlinux.o
MODPOST vmlinux.symvers
MODINFO modules.builtin.modinfo
GEN modules.builtin
LD .tmp_vmlinux.kallsyms1
KSYMS .tmp_vmlinux.kallsyms1.S
AS .tmp_vmlinux.kallsyms1.S
LD .tmp_vmlinux.kallsyms2
KSYMS .tmp_vmlinux.kallsyms2.S
AS .tmp_vmlinux.kallsyms2.S
LD vmlinux
SYSMAP System.map
MODPOST modules-only.symvers
GEN Module.symvers
CC [M] fs/efivarfs/efivarfs.mod.o
OBJCOPY arch/riscv/boot/Image
GZIP arch/riscv/boot/Image.gz
LD [M] fs/efivarfs/efivarfs.ko
Kernel: arch/riscv/boot/Image.gz is ready
&lt;/code>&lt;/pre>
&lt;h3 id="2-制作-rootfs">2. 制作 rootfs&lt;/h3>
&lt;h4 id="21-下载-busybox-源码">2.1 下载 busybox 源码&lt;/h4>
&lt;pre>&lt;code class="language-bash">$ wget https://busybox.net/downloads/busybox-1.33.1.tar.bz2
&lt;/code>&lt;/pre>
&lt;p>下载完后进入 busybox 源码目录&lt;/p>
&lt;pre>&lt;code class="language-bash">cd busybox-1.33.1
&lt;/code>&lt;/pre>
&lt;h4 id="22-配置-busybox">2.2 配置 busybox&lt;/h4>
&lt;pre>&lt;code class="language-bash">$ make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- defconfig
$ make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- menuconfig
&lt;/code>&lt;/pre>
&lt;p>打开配置菜单后进入第一行的 &amp;ldquo;Settings&amp;rdquo;，在 &amp;ldquo;Build Options&amp;rdquo; 节中，选中 “Build static binary (no shared libs)”，设置好后退出保存配置。&lt;/p>
&lt;p>检查 &lt;code>.config&lt;/code> 文件中是否有 &lt;code>CONFIG_STATIC=y&lt;/code> ，如果没有则手动添加。&lt;/p>
&lt;h4 id="23-编译和安装-busybox">2.3 编译和安装 busybox&lt;/h4>
&lt;pre>&lt;code class="language-bash">$ make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- -j $(nproc)
$ make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- install
&lt;/code>&lt;/pre>
&lt;p>此时源码目录 busyboxsource 下会新出现一个 &lt;code>_install&lt;/code> 目录 ，可以看到生成的东西。&lt;/p>
&lt;pre>&lt;code class="language-bash">$ ls _install
bin linuxrc sbin usr
&lt;/code>&lt;/pre>
&lt;p>进入 &lt;code>_install&lt;/code> 目录，创建以下目录&lt;/p>
&lt;pre>&lt;code class="language-bash">$ cd _install
$ mkdir proc sys dev etc etc/init.d
$ ls
bin dev etc linuxrc proc sbin sys usr
&lt;/code>&lt;/pre>
&lt;p>然后另外再新建一个最简单的 init 的 RC 文件：&lt;/p>
&lt;pre>&lt;code class="language-bash">$ cd etc/init.d/
$ touch rcS
$ vim rcS
&lt;/code>&lt;/pre>
&lt;p>编辑该文件内容为：&lt;/p>
&lt;pre>&lt;code class="language-bash">#!/bin/sh
mount -t proc none /proc
mount -t sysfs none /sys
/sbin/mdev -s
&lt;/code>&lt;/pre>
&lt;p>然后修改 rcS 文件权限，加上可执行权限&lt;/p>
&lt;pre>&lt;code class="language-bash">$ chmod +x rcS
&lt;/code>&lt;/pre>
&lt;h4 id="24-制作文件系统">2.4 制作文件系统&lt;/h4>
&lt;p>继续在 &lt;code>_install&lt;/code> 目录下执行如下命令：&lt;/p>
&lt;pre>&lt;code class="language-bash">$ find -print0 | cpio -0oH newc | gzip -9 &amp;gt; ../rootfs.img
3276 blocks
&lt;/code>&lt;/pre>
&lt;h3 id="3-启动运行">3. 启动运行&lt;/h3>
&lt;p>创建一个新的目录，将编译好的内核 &lt;code>Image&lt;/code> 和制作好的 &lt;code>rootfs.img&lt;/code> 移动到该目录下。&lt;/p>
&lt;pre>&lt;code class="language-bash">$ mkdir riscv64-linux
$ cd riscv64-linux
$ cp ../linux-5.10.42/arch/riscv/boot/Image .
$ cp ../busybox-1.33.1/rootfs.img .
&lt;/code>&lt;/pre>
&lt;p>执行如下命令：&lt;/p>
&lt;pre>&lt;code class="language-bash">$ qemu-system-riscv64 \
-nographic -machine virt \
-kernel Image \
-initrd rootfs.img \
-append &amp;quot;root=/dev/ram rdinit=/sbin/init&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>将显示 Linux Kernel 启动流程：&lt;/p>
&lt;details>
&lt;summary>&lt;strong>点击展开&lt;/strong>&lt;/summary>
&lt;pre>&lt;code class="language-bash">OpenSBI v0.9
____ _____ ____ _____
/ __ \ / ____| _ \_ _|
| | | |_ __ ___ _ __ | (___ | |_) || |
| | | | '_ \ / _ \ '_ \ \___ \| _ &amp;lt; | |
| |__| | |_) | __/ | | |____) | |_) || |_
\____/| .__/ \___|_| |_|_____/|____/_____|
| |
|_|
Platform Name : riscv-virtio,qemu
Platform Features : timer,mfdeleg
Platform HART Count : 1
Firmware Base : 0x80000000
Firmware Size : 100 KB
Runtime SBI Version : 0.2
Domain0 Name : root
Domain0 Boot HART : 0
Domain0 HARTs : 0*
Domain0 Region00 : 0x0000000080000000-0x000000008001ffff ()
Domain0 Region01 : 0x0000000000000000-0xffffffffffffffff (R,W,X)
Domain0 Next Address : 0x0000000080200000
Domain0 Next Arg1 : 0x0000000087000000
Domain0 Next Mode : S-mode
Domain0 SysReset : yes
Boot HART ID : 0
Boot HART Domain : root
Boot HART ISA : rv64imafdcvsu
Boot HART Features : scounteren,mcounteren,time
Boot HART PMP Count : 16
Boot HART PMP Granularity : 4
Boot HART PMP Address Bits: 54
Boot HART MHPM Count : 0
Boot HART MHPM Count : 0
Boot HART MIDELEG : 0x0000000000000222
Boot HART MEDELEG : 0x000000000000b109
[ 0.000000] Linux version 5.10.42 (root@centos) (riscv64-unknown-linux-gnu-gcc (Xuantie-900 linux-5.10.4 glibc gcc Toolchain V2.8.0 B-20231018) 10.4.0, GNU ld (GNU Binutils) 2.35) #1 SMP Wed Feb 21 02:07:46 CST 2024
[ 0.000000] OF: fdt: Ignoring memory range 0x80000000 - 0x80200000
[ 0.000000] efi: UEFI not found.
[ 0.000000] Initial ramdisk at: 0x(____ptrval____) (1085440 bytes)
[ 0.000000] Zone ranges:
[ 0.000000] DMA32 [mem 0x0000000080200000-0x0000000087ffffff]
[ 0.000000] Normal empty
[ 0.000000] Movable zone start for each node
[ 0.000000] Early memory node ranges
[ 0.000000] node 0: [mem 0x0000000080200000-0x0000000087ffffff]
[ 0.000000] Initmem setup node 0 [mem 0x0000000080200000-0x0000000087ffffff]
[ 0.000000] software IO TLB: Cannot allocate buffer
[ 0.000000] SBI specification v0.2 detected
[ 0.000000] SBI implementation ID=0x1 Version=0x9
[ 0.000000] SBI v0.2 TIME extension detected
[ 0.000000] SBI v0.2 IPI extension detected
[ 0.000000] SBI v0.2 RFENCE extension detected
[ 0.000000] SBI v0.2 HSM extension detected
[ 0.000000] riscv: ISA extensions acdfimsuv
[ 0.000000] riscv: ELF capabilities acdfim
[ 0.000000] percpu: Embedded 17 pages/cpu s32360 r8192 d29080 u69632
[ 0.000000] Built 1 zonelists, mobility grouping on. Total pages: 31815
[ 0.000000] Kernel command line: root=/dev/ram rdinit=/sbin/init
[ 0.000000] Dentry cache hash table entries: 16384 (order: 5, 131072 bytes, linear)
[ 0.000000] Inode-cache hash table entries: 8192 (order: 4, 65536 bytes, linear)
[ 0.000000] Sorting __ex_table...
[ 0.000000] mem auto-init: stack:off, heap alloc:off, heap free:off
[ 0.000000] Memory: 108240K/129024K available (7084K kernel code, 3993K rwdata, 4096K rodata, 223K init, 342K bss, 20784K reserved, 0K cma-reserved)
[ 0.000000] Virtual kernel memory layout:
[ 0.000000] fixmap : 0xffffffcefee00000 - 0xffffffceff000000 (2048 kB)
[ 0.000000] pci io : 0xffffffceff000000 - 0xffffffcf00000000 ( 16 MB)
[ 0.000000] vmemmap : 0xffffffcf00000000 - 0xffffffcfffffffff (4095 MB)
[ 0.000000] vmalloc : 0xffffffd000000000 - 0xffffffdfffffffff (65535 MB)
[ 0.000000] lowmem : 0xffffffe000000000 - 0xffffffe007e00000 ( 126 MB)
[ 0.000000] SLUB: HWalign=64, Order=0-3, MinObjects=0, CPUs=1, Nodes=1
[ 0.000000] rcu: Hierarchical RCU implementation.
[ 0.000000] rcu: RCU restricting CPUs from NR_CPUS=8 to nr_cpu_ids=1.
[ 0.000000] rcu: RCU debug extended QS entry/exit.
[ 0.000000] Tracing variant of Tasks RCU enabled.
[ 0.000000] rcu: RCU calculated value of scheduler-enlistment delay is 25 jiffies.
[ 0.000000] rcu: Adjusting geometry for rcu_fanout_leaf=16, nr_cpu_ids=1
[ 0.000000] NR_IRQS: 64, nr_irqs: 64, preallocated irqs: 0
[ 0.000000] riscv-intc: 64 local interrupts mapped
[ 0.000000] plic: plic@c000000: mapped 53 interrupts with 1 handlers for 2 contexts.
[ 0.000000] random: get_random_bytes called from start_kernel+0x31a/0x48c with crng_init=0
[ 0.000000] riscv_timer_init_dt: Registering clocksource cpuid [0] hartid [0]
[ 0.000000] clocksource: riscv_clocksource: mask: 0xffffffffffffffff max_cycles: 0x24e6a1710, max_idle_ns: 440795202120 ns
[ 0.000150] sched_clock: 64 bits at 10MHz, resolution 100ns, wraps every 4398046511100ns
[ 0.003557] Console: colour dummy device 80x25
[ 0.008887] printk: console [tty0] enabled
[ 0.012368] Calibrating delay loop (skipped), value calculated using timer frequency.. 20.00 BogoMIPS (lpj=40000)
[ 0.012666] pid_max: default: 32768 minimum: 301
[ 0.014227] Mount-cache hash table entries: 512 (order: 0, 4096 bytes, linear)
[ 0.014306] Mountpoint-cache hash table entries: 512 (order: 0, 4096 bytes, linear)
[ 0.040922] rcu: Hierarchical SRCU implementation.
[ 0.042741] EFI services will not be available.
[ 0.044926] smp: Bringing up secondary CPUs ...
[ 0.045062] smp: Brought up 1 node, 1 CPU
[ 0.054128] devtmpfs: initialized
[ 0.061463] clocksource: jiffies: mask: 0xffffffff max_cycles: 0xffffffff, max_idle_ns: 7645041785100000 ns
[ 0.061753] futex hash table entries: 256 (order: 2, 16384 bytes, linear)
[ 0.067460] NET: Registered protocol family 16
[ 0.131233] vgaarb: loaded
[ 0.132530] SCSI subsystem initialized
[ 0.134485] usbcore: registered new interface driver usbfs
[ 0.134834] usbcore: registered new interface driver hub
[ 0.135035] usbcore: registered new device driver usb
[ 0.150024] clocksource: Switched to clocksource riscv_clocksource
[ 0.167109] NET: Registered protocol family 2
[ 0.168330] IP idents hash table entries: 2048 (order: 2, 16384 bytes, linear)
[ 0.172076] tcp_listen_portaddr_hash hash table entries: 128 (order: 0, 5120 bytes, linear)
[ 0.172242] TCP established hash table entries: 1024 (order: 1, 8192 bytes, linear)
[ 0.172480] TCP bind hash table entries: 1024 (order: 3, 32768 bytes, linear)
[ 0.172690] TCP: Hash tables configured (established 1024 bind 1024)
[ 0.173861] UDP hash table entries: 256 (order: 2, 24576 bytes, linear)
[ 0.174481] UDP-Lite hash table entries: 256 (order: 2, 24576 bytes, linear)
[ 0.175963] NET: Registered protocol family 1
[ 0.179024] RPC: Registered named UNIX socket transport module.
[ 0.179111] RPC: Registered udp transport module.
[ 0.179150] RPC: Registered tcp transport module.
[ 0.179186] RPC: Registered tcp NFSv4.1 backchannel transport module.
[ 0.179332] PCI: CLS 0 bytes, default 64
[ 0.182716] Unpacking initramfs...
[ 0.263706] Freeing initrd memory: 1056K
[ 0.265678] workingset: timestamp_bits=62 max_order=15 bucket_order=0
[ 0.281052] NFS: Registering the id_resolver key type
[ 0.282003] Key type id_resolver registered
[ 0.282074] Key type id_legacy registered
[ 0.282505] nfs4filelayout_init: NFSv4 File Layout Driver Registering...
[ 0.282631] nfs4flexfilelayout_init: NFSv4 Flexfile Layout Driver Registering...
[ 0.283481] 9p: Installing v9fs 9p2000 file system support
[ 0.284918] NET: Registered protocol family 38
[ 0.285416] Block layer SCSI generic (bsg) driver version 0.4 loaded (major 251)
[ 0.285593] io scheduler mq-deadline registered
[ 0.285692] io scheduler kyber registered
[ 0.295484] pci-host-generic 30000000.pci: host bridge /soc/pci@30000000 ranges:
[ 0.296336] pci-host-generic 30000000.pci: IO 0x0003000000..0x000300ffff -&amp;gt; 0x0000000000
[ 0.296861] pci-host-generic 30000000.pci: MEM 0x0040000000..0x007fffffff -&amp;gt; 0x0040000000
[ 0.296961] pci-host-generic 30000000.pci: MEM 0x0400000000..0x07ffffffff -&amp;gt; 0x0400000000
[ 0.299940] pci-host-generic 30000000.pci: ECAM at [mem 0x30000000-0x3fffffff] for [bus 00-ff]
[ 0.301083] pci-host-generic 30000000.pci: PCI host bridge to bus 0000:00
[ 0.301328] pci_bus 0000:00: root bus resource [bus 00-ff]
[ 0.301486] pci_bus 0000:00: root bus resource [io 0x0000-0xffff]
[ 0.301528] pci_bus 0000:00: root bus resource [mem 0x40000000-0x7fffffff]
[ 0.301568] pci_bus 0000:00: root bus resource [mem 0x400000000-0x7ffffffff]
[ 0.302864] pci 0000:00:00.0: [1b36:0008] type 00 class 0x060000
[ 0.377412] Serial: 8250/16550 driver, 4 ports, IRQ sharing disabled
[ 0.389894] 10000000.uart: ttyS0 at MMIO 0x10000000 (irq = 2, base_baud = 230400) is a 16550A
[ 0.428017] printk: console [ttyS0] enabled
[ 0.430410] [drm] radeon kernel modesetting enabled.
[ 0.457312] loop: module loaded
[ 0.460726] libphy: Fixed MDIO Bus: probed
[ 0.464996] e1000e: Intel(R) PRO/1000 Network Driver
[ 0.465383] e1000e: Copyright(c) 1999 - 2015 Intel Corporation.
[ 0.466272] ehci_hcd: USB 2.0 'Enhanced' Host Controller (EHCI) Driver
[ 0.466724] ehci-pci: EHCI PCI platform driver
[ 0.467203] ehci-platform: EHCI generic platform driver
[ 0.467683] ohci_hcd: USB 1.1 'Open' Host Controller (OHCI) Driver
[ 0.468129] ohci-pci: OHCI PCI platform driver
[ 0.468593] ohci-platform: OHCI generic platform driver
[ 0.469968] usbcore: registered new interface driver uas
[ 0.470477] usbcore: registered new interface driver usb-storage
[ 0.471603] mousedev: PS/2 mouse device common for all mice
[ 0.475055] goldfish_rtc 101000.rtc: registered as rtc0
[ 0.476070] goldfish_rtc 101000.rtc: setting system clock to 2024-02-20T19:37:51 UTC (1708457871)
[ 0.478889] syscon-poweroff soc:poweroff: pm_power_off already claimed (____ptrval____) sbi_shutdown
[ 0.479494] syscon-poweroff: probe of soc:poweroff failed with error -16
[ 0.480977] usbcore: registered new interface driver usbhid
[ 0.481324] usbhid: USB HID core driver
[ 0.483516] NET: Registered protocol family 10
[ 0.491589] Segment Routing with IPv6
[ 0.492256] sit: IPv6, IPv4 and MPLS over IPv4 tunneling driver
[ 0.495528] NET: Registered protocol family 17
[ 0.497086] 9pnet: Installing 9P2000 support
[ 0.497667] Key type dns_resolver registered
[ 0.498706] debug_vm_pgtable: [debug_vm_pgtable ]: Validating architecture page table helpers
[ 0.533266] Freeing unused kernel memory: 220K
[ 0.539682] Run /sbin/init as init process
Please press Enter to activate this console.
&lt;/code>&lt;/pre>
&lt;/details>
&lt;p>见到 &lt;code>&amp;quot;Please press Enter to activate this console.&amp;quot;&lt;/code> 提示后直接回车，无需密码就进入系统了。&lt;/p>
&lt;p>执行几个常用命令测试一下，都能正常工作：&lt;/p>
&lt;pre>&lt;code class="language-bash">/ # ls
bin etc proc sbin usr
dev linuxrc root sys
/ # pwd
/
/ # cd bin
/bin #
/ # ls
arch dumpkmap kill netstat setarch
ash echo link nice setpriv
base32 ed linux32 nuke setserial
base64 egrep linux64 pidof sh
busybox false ln ping sleep
cat fatattr login ping6 stat
chattr fdflush ls pipe_progress stty
chgrp fgrep lsattr printenv su
chmod fsync lzop ps sync
chown getopt makemime pwd tar
conspy grep mkdir reformime touch
cp gunzip mknod resume true
cpio gzip mktemp rev umount
cttyhack hostname more rm uname
date hush mount rmdir usleep
dd ionice mountpoint rpm vi
df iostat mpstat run-parts watch
dmesg ipcalc mt scriptreplay zcat
dnsdomainname kbd_mode mv sed
/bin #
&lt;/code>&lt;/pre>
&lt;p>退出 QEMU 的方法是按下 &lt;code>Ctrl + A&lt;/code> ，松开后再按下 &lt;code>x&lt;/code> 键即可退出 QEMU 。&lt;/p>
&lt;p>如果想要往 QEMU 里面传输文件，可以使用挂载的方式，如下所示：&lt;/p>
&lt;pre>&lt;code class="language-bash">$ mkdir rootfs
$ sudo mount -o loop rootfs.img rootfs
$ cp [-r] [file] ./rootfs/
$ sudo umount rootfs
&lt;/code>&lt;/pre>
&lt;h2 id="五总结">五、总结&lt;/h2>
&lt;p>至此，我们已经成功搭建了玄铁 900 系列的工具链环境以及 xuantie-qemu 仿真环境，这为后续的开发、编译、链接以及运行和调试基于玄铁 900 系列芯片的 RISC-V 应用程序奠定了基础。&lt;/p></description></item><item><title>OpenMP 简介</title><link>https://cuterwrite.top/p/openmp-intro/</link><pubDate>Mon, 19 Feb 2024 01:36:00 +0000</pubDate><guid>https://cuterwrite.top/p/openmp-intro/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/c17b7451a44c1d4370d5ba2b966298ea195413_crop-2024-02-19.webp" alt="Featured image of post OpenMP 简介" />&lt;h1 id="openmp-简介">OpenMP 简介&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;h3 id="什么是-openmp">什么是 OpenMP？&lt;/h3>
&lt;p>OpenMP（Open Multi-Processing）是一种广泛应用的多线程并行编程模型，它为共享内存系统上的并行计算提供了丰富的指令集和 API。起源于 1997 年，OpenMP 由多个领先硬件和软件供应商共同制定标准，旨在简化并行程序的设计与实现过程，以充分利用现代多核处理器的计算能力。&lt;/p>
&lt;p>OpenMP 支持多种编程语言，包括 C、C++ 以及 Fortran 等，并通过在源代码中插入特定的编译指示（pragma），使得开发者能够轻松地将串行代码转化为高效的并行代码。其主要优势在于其简洁性和易用性，允许程序员使用熟悉的编程语言和开发环境，同时提供良好的可移植性和扩展性。&lt;/p>
&lt;p>OpenMP 由非营利性组织管理，由多家软硬件厂家参与，包括 Arm，IBM，Intel，AMD，NVIDIA，Cray，Oracle 等。&lt;/p>
&lt;h3 id="历史版本">历史版本&lt;/h3>
&lt;ul>
&lt;li>在 &lt;a class="link" href="https://www.openmp.org/" target="_blank" rel="noopener" >官网页面
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
可以查询到 OpenMP 的历史版本和发布日期。&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>版本&lt;/th>
&lt;th>发布日期&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Fortran 1.0&lt;/td>
&lt;td>October 1997&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>C/C++ 1.0&lt;/td>
&lt;td>October 1998&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>C/C++ 2.0&lt;/td>
&lt;td>March 2002&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 2.5&lt;/td>
&lt;td>May 2005&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 3.0&lt;/td>
&lt;td>May 2008&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 3.1&lt;/td>
&lt;td>July 2011&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 4.0&lt;/td>
&lt;td>July 2013&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 4.5&lt;/td>
&lt;td>November 2015&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 5.0&lt;/td>
&lt;td>November 2018&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 5.1&lt;/td>
&lt;td>November 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenMP 5.2&lt;/td>
&lt;td>November 2021&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="基础知识">基础知识&lt;/h2>
&lt;h3 id="技术框架">技术框架&lt;/h3>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/openmp-arch-2024-02-20.webp"
alt="openmp-arch-2024-02-20" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>OpenMP 技术框架&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>&lt;strong>OpenMP Runtime Library&lt;/strong> 是 OpenMP 规范中定义的一组函数和运行时支持结构，它是 OpenMP 并行编程框架的关键组成部分。这个库在编译器的支持下与用户程序链接，在程序执行时负责管理线程的创建、同步、调度以及数据共享等任务。它实现了 OpenMP 编译指导所指示的所有并行化机制。&lt;/p>
&lt;p>&lt;strong>OpenMP Runtime Library&lt;/strong> 包括了如下功能：&lt;/p>
&lt;ul>
&lt;li>线程管理（创建、销毁、同步）
= 工作共享（动态工作分配给各个线程）
= 任务调度
= 同步原语（如 barriers, locks, atomic operations）
= 动态调整线程数量&lt;/li>
&lt;li>内存模型支持（数据环境变量、private, shared, reduction 变量等）&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Compiler Directives&lt;/strong> 编译指导是以 &lt;code>#pragma omp&lt;/code> 开头的预处理器指令，程序员在源代码中插入这些指令来指导编译器如何将串行程序转换为并行程序。例如，使用 &lt;code>#pragma omp parallel&lt;/code> 指令定义一个并行区域，编译器会在此区域内生成多线程执行逻辑。&lt;/p>
&lt;p>&lt;strong>Environment Variables&lt;/strong> 环境变量是 OpenMP 运行时库的一部分，它们用于控制运行时行为，例如线程数量、调度策略等。&lt;/p>
&lt;p>&lt;strong>OpenMP Library&lt;/strong> 是一组函数库，包括了一些用于线程同步、原子操作、锁、并行循环等的函数。这些函数可以在用户程序中直接调用，以实现更细粒度的并行化。&lt;/p>
&lt;p>总的来说，OpenMP 技术框架包括了编译器指导、运行时库、环境变量和函数库等多个组成部分，它们共同构成了一个完整的并行编程环境，共同协作以支持在共享内存系统上的并行编程。&lt;/p>
&lt;h3 id="执行模型fork-join-model">执行模型：Fork-Join Model&lt;/h3>
&lt;p>OpenMP 的执行模型采用的是 Fork-Join 机制，这是一种用于并行编程中的同步原语模型。在该模型下，程序执行遵循以下步骤：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Fork（派生）阶段&lt;/strong>： 程序开始时以单个主线程执行，当遇到 OpenMP 编译指导（pragma）指示的并行区域时，主线程会通过 Runtime Library 创建一个或多个工作线程（worker threads）。这些工作线程是对主线程的派生，每个线程负责执行并行区域内的部分任务。并行区域可以是循环、段（sections）、单一任务或其他可并行化的代码块。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Parallel Execution（并行执行）阶段&lt;/strong>： 创建出的工作线程独立并发地执行分配给它们的任务，并且能够访问共享的数据结构。OpenMP 提供了一套丰富的指令来管理数据的同步和通信，确保在多线程环境下的正确性和一致性。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Join（合并）阶段&lt;/strong>： 当所有工作线程完成其在并行区域内的任务后，它们会自动或者通过显式同步指令（如 &lt;code>omp barrier&lt;/code> ）汇聚到 join 点。在此阶段，所有线程等待直至所有其他线程都到达了同步点，随后 join 操作发生。这意味着主线程和其他工作线程重新同步，恢复为串行执行模式或继续执行后续的非并行代码。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Synchronization and Data Consistency（同步与数据一致性）&lt;/strong>： Fork-Join 模型确保了在并行执行过程中，通过适当的锁机制、原子操作和同步原语保证了对共享资源的互斥访问以及数据的一致性。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>总结来说，OpenMP 的 Fork-Join 执行模型是一种基于动态线程创建和同步的并行处理框架，它允许开发者方便地将串行代码转化为并行执行的代码片段，同时简化了并行编程中常见的复杂性，如线程管理和数据同步问题。&lt;/p>
&lt;h3 id="线程与进程">线程与进程&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>进程&lt;/p>
&lt;ul>
&lt;li>每个进程都有自己独立的地址空间&lt;/li>
&lt;li>CPU 在进程间切换时需要进行上下文切换&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>线程&lt;/p>
&lt;ul>
&lt;li>一个进程下的线程共享相同的地址空间&lt;/li>
&lt;li>CPU 在线程之间切换开销较小&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>操作系统的线程设计&lt;/p>
&lt;ul>
&lt;li>现代操作系统如 Linux、Windows 等都支持一个进程下有多个线程。&lt;/li>
&lt;li>线程是操作系统调度的基本单位，进程是资源分配的基本单位。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/slide_10-2024-02-20.webp"
alt="slide_10-2024-02-20" width="80%" loading="lazy">&lt;figcaption>
&lt;h4>操作系统的线程设计&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="线程的硬件调度">线程的硬件调度&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>硬件调度机制与操作系统协同，负责将线程智能地映射至可用的 CPU 物理核心上执行。&lt;/strong>&lt;/li>
&lt;li>因此，在多线程应用中，当活跃线程数超过了实际 CPU 物理核心的数量时，操作系统将不得不进行密集的上下文切换，以确保多个线程在有限的核心资源上交替运行，这种线程竞争过载的现象会导致整体性能瓶颈和效率下降。&lt;/li>
&lt;li>&lt;strong>超线程技术（Hyper-Threading）&lt;/strong> 通过在单个物理 CPU 核心上虚拟化出额外的逻辑处理单元，当前通常配置为每个物理核心承载两个逻辑核心。这些逻辑核心能够并行执行独立的任务流，尽管它们共享同一物理核心的基础计算资源，如执行引擎、缓存和其他底层硬件结构。通过这种方式，超线程旨在提高资源利用率和并发处理能力，尤其是在存在大量并行任务且其对计算资源需求相对较小的情况下，可以有效提升系统的总体吞吐量。然而，在某些高度依赖单一核心性能或内存带宽的应用场景下，如部分对 CPU 敏感的游戏或特定类型的数据密集型运算，增加逻辑核心可能并不一定能带来显著的性能提升。&lt;/li>
&lt;/ul>
&lt;h3 id="硬件的内存模型">硬件的内存模型&lt;/h3>
&lt;ul>
&lt;li>在现代多核处理器体系结构中，每个 CPU 核心为了进一步提升数据访问速度，在与主存之间设计有多级缓存层次结构。最靠近 CPU 核心的是 L1 缓存，通常其后是 L2 缓存，部分高端架构还包含 L3 缓存，这些高速缓存层级存储容量逐层增大，但访问延迟逐层增加。&lt;/li>
&lt;li>L1 和 L2 缓存通常是与特定 CPU 核心紧密耦合且私有的，这意味着每个核心拥有自己的独立缓存空间，以降低数据访问冲突并提高缓存命中率。L1 缓存由于距离计算单元最近，其访问速度最快，但容量最小；而 L2 缓存作为 L1 缓存的有效补充，具有相对较大的容量。&lt;/li>
&lt;li>为确保在多核环境中不同 CPU 核心的缓存中对共享数据的一致性，硬件和操作系统共同实现了缓存一致性协议（如 MESI 协议）。这种机制允许系统自动维护一个全局一致的数据视图，即使数据在多个核心的缓存中存在副本也能保证它们同步更新，这一特性在某些架构中被称作 &lt;strong>ccNUMA（cache-coherent non-uniform memory access）&lt;/strong> ，即缓存一致性非统一内存访问。&lt;/li>
&lt;li>然而，这种缓存一致性管理也带来了一些挑战，其中之一就是“伪共享”(False Sharing)问题。当不同的线程修改位于同一缓存行内的各自独立变量时，尽管这些变量本身并无关联，但由于它们物理上相邻而被存储在同一缓存行内，因此任何针对其中一个变量的写操作都会导致整个缓存行失效并在所有核心间重新同步。这会引发不必要的缓存无效化与重新填充操作，从而显著降低性能。解决伪共享问题通常需要精心设计数据布局或利用缓存行对齐等技术手段来避免无关数据之间的争用。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/20170115165700476-2024-02-20.webp"
alt="20170115165700476-2024-02-20" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>典型的现代 CPU 内存结构&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="线程亲和性和线程绑定">线程亲和性和线程绑定&lt;/h3>
&lt;ul>
&lt;li>线程亲和性（Thread Affinity）是指操作系统或应用程序控制特定线程与处理器核心之间关联的能力。在多核或多处理器系统中，线程亲和性允许程序员或调度器决定将某个线程固定在特定的 CPU 核心上运行，而不是让操作系统自由地在所有可用的核心间进行动态调度。这种机制有助于减少上下文切换开销，提高缓存命中率，并且对于需要保持数据局部性的并行计算任务特别有益。&lt;/li>
&lt;li>线程绑定（Thread Pinning）是实现线程亲和性的具体技术手段，它指明了将特定线程与特定硬件资源（如 CPU 核心或 NUMA 节点）之间的强制关联。通过线程绑定，可以确保指定的线程始终在其分配的核心上执行，避免被操作系统迁移到其他核心，从而优化性能、减少延迟并解决伪共享等问题。在 OpenMP 等并行编程模型中，可以通过相关的环境变量或编译指导来设置线程绑定策略，以适应不同的并行计算需求和硬件特性。&lt;/li>
&lt;li>同一个插槽上的 CPU 对 L3 缓存的访问延迟是一致的，但不同插槽上的 CPU 对 L3 缓存的访问延迟是不一致的。因此，线程绑定的目的是为了减少线程在不同 CPU 之间的迁移，从而减少访存延迟。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/u=237293070,3563798054&amp;amp;fm=253&amp;amp;app=138&amp;amp;f=JPEG-2024-02-20.webp"
alt="u=237293070,3563798054&amp;amp;fm=253&amp;amp;app=138&amp;amp;f=JPEG-2024-02-20" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>线程亲和性和线程绑定&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>OpenMP 支持控制线程的绑定
&lt;ul>
&lt;li>环境变量 &lt;code>OMP_PROC_BIND&lt;/code> 或从句 &lt;code>proc_bind(master|close|spread)&lt;/code> 控制线程绑定与否，以及线程对于绑定单元（称为 place）分布&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="openmp-编程">OpenMP 编程&lt;/h2>
&lt;h3 id="安装">安装&lt;/h3>
&lt;p>对于 Linux 系统，GCC 是常用的编译器，现代版本的 GCC 一般已默认支持 OpenMP。例如在 Ubuntu 20.04 LTS 上，可以通过以下命令安装含有 OpenMP 的 build-essential 包：&lt;/p>
&lt;pre>&lt;code class="language-bash">$ sudo apt-get update
$ sudo apt-get install -y build-essential
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>查看 OpenMP 版本&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">$ echo |cpp -fopenmp -dM |grep -i open
#define _OPENMP 201511
&lt;/code>&lt;/pre>
&lt;h3 id="编译使用">编译使用&lt;/h3>
&lt;ul>
&lt;li>直接在编译语句中添加 &lt;code>-fopenmp&lt;/code> 选项即可开启 OpenMP 支持。&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">g++ -O2 -std=c++17 -fopenmp hello.cpp -o hello
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>如果使用 CMake 构建项目, 加入 &lt;code>-Wunknown-pragmas&lt;/code> 选项可以在编译时报告未处理的 &lt;code>#pragma&lt;/code> 指令。&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-cmake">find_package(OpenMP REQUIRED)
add_compile_options(-Wunknown-pragmas)
add_executable(hello hello.cpp)
target_link_libraries(hello PRIVATE OpenMP::OpenMP_CXX)
&lt;/code>&lt;/pre>
&lt;h3 id="hello-world">Hello World!&lt;/h3>
&lt;ul>
&lt;li>第一个 OpenMP 程序&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-c">#include &amp;lt;omp.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
int main() {
#pragma omp parallel num_threads(8)
{
int id = omp_get_thread_num();
int num_threads = omp_get_num_threads();
printf(&amp;quot;Hello World from thread %d of %d \n&amp;quot;, id, num_threads);
}
return 0;
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>运行结果&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">$ gcc -fopenmp hello.c -o hello
$ ./hello
Hello World from thread 7 of 8
Hello World from thread 6 of 8
Hello World from thread 0 of 8
Hello World from thread 3 of 8
Hello World from thread 1 of 8
Hello World from thread 2 of 8
Hello World from thread 5 of 8
Hello World from thread 4 of 8
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>同一类 openmp 制导语句称为一种构造(construct)&lt;/li>
&lt;li>形式为 &lt;code>#pragma omp &amp;lt;directive name&amp;gt; &amp;lt;clause&amp;gt;&lt;/code>&lt;/li>
&lt;li>使用 &lt;code>{}&lt;/code> 包裹的代码块称为并行区域(parallel region)&lt;/li>
&lt;/ul>
&lt;h3 id="线程数设置">线程数设置&lt;/h3>
&lt;ul>
&lt;li>优先级由低到高
&lt;ul>
&lt;li>什么都不做，系统选择运行线程数&lt;/li>
&lt;li>设置环境变量 &lt;code>export OMP_NUM_THREADS=4&lt;/code>&lt;/li>
&lt;li>代码中使用库函数 &lt;code>void omp_set_num_threads(int)&lt;/code>&lt;/li>
&lt;li>通过制导语句 &lt;code>num_threads(4)&lt;/code>&lt;/li>
&lt;li>if 从句判断串行还是并行执行&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="常用库函数">常用库函数&lt;/h3>
&lt;ul>
&lt;li>设置并行区运行线程数：&lt;code>void omp_set_num_threads(int)&lt;/code>&lt;/li>
&lt;li>获取并行区运行线程数：&lt;code>int omp_get_num_threads()&lt;/code>&lt;/li>
&lt;li>获取当前线程编号：&lt;code>int omp_get_thread_num()&lt;/code>&lt;/li>
&lt;li>获得 OpenMP Wall Clock 时间（单位：秒）：&lt;code>double omp_get_wtime()&lt;/code>&lt;/li>
&lt;li>获得时间精度：&lt;code>double omp_get_wtick()&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="parallel-构造">Parallel 构造&lt;/h3>
&lt;p>&lt;strong>支持的从句&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;code>if(scalar_expression)&lt;/code>：如果 &lt;code>scalar_expression&lt;/code> 为真，则并行执行，否则串行执行。&lt;/li>
&lt;li>&lt;code>num_threads(integer_expression)&lt;/code>：指定并行区域中的线程数。&lt;/li>
&lt;li>&lt;code>default(shared|none)&lt;/code>：指定变量的默认共享性。
&lt;ul>
&lt;li>&lt;code>shared&lt;/code>：所有变量默认为共享。&lt;/li>
&lt;li>&lt;code>none&lt;/code>：无默认变量类型，每个变量都需要显式声明共享或私有。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>shared(list)&lt;/code>：指定共享变量列表。
&lt;ul>
&lt;li>共享变量在内存中只有一份，所有线程都可以访问。&lt;/li>
&lt;li>请保证共享变量访问不会冲突。&lt;/li>
&lt;li>不特别指定并行区变量默认为 &lt;strong>shared&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>private(list)&lt;/code>：指定私有变量列表。
&lt;ul>
&lt;li>私有变量在每个线程中都有一份独立的拷贝。&lt;/li>
&lt;li>变量需要 &lt;strong>重新初始化&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>firstprivate(list)&lt;/code>：指定首次私有变量列表。
&lt;ul>
&lt;li>同 &lt;code>private&lt;/code>&lt;/li>
&lt;li>对变量根据主线程中的数据进行初始化。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 1： no clause、private、firstprivate&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">int results[4];
int cnt;
cnt = 1;
#pragma omp parallel num_threads(4)
{
int tid = omp_get_thread_num();
for (int i = 0; i &amp;lt; 4; i++) {
cnt += 1;
}
results[tid] = cnt;
}
printf(&amp;quot;no clause: &amp;quot;);
for (int i = 0; i &amp;lt; 4; i++) {
printf(&amp;quot;%d &amp;quot;, results[i]);
}
printf(&amp;quot;\n&amp;quot;);
cnt = 1;
#pragma omp parallel num_threads(4) private(cnt)
{
int tid = omp_get_thread_num();
for (int i = 0; i &amp;lt; 4; i++) {
cnt += 1;
}
results[tid] = cnt;
}
printf(&amp;quot;private(not init): &amp;quot;);
for (int i = 0; i &amp;lt; 4; i++) {
printf(&amp;quot;%d &amp;quot;, results[i]);
}
printf(&amp;quot;\n&amp;quot;);
cnt = 1;
#pragma omp parallel num_threads(4) firstprivate(cnt)
{
int tid = omp_get_thread_num();
for (int i = 0; i &amp;lt; 4; i++) {
cnt += 1;
}
results[tid] = cnt;
}
printf(&amp;quot;firstprivate: &amp;quot;);
for (int i = 0; i &amp;lt; 4; i++) {
printf(&amp;quot;%d &amp;quot;, results[i]);
}
printf(&amp;quot;\n&amp;quot;);
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>打印结果&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">no clause: 5 9 13 17
private(not init): 4 1572916964 1572916964 1572916964
firstprivate: 5 5 5 5
&lt;/code>&lt;/pre>
&lt;h3 id="for-构造">For 构造&lt;/h3>
&lt;ul>
&lt;li>最常用的并行化构造之一&lt;/li>
&lt;/ul>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 2：并行化 for 循环&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">#pragma omp parallel num_threads(8)
{
int tid = omp_get_thread_num();
int num_threads = omp_get_num_threads();
#pragma omp for
for (int i = 0; i &amp;lt; num_threads; i++) {
#pragma omp ordered
printf(&amp;quot;Hello from thread %d of %d \n&amp;quot;, tid, num_threads);
}
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>打印结果&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">Hello from thread 0 of 8
Hello from thread 1 of 8
Hello from thread 2 of 8
Hello from thread 3 of 8
Hello from thread 4 of 8
Hello from thread 5 of 8
Hello from thread 6 of 8
Hello from thread 7 of 8
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>在并行区内对 for 循环进行线程划分，且 for 循环满足格式要求
&lt;ul>
&lt;li>init-expr:需要是 &lt;code>var=lb&lt;/code> 形式，类型也有限制&lt;/li>
&lt;li>test-expr:限制为 &lt;code>var relational-opb&lt;/code> 或者 &lt;code>b relational-op var&lt;/code>&lt;/li>
&lt;li>incr-expr:仅限加减法&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="parallel-for-构造">Parallel for 构造&lt;/h3>
&lt;ul>
&lt;li>常常将 &lt;code>parallel&lt;/code> 和 &lt;code>for&lt;/code> 结合使用，合并为 &lt;code>parallel for&lt;/code> 制导语句&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>parallel&lt;/th>
&lt;th>for   &lt;/th>
&lt;th>parallel for&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>if&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>num_threads&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>default&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>copyin&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>private&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>firstprivate&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>shared&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>reduction&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>lastprivate&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>schedule&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ordered&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>collapse&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>nowait&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>
&lt;p>&lt;code>lastprivate(list)&lt;/code>&lt;/p>
&lt;ul>
&lt;li>同 &lt;code>private&lt;/code>&lt;/li>
&lt;li>执行完 for 循环后，将最后一个线程的值赋给主线程的变量。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>nowait&lt;/code>：取消代码块结束时的栅栏同步(barrier)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>collapse(n)&lt;/code>：应用于 n 重循环，合并(展开)循环&lt;/p>
&lt;ul>
&lt;li>注意循环之间是否有数据依赖&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>ordered&lt;/code>：声明有潜在的顺序执行部分&lt;/p>
&lt;ul>
&lt;li>使用 &lt;code>#pragma omp ordered&lt;/code> 标记顺序执行代码(搭配使用)&lt;/li>
&lt;li>ordered 区内的语句任意时刻仅由最多一个线程执行&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>shedule(type[,chunk])&lt;/code>&lt;/p>
&lt;ul>
&lt;li>&lt;code>type&lt;/code>：指定循环迭代的调度策略
&lt;ul>
&lt;li>&lt;code>static&lt;/code>：静态调度，chunk 大小固定（默认 n/p ）&lt;/li>
&lt;li>&lt;code>dynamic&lt;/code>：动态调度，chunk 大小固定（默认为 1）&lt;/li>
&lt;li>&lt;code>guided&lt;/code>：引导调度，chunk 大小动态调整&lt;/li>
&lt;li>&lt;code>runtime&lt;/code>：由系统环境变量 &lt;code>OMP_SCHEDULE&lt;/code> 指定&lt;/li>
&lt;li>&lt;code>auto&lt;/code>：自动调度&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>chunk&lt;/code>：指定每个线程获取的迭代次数&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="特殊的数据从句reduction">特殊的数据从句：Reduction&lt;/h3>
&lt;p>在 OpenMP 中，reduction 是一种并行编程技术，用于解决多线程环境下的数据竞争问题，特别是在计算全局变量的累加或类似操作时。当多个线程需要同时修改同一个共享变量，并且这些修改可以通过某种二元运算符（如加法、乘法等）将所有线程的结果合并成一个最终结果时，可以使用 &lt;code>reduction&lt;/code> 子句。&lt;/p>
&lt;p>具体来说，reducton 的执行过程为：&lt;/p>
&lt;ul>
&lt;li>fork 线程并分配任务&lt;/li>
&lt;li>每一个线程定义一个私有变量 &lt;code>omp_priv&lt;/code>
&lt;ul>
&lt;li>同 &lt;code>private&lt;/code>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>各个线程执行计算&lt;/li>
&lt;li>所有 &lt;code>omp_priv&lt;/code> 和 &lt;code>omp_in&lt;/code> 一起顺序进行 reduction，写回原变量。&lt;/li>
&lt;/ul>
&lt;p>相比之下，&lt;strong>atomic&lt;/strong> 是 OpenMP 提供的另一种同步机制，它确保对单个内存位置的访问在多线程环境中是原子性的，即一次只允许一个线程对该内存位置进行读取或写入操作。通过 &lt;code>#pragma omp atomic&lt;/code> 指令，可以保证一条简单的赋值语句（或某些特定类型的读改写操作）在并发环境下不会发生数据竞争。&lt;/p>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 3：Reduction&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">int sum = 0;
double start = omp_get_wtime();
#pragma omp parallel for num_threads(8) reduction(+ : sum)
for (int i = 0; i &amp;lt; 100000; i++) {
sum += i;
}
printf(&amp;quot;sum = %d\n&amp;quot;, sum);
printf(&amp;quot;Reduction time: %.5lf s\n&amp;quot;, omp_get_wtime() - start);
// no reduction
sum = 0;
start = omp_get_wtime();
#pragma omp parallel for num_threads(8)
for (int i = 0; i &amp;lt; 100000; i++) {
#pragma omp atomic
sum += i;
}
printf(&amp;quot;sum = %d\n&amp;quot;, sum);
printf(&amp;quot;Atomic time: %.5lf s\n&amp;quot;, omp_get_wtime() - start);
return 0;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>打印结果&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">sum = 704982704
Reduction time: 0.00062 s
sum = 704982704
Atomic time: 0.01021 s
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>两者结果相同，但是 reduction 的执行时间更短，这是因为 reduction 通过为每个线程分配一个私有副本，线程可以在其私有空间内自由地执行归约操作，而不需要在更新全局结果时与其他线程争夺锁资源，加上高效的数据合并方法等。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/reduction-omp-2024-02-20.webp"
alt="reduction-omp-2024-02-20" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>OpenMP reducton operation&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="同步构造">同步构造&lt;/h3>
&lt;h4 id="sections-构造">Sections 构造&lt;/h4>
&lt;ul>
&lt;li>将并行区的代码块划分为多个 section 分配执行。&lt;/li>
&lt;li>可以搭配 parallel 合成为 parallel sections 构造。&lt;/li>
&lt;li>每个 section 由一个线程执行
&lt;ul>
&lt;li>线程数大于 section 数目：部分线程空闲&lt;/li>
&lt;li>线程数小于 section 数目：部分线程分配多个 section&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>示例代码：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-c">#pragma omp sections
{
#pragma omp section
method1();
#pragma omp section
method2();
}
&lt;/code>&lt;/pre>
&lt;h4 id="barrier-构造">Barrier 构造&lt;/h4>
&lt;ul>
&lt;li>在特定位置进行栅栏同步&lt;/li>
&lt;li>在存在数据依赖的情况下，可以使用 barrier 保证数据的一致性&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/Barrier-2024-02-20.webp"
alt="Barrier-2024-02-20" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Barrier 同步示意图&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h4 id="single-构造">Single 构造&lt;/h4>
&lt;ul>
&lt;li>用于标记只有一个线程执行的代码块，带有隐式的 barrier 同步，可以使用 nowait 取消隐式的 barrier 同步。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/omp-single-2024-02-20.webp"
alt="omp-single-2024-02-20" width="70%" loading="lazy">&lt;figcaption>
&lt;h4>pragma single&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h4 id="atomic-构造">Atomic 构造&lt;/h4>
&lt;ul>
&lt;li>用于保证对共享变量的原子操作，避免数据竞争。&lt;/li>
&lt;/ul>
&lt;h3 id="false-sharing">False Sharing&lt;/h3>
&lt;ul>
&lt;li>伪共享简单来说就是指多个线程同时访问同一缓存行的不同部分，导致缓存行的无效化和重新填充，从而降低了程序的性能。&lt;/li>
&lt;li>不同核心对同一 Cache line 的同时读写会造成严重的冲突，导致改级缓存失效。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/false-sharing-2024-02-20.webp"
alt="false-sharing-2024-02-20" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>False Sharing 问题&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>在 OpenMP 中，解决伪共享的方法主要有：
&lt;ul>
&lt;li>&lt;strong>数据结构对齐&lt;/strong> ：通过使用编译器提供的对齐指令或关键字确保相关变量分别处于不同的缓存行中。例如，在 C++中可以使用 &lt;code>alignas&lt;/code> 关键字来指定变量的内存对齐方式，确保每个线程的数据独立位于不同的缓存行。&lt;/li>
&lt;li>&lt;strong>增大缓存行之间的间距&lt;/strong> ：在相邻变量之间插入足够的填充空间，使得它们不会出现在同一个缓存行内。&lt;/li>
&lt;li>&lt;strong>避免无意义的竞争&lt;/strong> ：设计算法和数据结构以减少不必要的共享数据访问。如果可能，尽量让线程操作各自独立的数据段。&lt;/li>
&lt;li>&lt;strong>自定义内存分配&lt;/strong> ：使用特殊的内存分配函数，确保分配的连续内存区域对齐到缓存行边界，这样分配给不同线程的数据就不会落在同一缓存行上。&lt;/li>
&lt;li>在某些情况下，可以利用特定平台提供的硬件特性或者编译器支持的扩展，比如 Intel 的 &lt;code>__declspec(align(#))&lt;/code> 属性（对于 MSVC）或者 &lt;code>__attribute__((aligned(#)))&lt;/code>（对于 GCC/Clang）。&lt;/li>
&lt;li>也可以通过控制变量的作用域或者利用动态创建私有副本等技术来间接避免伪共享问题。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="任务构造">任务构造&lt;/h3>
&lt;ul>
&lt;li>除了 Fork-Join 模型外，OpenMP 还支持任务并行模型，通过 &lt;code>task&lt;/code> 制导语句来实现。&lt;/li>
&lt;li>即动态地管理线程池和任务池，线程池中的线程可以动态地获取任务池中的任务进行执行，从而实现任务的并行执行。&lt;/li>
&lt;/ul>
&lt;div class="notice notice-info" >
&lt;div class="notice-title">&lt;svg t="1705940100069" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6252" width="200" height="200">&lt;path d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m32 664c0 4.4-3.6 8-8 8h-48c-4.4 0-8-3.6-8-8V456c0-4.4 3.6-8 8-8h48c4.4 0 8 3.6 8 8v272z m-32-344c-26.5 0-48-21.5-48-48s21.5-48 48-48 48 21.5 48 48-21.5 48-48 48z" p-id="6253" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>示例 4：任务并行&lt;/p>&lt;/div>
&lt;pre>&lt;code class="language-c">#include &amp;lt;iostream&amp;gt;
#include &amp;lt;omp.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
#include &amp;lt;iomanip&amp;gt;
void big_task(int i) {
sleep(10);
}
void small_task(int i) {
sleep(1);
}
int main() {
int ntasks = 8;
double start = omp_get_wtime();
#pragma omp parallel
{
#pragma omp single
{
std::cout &amp;lt;&amp;lt; &amp;quot;Task 0 Created&amp;quot; &amp;lt;&amp;lt; std::endl;
#pragma omp task
big_task(0);
std::cout &amp;lt;&amp;lt; &amp;quot;Task 1 Created&amp;quot; &amp;lt;&amp;lt; std::endl;
#pragma omp task
big_task(1);
for (int i = 2; i &amp;lt; ntasks; i++) {
std::cout &amp;lt;&amp;lt; &amp;quot;Task &amp;quot; &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &amp;quot; Created&amp;quot; &amp;lt;&amp;lt; std::endl;
#pragma omp task
small_task(i);
}
}
#pragma omp taskwait
}
std::cout &amp;lt;&amp;lt; &amp;quot;All tasks finished&amp;quot; &amp;lt;&amp;lt; std::endl;
std::cout &amp;lt;&amp;lt; &amp;quot;Time: &amp;quot; &amp;lt;&amp;lt; std::fixed &amp;lt;&amp;lt; std::setprecision(2) &amp;lt;&amp;lt; omp_get_wtime() - start &amp;lt;&amp;lt; &amp;quot;s&amp;quot; &amp;lt;&amp;lt; std::endl;
return 0;
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>运行结果&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">$ g++ -fopenmp task.cpp -o task
$ ./task
Task 0 Created
Task 1 Created
Task 2 Created
Task 3 Created
Task 4 Created
Task 5 Created
Task 6 Created
Task 7 Created
All tasks finished
Time: 10.00s
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>这段代码中，我们使用了 &lt;code>#pragma omp task&lt;/code> 制导语句来创建任务，任务的执行由线程池中的线程动态获取并执行。在任务创建后，我们使用 &lt;code>#pragma omp taskwait&lt;/code> 来等待所有任务执行完毕。达到了一个异步执行的效果。&lt;/li>
&lt;/ul>
&lt;h3 id="向量化simd-构造">向量化：SIMD 构造&lt;/h3>
&lt;ul>
&lt;li>SIMD（Single Instruction, Multiple Data）是一种并行计算模式，它通过一条指令同时对多个数据进行操作，从而实现高效的数据并行计算。&lt;/li>
&lt;li>在 OpenMP 中，可以使用 &lt;code>#pragma omp simd&lt;/code> 制导语句来实现向量化并行计算。
&lt;ul>
&lt;li>&lt;code>aligned&lt;/code> 用于列出内存对齐的指针&lt;/li>
&lt;li>&lt;code>safelen&lt;/code> 用于标记循环展开时的数据依赖&lt;/li>
&lt;li>&lt;code>linear&lt;/code> 用于标记循环变量的线性关系&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>编译器例如 gcc 也自带向量化功能，一般使用以下编译选项
&lt;ul>
&lt;li>-O3&lt;/li>
&lt;li>-ffast-math&lt;/li>
&lt;li>-fivopts&lt;/li>
&lt;li>-march=native&lt;/li>
&lt;li>-fopt-info-vec&lt;/li>
&lt;li>-fopt-info-vec-missed&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>RDMA 基本元素</title><link>https://cuterwrite.top/p/rdma-element/</link><pubDate>Fri, 02 Feb 2024 01:01:01 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-element/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/73c30b990886bf6988c97858a3e16011195413_crop-2024-02-04.webp" alt="Featured image of post RDMA 基本元素" />&lt;h1 id="rdma-基本元素">RDMA 基本元素&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/141267386">&lt;cite>知乎专栏：3. RDMA 基本元素&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>RDMA 技术中经常使用缩略语，很容易让刚接触的人一头雾水，本篇的目的是讲解 RDMA 中最基本的元素及其含义。&lt;/p>
&lt;p>我将常见的缩略语对照表写在前面，阅读的时候如果忘记了可以翻到前面查阅。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-b6723caa5b291ee161d94fd8fd8ce09c_720w-2024-02-03.webp"
alt="v2-b6723caa5b291ee161d94fd8fd8ce09c_720w-2024-02-03" width="auto" loading="lazy">
&lt;/figure>
&lt;h2 id="wq">WQ&lt;/h2>
&lt;p>Work Queue 简称 WQ，是 RDMA 技术中最重要的概念之一。WQ 是一个储存工作请求的队列，为了讲清楚 WQ 是什么，我们先介绍这个队列中的元素 WQE（Work Queue Element，工作队列元素）。&lt;/p>
&lt;h3 id="wqe">WQE&lt;/h3>
&lt;p>WQE 可以认为是一种“任务说明”，这个工作请求是软件下发给硬件的，这份说明中包含了软件所希望硬件去做的任务以及有关这个任务的详细信息。比如，某一份任务是这样的：“我想把位于地址 0x12345678 的长度为 10 字节的数据发送给对面的节点”，硬件接到任务之后，就会通过 DMA 去内存中取数据，组装数据包，然后发送。&lt;/p>
&lt;p>WQE 的含义应该比较明确了，那么我们最开始提到的 WQ 是什么呢？它就是用来存放“任务书”的“文件夹”，WQ 里面可以容纳很多 WQE。有数据结构基础的读者应该都了解，队列是一种先进先出的数据结构，在计算机系统中非常常见，我们可以用下图表示上文中描述的 WQ 和 WQE 的关系：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-40c7e57f2760323c6b6665306e8f8896_720w-2024-02-03.webp"
alt="v2-40c7e57f2760323c6b6665306e8f8896_720w-2024-02-03" width="auto" loading="lazy">
&lt;/figure>
&lt;p>WQ 这个队列总是由软件向其中增加 WQE（入队），硬件从中取出 WQE，这就是软件给硬件“下发任务”的过程。为什么用队列而不是栈？因为进行“存”和“取“操作的分别是软件和硬件，并且需要保证用户的请求按照顺序被处理在 RDMA 技术中，所有的通信请求都要按照上图这种方式告知硬件，这种方式常被称为“Post”。&lt;/p>
&lt;h3 id="qp">QP&lt;/h3>
&lt;p>Queue Pair 简称 QP，就是“一对”WQ 的意思。&lt;/p>
&lt;h3 id="sq-和-rq">SQ 和 RQ&lt;/h3>
&lt;p>任何通信过程都要有收发两端，QP 就是一个发送工作队列和一个接受工作队列的组合，这两个队列分别称为 SQ（Send Queue）和 RQ（Receive Queue）。我们再把上面的图丰富一下，左边是发送端，右边是接收端：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-b89b321b8d1ae5ab6dcbaf8d6085f107_720w-2024-02-03.webp"
alt="v2-b89b321b8d1ae5ab6dcbaf8d6085f107_720w-2024-02-03" width="auto" loading="lazy">
&lt;/figure>
&lt;p>WQ 怎么不见了？SQ 和 RQ 都是 WQ，WQ 只是表示一种可以存储 WQE 的单元，SQ 和 RQ 才是实例。&lt;/p>
&lt;p>SQ 专门用来存放发送任务，RQ 专门用来存放接收任务。在一次 SEND-RECV 流程中，发送端需要把表示一次发送任务的 WQE 放到 SQ 里面。同样的，接收端软件需要给硬件下发一个表示接收任务的 WQE，这样硬件才知道收到数据之后放到内存中的哪个位置。上文我们提到的 Post 操作，对于 SQ 来说称为 Post Send，对于 RQ 来说称为 Post Receive。&lt;/p>
&lt;p>需要注意的是，在 RDMA 技术中&lt;strong>通信的基本单元是 QP&lt;/strong>，而不是节点。如下图所示，对于每个节点来说，每个进程都可以使用若干个 QP，而每个本地 QP 可以“关联”一个远端的 QP。我们用“节点 A 给节点 B 发送数据”并不足以完整的描述一次 RDMA 通信，而应该是类似于“节点 A 上的 QP3 给节点 C 上的 QP4 发送数据”。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-71b3b17ef8aec45d74ef9e4a42a69201_720w-2024-02-03.webp"
alt="v2-71b3b17ef8aec45d74ef9e4a42a69201_720w-2024-02-03" width="auto" loading="lazy">
&lt;/figure>
&lt;p>每个节点的每个 QP 都有一个唯一的编号，称为 QPN（Queue Pair Number），通过 QPN 可以唯一确定一个节点上的 QP。&lt;/p>
&lt;h3 id="srq">SRQ&lt;/h3>
&lt;p>Shared Receive Queue 简称 SRQ，意为共享接收队列。概念很好理解，就是一种几个 QP 共享同一个 RQ 时，我们称其为 SRQ。以后我们会了解到，使用 RQ 的情况要远远小于使用 SQ，而每个队列都是要消耗内存资源的。当我们需要使用大量的 QP 时，可以通过 SRQ 来节省内存。如下图所示，QP2~QP4 一起使用同一个 RQ：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-4a21f2b1333877b4b0d97a1ca91d4096_720w-2024-02-03.webp"
alt="v2-4a21f2b1333877b4b0d97a1ca91d4096_720w-2024-02-03" width="auto" loading="lazy">
&lt;/figure>
&lt;h2 id="cq">CQ&lt;/h2>
&lt;p>Completion Queue 简称 CQ，意为完成队列。跟 WQ 一样，我们先介绍 CQ 这个队列当中的元素——CQE（Completion Queue Element）。可以认为 CQE 跟 WQE 是相反的概念，如果 WQE 是软件下发给硬件的“任务书”的话，那么 CQE 就是硬件完成任务之后返回给软件的“任务报告”。CQE 中描述了某个任务是被正确无误的执行，还是遇到了错误，如果遇到了错误，那么错误的原因是什么。&lt;/p>
&lt;p>而 CQ 就是承载 CQE 的容器——一个先进先出的队列。我们把表示 WQ 和 WQE 关系的图倒过来画，就得到了 CQ 和 CQE 的关系：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-31f9a407ab66381fbc557d8acc5573cb_720w-2024-02-03.webp"
alt="v2-31f9a407ab66381fbc557d8acc5573cb_720w-2024-02-03" width="auto" loading="lazy">
&lt;/figure>
&lt;p>每个 CQE 都包含某个 WQE 的完成信息，他们的关系如下图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-701fa8eacb10c90c45b0241c75254a01_720w-2024-02-03.webp"
alt="v2-701fa8eacb10c90c45b0241c75254a01_720w-2024-02-03" width="auto" loading="lazy">
&lt;/figure>
&lt;p>下面我们把 CQ 和 WQ（QP）放在一起，看一下一次 SEND-RECV 操作中，软硬件的互动（图中序号顺序不表示实际时序）：&lt;/p>
&lt;blockquote>
&lt;p>2022/5/23：下图及后面的列表顺序有修改，将原来第 2 条的“接收端硬件从 RQ 中拿到任务书，准备接收数据”移动到“接收端收到数据，进行校验后回复 ACK 报文给发送端”之后，并且修改了描述，现在为第 6 条。&lt;/p>
&lt;p>这里我犯了错误的点是 RQ 和 SQ 不同，是一个“被动接收”的过程，只有收到 Send 报文（或者带立即数的 Write 报文）时硬件才会消耗 RQ WQE。感谢 @连接改变世界 的指正。&lt;/p>
&lt;/blockquote>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-a8d38721903672037b27cc7e49ecee03_720w-2024-02-03.webp"
alt="v2-a8d38721903672037b27cc7e49ecee03_720w-2024-02-03" width="auto" loading="lazy">
&lt;/figure>
&lt;ol>
&lt;li>接收端 APP 以 WQE 的形式下发一次 RECV 任务到 RQ。&lt;/li>
&lt;li>发送端 APP 以 WQE 的形式下发一次 SEND 任务到 SQ。&lt;/li>
&lt;li>发送端硬件从 SQ 中拿到任务书，从内存中拿到待发送数据，组装数据包。&lt;/li>
&lt;li>发送端网卡将数据包通过物理链路发送给接收端网卡。&lt;/li>
&lt;li>接收端收到数据，进行校验后回复 ACK 报文给发送端。&lt;/li>
&lt;li>接收端硬件从 RQ 中取出一个任务书（WQE）。&lt;/li>
&lt;li>接收端硬件将数据放到 WQE 中指定的位置，然后生成“任务报告”CQE，放置到 CQ 中。&lt;/li>
&lt;li>接收端 APP 取得任务完成信息。&lt;/li>
&lt;li>发送端网卡收到 ACK 后，生成 CQE，放置到 CQ 中。&lt;/li>
&lt;li>发送端 APP 取得任务完成信息。&lt;/li>
&lt;/ol>
&lt;div class="notice notice-note" >
&lt;div class="notice-title">&lt;svg t="1705946198814" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23141" width="200" height="200">&lt;path d="M195.541333 739.029333C151.594667 692.352 128 640 128 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642667 148.864a149.333333 149.333333 0 0 1-149.333334 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z m426.666667 0C578.261333 692.352 554.666667 640 554.666667 555.136c0-149.333333 104.832-283.178667 257.28-349.354667l38.101333 58.794667c-142.293333 76.970667-170.112 176.853333-181.205333 239.829333 22.912-11.861333 52.906667-16 82.304-13.269333 76.970667 7.125333 137.642667 70.314667 137.642666 148.864a149.333333 149.333333 0 0 1-149.333333 149.333333 165.162667 165.162667 0 0 1-117.248-50.304z" p-id="23142" fill="#ffffff">&lt;/path>&lt;/svg>&lt;/div>&lt;p>NOTE: 需要注意的一点是，上图中的例子是可靠服务类型的交互流程，如果是不可靠服务，那么不会有步骤 5 的 ACK 回复，而且步骤 9 以及之后的步骤会在步骤 5 之后立即触发。关于服务类型以及可靠与不可靠，我们将在《RDMA 基本服务类型》一文中讲解。&lt;/p>&lt;/div>
&lt;p>至此，通过 WQ 和 CQ 这两种媒介，两端软硬件共同完成了一次收发过程。&lt;/p>
&lt;h2 id="wr-和-wc">WR 和 WC&lt;/h2>
&lt;p>说完了几个 Queue 之后，其实还有两个文章开头提到的概念没有解释，那就是 WR 和 WC（不是 Water Closet 的缩写）。&lt;/p>
&lt;p>WR 全称为 Work Request，意为工作请求；WC 全称 Work Completion，意为工作完成。这两者其实是 WQE 和 CQE 在用户层的“映射”。因为 APP 是通过调用协议栈接口来完成 RDMA 通信的，WQE 和 CQE 本身并不对用户可见，是驱动中的概念。用户真正通过 API 下发的是 WR，收到的是 WC。&lt;/p>
&lt;p>WR/WC 和 WQE/CQE 是相同的概念在不同层次的实体，他们都是“任务书”和“任务报告”。于是我们把前文的两个图又加了点内容：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-00b87c111a8e1701f96fbfb78e078b29_720w-2024-02-03.webp"
alt="v2-00b87c111a8e1701f96fbfb78e078b29_720w-2024-02-03" width="auto" loading="lazy">
&lt;/figure>
&lt;h2 id="代码示例">代码示例&lt;/h2>
&lt;p>最后，下面是一个简单的例子，展示了如何使用 libibverbs 来创建一个 QP，然后通过这个 QP 来发送一次数据。这是一个非常简单的例子，只是为了让读者对上文中的概念有一个直观的认识。&lt;/p>
&lt;pre>&lt;code class="language-c">#include &amp;lt;infiniband/verbs.h&amp;gt;
int main() {
struct ibv_context *ctx;
struct ibv_pd *pd;
struct ibv_cq *cq;
struct ibv_qp *qp;
struct ibv_mr *mr;
struct ibv_sge sge;
struct ibv_send_wr wr;
struct ibv_send_wr *bad_wr;
struct ibv_wc wc;
ctx = ibv_open_device();
pd = ibv_alloc_pd(ctx);
cq = ibv_create_cq(ctx, 100, NULL, NULL, 0);
qp = ibv_create_qp(pd, NULL, NULL);
mr = ibv_reg_mr(pd, buf, size, IBV_ACCESS_LOCAL_WRITE | IBV_ACCESS_REMOTE_WRITE);
sge.addr = (uintptr_t)buf;
sge.length = size;
sge.lkey = mr-&amp;gt;lkey;
wr.wr_id = 1;
wr.sg_list = &amp;amp;sge;
wr.num_sge = 1;
wr.opcode = IBV_WR_SEND;
wr.send_flags = IBV_SEND_SIGNALED;
wr.next = NULL;
ibv_post_send(qp, &amp;amp;wr, &amp;amp;bad_wr);
ibv_poll_cq(cq, 1, &amp;amp;wc);
return 0;
}
&lt;/code>&lt;/pre>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>好了，我们用 IB 协议[1]3.2.1 中的 Figure 11 这张图总结一下本篇文章的内容：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/v2-2107a9bf8230c45ad73aa5ff0b8626ff_720w-2024-02-03.webp"
alt="v2-2107a9bf8230c45ad73aa5ff0b8626ff_720w-2024-02-03" width="auto" loading="lazy">
&lt;/figure>
&lt;p>用户态的 WR，由驱动转化成了 WQE 填写到了 WQ 中，WQ 可以是负责发送的 SQ，也可以是负责接收的 RQ。硬件会从各个 WQ 中取出 WQE，并根据 WQE 中的要求完成发送或者接收任务。任务完成后，会给这个任务生成一个 CQE 填写到 CQ 中。驱动会从 CQ 中取出 CQE，并转换成 WC 返回给用户。&lt;/p>
&lt;p>基础概念就介绍到这里，下一篇将介绍 RDMA 的几种常见操作类型。&lt;/p>
&lt;h2 id="参考文献">参考文献&lt;/h2>
&lt;p>[1]《IB Specification Vol 1-Release-1.3-2015-03-03》&lt;/p></description></item></channel></rss>