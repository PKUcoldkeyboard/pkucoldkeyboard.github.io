<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>人工智能与数据科学 on Cuterwrite's Blog</title><link>https://cuterwrite.top/categories/ai/</link><description>Recent content in 人工智能与数据科学 on Cuterwrite's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>cuterwrite</copyright><lastBuildDate>Fri, 05 Jul 2024 22:46:00 +0000</lastBuildDate><atom:link href="https://cuterwrite.top/categories/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>LLM 生态介绍：从模型微调到应用落地</title><link>https://cuterwrite.top/p/llm-ecosystem/</link><pubDate>Fri, 05 Jul 2024 22:46:00 +0000</pubDate><guid>https://cuterwrite.top/p/llm-ecosystem/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_119269220_p0_master1200.webp" alt="Featured image of post LLM 生态介绍：从模型微调到应用落地" />&lt;h1 id="llm-生态介绍从模型微调到应用落地">LLM 生态介绍：从模型微调到应用落地&lt;/h1>
&lt;h2 id="模型微调">模型微调&lt;/h2>
&lt;p>预训练的 LLM 通常具备广泛的知识，但要使其在特定任务上表现出色，微调是必不可少的。以下是一些常用的 LLM 微调工具：&lt;/p>
&lt;h3 id="axolotl">Axolotl&lt;/h3>
&lt;a href="https://github.com/OpenAccess-AI-Collective/axolotl" target="_blank" class="card-github fetch-waiting no-styling"
repo="OpenAccess-AI-Collective/axolotl" id="repo-Dbo3QhrBcsutclai-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-Dbo3QhrBcsutclai-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">OpenAccess-AI-Collective&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">axolotl&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-Dbo3QhrBcsutclai-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-Dbo3QhrBcsutclai-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-Dbo3QhrBcsutclai-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-Dbo3QhrBcsutclai-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-Dbo3QhrBcsutclai-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-Dbo3QhrBcsutclai-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/OpenAccess-AI-Collective\/axolotl', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-Dbo3QhrBcsutclai-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-Dbo3QhrBcsutclai-language').innerText = data.language;
document.getElementById('repo-Dbo3QhrBcsutclai-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-Dbo3QhrBcsutclai-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-Dbo3QhrBcsutclai-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-Dbo3QhrBcsutclai-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-Dbo3QhrBcsutclai-license').classList.add = "no-license"
};
document.getElementById('repo-Dbo3QhrBcsutclai-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for OpenAccess-AI-Collective\/axolotl.")
}).catch(err => {
const c = document.getElementById('repo-Dbo3QhrBcsutclai-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for OpenAccess-AI-Collective\/axolotl.")
})
&lt;/script>
&lt;p>Axolotl 是一款旨在简化各种人工智能模型微调的工具，支持多种配置和架构。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>训练各种 Huggingface 模型，如 llama、pythia、falcon、mpt&lt;/li>
&lt;li>支持 fullfinetune、lora、qlora、relora 和 gptq&lt;/li>
&lt;li>使用简单的 yaml 文件或 CLI 重写功能自定义配置&lt;/li>
&lt;li>加载不同的数据集格式，使用自定义格式，或自带标记化数据集&lt;/li>
&lt;li>与 xformer、闪存关注、绳索缩放和多重包装集成&lt;/li>
&lt;li>可通过 FSDP 或 Deepspeed 与单 GPU 或多 GPU 协同工作&lt;/li>
&lt;li>使用 Docker 在本地或云端轻松运行&lt;/li>
&lt;li>将结果和可选的检查点记录到 wandb 或 mlflow 中&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>快速入门：&lt;/strong>
要求： Python &amp;gt;=3.10 和 Pytorch &amp;gt;=2.1.1&lt;/p>
&lt;pre>&lt;code class="language-bash">git clone https://github.com/OpenAccess-AI-Collective/axolotl
cd axolotl
pip3 install packaging ninja
pip3 install -e '.[flash-attn,deepspeed]'
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>使用方法：&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash"># preprocess datasets - optional but recommended
CUDA_VISIBLE_DEVICES=&amp;quot;&amp;quot; python -m axolotl.cli.preprocess examples/openllama-3b/lora.yml
# finetune lora
accelerate launch -m axolotl.cli.train examples/openllama-3b/lora.yml
# inference
accelerate launch -m axolotl.cli.inference examples/openllama-3b/lora.yml \
--lora_model_dir=&amp;quot;./outputs/lora-out&amp;quot;
# gradio
accelerate launch -m axolotl.cli.inference examples/openllama-3b/lora.yml \
--lora_model_dir=&amp;quot;./outputs/lora-out&amp;quot; --gradio
# remote yaml files - the yaml config can be hosted on a public URL
# Note: the yaml config must directly link to the **raw** yaml
accelerate launch -m axolotl.cli.train https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/examples/openllama-3b/lora.yml
&lt;/code>&lt;/pre>
&lt;p>其它详细信息，请访问 &lt;a class="link" href="https://github.com/OpenAccess-AI-Collective/axolotl" target="_blank" rel="noopener" >Axolotl
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
项目主页。&lt;/p>
&lt;h3 id="llama-factory">Llama-Factory&lt;/h3>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-07-07_llama-factory-logo.webp"
alt="Llama Factory Logo" width="80%" loading="lazy">
&lt;/figure>
&lt;a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" class="card-github fetch-waiting no-styling"
repo="hiyouga/LLaMA-Factory" id="repo-EqZYlIcSa3Qffrnt-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-EqZYlIcSa3Qffrnt-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">hiyouga&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">LLaMA-Factory&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-EqZYlIcSa3Qffrnt-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-EqZYlIcSa3Qffrnt-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-EqZYlIcSa3Qffrnt-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-EqZYlIcSa3Qffrnt-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-EqZYlIcSa3Qffrnt-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-EqZYlIcSa3Qffrnt-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/hiyouga\/LLaMA-Factory', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-EqZYlIcSa3Qffrnt-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-EqZYlIcSa3Qffrnt-language').innerText = data.language;
document.getElementById('repo-EqZYlIcSa3Qffrnt-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-EqZYlIcSa3Qffrnt-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-EqZYlIcSa3Qffrnt-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-EqZYlIcSa3Qffrnt-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-EqZYlIcSa3Qffrnt-license').classList.add = "no-license"
};
document.getElementById('repo-EqZYlIcSa3Qffrnt-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for hiyouga\/LLaMA-Factory.")
}).catch(err => {
const c = document.getElementById('repo-EqZYlIcSa3Qffrnt-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for hiyouga\/LLaMA-Factory.")
})
&lt;/script>
&lt;p>Llama-Factory 是 Meta 推出的，专注于 Llama 模型微调的框架。它构建于 PyTorch 生态之上，并提供高效的训练和评估工具。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>多种模型&lt;/strong>：LLaMA、LLaVA、Mistral、Mixtral-MoE、Qwen、Yi、Gemma、Baichuan、ChatGLM、Phi 等等。&lt;/li>
&lt;li>&lt;strong>集成方法&lt;/strong>：（增量）预训练、（多模态）指令监督微调、奖励模型训练、PPO 训练、DPO 训练、KTO 训练、ORPO 训练等等。&lt;/li>
&lt;li>&lt;strong>多种精度&lt;/strong>：16 比特全参数微调、冻结微调、LoRA 微调和基于 AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ 的 2/3/4/5/6/8 比特 QLoRA 微调。&lt;/li>
&lt;li>&lt;strong>先进算法&lt;/strong>：GaLore、BAdam、DoRA、LongLoRA、LLaMA Pro、Mixture-of-Depths、LoRA+、LoftQ、PiSSA 和 Agent 微调。&lt;/li>
&lt;li>&lt;strong>实用技巧&lt;/strong>：FlashAttention-2、Unsloth、RoPE scaling、NEFTune 和 rsLoRA。&lt;/li>
&lt;li>&lt;strong>实验监控&lt;/strong>：LlamaBoard、TensorBoard、Wandb、MLflow 等等。&lt;/li>
&lt;li>&lt;strong>极速推理&lt;/strong>：基于 vLLM 的 OpenAI 风格 API、浏览器界面和命令行接口。&lt;/li>
&lt;/ul>
&lt;link href="https://web.sdk.qcloud.com/player/tcplayer/release/v4.2.1/tcplayer.min.css" rel="stylesheet">
&lt;script src="https://web.sdk.qcloud.com/player/tcplayer/release/v4.2.1/libs/hls.min.0.13.2m.js">&lt;/script>
&lt;script src="https://web.sdk.qcloud.com/player/tcplayer/release/v4.2.1/tcplayer.v4.2.1.min.js">&lt;/script>
&lt;style>
.tcplayer {
position: absolute;
width: 100%;
height: 100%;
left: 0;
top: 0;
border: 0;
}
&lt;/style>
&lt;div class="video-wrapper">
&lt;video
id="player-container-id"
preload="auto"
width="100%"
height="100%"
playsinline
webkit-playsinline>
&lt;/video>
&lt;/div>
&lt;script>
var tcplayer = TCPlayer("player-container-id", {
reportable: false,
poster: "",
});
tcplayer.src('https:\/\/cuterwrite-1302252842.file.myqcloud.com\/img\/2024-07-07_309488092-ec36a9dd-37f4-4f72-81bd-d76c6d0a6594_output.mp4');
&lt;/script>
&lt;p>&lt;strong>性能指标&lt;/strong>&lt;/p>
&lt;p>与 ChatGLM 官方的 &lt;a class="link" href="https://github.com/THUDM/ChatGLM2-6B/tree/main/ptuning" target="_blank" rel="noopener" >P-Tuning
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
微调相比，LLaMA Factory 的 LoRA 微调提供了 &lt;strong>3.7 倍&lt;/strong>的加速比，同时在广告文案生成任务上取得了更高的 Rouge 分数。结合 4 比特量化技术，LLaMA Factory 的 QLoRA 微调进一步降低了 GPU 显存消耗。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-07-07_benchmark.webp"
alt="Performance" width="auto" loading="lazy">
&lt;/figure>
&lt;details>
&lt;summary>变量定义&lt;/summary>
&lt;ul>
&lt;li>&lt;strong>Training Speed&lt;/strong>: 训练阶段每秒处理的样本数量。（批处理大小=4，截断长度=1024）&lt;/li>
&lt;li>&lt;strong>Rouge Score&lt;/strong>: &lt;a class="link" href="https://aclanthology.org/D19-1321.pdf" target="_blank" rel="noopener" >广告文案生成
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
任务验证集上的 Rouge-2 分数。（批处理大小=4，截断长度=1024）&lt;/li>
&lt;li>&lt;strong>GPU Memory&lt;/strong>: 4 比特量化训练的 GPU 显存峰值。（批处理大小=1，截断长度=1024）&lt;/li>
&lt;li>我们在 ChatGLM 的 P-Tuning 中采用 &lt;code>pre_seq_len=128&lt;/code>，在 LLaMA Factory 的 LoRA 微调中采用 &lt;code>lora_rank=32&lt;/code>。&lt;/li>
&lt;/ul>
&lt;/details>
&lt;p>&lt;strong>快速入门&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash">git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e &amp;quot;.[torch,metrics]&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>可选的额外依赖项：torch、torch-npu、metrics、deepspeed、bitsandbytes、hqq、eetq、gptq、awq、aqlm、vllm、galore、badam、qwen、modelscope、quality&lt;/p>
&lt;div class="notice notice-tip" >
&lt;div class="notice-title">&lt;svg t="1705945832245" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="19409" width="200" height="200">&lt;path d="M512 74.666667C270.933333 74.666667 74.666667 270.933333 74.666667 512S270.933333 949.333333 512 949.333333 949.333333 753.066667 949.333333 512 753.066667 74.666667 512 74.666667z m238.933333 349.866666l-2.133333 2.133334-277.333333 277.333333c-10.666667 10.666667-29.866667 12.8-42.666667 2.133333L426.666667 704l-149.333334-149.333333c-12.8-12.8-12.8-32 0-44.8 10.666667-10.666667 29.866667-12.8 42.666667-2.133334l2.133333 2.133334 125.866667 125.866666 253.866667-253.866666c10.666667-10.666667 29.866667-12.8 42.666666-2.133334l2.133334 2.133334c12.8 12.8 12.8 32 4.266666 42.666666z" fill="#ffffff" p-id="19410">&lt;/path>&lt;/svg>&lt;/div>&lt;p>遇到包冲突时，可使用 pip install &amp;ndash;no-deps -e . 解决。&lt;/p>&lt;/div>
&lt;details>
&lt;summary>Windows 用户指南&lt;/summary>
&lt;p>如果要在 Windows 平台上开启量化 LoRA（QLoRA），需要安装预编译的 &lt;code>bitsandbytes&lt;/code> 库, 支持 CUDA 11.1 到 12.2, 请根据您的 CUDA 版本情况选择适合的&lt;a class="link" href="https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels" target="_blank" rel="noopener" >发布版本
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl
&lt;/code>&lt;/pre>
&lt;p>如果要在 Windows 平台上开启 FlashAttention-2，需要安装预编译的 &lt;code>flash-attn&lt;/code> 库，支持 CUDA 12.1 到 12.2，请根据需求到 &lt;a class="link" href="https://github.com/bdashore3/flash-attention/releases" target="_blank" rel="noopener" >flash-attention
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
下载对应版本安装。&lt;/p>
&lt;/details>
&lt;details>
&lt;summary>昇腾 NPU 用户指南&lt;/summary>
&lt;p>在昇腾 NPU 设备上安装 LLaMA Factory 时，需要指定额外依赖项，使用 &lt;code>pip install -e &amp;quot;.[torch-npu,metrics]&amp;quot;&lt;/code> 命令安装。此外，还需要安装 &lt;strong>&lt;a class="link" href="https://www.hiascend.com/developer/download/community/result?module=cann" target="_blank" rel="noopener" >Ascend CANN Toolkit 与 Kernels
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/strong>，安装方法请参考&lt;a class="link" href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/80RC2alpha002/quickstart/quickstart/quickstart_18_0004.html" target="_blank" rel="noopener" >安装教程
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
或使用以下命令：&lt;/p>
&lt;pre>&lt;code class="language-bash"># 请替换 URL 为 CANN 版本和设备型号对应的 URL
# 安装 CANN Toolkit
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-toolkit_8.0.RC1.alpha001_linux-&amp;quot;$(uname -i)&amp;quot;.run
bash Ascend-cann-toolkit_8.0.RC1.alpha001_linux-&amp;quot;$(uname -i)&amp;quot;.run --install
# 安装 CANN Kernels
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run
bash Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run --install
# 设置环境变量
source /usr/local/Ascend/ascend-toolkit/set_env.sh
&lt;/code>&lt;/pre>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>依赖项&lt;/th>
&lt;th>至少&lt;/th>
&lt;th>推荐&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>CANN&lt;/td>
&lt;td>8.0.RC1&lt;/td>
&lt;td>8.0.RC1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>torch&lt;/td>
&lt;td>2.1.0&lt;/td>
&lt;td>2.1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>torch-npu&lt;/td>
&lt;td>2.1.0&lt;/td>
&lt;td>2.1.0.post3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>deepspeed&lt;/td>
&lt;td>0.13.2&lt;/td>
&lt;td>0.13.2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>请使用 &lt;code>ASCEND_RT_VISIBLE_DEVICES&lt;/code> 而非 &lt;code>CUDA_VISIBLE_DEVICES&lt;/code> 来指定运算设备。&lt;/p>
&lt;p>如果遇到无法正常推理的情况，请尝试设置 &lt;code>do_sample: false&lt;/code>。&lt;/p>
&lt;p>下载预构建 Docker 镜像：&lt;a class="link" href="http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html" target="_blank" rel="noopener" >32GB
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
| &lt;a class="link" href="http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html" target="_blank" rel="noopener" >64GB
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;/details>
&lt;p>下面三行命令分别对 Llama3-8B-Instruct 模型进行 LoRA 微调、推理和合并。&lt;/p>
&lt;pre>&lt;code class="language-bash">llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml
&lt;/code>&lt;/pre>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" rel="noopener" >Llama-Factory
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
项目主页。&lt;/p>
&lt;h3 id="firefly">Firefly&lt;/h3>
&lt;a href="https://github.com/yangjianxin1/Firefly" target="_blank" class="card-github fetch-waiting no-styling"
repo="yangjianxin1/Firefly" id="repo-dAp2H8ipgeiwq3b3-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-dAp2H8ipgeiwq3b3-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">yangjianxin1&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">Firefly&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-dAp2H8ipgeiwq3b3-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-dAp2H8ipgeiwq3b3-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-dAp2H8ipgeiwq3b3-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-dAp2H8ipgeiwq3b3-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-dAp2H8ipgeiwq3b3-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-dAp2H8ipgeiwq3b3-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/yangjianxin1\/Firefly', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-dAp2H8ipgeiwq3b3-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-dAp2H8ipgeiwq3b3-language').innerText = data.language;
document.getElementById('repo-dAp2H8ipgeiwq3b3-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-dAp2H8ipgeiwq3b3-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-dAp2H8ipgeiwq3b3-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-dAp2H8ipgeiwq3b3-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-dAp2H8ipgeiwq3b3-license').classList.add = "no-license"
};
document.getElementById('repo-dAp2H8ipgeiwq3b3-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for yangjianxin1\/Firefly.")
}).catch(err => {
const c = document.getElementById('repo-dAp2H8ipgeiwq3b3-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for yangjianxin1\/Firefly.")
})
&lt;/script>
&lt;p>&lt;strong>Firefly&lt;/strong> 是一个开源的大模型训练项目，支持对主流的大模型进行预训练、指令微调和 DPO，包括但不限于 Qwen2、Yi-1.5、Llama3、Gemma、Qwen1.5、MiniCPM、Llama、InternLM、Baichuan、ChatGLM、Yi、Deepseek、Qwen、Orion、Ziya、Xverse、Mistral、Mixtral-8x7B、Zephyr、Vicuna、Bloom 等。
本项目支持&lt;strong>全量参数训练、LoRA、QLoRA 高效训练&lt;/strong>，支持&lt;strong>预训练、SFT、DPO&lt;/strong>。 如果你的训练资源有限，我们极力推荐使用 QLoRA 进行指令微调，因为我们在 Open LLM Leaderboard 上验证了该方法的有效性，并且取得了非常不错的成绩。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>📗 支持预训练、指令微调、DPO，支持全量参数训练、LoRA、QLoRA 高效训练。通过配置文件的方式训练不同的模型，小白亦可快速上手训练模型。&lt;/li>
&lt;li>📗 支持使用&lt;a class="link" href="https://github.com/yangjianxin1/unsloth" target="_blank" rel="noopener" >Unsloth
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
加速训练，并且节省显存。&lt;/li>
&lt;li>📗 支持绝大部分主流的开源大模型，如 Llama3、Gemma、MiniCPM、Llama、InternLM、Baichuan、ChatGLM、Yi、Deepseek、Qwen、Orion、Ziya、Xverse、Mistral、Mixtral-8x7B、Zephyr、Vicuna、Bloom，训练时与各个官方的 chat 模型的 template 对齐。&lt;/li>
&lt;li>📗 整理并开源指令微调数据集：firefly-train-1.1M 、moss-003-sft-data、ultrachat、 WizardLM_evol_instruct_V2_143k、school_math_0.25M。&lt;/li>
&lt;li>📗 开源&lt;a class="link" href="https://huggingface.co/YeungNLP" target="_blank" rel="noopener" >Firefly 系列指令微调模型权重
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/li>
&lt;li>📗 在 Open LLM Leaderboard 上验证了 QLoRA 训练流程的有效性。&lt;/li>
&lt;/ul>
&lt;p>该项目的 README 中包含了详细的使用说明，包括如何安装、如何训练、如何微调、如何评估等。请访问 &lt;a class="link" href="https://github.com/yangjianxin1/Firefly" target="_blank" rel="noopener" >Firefly
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
项目主页。&lt;/p>
&lt;h3 id="xtuner">XTuner&lt;/h3>
&lt;a href="https://github.com/InternLM/xtuner" target="_blank" class="card-github fetch-waiting no-styling"
repo="InternLM/xtuner" id="repo-YoDfM4UTWYfxMsbZ-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-YoDfM4UTWYfxMsbZ-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">InternLM&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">xtuner&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-YoDfM4UTWYfxMsbZ-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-YoDfM4UTWYfxMsbZ-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-YoDfM4UTWYfxMsbZ-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-YoDfM4UTWYfxMsbZ-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-YoDfM4UTWYfxMsbZ-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-YoDfM4UTWYfxMsbZ-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/InternLM\/xtuner', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-YoDfM4UTWYfxMsbZ-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-YoDfM4UTWYfxMsbZ-language').innerText = data.language;
document.getElementById('repo-YoDfM4UTWYfxMsbZ-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-YoDfM4UTWYfxMsbZ-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-YoDfM4UTWYfxMsbZ-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-YoDfM4UTWYfxMsbZ-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-YoDfM4UTWYfxMsbZ-license').classList.add = "no-license"
};
document.getElementById('repo-YoDfM4UTWYfxMsbZ-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for InternLM\/xtuner.")
}).catch(err => {
const c = document.getElementById('repo-YoDfM4UTWYfxMsbZ-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for InternLM\/xtuner.")
})
&lt;/script>
&lt;p>XTuner 是一个高效、灵活、全能的轻量化大模型微调工具库。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>高效&lt;/strong>
&lt;ul>
&lt;li>支持大语言模型 LLM、多模态图文模型 VLM 的预训练及轻量级微调。XTuner 支持在 8GB 显存下微调 7B 模型，同时也支持多节点跨设备微调更大尺度模型（70B+）。&lt;/li>
&lt;li>自动分发高性能算子（如 FlashAttention、Triton kernels 等）以加速训练吞吐。&lt;/li>
&lt;li>兼容 &lt;a class="link" href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener" >DeepSpeed
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
🚀，轻松应用各种 ZeRO 训练优化策略。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>灵活&lt;/strong>
&lt;ul>
&lt;li>支持多种大语言模型，包括但不限于 &lt;a class="link" href="https://huggingface.co/internlm" target="_blank" rel="noopener" >InternLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="https://huggingface.co/mistralai" target="_blank" rel="noopener" >Mixtral-8x7B
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="https://huggingface.co/meta-llama" target="_blank" rel="noopener" >Llama 2
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="https://huggingface.co/THUDM" target="_blank" rel="noopener" >ChatGLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="https://huggingface.co/Qwen" target="_blank" rel="noopener" >Qwen
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="https://huggingface.co/baichuan-inc" target="_blank" rel="noopener" >Baichuan
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/li>
&lt;li>支持多模态图文模型 LLaVA 的预训练与微调。利用 XTuner 训得模型 &lt;a class="link" href="https://huggingface.co/xtuner/llava-internlm2-20b" target="_blank" rel="noopener" >LLaVA-InternLM2-20B
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
表现优异。&lt;/li>
&lt;li>精心设计的数据管道，兼容任意数据格式，开源数据或自定义数据皆可快速上手。&lt;/li>
&lt;li>支持 &lt;a class="link" href="http://arxiv.org/abs/2305.14314" target="_blank" rel="noopener" >QLoRA
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="http://arxiv.org/abs/2106.09685" target="_blank" rel="noopener" >LoRA
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、全量参数微调等多种微调算法，支撑用户根据具体需求作出最优选择。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>全能&lt;/strong>
&lt;ul>
&lt;li>支持增量预训练、指令微调与 Agent 微调。&lt;/li>
&lt;li>预定义众多开源对话模版，支持与开源或训练所得模型进行对话。&lt;/li>
&lt;li>训练所得模型可无缝接入部署工具库 &lt;a class="link" href="https://github.com/InternLM/lmdeploy" target="_blank" rel="noopener" >LMDeploy
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、大规模评测工具库 &lt;a class="link" href="https://github.com/open-compass/opencompass" target="_blank" rel="noopener" >OpenCompass
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
及 &lt;a class="link" href="https://github.com/open-compass/VLMEvalKit" target="_blank" rel="noopener" >VLMEvalKit
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>快速上手：&lt;/strong>
&lt;details>
&lt;summary>安装&lt;/summary>
&lt;ul>
&lt;li>
&lt;p>推荐使用 conda 先构建一个 Python-3.10 的虚拟环境&lt;/p>
&lt;pre>&lt;code class="language-bash">conda create --name xtuner-env python=3.10 -y
conda activate xtuner-env
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>通过 pip 安装 XTuner：&lt;/p>
&lt;pre>&lt;code class="language-shell">pip install -U xtuner
&lt;/code>&lt;/pre>
&lt;p>亦可集成 DeepSpeed 安装：&lt;/p>
&lt;pre>&lt;code class="language-shell">pip install -U 'xtuner[deepspeed]'
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>从源码安装 XTuner：&lt;/p>
&lt;pre>&lt;code class="language-shell">git clone https://github.com/InternLM/xtuner.git
cd xtuner
pip install -e '.[all]'
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul>
&lt;/details>
&lt;/p>
&lt;details>
&lt;summary>微调&lt;/summary>
&lt;p>XTuner 支持微调大语言模型。数据集预处理指南请查阅&lt;a class="link" href="./docs/zh_cn/user_guides/dataset_prepare.md" >文档
&lt;/a>
。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>步骤 0&lt;/strong>，准备配置文件。XTuner 提供多个开箱即用的配置文件，用户可以通过下列命令查看：&lt;/p>
&lt;pre>&lt;code class="language-shell">xtuner list-cfg
&lt;/code>&lt;/pre>
&lt;p>或者，如果所提供的配置文件不能满足使用需求，请导出所提供的配置文件并进行相应更改：&lt;/p>
&lt;pre>&lt;code class="language-shell">xtuner copy-cfg ${CONFIG_NAME} ${SAVE_PATH}
vi ${SAVE_PATH}/${CONFIG_NAME}_copy.py
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>步骤 1&lt;/strong>，开始微调。&lt;/p>
&lt;pre>&lt;code class="language-shell">xtuner train ${CONFIG_NAME_OR_PATH}
&lt;/code>&lt;/pre>
&lt;p>例如，我们可以利用 QLoRA 算法在 oasst1 数据集上微调 InternLM2.5-Chat-7B：&lt;/p>
&lt;pre>&lt;code class="language-shell"># 单卡
xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2
# 多卡
(DIST) NPROC_PER_NODE=${GPU_NUM} xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2
(SLURM) srun ${SRUN_ARGS} xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --launcher slurm --deepspeed deepspeed_zero2
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>
&lt;p>&lt;code>--deepspeed&lt;/code> 表示使用 &lt;a class="link" href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener" >DeepSpeed
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
🚀 来优化训练过程。XTuner 内置了多种策略，包括 ZeRO-1、ZeRO-2、ZeRO-3 等。如果用户期望关闭此功能，请直接移除此参数。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>更多示例，请查阅&lt;a class="link" href="./docs/zh_cn/user_guides/finetune.md" >文档
&lt;/a>
。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>步骤 2&lt;/strong>，将保存的 PTH 模型（如果使用的 DeepSpeed，则将会是一个文件夹）转换为 HuggingFace 模型：&lt;/p>
&lt;pre>&lt;code class="language-shell">xtuner convert pth_to_hf ${CONFIG_NAME_OR_PATH} ${PTH} ${SAVE_PATH}
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul>
&lt;/details>
&lt;p>其它详细信息，请访问 &lt;a class="link" href="https://github.com/InternLM/xtuner" target="_blank" rel="noopener" >XTuner
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h2 id="模型量化">模型量化&lt;/h2>
&lt;p>LLM 通常体积庞大，对计算资源要求高。模型量化技术可以压缩模型大小，提高运行效率，使其更易于部署：&lt;/p>
&lt;h3 id="autogptq">AutoGPTQ&lt;/h3>
&lt;a href="https://github.com/PanQiWei/AutoGPTQ" target="_blank" class="card-github fetch-waiting no-styling"
repo="PanQiWei/AutoGPTQ" id="repo-TYLKLvIqJNKEKaVc-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-TYLKLvIqJNKEKaVc-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">PanQiWei&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">AutoGPTQ&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-TYLKLvIqJNKEKaVc-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-TYLKLvIqJNKEKaVc-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-TYLKLvIqJNKEKaVc-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-TYLKLvIqJNKEKaVc-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-TYLKLvIqJNKEKaVc-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-TYLKLvIqJNKEKaVc-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/PanQiWei\/AutoGPTQ', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-TYLKLvIqJNKEKaVc-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-TYLKLvIqJNKEKaVc-language').innerText = data.language;
document.getElementById('repo-TYLKLvIqJNKEKaVc-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-TYLKLvIqJNKEKaVc-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-TYLKLvIqJNKEKaVc-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-TYLKLvIqJNKEKaVc-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-TYLKLvIqJNKEKaVc-license').classList.add = "no-license"
};
document.getElementById('repo-TYLKLvIqJNKEKaVc-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for PanQiWei\/AutoGPTQ.")
}).catch(err => {
const c = document.getElementById('repo-TYLKLvIqJNKEKaVc-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for PanQiWei\/AutoGPTQ.")
})
&lt;/script>
&lt;p>AutoGPTQ 一个基于 GPTQ 算法，简单易用且拥有用户友好型接口的大语言模型量化工具包。&lt;/p>
&lt;p>&lt;strong>快速安装&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>对于 CUDA 11.7：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>对于 CUDA 11.8：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>对于 RoCm 5.4.2：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/rocm542/
&lt;/code>&lt;/pre>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/AutoGPTQ/AutoGPTQ/" target="_blank" rel="noopener" >AutoGPTQ
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="autoawq">AutoAWQ&lt;/h3>
&lt;a href="https://github.com/casper-hansen/AutoAWQ" target="_blank" class="card-github fetch-waiting no-styling"
repo="casper-hansen/AutoAWQ" id="repo-sWng6mu5QE6LyyUA-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-sWng6mu5QE6LyyUA-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">casper-hansen&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">AutoAWQ&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-sWng6mu5QE6LyyUA-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-sWng6mu5QE6LyyUA-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-sWng6mu5QE6LyyUA-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-sWng6mu5QE6LyyUA-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-sWng6mu5QE6LyyUA-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-sWng6mu5QE6LyyUA-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/casper-hansen\/AutoAWQ', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-sWng6mu5QE6LyyUA-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-sWng6mu5QE6LyyUA-language').innerText = data.language;
document.getElementById('repo-sWng6mu5QE6LyyUA-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-sWng6mu5QE6LyyUA-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-sWng6mu5QE6LyyUA-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-sWng6mu5QE6LyyUA-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-sWng6mu5QE6LyyUA-license').classList.add = "no-license"
};
document.getElementById('repo-sWng6mu5QE6LyyUA-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for casper-hansen\/AutoAWQ.")
}).catch(err => {
const c = document.getElementById('repo-sWng6mu5QE6LyyUA-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for casper-hansen\/AutoAWQ.")
})
&lt;/script>
&lt;p>AutoAWQ 是另一款自动化模型量化工具，支持多种量化精度，并提供灵活的配置选项，可以根据不同的硬件平台和性能需求进行调整。&lt;/p>
&lt;p>AutoAWQ 是一个易于使用的 4 位量化模型软件包。与 FP16 相比，AutoAWQ 可将模型速度提高 3 倍，内存需求减少 3 倍。AutoAWQ 实现了用于量化 LLMs 的激活感知权重量化（AWQ）算法。AutoAWQ 是在 MIT 的原始工作 &lt;a class="link" href="https://github.com/mit-han-lab/llm-awq" target="_blank" rel="noopener" >AWQ
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
基础上创建和改进的。&lt;/p>
&lt;p>&lt;strong>安装方法：&lt;/strong>&lt;/p>
&lt;p>安装前，确保安装了 CUDA &amp;gt;= 12.1（注意：以下只是最快捷的安装方法）&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install autoawq
&lt;/code>&lt;/pre>
&lt;p>更多详细信息和示例，请访问 &lt;a class="link" href="https://github.com/casper-hansen/AutoAWQ" target="_blank" rel="noopener" >AutoAWQ
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="neural-compressor">Neural Compressor&lt;/h3>
&lt;a href="https://github.com/intel/neural-compressor" target="_blank" class="card-github fetch-waiting no-styling"
repo="intel/neural-compressor" id="repo-b9OIzNU2ZMJzGuQk-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-b9OIzNU2ZMJzGuQk-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">intel&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">neural-compressor&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-b9OIzNU2ZMJzGuQk-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-b9OIzNU2ZMJzGuQk-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-b9OIzNU2ZMJzGuQk-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-b9OIzNU2ZMJzGuQk-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-b9OIzNU2ZMJzGuQk-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-b9OIzNU2ZMJzGuQk-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/intel\/neural-compressor', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-b9OIzNU2ZMJzGuQk-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-b9OIzNU2ZMJzGuQk-language').innerText = data.language;
document.getElementById('repo-b9OIzNU2ZMJzGuQk-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-b9OIzNU2ZMJzGuQk-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-b9OIzNU2ZMJzGuQk-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-b9OIzNU2ZMJzGuQk-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-b9OIzNU2ZMJzGuQk-license').classList.add = "no-license"
};
document.getElementById('repo-b9OIzNU2ZMJzGuQk-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for intel\/neural-compressor.")
}).catch(err => {
const c = document.getElementById('repo-b9OIzNU2ZMJzGuQk-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for intel\/neural-compressor.")
})
&lt;/script>
&lt;p>Neural Compressor 是英特尔开发的模型压缩工具包，支持所有主流深度学习框架（TensorFlow、PyTorch、ONNX Runtime 和 MXNet）上流行的模型压缩技术。&lt;/p>
&lt;p>&lt;strong>安装方法：&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install &amp;quot;neural-compressor&amp;gt;=2.3&amp;quot; &amp;quot;transformers&amp;gt;=4.34.0&amp;quot; torch torchvision
&lt;/code>&lt;/pre>
&lt;p>更多详细信息和示例，请访问 &lt;a class="link" href="https://github.com/intel/neural-compressor" target="_blank" rel="noopener" >Neural Compressor
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h2 id="模型部署">模型部署&lt;/h2>
&lt;p>将训练好的 LLM 部署到生产环境至关重要。以下是一些常用的 LLM 部署工具：&lt;/p>
&lt;h3 id="vllm">vLLM&lt;/h3>
&lt;a href="https://github.com/vllm-project/vllm" target="_blank" class="card-github fetch-waiting no-styling"
repo="vllm-project/vllm" id="repo-sZeopj7Y75srxWx5-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-sZeopj7Y75srxWx5-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">vllm-project&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">vllm&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-sZeopj7Y75srxWx5-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-sZeopj7Y75srxWx5-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-sZeopj7Y75srxWx5-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-sZeopj7Y75srxWx5-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-sZeopj7Y75srxWx5-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-sZeopj7Y75srxWx5-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/vllm-project\/vllm', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-sZeopj7Y75srxWx5-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-sZeopj7Y75srxWx5-language').innerText = data.language;
document.getElementById('repo-sZeopj7Y75srxWx5-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-sZeopj7Y75srxWx5-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-sZeopj7Y75srxWx5-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-sZeopj7Y75srxWx5-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-sZeopj7Y75srxWx5-license').classList.add = "no-license"
};
document.getElementById('repo-sZeopj7Y75srxWx5-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for vllm-project\/vllm.")
}).catch(err => {
const c = document.getElementById('repo-sZeopj7Y75srxWx5-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for vllm-project\/vllm.")
})
&lt;/script>
&lt;p>vLLM 是一个快速、易用的 LLM 推理服务库。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>快速&lt;/strong>
&lt;ul>
&lt;li>SOTA 服务吞吐量&lt;/li>
&lt;li>利用 PagedAttention 高效管理注意力键值内存&lt;/li>
&lt;li>持续批量处理收到的请求&lt;/li>
&lt;li>利用 CUDA/HIP 图进行加速&lt;/li>
&lt;li>量化：支持 GPTQ、AWQ、SqueezeLLM、FP8 KV 高速缓存&lt;/li>
&lt;li>优化的 CUDA 内核&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>灵活&lt;/strong>
&lt;ul>
&lt;li>与流行的 Hugging Face 模型无缝集成&lt;/li>
&lt;li>利用各种解码算法（包括并行采样、波束搜索等）提供高吞吐量服务&lt;/li>
&lt;li>为分布式推理提供张量并行支持&lt;/li>
&lt;li>流输出&lt;/li>
&lt;li>兼容 OpenAI 的应用程序接口服务器&lt;/li>
&lt;li>支持 NVIDIA GPU、AMD GPU、Intel CPU 和 GPU
-（实验性）支持前缀缓存
-（试验性）支持多种语言&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>无缝支持&lt;/strong>
&lt;ul>
&lt;li>基于 Transformer 的模型，例如 Llama&lt;/li>
&lt;li>基于 MoE 的模型，例如 Mixtral&lt;/li>
&lt;li>多模态模型，例如 LLaVA&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>快速安装：&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install vllm
&lt;/code>&lt;/pre>
&lt;p>更多详细信息，请查看 &lt;a class="link" href="https://vllm.readthedocs.io/en/latest/" target="_blank" rel="noopener" >vLLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
官方文档。&lt;/p>
&lt;h3 id="sgl">SGL&lt;/h3>
&lt;a href="https://github.com/sgl-project/sglang" target="_blank" class="card-github fetch-waiting no-styling"
repo="sgl-project/sglang" id="repo-6lg0E0NRrLIyDK58-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-6lg0E0NRrLIyDK58-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">sgl-project&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">sglang&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-6lg0E0NRrLIyDK58-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-6lg0E0NRrLIyDK58-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-6lg0E0NRrLIyDK58-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-6lg0E0NRrLIyDK58-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-6lg0E0NRrLIyDK58-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-6lg0E0NRrLIyDK58-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/sgl-project\/sglang', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-6lg0E0NRrLIyDK58-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-6lg0E0NRrLIyDK58-language').innerText = data.language;
document.getElementById('repo-6lg0E0NRrLIyDK58-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-6lg0E0NRrLIyDK58-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-6lg0E0NRrLIyDK58-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-6lg0E0NRrLIyDK58-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-6lg0E0NRrLIyDK58-license').classList.add = "no-license"
};
document.getElementById('repo-6lg0E0NRrLIyDK58-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for sgl-project\/sglang.")
}).catch(err => {
const c = document.getElementById('repo-6lg0E0NRrLIyDK58-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for sgl-project\/sglang.")
})
&lt;/script>
&lt;p>SGLang 是一种结构化生成语言，专为大型语言模型（LLMs）而设计。它通过共同设计前端语言和运行系统，使你与 LLMs 的交互更快、更可控。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>灵活的前端语言&lt;/strong>：通过链式生成调用、高级提示、控制流、多种模式、并行性和外部交互，可轻松编写 LLM 应用程序。&lt;/li>
&lt;li>&lt;strong>高性能后端运行时&lt;/strong>：具有 RadixAttention 功能，可通过在多次调用中重复使用 KV 缓存来加速复杂的 LLM 程序。它还可以作为独立的推理引擎，实现所有常用技术（如连续批处理和张量并行）。&lt;/li>
&lt;/ul>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/sgl-project/" target="_blank" rel="noopener" >SGL
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="skypilot">SkyPilot&lt;/h3>
&lt;a href="https://github.com/skypilot-org/skypilot" target="_blank" class="card-github fetch-waiting no-styling"
repo="skypilot-org/skypilot" id="repo-oCyjKqYzgTJ7pcOy-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-oCyjKqYzgTJ7pcOy-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">skypilot-org&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">skypilot&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-oCyjKqYzgTJ7pcOy-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-oCyjKqYzgTJ7pcOy-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-oCyjKqYzgTJ7pcOy-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-oCyjKqYzgTJ7pcOy-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-oCyjKqYzgTJ7pcOy-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-oCyjKqYzgTJ7pcOy-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/skypilot-org\/skypilot', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-oCyjKqYzgTJ7pcOy-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-oCyjKqYzgTJ7pcOy-language').innerText = data.language;
document.getElementById('repo-oCyjKqYzgTJ7pcOy-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-oCyjKqYzgTJ7pcOy-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-oCyjKqYzgTJ7pcOy-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-oCyjKqYzgTJ7pcOy-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-oCyjKqYzgTJ7pcOy-license').classList.add = "no-license"
};
document.getElementById('repo-oCyjKqYzgTJ7pcOy-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for skypilot-org\/skypilot.")
}).catch(err => {
const c = document.getElementById('repo-oCyjKqYzgTJ7pcOy-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for skypilot-org\/skypilot.")
})
&lt;/script>
&lt;p>SkyPilot 是 UC Berkeley RISELab 推出的灵活的云端 LLM 部署工具，支持多种云平台和硬件加速器，可以自动选择最优的部署方案，并提供成本优化功能。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>多云支持:&lt;/strong> 支持 AWS, GCP, Azure 等多种云平台，方便用户选择合适的部署环境。&lt;/li>
&lt;li>&lt;strong>轻松扩展&lt;/strong>：排队和运行多个作业，自动管理&lt;/li>
&lt;li>&lt;strong>轻松接入对象存储&lt;/strong>：轻松访问对象存储（S3、GCS、R2）&lt;/li>
&lt;/ul>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/skypilot-org/skypilot" target="_blank" rel="noopener" >SkyPilot
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="tensorrt-llm">TensorRT-LLM&lt;/h3>
&lt;a href="https://github.com/NVIDIA/TensorRT-LLM" target="_blank" class="card-github fetch-waiting no-styling"
repo="NVIDIA/TensorRT-LLM" id="repo-qaACQwOZLuCrkZL0-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-qaACQwOZLuCrkZL0-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">NVIDIA&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">TensorRT-LLM&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-qaACQwOZLuCrkZL0-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-qaACQwOZLuCrkZL0-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-qaACQwOZLuCrkZL0-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-qaACQwOZLuCrkZL0-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-qaACQwOZLuCrkZL0-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-qaACQwOZLuCrkZL0-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/NVIDIA\/TensorRT-LLM', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-qaACQwOZLuCrkZL0-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-qaACQwOZLuCrkZL0-language').innerText = data.language;
document.getElementById('repo-qaACQwOZLuCrkZL0-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-qaACQwOZLuCrkZL0-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-qaACQwOZLuCrkZL0-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-qaACQwOZLuCrkZL0-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-qaACQwOZLuCrkZL0-license').classList.add = "no-license"
};
document.getElementById('repo-qaACQwOZLuCrkZL0-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for NVIDIA\/TensorRT-LLM.")
}).catch(err => {
const c = document.getElementById('repo-qaACQwOZLuCrkZL0-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for NVIDIA\/TensorRT-LLM.")
})
&lt;/script>
&lt;p>TensorRT-LLM 是 NVIDIA 推出的高性能 LLM 推理引擎，能够充分利用 GPU 加速计算，并针对 Transformer 模型结构进行了优化，大幅提升推理速度。&lt;/p>
&lt;p>TensorRT-LLM 为用户提供了易于使用的 Python API，用于定义大型语言模型 (LLMs) 和构建 TensorRT 引擎，这些引擎包含最先进的优化技术，可在英伟达™（NVIDIA®）图形处理器上高效执行推理。TensorRT-LLM 还包含用于创建执行这些 TensorRT 引擎的 Python 和 C++ 运行时的组件。&lt;/p>
&lt;p>详细信息，请访问 &lt;a class="link" href="https://github.com/NVIDIA/TensorRT-LLM" target="_blank" rel="noopener" >TensorRT-LLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="openvino">OpenVino&lt;/h3>
&lt;a href="https://github.com/openvinotoolkit/openvino" target="_blank" class="card-github fetch-waiting no-styling"
repo="openvinotoolkit/openvino" id="repo-co0sKqgKuz8yyl9U-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-co0sKqgKuz8yyl9U-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">openvinotoolkit&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">openvino&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-co0sKqgKuz8yyl9U-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-co0sKqgKuz8yyl9U-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-co0sKqgKuz8yyl9U-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-co0sKqgKuz8yyl9U-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-co0sKqgKuz8yyl9U-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-co0sKqgKuz8yyl9U-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/openvinotoolkit\/openvino', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-co0sKqgKuz8yyl9U-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-co0sKqgKuz8yyl9U-language').innerText = data.language;
document.getElementById('repo-co0sKqgKuz8yyl9U-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-co0sKqgKuz8yyl9U-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-co0sKqgKuz8yyl9U-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-co0sKqgKuz8yyl9U-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-co0sKqgKuz8yyl9U-license').classList.add = "no-license"
};
document.getElementById('repo-co0sKqgKuz8yyl9U-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for openvinotoolkit\/openvino.")
}).catch(err => {
const c = document.getElementById('repo-co0sKqgKuz8yyl9U-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for openvinotoolkit\/openvino.")
})
&lt;/script>
&lt;p>OpenVINO™ 是用于优化和部署人工智能推理的开源工具包。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>推理优化&lt;/strong>：提升深度学习在计算机视觉、自动语音识别、生成式人工智能、使用大型和小型语言模型的自然语言处理以及许多其他常见任务中的性能。&lt;/li>
&lt;li>&lt;strong>灵活的模型支持&lt;/strong>：使用 TensorFlow、PyTorch、ONNX、Keras 和 PaddlePaddle 等流行框架训练的模型。无需原始框架即可转换和部署模型。&lt;/li>
&lt;li>&lt;strong>广泛的平台兼容性&lt;/strong>：减少资源需求，在从边缘到云的一系列平台上高效部署。OpenVINO™ 支持在 CPU（x86、ARM）、GPU（支持 OpenCL 的集成和独立 GPU）和 AI 加速器（英特尔 NPU）上进行推理。&lt;/li>
&lt;li>&lt;strong>社区和生态系统&lt;/strong>：加入一个活跃的社区，为提高各个领域的深度学习性能做出贡献。&lt;/li>
&lt;/ul>
&lt;p>详细信息，请访问 &lt;a class="link" href="https://github.com/openvinotoolkit/openvino" target="_blank" rel="noopener" >OpenVino
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="tgi">TGI&lt;/h3>
&lt;a href="https://github.com/huggingface/text-generation-inference" target="_blank" class="card-github fetch-waiting no-styling"
repo="huggingface/text-generation-inference" id="repo-b7bGIC7KKPbloIuC-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-b7bGIC7KKPbloIuC-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">huggingface&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">text-generation-inference&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-b7bGIC7KKPbloIuC-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-b7bGIC7KKPbloIuC-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-b7bGIC7KKPbloIuC-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-b7bGIC7KKPbloIuC-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-b7bGIC7KKPbloIuC-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-b7bGIC7KKPbloIuC-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/huggingface\/text-generation-inference', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-b7bGIC7KKPbloIuC-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-b7bGIC7KKPbloIuC-language').innerText = data.language;
document.getElementById('repo-b7bGIC7KKPbloIuC-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-b7bGIC7KKPbloIuC-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-b7bGIC7KKPbloIuC-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-b7bGIC7KKPbloIuC-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-b7bGIC7KKPbloIuC-license').classList.add = "no-license"
};
document.getElementById('repo-b7bGIC7KKPbloIuC-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for huggingface\/text-generation-inference.")
}).catch(err => {
const c = document.getElementById('repo-b7bGIC7KKPbloIuC-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for huggingface\/text-generation-inference.")
})
&lt;/script>
&lt;p>文本生成推理（TGI）是一个用于部署和服务大型语言模型（LLMs）的工具包。TGI 可为最流行的开源 LLMs 实现高性能文本生成，包括 Llama、Falcon、StarCoder、BLOOM、GPT-NeoX 等。&lt;/p>
&lt;p>TGI 实现了许多功能，可以在 &lt;a class="link" href="https://github.com/huggingface/text-generation-inference" target="_blank" rel="noopener" >TGI
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页上找到详细信息。&lt;/p>
&lt;h2 id="本地运行">本地运行&lt;/h2>
&lt;p>得益于模型压缩和优化技术，我们也可以在个人设备上运行 LLM：&lt;/p>
&lt;h3 id="mlx">MLX&lt;/h3>
&lt;a href="https://github.com/ml-explore/mlx" target="_blank" class="card-github fetch-waiting no-styling"
repo="ml-explore/mlx" id="repo-QkQq8UCS1HpCyX8q-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-QkQq8UCS1HpCyX8q-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">ml-explore&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">mlx&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-QkQq8UCS1HpCyX8q-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-QkQq8UCS1HpCyX8q-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-QkQq8UCS1HpCyX8q-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-QkQq8UCS1HpCyX8q-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-QkQq8UCS1HpCyX8q-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-QkQq8UCS1HpCyX8q-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/ml-explore\/mlx', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-QkQq8UCS1HpCyX8q-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-QkQq8UCS1HpCyX8q-language').innerText = data.language;
document.getElementById('repo-QkQq8UCS1HpCyX8q-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-QkQq8UCS1HpCyX8q-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-QkQq8UCS1HpCyX8q-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-QkQq8UCS1HpCyX8q-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-QkQq8UCS1HpCyX8q-license').classList.add = "no-license"
};
document.getElementById('repo-QkQq8UCS1HpCyX8q-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for ml-explore\/mlx.")
}).catch(err => {
const c = document.getElementById('repo-QkQq8UCS1HpCyX8q-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for ml-explore\/mlx.")
})
&lt;/script>
&lt;p>MLX 是一个专门支持在 Apple 设备上运行 LLM 的框架，充分利用 Metal 加速计算，并提供简单易用的 API，方便开发者将 LLM 集成到 iOS 应用中.&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>相似的应用程序接口&lt;/strong>：MLX 的 Python API 与 NumPy 非常相似。MLX 还拥有功能齐全的 C++、C 和 Swift API，这些 API 与 Python API 非常相似。MLX 拥有更高级别的软件包，如 &lt;code>mlx.nn&lt;/code> 和 &lt;code>mlx.optimizers&lt;/code> ，其 API 与 PyTorch 非常接近，可简化更复杂模型的构建。&lt;/li>
&lt;li>&lt;strong>可组合函数变换&lt;/strong>：MLX 支持用于自动微分、自动矢量化和计算图优化的可组合函数变换。&lt;/li>
&lt;li>&lt;strong>懒计算&lt;/strong>：MLX 中的计算只有在需要时才将数组实体化。&lt;/li>
&lt;li>&lt;strong>动态图构建&lt;/strong>：MLX 中的计算图形是动态构建的。改变函数参数的形状不会导致编译速度变慢，而且调试简单直观。&lt;/li>
&lt;li>&lt;strong>多设备&lt;/strong>：操作可在任何支持的设备（目前是 CPU 和 GPU）上运行。&lt;/li>
&lt;li>&lt;strong>统一内存&lt;/strong>：统一内存模型是 MLX 与其他框架的一个显著区别。MLX 中的阵列位于共享内存中。对 MLX 数组的操作可在任何支持的设备类型上执行，而无需传输数据。&lt;/li>
&lt;/ul>
&lt;p>MLX 是机器学习研究人员为机器学习研究人员设计的。该框架旨在方便用户使用，但仍能高效地训练和部署模型。框架本身的设计概念也很简单。我们的目标是让研究人员能够轻松扩展和改进 MLX，从而快速探索新思路。更多详细信息，请访问 &lt;a class="link" href="https://github.com/ml-explore/mlx" target="_blank" rel="noopener" >MLX
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="llamacpp">Llama.cpp&lt;/h3>
&lt;p>Llama.cpp 是使用 C++ 实现的 Llama 模型推理引擎，可以在 CPU 上高效运行，并支持多种操作系统和硬件平台，方便开发者在资源受限的设备上运行 LLM。&lt;/p>
&lt;a href="https://github.com/ggerganov/llama.cpp" target="_blank" class="card-github fetch-waiting no-styling"
repo="ggerganov/llama.cpp" id="repo-Cevmp6LQ9F2xG8HI-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-Cevmp6LQ9F2xG8HI-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">ggerganov&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">llama.cpp&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-Cevmp6LQ9F2xG8HI-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-Cevmp6LQ9F2xG8HI-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-Cevmp6LQ9F2xG8HI-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-Cevmp6LQ9F2xG8HI-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-Cevmp6LQ9F2xG8HI-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-Cevmp6LQ9F2xG8HI-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/ggerganov\/llama.cpp', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-Cevmp6LQ9F2xG8HI-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-Cevmp6LQ9F2xG8HI-language').innerText = data.language;
document.getElementById('repo-Cevmp6LQ9F2xG8HI-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-Cevmp6LQ9F2xG8HI-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-Cevmp6LQ9F2xG8HI-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-Cevmp6LQ9F2xG8HI-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-Cevmp6LQ9F2xG8HI-license').classList.add = "no-license"
};
document.getElementById('repo-Cevmp6LQ9F2xG8HI-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for ggerganov\/llama.cpp.")
}).catch(err => {
const c = document.getElementById('repo-Cevmp6LQ9F2xG8HI-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for ggerganov\/llama.cpp.")
})
&lt;/script>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>CPU 推理:&lt;/strong> 针对 CPU 平台进行优化，可以在没有 GPU 的设备上运行 LLM。&lt;/li>
&lt;li>&lt;strong>跨平台支持:&lt;/strong> 支持 Linux, macOS, Windows 等多种操作系统，方便用户在不同平台上使用。&lt;/li>
&lt;li>&lt;strong>轻量级部署:&lt;/strong> 编译后的二进制文件体积小，方便用户部署和使用.&lt;/li>
&lt;/ul>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener" >Llama.cpp
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="ollama">Ollama&lt;/h3>
&lt;a href="https://github.com/ollama/ollama" target="_blank" class="card-github fetch-waiting no-styling"
repo="ollama/ollama" id="repo-oejUcoMxoATC0q0c-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-oejUcoMxoATC0q0c-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">ollama&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">ollama&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-oejUcoMxoATC0q0c-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-oejUcoMxoATC0q0c-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-oejUcoMxoATC0q0c-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-oejUcoMxoATC0q0c-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-oejUcoMxoATC0q0c-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-oejUcoMxoATC0q0c-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/ollama\/ollama', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-oejUcoMxoATC0q0c-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-oejUcoMxoATC0q0c-language').innerText = data.language;
document.getElementById('repo-oejUcoMxoATC0q0c-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-oejUcoMxoATC0q0c-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-oejUcoMxoATC0q0c-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-oejUcoMxoATC0q0c-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-oejUcoMxoATC0q0c-license').classList.add = "no-license"
};
document.getElementById('repo-oejUcoMxoATC0q0c-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for ollama\/ollama.")
}).catch(err => {
const c = document.getElementById('repo-oejUcoMxoATC0q0c-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for ollama\/ollama.")
})
&lt;/script>
&lt;p>在 &lt;a class="link" href="https://cuterwrite.top/p/ollama/" >【Ollama：从入门到进阶】
&lt;/a>
一文中介绍过，Ollama 是一个用于构建大型语言模型应用的工具，它提供了一个简洁易用的命令行界面和服务器，让你能够轻松下载、运行和管理各种开源 LLM。与需要复杂配置和强大硬件的传统 LLM 不同，Ollama 让你能够方便地像使用手机 App 一样体验 LLM 的强大功能。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>简单易用&lt;/strong>：Ollama 提供了一个简洁易用的命令行工具，方便用户下载、运行和管理 LLM。&lt;/li>
&lt;li>&lt;strong>多种模型&lt;/strong>：Ollama 支持多种开源 LLM，包括 Qwen2、Llama3、Mistral 等。&lt;/li>
&lt;li>&lt;strong>兼容 OpenAI 接口&lt;/strong>：Ollama 支持 OpenAI API 接口，便于切换原有应用到 Ollama 上。&lt;/li>
&lt;/ul>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/ollama/ollama" target="_blank" rel="noopener" >Ollama
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h2 id="agent-及-rag-框架">Agent 及 RAG 框架&lt;/h2>
&lt;p>将 LLM 与外部数据和工具结合，可以构建更强大的应用。以下是一些常用的 Agent 及 RAG 框架：&lt;/p>
&lt;h3 id="llamaindex">LlamaIndex&lt;/h3>
&lt;a href="https://github.com/run-llama/llama_index" target="_blank" class="card-github fetch-waiting no-styling"
repo="run-llama/llama_index" id="repo-oitqYi7WmypEnI1i-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-oitqYi7WmypEnI1i-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">run-llama&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">llama_index&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-oitqYi7WmypEnI1i-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-oitqYi7WmypEnI1i-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-oitqYi7WmypEnI1i-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-oitqYi7WmypEnI1i-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-oitqYi7WmypEnI1i-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-oitqYi7WmypEnI1i-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/run-llama\/llama_index', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-oitqYi7WmypEnI1i-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-oitqYi7WmypEnI1i-language').innerText = data.language;
document.getElementById('repo-oitqYi7WmypEnI1i-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-oitqYi7WmypEnI1i-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-oitqYi7WmypEnI1i-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-oitqYi7WmypEnI1i-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-oitqYi7WmypEnI1i-license').classList.add = "no-license"
};
document.getElementById('repo-oitqYi7WmypEnI1i-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for run-llama\/llama_index.")
}).catch(err => {
const c = document.getElementById('repo-oitqYi7WmypEnI1i-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for run-llama\/llama_index.")
})
&lt;/script>
&lt;p>LlamaIndex（GPT 索引）是用于 LLM 应用程序的数据框架。使用 LlamaIndex 构建应用程序通常需要使用 LlamaIndex 核心和一组选定的集成（或插件）。在 Python 中使用 LlamaIndex 构建应用程序有两种方法：&lt;/p>
&lt;ul>
&lt;li>启动器： &lt;code>llama-index&lt;/code> ( &lt;a class="link" href="https://pypi.org/project/llama-index/" target="_blank" rel="noopener" >https://pypi.org/project/llama-index/
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
)。Python 入门包，包括核心 LlamaIndex 以及部分集成。&lt;/li>
&lt;li>定制化：&lt;code>llama-index-core&lt;/code> ( &lt;a class="link" href="https://pypi.org/project/llama-index-core/" target="_blank" rel="noopener" >https://pypi.org/project/llama-index-core/
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
)。安装核心 LlamaIndex，并在 LlamaHub 上添加应用程序所需的 LlamaIndex 集成包。目前有 300 多个 LlamaIndex 集成包可与核心无缝协作，让你可以使用自己喜欢的 LLM、嵌入和向量存储数据库进行构建&lt;/li>
&lt;/ul>
&lt;p>LlamaIndex Python 库是以名字命名的，因此包含 &lt;code>core&lt;/code> 的导入语句意味着使用的是核心包。相反，那些不含 &lt;code>core&lt;/code> 的语句则意味着使用的是集成包。&lt;/p>
&lt;pre>&lt;code class="language-python"># typical pattern
from llama_index.core.xxx import ClassABC # core submodule xxx
from llama_index.xxx.yyy import (
SubclassABC,
) # integration yyy for submodule xxx
# concrete example
from llama_index.core.llms import LLM
from llama_index.llms.openai import OpenAI
&lt;/code>&lt;/pre>
&lt;h3 id="crewai">CrewAI&lt;/h3>
&lt;a href="https://github.com/joaomdmoura/crewAI" target="_blank" class="card-github fetch-waiting no-styling"
repo="joaomdmoura/crewAI" id="repo-mYBLzNJyIM5dUei7-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-mYBLzNJyIM5dUei7-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">joaomdmoura&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">crewAI&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-mYBLzNJyIM5dUei7-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-mYBLzNJyIM5dUei7-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-mYBLzNJyIM5dUei7-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-mYBLzNJyIM5dUei7-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-mYBLzNJyIM5dUei7-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-mYBLzNJyIM5dUei7-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/joaomdmoura\/crewAI', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-mYBLzNJyIM5dUei7-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-mYBLzNJyIM5dUei7-language').innerText = data.language;
document.getElementById('repo-mYBLzNJyIM5dUei7-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-mYBLzNJyIM5dUei7-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-mYBLzNJyIM5dUei7-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-mYBLzNJyIM5dUei7-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-mYBLzNJyIM5dUei7-license').classList.add = "no-license"
};
document.getElementById('repo-mYBLzNJyIM5dUei7-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for joaomdmoura\/crewAI.")
}).catch(err => {
const c = document.getElementById('repo-mYBLzNJyIM5dUei7-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for joaomdmoura\/crewAI.")
})
&lt;/script>
&lt;p>CrewAI 是一个构建 AI Agent 的框架，可以将 LLM 与其他工具和 API 集成，实现更复杂的任务，例如自动执行网页操作、生成代码等。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>基于角色的智能体设计&lt;/strong>：你可以使用特定的角色、目标和工具来自定义智能体。&lt;/li>
&lt;li>&lt;strong>自主智能体间委托&lt;/strong>：智能体可以自主地将任务委托给其他智能体，并相互查询信息，从而提高解决问题的效率。&lt;/li>
&lt;li>&lt;strong>灵活的任务管理&lt;/strong>：可以使用可定制的工具来定义任务，并动态地将任务分配给智能体。&lt;/li>
&lt;li>&lt;strong>流程驱动&lt;/strong>：该系统以流程为中心，目前支持按顺序执行任务和分层流程。未来还会支持更复杂的流程，例如协商和自主流程。&lt;/li>
&lt;li>&lt;strong>保存输出为文件&lt;/strong>：可以将单个任务的输出保存为文件，以便以后使用。&lt;/li>
&lt;li>&lt;strong>将输出解析为 Pydantic 或 Json&lt;/strong>：可以将单个任务的输出解析为 Pydantic 模型或 Json 格式，以便于后续处理和分析。&lt;/li>
&lt;li>&lt;strong>支持开源模型&lt;/strong>：可以使用 OpenAI 或其他开源模型来运行您的智能体团队。更多关于配置智能体与模型连接的信息，包括如何连接到本地运行的模型，请参阅&lt;a class="link" href="https://docs.crewai.com/how-to/LLM-Connections/" target="_blank" rel="noopener" >将 crewAI 连接到大型语言模型
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/li>
&lt;/ul>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/joaomdmoura/crewAI" target="_blank" rel="noopener" >CrewAI
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="opendevin">OpenDevin&lt;/h3>
&lt;a href="https://github.com/opendevin/opendevin" target="_blank" class="card-github fetch-waiting no-styling"
repo="opendevin/opendevin" id="repo-1t0UaKxBOfFoZp82-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-1t0UaKxBOfFoZp82-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">opendevin&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">opendevin&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-1t0UaKxBOfFoZp82-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-1t0UaKxBOfFoZp82-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-1t0UaKxBOfFoZp82-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-1t0UaKxBOfFoZp82-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-1t0UaKxBOfFoZp82-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-1t0UaKxBOfFoZp82-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/opendevin\/opendevin', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-1t0UaKxBOfFoZp82-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-1t0UaKxBOfFoZp82-language').innerText = data.language;
document.getElementById('repo-1t0UaKxBOfFoZp82-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-1t0UaKxBOfFoZp82-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-1t0UaKxBOfFoZp82-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-1t0UaKxBOfFoZp82-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-1t0UaKxBOfFoZp82-license').classList.add = "no-license"
};
document.getElementById('repo-1t0UaKxBOfFoZp82-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for opendevin\/opendevin.")
}).catch(err => {
const c = document.getElementById('repo-1t0UaKxBOfFoZp82-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for opendevin\/opendevin.")
})
&lt;/script>
&lt;p>OpenDevin 是一个由人工智能和 LLMs 驱动的自主软件工程师平台。&lt;/p>
&lt;p>OpenDevin 智能体与人类开发人员合作编写代码、修复错误和发布功能。&lt;/p>
&lt;p>详细信息，请访问 &lt;a class="link" href="https://github.com/opendevin/opendevin" target="_blank" rel="noopener" >OpenDevin
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h2 id="模型评测">模型评测&lt;/h2>
&lt;p>为了选择合适的 LLM 并评估其性能，我们需要进行模型评测：&lt;/p>
&lt;h3 id="lmsys">LMSys&lt;/h3>
&lt;p>LMSys Org 是由加州大学伯克利分校的学生和教师与加州大学圣地亚哥分校以及卡内基梅隆大学合作成立的开放式研究组织。&lt;/p>
&lt;p>目标是通过共同开发开放模型、数据集、系统和评估工具，使大型模型对每个人都可访问。训练大型语言模型并广泛提供它们的应用，同时也在开发分布式系统以加速它们的训练和推理过程。&lt;/p>
&lt;p>目前，LMSys Chatbot Area 是最被认可的大模型排行榜之一，受多家公司和研究机构的认可。&lt;/p>
&lt;p>排行榜地址：&lt;a class="link" href="https://arena.lmsys.org/" target="_blank" rel="noopener" >https://arena.lmsys.org/
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h3 id="opencompass">OpenCompass&lt;/h3>
&lt;p>OpenCompass 是一个 LLM 评估平台，支持 100 多个数据集上的各种模型（Llama3、Mistral、InternLM2、GPT-4、LLaMa2、Qwen、GLM、Claude 等）。&lt;/p>
&lt;a href="https://github.com/open-compass/opencompass" target="_blank" class="card-github fetch-waiting no-styling"
repo="open-compass/opencompass" id="repo-rRbp7FKOQ43iju6l-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-rRbp7FKOQ43iju6l-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">open-compass&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">opencompass&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-rRbp7FKOQ43iju6l-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-rRbp7FKOQ43iju6l-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-rRbp7FKOQ43iju6l-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-rRbp7FKOQ43iju6l-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-rRbp7FKOQ43iju6l-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-rRbp7FKOQ43iju6l-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/open-compass\/opencompass', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-rRbp7FKOQ43iju6l-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-rRbp7FKOQ43iju6l-language').innerText = data.language;
document.getElementById('repo-rRbp7FKOQ43iju6l-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-rRbp7FKOQ43iju6l-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-rRbp7FKOQ43iju6l-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-rRbp7FKOQ43iju6l-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-rRbp7FKOQ43iju6l-license').classList.add = "no-license"
};
document.getElementById('repo-rRbp7FKOQ43iju6l-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for open-compass\/opencompass.")
}).catch(err => {
const c = document.getElementById('repo-rRbp7FKOQ43iju6l-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for open-compass\/opencompass.")
})
&lt;/script>
&lt;h3 id="open-llm-leaderboard">Open LLM Leaderboard&lt;/h3>
&lt;p>Open LLM Leaderboard 是一个持续更新的 LLM 排行榜，根据多个评测指标对不同模型进行排名，方便开发者了解最新的模型性能和发展趋势。&lt;/p>
&lt;p>排行榜地址：&lt;a class="link" href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard" target="_blank" rel="noopener" >https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>LLM 生态正在蓬勃发展，涵盖了从模型训练到应用落地的各个环节。相信随着技术的不断进步，LLM 将会在更多领域发挥重要作用，为我们带来更加智能的应用体验。&lt;/p></description></item><item><title>Ollama：从入门到进阶</title><link>https://cuterwrite.top/p/ollama/</link><pubDate>Sat, 15 Jun 2024 23:10:00 +0000</pubDate><guid>https://cuterwrite.top/p/ollama/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116903387_p0_master1200.webp" alt="Featured image of post Ollama：从入门到进阶" />&lt;p>近年来，大型语言模型（LLM）以其强大的文本生成和理解能力，成为了人工智能领域的中坚力量。商业 LLM 的价格通常高昂且代码封闭，限制了研究者和开发者的探索空间。幸运的是，开源社区提供了像 Ollama 这样优秀的替代方案，让每个人都能够轻松体验 LLM 的魅力，并能结合 HPC 和 IDE 插件，打造更强大的个人助手。&lt;/p>
&lt;h2 id="什么是-ollama">什么是 Ollama？&lt;/h2>
&lt;p>Ollama 是一个用于构建大型语言模型应用的工具，它提供了一个简洁易用的命令行界面和服务器，让你能够轻松下载、运行和管理各种开源 LLM。与需要复杂配置和强大硬件的传统 LLM 不同，Ollama 让你能够方便地像使用手机 App 一样体验 LLM 的强大功能。&lt;/p>
&lt;h2 id="ollama-的优势">Ollama 的优势&lt;/h2>
&lt;p>Ollama 拥有以下显著优势：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>开源免费&lt;/strong>： Ollama 及其支持的模型完全开源免费，任何人都可以自由使用、修改和分发。&lt;/li>
&lt;li>&lt;strong>简单易用&lt;/strong>： 无需复杂的配置和安装过程，只需几条命令即可启动和运行 Ollama。&lt;/li>
&lt;li>&lt;strong>模型丰富&lt;/strong>： Ollama 支持 Llama 3、Mistral、Qwen2 等众多热门开源 LLM，并提供一键下载和切换功能。&lt;/li>
&lt;li>&lt;strong>资源占用低&lt;/strong>： 相比于商业 LLM，Ollama 对硬件要求更低，即使在普通笔记本电脑上也能流畅运行。&lt;/li>
&lt;li>&lt;strong>社区活跃&lt;/strong>： Ollama 拥有庞大且活跃的社区，用户可以轻松获取帮助、分享经验和参与模型开发。&lt;/li>
&lt;/ul>
&lt;h2 id="如何使用-ollama">如何使用 Ollama？&lt;/h2>
&lt;p>使用 Ollama 非常简单，只需要按照以下步骤：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>安装 Ollama&lt;/strong>： 根据你的操作系统，从 &lt;a class="link" href="https://ollama.com/" target="_blank" rel="noopener" >Ollama 官网
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
下载并安装最新版本。&lt;/li>
&lt;li>&lt;strong>启动 Ollama&lt;/strong>： 打开终端或命令行，输入 &lt;code>ollama serve&lt;/code> 命令启动 Ollama 服务器。&lt;/li>
&lt;li>&lt;strong>下载模型&lt;/strong>： 在&lt;a class="link" href="https://ollama.com/library" target="_blank" rel="noopener" >模型仓库
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
找到想要的模型，然后使用 &lt;code>ollama pull&lt;/code> 命令下载，例如 &lt;code>ollama pull llama3:70b&lt;/code> 。&lt;/li>
&lt;li>&lt;strong>运行模型&lt;/strong>： 使用 &lt;code>ollama run&lt;/code> 命令启动模型，例如 &lt;code>ollama run llama3:70b&lt;/code> 。&lt;/li>
&lt;li>&lt;strong>开始聊天&lt;/strong>： 在终端中输入你的问题或指令，Ollama 会根据模型生成相应的回复。&lt;/li>
&lt;/ol>
&lt;h3 id="安装-ollama">安装 Ollama&lt;/h3>
&lt;h4 id="macos">macOS&lt;/h4>
&lt;p>&lt;a class="link" href="https://ollama.com/download/Ollama-darwin.zip" target="_blank" rel="noopener" >下载 Ollama for macOS
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h4 id="windows">Windows&lt;/h4>
&lt;p>&lt;a class="link" href="https://ollama.com/download/OllamaSetup.exe" target="_blank" rel="noopener" >下载 Ollama for Windows
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h4 id="linux">Linux&lt;/h4>
&lt;pre>&lt;code class="language-bash">curl -fsSL https://ollama.com/install.sh | sh
&lt;/code>&lt;/pre>
&lt;h4 id="docker">Docker&lt;/h4>
&lt;h5 id="cpu-版本">CPU 版本&lt;/h5>
&lt;pre>&lt;code class="language-bash">docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
&lt;/code>&lt;/pre>
&lt;h5 id="gpu-版本">GPU 版本&lt;/h5>
&lt;ol>
&lt;li>安装 &lt;a class="link" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation" target="_blank" rel="noopener" >Nvidia container toolkit
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;li>在 Docker 容器中运行 Ollama&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
&lt;/code>&lt;/pre>
&lt;h3 id="启动-ollama">启动 Ollama&lt;/h3>
&lt;pre>&lt;code class="language-bash">ollama serve
&lt;/code>&lt;/pre>
&lt;p>输出以下信息表示 Ollama 服务器已成功启动（V100 机器）：&lt;/p>
&lt;pre>&lt;code class="language-bash">$ ollama serve
### 省略的日志输出 ###
Listening on [::]:11434 (version 0.1.42)
&lt;/code>&lt;/pre>
&lt;h3 id="下载模型">下载模型&lt;/h3>
&lt;pre>&lt;code class="language-bash">ollama pull qwen2:72b
&lt;/code>&lt;/pre>
&lt;h3 id="运行模型">运行模型&lt;/h3>
&lt;pre>&lt;code class="language-bash">ollama run qwen2:72b
&lt;/code>&lt;/pre>
&lt;p>例如，运行如下命令后：&lt;/p>
&lt;pre>&lt;code class="language-bash">$ ollama run qwen2:72b
&amp;gt;&amp;gt;&amp;gt; Who are you?
I am Qwen, a pre-trained language model developed by Alibaba Cloud. My purpose is to assist users in generating various types of text, such as articles, stories, poems, and answering
questions by using the natural language processing techniques. How may I assist you today?
&amp;gt;&amp;gt;&amp;gt; Send a message(/? for help)
&lt;/code>&lt;/pre>
&lt;h4 id="docker-容器中运行模型">Docker 容器中运行模型&lt;/h4>
&lt;pre>&lt;code class="language-bash">docker exec -it ollama ollama run qwen2:72b
&lt;/code>&lt;/pre>
&lt;h3 id="配置-ollama">配置 Ollama&lt;/h3>
&lt;p>Ollama 提供了多种环境变量以供配置：&lt;/p>
&lt;ul>
&lt;li>&lt;code>OLLAMA_DEBUG&lt;/code>：是否开启调试模式，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_FLASH_ATTENTION&lt;/code>：是否闪烁注意力，默认为 &lt;code>true&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_HOST&lt;/code>：Ollama 服务器的主机地址，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_KEEP_ALIVE&lt;/code>：保持连接的时间，默认为 &lt;code>5m&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_LLM_LIBRARY&lt;/code>：LLM 库，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_LOADED_MODELS&lt;/code>：最大加载模型数，默认为 &lt;code>1&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_QUEUE&lt;/code>：最大队列数，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_VRAM&lt;/code>：最大虚拟内存，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MODELS&lt;/code>：模型目录，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_NOHISTORY&lt;/code>：是否保存历史记录，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_NOPRUNE&lt;/code>：是否启用剪枝，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_NUM_PARALLEL&lt;/code>：并行数，默认为 &lt;code>1&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_ORIGINS&lt;/code>：允许的来源，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_RUNNERS_DIR&lt;/code>：运行器目录，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_SCHED_SPREAD&lt;/code>：调度分布，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_TMPDIR&lt;/code>：临时文件目录，默认为空。Here is the optimized list in the desired format:&lt;/li>
&lt;li>&lt;code>OLLAMA_DEBUG&lt;/code>：是否开启调试模式，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_FLASH_ATTENTION&lt;/code>：是否闪烁注意力，默认为 &lt;code>true&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_HOST&lt;/code>：Ollama 服务器的主机地址，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_KEEP_ALIVE&lt;/code>：保持连接的时间，默认为 &lt;code>5m&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_LLM_LIBRARY&lt;/code>：LLM 库，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_LOADED_MODELS&lt;/code>：最大加载模型数，默认为 &lt;code>1&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_QUEUE&lt;/code>：最大队列数，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_VRAM&lt;/code>：最大虚拟内存，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MODELS&lt;/code>：模型目录，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_NOHISTORY&lt;/code>：是否保存历史记录，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_NOPRUNE&lt;/code>：是否启用剪枝，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_NUM_PARALLEL&lt;/code>：并行数，默认为 &lt;code>1&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_ORIGINS&lt;/code>：允许的来源，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_RUNNERS_DIR&lt;/code>：运行器目录，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_SCHED_SPREAD&lt;/code>：调度分布，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_TMPDIR&lt;/code>：临时文件目录，默认为空。&lt;/li>
&lt;/ul>
&lt;h2 id="进阶用法hpc-集群上部署-ollama">进阶用法：HPC 集群上部署 Ollama&lt;/h2>
&lt;p>对于大型模型或需要更高性能的情况，可以利用 HPC 集群的强大算力来运行 Ollama。结合 Slurm 进行任务管理，并使用端口映射将服务暴露到本地，即可方便地进行远程访问和使用：&lt;/p>
&lt;ol>
&lt;li>在登录节点配置 Ollama 环境： 安装 Ollama，并下载需要的模型。&lt;/li>
&lt;li>&lt;strong>编写 slurm 脚本&lt;/strong>： 指定资源需求（CPU、内存、GPU 等），并使用 &lt;code>ollama serve&lt;/code> 命令启动模型服务，并绑定到特定端口。&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">#!/bin/bash
#SBATCH --job-name=ollama
#SBATCH -N 1
#SBATCH -p GPU
#SBATCH --gres=gpu:1
module load CUDA
ollama serve
&lt;/code>&lt;/pre>
&lt;ol start="3">
&lt;li>&lt;strong>提交 slurm 任务&lt;/strong>: 使用 &lt;code>sbatch&lt;/code> 命令提交脚本，Slurm 会将任务分配到计算节点运行。&lt;/li>
&lt;li>&lt;strong>本地端口映射&lt;/strong>： 使用 ssh -L 命令将计算节点的端口映射到本地，例如:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">ssh -t -t 用户名@登录节点 ip -L 11434:localhost:11434 -i 登录节点私钥 ssh 计算节点 IP -L 11434:127.0.0.1:11434
&lt;/code>&lt;/pre>
&lt;ol start="5">
&lt;li>&lt;strong>本地访问&lt;/strong>： 在浏览器或应用程序中访问 http://localhost:11434 即可使用 Ollama 服务。&lt;/li>
&lt;/ol>
&lt;div class="notice notice-tip" >
&lt;div class="notice-title">&lt;svg t="1705945832245" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="19409" width="200" height="200">&lt;path d="M512 74.666667C270.933333 74.666667 74.666667 270.933333 74.666667 512S270.933333 949.333333 512 949.333333 949.333333 753.066667 949.333333 512 753.066667 74.666667 512 74.666667z m238.933333 349.866666l-2.133333 2.133334-277.333333 277.333333c-10.666667 10.666667-29.866667 12.8-42.666667 2.133333L426.666667 704l-149.333334-149.333333c-12.8-12.8-12.8-32 0-44.8 10.666667-10.666667 29.866667-12.8 42.666667-2.133334l2.133333 2.133334 125.866667 125.866666 253.866667-253.866666c10.666667-10.666667 29.866667-12.8 42.666666-2.133334l2.133334 2.133334c12.8 12.8 12.8 32 4.266666 42.666666z" fill="#ffffff" p-id="19410">&lt;/path>&lt;/svg>&lt;/div>&lt;p>注意：由于计算节点不联网，需要提前在登录节点使用 &lt;code>ollama pull&lt;/code> 下载所需模型。此外，需要设置 &lt;code>OLLAMA_ORIGINS&lt;/code> 为 &lt;code>*&lt;/code>，设置 &lt;code>OLLAMA_HOST&lt;/code> 为 &lt;code>0.0.0.0&lt;/code>，以允许所有来源访问服务。&lt;/p>&lt;/div>
&lt;h2 id="进阶用法本地代码补全助手">进阶用法：本地代码补全助手&lt;/h2>
&lt;p>Ollama 不仅可以用于聊天和文本创作，还可以结合代码生成模型和 IDE 插件，打造强大的代码补全助手。例如，使用 Codeqwen 7B 模型和 VS Code 插件 &lt;a class="link" href="https://continue.dev/" target="_blank" rel="noopener" >Continue
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
，可以实现高效便捷的代码补全功能。&lt;/p>
&lt;p>首先介绍一下 Continue :
&lt;blockquote>
&lt;p>&lt;p>&lt;a class="link" href="https://continue.dev/" target="_blank" rel="noopener" >Continue
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
enables you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs. All this can run entirely on your own laptop or have Ollama deployed on a server to remotely power code completion and chat experiences based on your needs.&lt;/p>
&lt;p>Continue 使您能够轻松地在 Visual Studio Code 和 JetBrains 中创建自己的代码助手，利用开源 LLM。这一切都可以完全在您的笔记本电脑上运行，或者在服务器上部署 Ollama，远程根据您的需求提供代码补全和聊天体验。&lt;/p>
&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Continue&lt;/span>&lt;cite>&lt;/cite>&lt;/span>&lt;/blockquote>&lt;/p>
&lt;p>在开始之前，你需要安装如下工具：&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://docs.continue.dev/quickstart" target="_blank" rel="noopener" >Continue
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
：&lt;a class="link" href="https://marketplace.visualstudio.com/items?itemName=Continue.continue" target="_blank" rel="noopener" >VS Code 版本
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
或 &lt;a class="link" href="https://plugins.jetbrains.com/plugin/22707-continue" target="_blank" rel="noopener" >JetBrains 版本
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;li>&lt;a class="link" href="https://ollama.com/" target="_blank" rel="noopener" >Ollama
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;/ul>
&lt;p>接下来，我们以 VS Code 为例，介绍如何使用 Ollama + Continue 实现代码补全功能：&lt;/p>
&lt;h3 id="codestral-22b-模型">Codestral 22B 模型&lt;/h3>
&lt;p>Codestral 既能完成代码自动补全，也支持聊天功能。但鉴于其拥有 220 亿参数且不具备生产许可，它对显存要求颇高，仅限于研究和测试使用，因此可能并不适合日常本地应用。&lt;/p>
&lt;h4 id="下载并运行-codestral-模型">下载并运行 Codestral 模型&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run codestral
&lt;/code>&lt;/pre>
&lt;h4 id="配置-configjson">配置 config.json&lt;/h4>
&lt;ul>
&lt;li>在 VS Code 侧边栏点击 Continue 插件图标，然后在面板右下角点击 “齿轮” 图标，打开 &lt;code>config.json&lt;/code> 文件。然后复制以下配置到 &lt;code>config.json&lt;/code> 文件中：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-json">{
&amp;quot;models&amp;quot;: [
{
&amp;quot;title&amp;quot;: &amp;quot;Codestral&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;codestral&amp;quot;
}
],
&amp;quot;tabAutocompleteModel&amp;quot;: {
&amp;quot;title&amp;quot;: &amp;quot;Codestral&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;codestral&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="deepseek-coder-67b-模型--llama-3-8b-模型">DeepSeek Coder 6.7B 模型 + Llama 3 8B 模型&lt;/h3>
&lt;p>根据机器的显存大小，可以利用 Ollama 同时运行多个模型并处理多个并发请求的能力，使用 &lt;code>DeepSeek Coder 6.7B&lt;/code> 进行自动补全，&lt;code>Llama 3 8B&lt;/code> 进行聊天。如果你的机器无法同时运行两者，那么可以分别尝试，决定你更偏好本地自动补全还是本地聊天体验。&lt;/p>
&lt;h4 id="下载并运行-deepseek-coder-模型">下载并运行 DeepSeek Coder 模型&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run deepseek-coder:6.7b-base
&lt;/code>&lt;/pre>
&lt;h4 id="下载并运行-llama-3-模型">下载并运行 Llama 3 模型&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run llama3:8b
&lt;/code>&lt;/pre>
&lt;h4 id="配置-configjson-1">配置 config.json&lt;/h4>
&lt;pre>&lt;code class="language-json">{
&amp;quot;models&amp;quot;: [
{
&amp;quot;title&amp;quot;: &amp;quot;Llama 3 8B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;llama3:8b&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
],
&amp;quot;tabAutocompleteModel&amp;quot;: {
&amp;quot;title&amp;quot;: &amp;quot;DeepSeek Coder 6.7B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;deepseek-coder:6.7b-base&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="codeqwen-7b-模型--qwen2-7b-模型">Codeqwen 7B 模型 + Qwen2 7B 模型&lt;/h3>
&lt;p>Codeqwen 7B 模型是一个专门用于代码补全的模型，而 Qwen2 7B 模型则是一个通用的聊天模型。这两个模型可以很好地结合在一起，实现代码补全和聊天功能。&lt;/p>
&lt;h4 id="下载并运行-codeqwen-模型">下载并运行 Codeqwen 模型&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run codeqwen
&lt;/code>&lt;/pre>
&lt;h4 id="下载并运行-qwen2-模型">下载并运行 Qwen2 模型&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run qwen2:7b
&lt;/code>&lt;/pre>
&lt;h4 id="配置-configjson-2">配置 config.json&lt;/h4>
&lt;pre>&lt;code class="language-json">{
&amp;quot;models&amp;quot;: [
{
&amp;quot;title&amp;quot;: &amp;quot;Codeqwen 7B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;codeqwen&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
],
&amp;quot;tabAutocompleteModel&amp;quot;: {
&amp;quot;title&amp;quot;: &amp;quot;Qwen2 7B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;qwen2:7b&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="利用-rag-向量检索优化聊天">利用 RAG 向量检索优化聊天&lt;/h3>
&lt;p>Continue 内置了 &lt;a class="link" href="https://docs.continue.dev/customization/context-providers#codebase-retrieval" target="_blank" rel="noopener" >@codebase
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
上下文提供器，能自动从代码库中检索到最相关的代码片段。假如你已经设置好了聊天模型（例如 Codestral、Llama 3），那么借助 Ollama 和 &lt;a class="link" href="https://blog.lancedb.com/lancedb-x-continue/" target="_blank" rel="noopener" >LanceDB
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的向量化技术，可以实现更高效的代码检索和聊天体验。&lt;/p>
&lt;p>这里，我们使用 &lt;code>nomic-embed-text&lt;/code> 模型作为向量检索模型：&lt;/p>
&lt;h4 id="下载并运行-nomic-embed-text-模型">下载并运行 Nomic Embed Text 模型&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run nomic-embed-text
&lt;/code>&lt;/pre>
&lt;h4 id="配置-configjson-3">配置 config.json&lt;/h4>
&lt;ul>
&lt;li>在文件中添加以下内容：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-json">{
&amp;quot;embeddingsProvider&amp;quot;: {
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;nomic-embed-text&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="代码补全效果">代码补全效果&lt;/h3>
&lt;ul>
&lt;li>&lt;code>Ctrl + I&lt;/code>: 根据指令生成代码片段。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_1-2024-06-17.webp"
alt="codeqwen_1-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_2-2024-06-17.webp"
alt="codeqwen_2-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_3-2024-06-17.webp"
alt="codeqwen_3-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>光标悬停自动补全代码&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_4-2024-06-17.webp"
alt="codeqwen_4-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;h3 id="与-ollama-聊天">与 Ollama 聊天&lt;/h3>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_5-2024-06-17.webp"
alt="codeqwen_5-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;h3 id="代码自动注释">代码自动注释&lt;/h3>
&lt;ul>
&lt;li>选中代码打开右键菜单&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_6-2024-06-17.webp"
alt="codeqwen_6-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_7-2024-06-17.webp"
alt="codeqwen_7-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>Ollama 为我们打开了通往开源 LLM 世界的大门，让每个人都能轻松体验 LLM 的强大功能，并可以根据自身需求进行定制化应用。无论是进行研究、开发，还是日常使用，Ollama 都能为你提供探索 LLM 无限可能的平台。相信随着 Ollama 的不断发展，它将为我们带来更多惊喜，推动 LLM 技术在各个领域的应用和发展。&lt;/p></description></item></channel></rss>