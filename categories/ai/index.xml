<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>人工智能与数据科学 on Cuterwrite's Blog</title><link>https://cuterwrite.top/categories/ai/</link><description>Recent content in 人工智能与数据科学 on Cuterwrite's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>cuterwrite</copyright><lastBuildDate>Fri, 20 Sep 2024 22:44:00 +0000</lastBuildDate><atom:link href="https://cuterwrite.top/categories/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>实现本地 RAG 服务：整合 Open WebUI、Ollama 和 Qwen2.5</title><link>https://cuterwrite.top/p/integrate-open-webui-ollama-qwen25-local-rag/</link><pubDate>Fri, 20 Sep 2024 22:44:00 +0000</pubDate><guid>https://cuterwrite.top/p/integrate-open-webui-ollama-qwen25-local-rag/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_116903285_p0_master1200.webp" alt="Featured image of post 实现本地 RAG 服务：整合 Open WebUI、Ollama 和 Qwen2.5" />&lt;h1 id="实现本地-rag-服务整合-open-webuiollama-和-qwen25">
&lt;a href="#%e5%ae%9e%e7%8e%b0%e6%9c%ac%e5%9c%b0-rag-%e6%9c%8d%e5%8a%a1%e6%95%b4%e5%90%88-open-webuiollama-%e5%92%8c-qwen25" class="header-anchor">#&lt;/a>
实现本地 RAG 服务：整合 Open WebUI、Ollama 和 Qwen2.5
&lt;/h1>
&lt;h2 id="引言">
&lt;a href="#%e5%bc%95%e8%a8%80" class="header-anchor">#&lt;/a>
引言
&lt;/h2>
&lt;p>在构建信息检索和生成式 AI 应用时，Retrieval-Augmented Generation (RAG) 模型凭借其能够从知识库中检索相关信息并生成准确答案的强大能力，受到越来越多开发者的青睐。然而，实现端到端的本地 RAG 服务，需求的不只是合适的模型，还需要集成强大的用户界面和高效的推理框架。&lt;/p>
&lt;p>在构建本地 RAG 服务时，利用易于部署的 Docker 方式，可以极大简化模型管理和服务集成。这里我们依赖 Open WebUI 提供的用户界面与模型推理服务，再通过 Ollama 来引入 &lt;code>bge-m3&lt;/code> embedding 模型以实现文档向量化方式的检索功能，从而帮助 Qwen2.5 完成更精准的答案生成。&lt;/p>
&lt;p>本文我们将讨论如何通过 Docker 快速启动 Open WebUI，同步 Ollama 的 RAG 能力，并结合 Qwen2.5 模型实现高效的文档检索与生成系统。&lt;/p>
&lt;h2 id="项目概览">
&lt;a href="#%e9%a1%b9%e7%9b%ae%e6%a6%82%e8%a7%88" class="header-anchor">#&lt;/a>
项目概览
&lt;/h2>
&lt;p>该项目将使用以下关键工具：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Open WebUI&lt;/strong> : 提供用户与模型交互的 web 界面。&lt;/li>
&lt;li>&lt;strong>Ollama&lt;/strong> : 用于管理 embedding 和大语言模型的模型推理任务。其中 Ollama 中的 &lt;code>bge-m3&lt;/code> 模型将用于文档检索，Qwen2.5 将负责回答生成。&lt;/li>
&lt;li>&lt;strong>Qwen2.5&lt;/strong> : 模型部分使用阿里推出的 Qwen 2.5 系列，为检索增强生成服务提供自然语言生成。&lt;/li>
&lt;/ol>
&lt;p>为了实现 RAG 服务，我们需要以下步骤：&lt;/p>
&lt;ol>
&lt;li>部署 Open WebUI 作为用户交互界面。&lt;/li>
&lt;li>配置 Ollama 以高效调度 Qwen2.5 系列模型。&lt;/li>
&lt;li>使用 Ollama 配置的名为 &lt;code>bge-m3&lt;/code> 的 embedding 模型实现检索向量化处理。&lt;/li>
&lt;/ol>
&lt;h2 id="部署-open-webui">
&lt;a href="#%e9%83%a8%e7%bd%b2-open-webui" class="header-anchor">#&lt;/a>
部署 Open WebUI
&lt;/h2>
&lt;p>Open WebUI 提供了一个简洁的 Docker 化解决方案，用户无需手动配置大量依赖，直接通过 Docker 启动 Web 界面。&lt;/p>
&lt;p>首先，服务器上需要确保已经安装了 &lt;a class="link" href="https://docs.docker.com/get-docker/" target="_blank" rel="noopener" >Docker
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
，如果没有安装，可以通过以下命令进行快速安装：&lt;/p>
&lt;pre>&lt;code class="language-bash">curl https://get.docker.com | sh
&lt;/code>&lt;/pre>
&lt;p>然后创建一个目录用于保存 Open WebUI 的数据，这样数据不会在项目更新后丢失：&lt;/p>
&lt;pre>&lt;code class="language-bash">sudo mkdir -p /DATA/open-webui
&lt;/code>&lt;/pre>
&lt;p>接下来，我们可以通过以下命令来启动 Open WebUI ：&lt;/p>
&lt;pre>&lt;code class="language-bash">docker run -d -p 3000:8080 \
--add-host=host.docker.internal:host-gateway \
-v /DATA/open-webui:/app/backend/data \
--name open-webui \
--restart always \
ghcr.io/open-webui/open-webui:main
&lt;/code>&lt;/pre>
&lt;p>如果想要运行支持 Nvidia GPU 的 Open WebUI ，可以使用以下命令：&lt;/p>
&lt;pre>&lt;code class="language-bash">docker run -d -p 3000:8080 \
--gpus all \
--add-host=host.docker.internal:host-gateway \
-v /DATA/open-webui:/app/backend/data \
--name open-webui \
--restart always \
ghcr.io/open-webui/open-webui:cuda
&lt;/code>&lt;/pre>
&lt;p>这里我们将 Open WebUI 的服务暴露在机器的 3000 端口，可以通过浏览器访问 &lt;code>http://localhost:3000&lt;/code> 即可使用（远程访问则使用公网 ip ，开放 3000 端口）。/DATA/open-webui 是数据存储目录，你可以根据需要调整这个路径。&lt;/p>
&lt;p>当然除了 Docker 安装方式外，你也可以通过 pip 、源码编译、Podman 等方式安装 Open WebUI 。更多安装方式请参考 &lt;a class="link" href="https://docs.openwebui.com/getting-started" target="_blank" rel="noopener" >Open WebUI 官方文档
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/p>
&lt;h3 id="基础设置">
&lt;a href="#%e5%9f%ba%e7%a1%80%e8%ae%be%e7%bd%ae" class="header-anchor">#&lt;/a>
基础设置
&lt;/h3>
&lt;ol>
&lt;li>输入要注册的账号信息，&lt;strong>设置强密码！！！&lt;/strong>&lt;/li>
&lt;/ol>
&lt;blockquote class="alert-blockquote alert-important">
&lt;p class="alert-heading">
&lt;svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16">
&lt;path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v9.5A1.75 1.75 0 0 1 14.25 13H8.06l-2.573 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25Zm7 2.25v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 9a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z">&lt;/path>
&lt;/svg>
&lt;span>重要&lt;/span>
&lt;/p>
&lt;p>第一个注册的用户将被自动设置为系统管理员，所以请确保你是第一个注册的用户。&lt;/p>
&lt;/blockquote>
&lt;ol start="2">
&lt;li>点击左下角头像，选择管理员面板&lt;/li>
&lt;li>点击面板中的设置&lt;/li>
&lt;li>关闭允许新用户注册（可选）&lt;/li>
&lt;li>点击右下角保存&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-09-21_openwebui_admin.webp"
alt="Open WebUI" width="85%" loading="lazy">
&lt;/figure>
&lt;h2 id="配置-ollama-和-qwen25">
&lt;a href="#%e9%85%8d%e7%bd%ae-ollama-%e5%92%8c-qwen25" class="header-anchor">#&lt;/a>
配置 Ollama 和 Qwen2.5
&lt;/h2>
&lt;h3 id="部署-ollama">
&lt;a href="#%e9%83%a8%e7%bd%b2-ollama" class="header-anchor">#&lt;/a>
部署 Ollama
&lt;/h3>
&lt;p>在本地服务器上安装 Ollama。目前 Ollama 提供多种安装方式，请参考 Ollama 的&lt;a class="link" href="https://ollama.com/" target="_blank" rel="noopener" >官方文档
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
下载安装最新的 &lt;code>0.3.11&lt;/code> 版本（Qwen2.5 在该版本才开始支持）。安装细节可以参考我之前写的一篇文章：&lt;a class="link" href="https://cuterwrite.top/p/ollama/" >Ollama：从入门到进阶
&lt;/a>
。&lt;/p>
&lt;p>启动 Ollama 服务（如果是 Docker 方式启动则不需要，但必须暴露 11434 端口）：&lt;/p>
&lt;pre>&lt;code class="language-bash">ollama serve
&lt;/code>&lt;/pre>
&lt;p>Ollama 服务启动后，可以通过访问地址 &lt;code>http://localhost:11434&lt;/code> 连接到 Ollama 服务。&lt;/p>
&lt;p>&lt;a class="link" href="https://ollama.com/library" target="_blank" rel="noopener" >Ollama Library
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
提供了语义向量模型 ( &lt;code>bge-m3&lt;/code> ) 以及各大文本生成模型（包括 Qwen2.5）。下一步我们将配置 Ollama 以适应本项目对文档检索和问答生成的需求。&lt;/p>
&lt;h3 id="下载-qwen25-模型">
&lt;a href="#%e4%b8%8b%e8%bd%bd-qwen25-%e6%a8%a1%e5%9e%8b" class="header-anchor">#&lt;/a>
下载 Qwen2.5 模型
&lt;/h3>
&lt;p>通过 Ollama 安装 Qwen2.5，你可以直接在命令行中运行 &lt;code>ollama pull&lt;/code> 命令来下载 Qwen2.5 模型，比如要下载 Qwen2.5 的 72B 模型，可以使用以下命令：&lt;/p>
&lt;pre>&lt;code class="language-bash">ollama pull qwen2.5:72b
&lt;/code>&lt;/pre>
&lt;p>该命令将从 Ollama 的模型仓库抓取 Qwen2.5 模型，并准备运行环境。&lt;/p>
&lt;p>Qwen2.5 提供了多种模型尺寸，包括 72B、32B、14B、7B、3B、1.5B、0.5B 等，你可以根据自己的需求和 GPU 显存大小选择合适的模型。我采用的是 4x V100 的服务器，所以可以直接选择 72B 模型。如果要求吐字速度快且能接收微小的性能损失的话，可以使用 &lt;code>q4_0&lt;/code> 量化版本 &lt;code>qwen2.5:72b-instruct-q4_0&lt;/code> ；如果能接受吐字速度慢一些，可以使用 &lt;code>qwen2.5:72b-instruct-q5_K_M&lt;/code> 。对于 4x V100 的服务器，虽然 &lt;code>q5_K_M&lt;/code> 模型的 token 生成明显卡顿，但是为了试验一下 Qwen2.5 的性能，我还是选择了 &lt;code>q5_K_M&lt;/code> 模型。&lt;/p>
&lt;p>对于显存较少的个人电脑，推荐使用 14B 或 7B 模型，通过以下命令下载：&lt;/p>
&lt;pre>&lt;code class="language-bash">ollama pull qwen2.5:14b
&lt;/code>&lt;/pre>
&lt;p>或者&lt;/p>
&lt;pre>&lt;code class="language-bash">ollama pull qwen2.5:7b
&lt;/code>&lt;/pre>
&lt;p>如果你同时启动好了 Open WebUI 和 Ollama 服务，那么也可以在管理员面板中下载模型。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-09-21_download_qwen2-5-7b.webp"
alt="Download Ollama Model in Open WebUI" width="85%" loading="lazy">
&lt;/figure>
&lt;h3 id="下载-bge-m3-模型">
&lt;a href="#%e4%b8%8b%e8%bd%bd-bge-m3-%e6%a8%a1%e5%9e%8b" class="header-anchor">#&lt;/a>
下载 bge-m3 模型
&lt;/h3>
&lt;p>在 Ollama 中下载 &lt;code>bge-m3&lt;/code> 模型，该模型用于文档向量化处理。在命令行中运行以下命令下载模型（或者在 Open WebUI 界面下载）：&lt;/p>
&lt;pre>&lt;code class="language-bash">ollama pull bge-m3:latest
&lt;/code>&lt;/pre>
&lt;p>到这里，我们已经完成了 Ollama 的配置，接下来我们将在 Open WebUI 中配置 RAG 服务。&lt;/p>
&lt;h2 id="rag-集成与配置">
&lt;a href="#rag-%e9%9b%86%e6%88%90%e4%b8%8e%e9%85%8d%e7%bd%ae" class="header-anchor">#&lt;/a>
RAG 集成与配置
&lt;/h2>
&lt;h3 id="在-open-webui-中配置-ollama-的-rag-接口">
&lt;a href="#%e5%9c%a8-open-webui-%e4%b8%ad%e9%85%8d%e7%bd%ae-ollama-%e7%9a%84-rag-%e6%8e%a5%e5%8f%a3" class="header-anchor">#&lt;/a>
在 Open WebUI 中配置 Ollama 的 RAG 接口
&lt;/h3>
&lt;h4 id="访问-open-webui-管理界面">
&lt;a href="#%e8%ae%bf%e9%97%ae-open-webui-%e7%ae%a1%e7%90%86%e7%95%8c%e9%9d%a2" class="header-anchor">#&lt;/a>
访问 Open WebUI 管理界面
&lt;/h4>
&lt;p>启动 Open WebUI 之后，你可以直接通过 Web 浏览器访问服务地址，登录你的管理员账户，然后进入管理员面板。&lt;/p>
&lt;h4 id="设置-ollama-接口">
&lt;a href="#%e8%ae%be%e7%bd%ae-ollama-%e6%8e%a5%e5%8f%a3" class="header-anchor">#&lt;/a>
设置 Ollama 接口
&lt;/h4>
&lt;p>在 Open WebUI 的管理员面板中，点击&lt;strong>设置&lt;/strong>，你会看到外部连接的选项，确保 Ollama API 的地址为 &lt;code>host.docker.internal:11434&lt;/code> ，然后点击右边的 &lt;strong>verify connection&lt;/strong> 按钮确认 Ollama 服务是否正常连接。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-09-21_ollama_api.webp"
alt="Open WebUI Ollama Setting" width="85%" loading="lazy">
&lt;/figure>
&lt;h4 id="设置语义向量模型">
&lt;a href="#%e8%ae%be%e7%bd%ae%e8%af%ad%e4%b9%89%e5%90%91%e9%87%8f%e6%a8%a1%e5%9e%8b" class="header-anchor">#&lt;/a>
设置语义向量模型
&lt;/h4>
&lt;p>在 Open WebUI 的管理员面板中，点击&lt;strong>设置&lt;/strong>，接着点击&lt;strong>文档&lt;/strong>，依次完成以下步骤：&lt;/p>
&lt;ol>
&lt;li>设置语义向量模型引擎为 Ollama 。&lt;/li>
&lt;li>设置语义向量模型为 &lt;code>bge-m3:latest&lt;/code> 。&lt;/li>
&lt;li>其余设置可以保持默认，这里我设置了文件最大上传大小为 10MB，最大上传数量为 3，Top K 设置为 5，块大小和块重叠分别设置为 1500 和 100 ，并开启 PDF 图像处理。&lt;/li>
&lt;li>点击右下角保存。&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-09-21_openwebui_embedding.webp"
alt="Open WebUI Embedding Setting" width="85%" loading="lazy">
&lt;/figure>
&lt;h3 id="测试-rag-服务">
&lt;a href="#%e6%b5%8b%e8%af%95-rag-%e6%9c%8d%e5%8a%a1" class="header-anchor">#&lt;/a>
测试 RAG 服务
&lt;/h3>
&lt;p>现在，你已经实现了完整的本地 RAG 系统。你可以在 Open WebUI 的主界面中输入任意自然语言问题，然后上传响应文档，系统会调用语义向量模型向量化文档，再使用 Qwen2.5 模型检索文档生成答案并返回给用户。&lt;/p>
&lt;p>在 Open WebUI 的用户聊天界面，上传你要检索的文档，然后输入你的问题，点击发送，Open WebUI 将会调用 Ollama 的 &lt;code>bge-m3&lt;/code> 模型进行文档向量化处理，然后调用 Qwen2.5 模型进行问答生成。&lt;/p>
&lt;p>这里我上传了一个简单的 &lt;code>txt&lt;/code> 文件（由 GPT 生成的文本），内容如下：&lt;/p>
&lt;pre>&lt;code class="language-md"># 奇幻森林的冒险
## 引言
在一个遥远的王国边界，有一片神秘的奇幻森林，传说中栖息着许多奇异的生物和古老的魔法。很少有人敢于进入，因为进入森林的人都没有再回来过。故事的主人公是一个年轻的冒险者，他名叫艾文。
## 第一章：艾文的决定
艾文是一个热爱冒险和探索的年轻人，他从小就听过很多关于奇幻森林的故事。尽管家人和朋友都劝他不要去，但他坚定地认为，自己注定要揭开这片森林的秘密。一天清晨，他收拾好行囊，带着勇气和好奇心，向森林进发。
### 1.1 出发前的准备
在出发前，艾文去了城里最有名的图书馆，查阅了关于奇幻森林的资料。他发现，有一本古老的手稿记录了进入森林的路线，以及如何避开其中一些危险的生物。艾文将这本手稿复印在自己的笔记本上，准备在需要的时候参考。
### 1.2 第一次穿越
艾文刚进入森林就感觉到这里的气息与外界完全不同。空气中弥漫着浓郁的花香，还有隐隐约约的奇怪声音。穿越森林的第一天，艾文没有遇到什么危险，但他能感觉到，有什么东西在暗中观察他。
## 第二章：神秘生物
第二天，艾文继续深入森林。然而，他没走多远，就遇到了一只奇异的生物。这是一只会发光的小鹿，全身散发着柔和的蓝色光芒。起初，艾文感到惊讶和畏惧，但这只小鹿却没有攻击他的意思，还带着他走向一个隐秘的洞穴。
### 2.1 洞穴中的秘密
在洞穴中，艾文发现了一块古老的石板，石板上刻有一些奇怪的符号。小鹿似乎知道这些符号的含义，带着艾文一步一步地解读。原来，这些符号记载着一种强大的魔法，可以帮助他在森林中找到失落的宝藏。
### 2.2 获得帮助
艾文决定接受小鹿的帮助，解开这些符号的秘密。他们在洞穴中度过了几天，艾文学会了如何利用森林中的资源制作药剂和武器。通过这些，他在森林中的生存能力大大提高。
## 第三章：最终的试炼
在小鹿的指引下，艾文终于来到了森林的深处，那里有一个古老的祭坛。据说，只有最勇敢的冒险者才能通过祭坛的试炼，获得最终的宝藏。
### 3.1 面对恐惧
祭坛周围布满了各种陷阱和幻觉。艾文必须面对自己内心深处的恐惧，才能通过这些障碍。最终，他用智慧和勇气克服了一切，获得了进入祭坛的资格。
### 3.2 发现宝藏
在祭坛的中心，艾文发现了一颗闪闪发光的宝石。据传，这颗宝石拥有改变命运的力量。艾文拿起宝石，感受到了其中的强大力量。他知道，这不仅仅是一件珍宝，还有可能是破解奇幻森林秘密的关键。
## 结论
艾文成功地揭开了奇幻森林的部分秘密，成为了传说中的英雄。他的冒险故事也激励了更多年轻的冒险者，带着勇气和智慧，踏上探索未知世界的旅程。
&lt;/code>&lt;/pre>
&lt;p>然后分别提了三个问题：&lt;/p>
&lt;ol>
&lt;li>艾文在森林中遇到的奇异生物是什么？&lt;/li>
&lt;li>艾文在洞穴中找到的古老石板上刻的是什么？&lt;/li>
&lt;li>艾文在祭坛中心发现了什么宝藏？&lt;/li>
&lt;/ol>
&lt;p>下图是回答结果：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-09-21_qwen2-5-QA.webp"
alt="Open WebUI Qwen2.5 Answer" width="90%" loading="lazy">
&lt;/figure>
&lt;h2 id="总结">
&lt;a href="#%e6%80%bb%e7%bb%93" class="header-anchor">#&lt;/a>
总结
&lt;/h2>
&lt;p>借助 Open WebUI 和 Ollama，我们可以轻松搭建一个高效、直观的本地 RAG 系统。通过将 &lt;code>bge-m3&lt;/code> 语义向量模型用于文本向量化，再结合 Qwen2.5 生成模型，用户可以在一个统一的 Web 界面中进行文档检索与增强生成任务的高效互动。不但保护了数据隐私，还大幅提升了生成式 AI 的本地化应用能力。&lt;/p></description></item><item><title>LLM 生态介绍：从模型微调到应用落地</title><link>https://cuterwrite.top/p/llm-ecosystem/</link><pubDate>Fri, 05 Jul 2024 22:46:00 +0000</pubDate><guid>https://cuterwrite.top/p/llm-ecosystem/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_119269220_p0_master1200.webp" alt="Featured image of post LLM 生态介绍：从模型微调到应用落地" />&lt;h1 id="llm-生态介绍从模型微调到应用落地">
&lt;a href="#llm-%e7%94%9f%e6%80%81%e4%bb%8b%e7%bb%8d%e4%bb%8e%e6%a8%a1%e5%9e%8b%e5%be%ae%e8%b0%83%e5%88%b0%e5%ba%94%e7%94%a8%e8%90%bd%e5%9c%b0" class="header-anchor">#&lt;/a>
LLM 生态介绍：从模型微调到应用落地
&lt;/h1>
&lt;h2 id="模型微调">
&lt;a href="#%e6%a8%a1%e5%9e%8b%e5%be%ae%e8%b0%83" class="header-anchor">#&lt;/a>
模型微调
&lt;/h2>
&lt;p>预训练的 LLM 通常具备广泛的知识，但要使其在特定任务上表现出色，微调是必不可少的。以下是一些常用的 LLM 微调工具：&lt;/p>
&lt;h3 id="axolotl">
&lt;a href="#axolotl" class="header-anchor">#&lt;/a>
Axolotl
&lt;/h3>
&lt;a href="https://github.com/OpenAccess-AI-Collective/axolotl" target="_blank" class="card-github fetch-waiting no-styling"
repo="OpenAccess-AI-Collective/axolotl" id="repo-TXKnyFv9M39JbAXC-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-TXKnyFv9M39JbAXC-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">OpenAccess-AI-Collective&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">axolotl&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-TXKnyFv9M39JbAXC-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-TXKnyFv9M39JbAXC-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-TXKnyFv9M39JbAXC-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-TXKnyFv9M39JbAXC-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-TXKnyFv9M39JbAXC-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-TXKnyFv9M39JbAXC-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/OpenAccess-AI-Collective\/axolotl', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-TXKnyFv9M39JbAXC-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-TXKnyFv9M39JbAXC-language').innerText = data.language;
document.getElementById('repo-TXKnyFv9M39JbAXC-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-TXKnyFv9M39JbAXC-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-TXKnyFv9M39JbAXC-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-TXKnyFv9M39JbAXC-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-TXKnyFv9M39JbAXC-license').classList.add = "no-license"
};
document.getElementById('repo-TXKnyFv9M39JbAXC-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for OpenAccess-AI-Collective\/axolotl.")
}).catch(err => {
const c = document.getElementById('repo-TXKnyFv9M39JbAXC-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for OpenAccess-AI-Collective\/axolotl.")
})
&lt;/script>
&lt;p>Axolotl 是一款旨在简化各种人工智能模型微调的工具，支持多种配置和架构。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>训练各种 Huggingface 模型，如 llama、pythia、falcon、mpt&lt;/li>
&lt;li>支持 fullfinetune、lora、qlora、relora 和 gptq&lt;/li>
&lt;li>使用简单的 yaml 文件或 CLI 重写功能自定义配置&lt;/li>
&lt;li>加载不同的数据集格式，使用自定义格式，或自带标记化数据集&lt;/li>
&lt;li>与 xformer、闪存关注、绳索缩放和多重包装集成&lt;/li>
&lt;li>可通过 FSDP 或 Deepspeed 与单 GPU 或多 GPU 协同工作&lt;/li>
&lt;li>使用 Docker 在本地或云端轻松运行&lt;/li>
&lt;li>将结果和可选的检查点记录到 wandb 或 mlflow 中&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>快速入门：&lt;/strong>
要求： Python &amp;gt;=3.10 和 Pytorch &amp;gt;=2.1.1&lt;/p>
&lt;pre>&lt;code class="language-bash">git clone https://github.com/OpenAccess-AI-Collective/axolotl
cd axolotl
pip3 install packaging ninja
pip3 install -e '.[flash-attn,deepspeed]'
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>使用方法：&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash"># preprocess datasets - optional but recommended
CUDA_VISIBLE_DEVICES=&amp;quot;&amp;quot; python -m axolotl.cli.preprocess examples/openllama-3b/lora.yml
# finetune lora
accelerate launch -m axolotl.cli.train examples/openllama-3b/lora.yml
# inference
accelerate launch -m axolotl.cli.inference examples/openllama-3b/lora.yml \
--lora_model_dir=&amp;quot;./outputs/lora-out&amp;quot;
# gradio
accelerate launch -m axolotl.cli.inference examples/openllama-3b/lora.yml \
--lora_model_dir=&amp;quot;./outputs/lora-out&amp;quot; --gradio
# remote yaml files - the yaml config can be hosted on a public URL
# Note: the yaml config must directly link to the **raw** yaml
accelerate launch -m axolotl.cli.train https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/examples/openllama-3b/lora.yml
&lt;/code>&lt;/pre>
&lt;p>其它详细信息，请访问 &lt;a class="link" href="https://github.com/OpenAccess-AI-Collective/axolotl" target="_blank" rel="noopener" >Axolotl
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
项目主页。&lt;/p>
&lt;h3 id="llama-factory">
&lt;a href="#llama-factory" class="header-anchor">#&lt;/a>
Llama-Factory
&lt;/h3>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-07-07_llama-factory-logo.webp"
alt="Llama Factory Logo" width="80%" loading="lazy">
&lt;/figure>
&lt;a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" class="card-github fetch-waiting no-styling"
repo="hiyouga/LLaMA-Factory" id="repo-iQW3W3yc9nhnWqhM-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-iQW3W3yc9nhnWqhM-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">hiyouga&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">LLaMA-Factory&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-iQW3W3yc9nhnWqhM-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-iQW3W3yc9nhnWqhM-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-iQW3W3yc9nhnWqhM-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-iQW3W3yc9nhnWqhM-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-iQW3W3yc9nhnWqhM-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-iQW3W3yc9nhnWqhM-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/hiyouga\/LLaMA-Factory', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-iQW3W3yc9nhnWqhM-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-iQW3W3yc9nhnWqhM-language').innerText = data.language;
document.getElementById('repo-iQW3W3yc9nhnWqhM-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-iQW3W3yc9nhnWqhM-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-iQW3W3yc9nhnWqhM-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-iQW3W3yc9nhnWqhM-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-iQW3W3yc9nhnWqhM-license').classList.add = "no-license"
};
document.getElementById('repo-iQW3W3yc9nhnWqhM-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for hiyouga\/LLaMA-Factory.")
}).catch(err => {
const c = document.getElementById('repo-iQW3W3yc9nhnWqhM-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for hiyouga\/LLaMA-Factory.")
})
&lt;/script>
&lt;p>Llama-Factory 是 Meta 推出的，专注于 Llama 模型微调的框架。它构建于 PyTorch 生态之上，并提供高效的训练和评估工具。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>多种模型&lt;/strong>：LLaMA、LLaVA、Mistral、Mixtral-MoE、Qwen、Yi、Gemma、Baichuan、ChatGLM、Phi 等等。&lt;/li>
&lt;li>&lt;strong>集成方法&lt;/strong>：（增量）预训练、（多模态）指令监督微调、奖励模型训练、PPO 训练、DPO 训练、KTO 训练、ORPO 训练等等。&lt;/li>
&lt;li>&lt;strong>多种精度&lt;/strong>：16 比特全参数微调、冻结微调、LoRA 微调和基于 AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ 的 2/3/4/5/6/8 比特 QLoRA 微调。&lt;/li>
&lt;li>&lt;strong>先进算法&lt;/strong>：GaLore、BAdam、DoRA、LongLoRA、LLaMA Pro、Mixture-of-Depths、LoRA+、LoftQ、PiSSA 和 Agent 微调。&lt;/li>
&lt;li>&lt;strong>实用技巧&lt;/strong>：FlashAttention-2、Unsloth、RoPE scaling、NEFTune 和 rsLoRA。&lt;/li>
&lt;li>&lt;strong>实验监控&lt;/strong>：LlamaBoard、TensorBoard、Wandb、MLflow 等等。&lt;/li>
&lt;li>&lt;strong>极速推理&lt;/strong>：基于 vLLM 的 OpenAI 风格 API、浏览器界面和命令行接口。&lt;/li>
&lt;/ul>
&lt;link href="https://web.sdk.qcloud.com/player/tcplayer/release/v4.2.1/tcplayer.min.css" rel="stylesheet">
&lt;script src="https://web.sdk.qcloud.com/player/tcplayer/release/v4.2.1/libs/hls.min.0.13.2m.js">&lt;/script>
&lt;script src="https://web.sdk.qcloud.com/player/tcplayer/release/v4.2.1/tcplayer.v4.2.1.min.js">&lt;/script>
&lt;style>
.tcplayer {
position: absolute;
width: 100%;
height: 100%;
left: 0;
top: 0;
border: 0;
}
&lt;/style>
&lt;div class="video-wrapper">
&lt;video
id="player-container-id"
preload="auto"
width="100%"
height="100%"
playsinline
webkit-playsinline>
&lt;/video>
&lt;/div>
&lt;script>
var tcplayer = TCPlayer("player-container-id", {
reportable: false,
poster: "",
});
tcplayer.src('https:\/\/cuterwrite-1302252842.file.myqcloud.com\/img\/2024-07-07_309488092-ec36a9dd-37f4-4f72-81bd-d76c6d0a6594_output.mp4');
&lt;/script>
&lt;p>&lt;strong>性能指标&lt;/strong>&lt;/p>
&lt;p>与 ChatGLM 官方的 &lt;a class="link" href="https://github.com/THUDM/ChatGLM2-6B/tree/main/ptuning" target="_blank" rel="noopener" >P-Tuning
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
微调相比，LLaMA Factory 的 LoRA 微调提供了 &lt;strong>3.7 倍&lt;/strong>的加速比，同时在广告文案生成任务上取得了更高的 Rouge 分数。结合 4 比特量化技术，LLaMA Factory 的 QLoRA 微调进一步降低了 GPU 显存消耗。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-07-07_benchmark.webp"
alt="Performance" width="auto" loading="lazy">
&lt;/figure>
&lt;details>
&lt;summary>变量定义&lt;/summary>
&lt;ul>
&lt;li>&lt;strong>Training Speed&lt;/strong>: 训练阶段每秒处理的样本数量。（批处理大小=4，截断长度=1024）&lt;/li>
&lt;li>&lt;strong>Rouge Score&lt;/strong>: &lt;a class="link" href="https://aclanthology.org/D19-1321.pdf" target="_blank" rel="noopener" >广告文案生成
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
任务验证集上的 Rouge-2 分数。（批处理大小=4，截断长度=1024）&lt;/li>
&lt;li>&lt;strong>GPU Memory&lt;/strong>: 4 比特量化训练的 GPU 显存峰值。（批处理大小=1，截断长度=1024）&lt;/li>
&lt;li>我们在 ChatGLM 的 P-Tuning 中采用 &lt;code>pre_seq_len=128&lt;/code>，在 LLaMA Factory 的 LoRA 微调中采用 &lt;code>lora_rank=32&lt;/code>。&lt;/li>
&lt;/ul>
&lt;/details>
&lt;p>&lt;strong>快速入门&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash">git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e &amp;quot;.[torch,metrics]&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>可选的额外依赖项：torch、torch-npu、metrics、deepspeed、bitsandbytes、hqq、eetq、gptq、awq、aqlm、vllm、galore、badam、qwen、modelscope、quality&lt;/p>
&lt;blockquote class="alert-blockquote alert-tip">
&lt;p class="alert-heading">
&lt;svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16">
&lt;path d="M8 1.5c-2.363 0-4 1.69-4 3.75 0 .984.424 1.625.984 2.304l.214.253c.223.264.47.556.673.848.284.411.537.896.621 1.49a.75.75 0 0 1-1.484.211c-.04-.282-.163-.547-.37-.847a8.456 8.456 0 0 0-.542-.68c-.084-.1-.173-.205-.268-.32C3.201 7.75 2.5 6.766 2.5 5.25 2.5 2.31 4.863 0 8 0s5.5 2.31 5.5 5.25c0 1.516-.701 2.5-1.328 3.259-.095.115-.184.22-.268.319-.207.245-.383.453-.541.681-.208.3-.33.565-.37.847a.751.751 0 0 1-1.485-.212c.084-.593.337-1.078.621-1.489.203-.292.45-.584.673-.848.075-.088.147-.173.213-.253.561-.679.985-1.32.985-2.304 0-2.06-1.637-3.75-4-3.75ZM5.75 12h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM6 15.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 0 1.5h-2.5a.75.75 0 0 1-.75-.75Z">&lt;/path>
&lt;/svg>
&lt;span>提示&lt;/span>
&lt;/p>
&lt;p>遇到包冲突时，可使用 pip install &amp;ndash;no-deps -e . 解决。&lt;/p>
&lt;/blockquote>
&lt;details>
&lt;summary>Windows 用户指南&lt;/summary>
&lt;p>如果要在 Windows 平台上开启量化 LoRA（QLoRA），需要安装预编译的 &lt;code>bitsandbytes&lt;/code> 库, 支持 CUDA 11.1 到 12.2, 请根据您的 CUDA 版本情况选择适合的&lt;a class="link" href="https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels" target="_blank" rel="noopener" >发布版本
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl
&lt;/code>&lt;/pre>
&lt;p>如果要在 Windows 平台上开启 FlashAttention-2，需要安装预编译的 &lt;code>flash-attn&lt;/code> 库，支持 CUDA 12.1 到 12.2，请根据需求到 &lt;a class="link" href="https://github.com/bdashore3/flash-attention/releases" target="_blank" rel="noopener" >flash-attention
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
下载对应版本安装。&lt;/p>
&lt;/details>
&lt;details>
&lt;summary>昇腾 NPU 用户指南&lt;/summary>
&lt;p>在昇腾 NPU 设备上安装 LLaMA Factory 时，需要指定额外依赖项，使用 &lt;code>pip install -e &amp;quot;.[torch-npu,metrics]&amp;quot;&lt;/code> 命令安装。此外，还需要安装 &lt;strong>&lt;a class="link" href="https://www.hiascend.com/developer/download/community/result?module=cann" target="_blank" rel="noopener" >Ascend CANN Toolkit 与 Kernels
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/strong>，安装方法请参考&lt;a class="link" href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/80RC2alpha002/quickstart/quickstart/quickstart_18_0004.html" target="_blank" rel="noopener" >安装教程
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
或使用以下命令：&lt;/p>
&lt;pre>&lt;code class="language-bash"># 请替换 URL 为 CANN 版本和设备型号对应的 URL
# 安装 CANN Toolkit
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-toolkit_8.0.RC1.alpha001_linux-&amp;quot;$(uname -i)&amp;quot;.run
bash Ascend-cann-toolkit_8.0.RC1.alpha001_linux-&amp;quot;$(uname -i)&amp;quot;.run --install
# 安装 CANN Kernels
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run
bash Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run --install
# 设置环境变量
source /usr/local/Ascend/ascend-toolkit/set_env.sh
&lt;/code>&lt;/pre>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">依赖项&lt;/th>
&lt;th style="text-align: left">至少&lt;/th>
&lt;th style="text-align: left">推荐&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">CANN&lt;/td>
&lt;td style="text-align: left">8.0.RC1&lt;/td>
&lt;td style="text-align: left">8.0.RC1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">torch&lt;/td>
&lt;td style="text-align: left">2.1.0&lt;/td>
&lt;td style="text-align: left">2.1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">torch-npu&lt;/td>
&lt;td style="text-align: left">2.1.0&lt;/td>
&lt;td style="text-align: left">2.1.0.post3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">deepspeed&lt;/td>
&lt;td style="text-align: left">0.13.2&lt;/td>
&lt;td style="text-align: left">0.13.2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>请使用 &lt;code>ASCEND_RT_VISIBLE_DEVICES&lt;/code> 而非 &lt;code>CUDA_VISIBLE_DEVICES&lt;/code> 来指定运算设备。&lt;/p>
&lt;p>如果遇到无法正常推理的情况，请尝试设置 &lt;code>do_sample: false&lt;/code>。&lt;/p>
&lt;p>下载预构建 Docker 镜像：&lt;a class="link" href="http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html" target="_blank" rel="noopener" >32GB
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
| &lt;a class="link" href="http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html" target="_blank" rel="noopener" >64GB
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;/details>
&lt;p>下面三行命令分别对 Llama3-8B-Instruct 模型进行 LoRA 微调、推理和合并。&lt;/p>
&lt;pre>&lt;code class="language-bash">llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml
&lt;/code>&lt;/pre>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" rel="noopener" >Llama-Factory
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
项目主页。&lt;/p>
&lt;h3 id="firefly">
&lt;a href="#firefly" class="header-anchor">#&lt;/a>
Firefly
&lt;/h3>
&lt;a href="https://github.com/yangjianxin1/Firefly" target="_blank" class="card-github fetch-waiting no-styling"
repo="yangjianxin1/Firefly" id="repo-aW4QMnPyzCdj3YBL-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-aW4QMnPyzCdj3YBL-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">yangjianxin1&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">Firefly&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-aW4QMnPyzCdj3YBL-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-aW4QMnPyzCdj3YBL-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-aW4QMnPyzCdj3YBL-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-aW4QMnPyzCdj3YBL-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-aW4QMnPyzCdj3YBL-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-aW4QMnPyzCdj3YBL-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/yangjianxin1\/Firefly', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-aW4QMnPyzCdj3YBL-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-aW4QMnPyzCdj3YBL-language').innerText = data.language;
document.getElementById('repo-aW4QMnPyzCdj3YBL-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-aW4QMnPyzCdj3YBL-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-aW4QMnPyzCdj3YBL-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-aW4QMnPyzCdj3YBL-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-aW4QMnPyzCdj3YBL-license').classList.add = "no-license"
};
document.getElementById('repo-aW4QMnPyzCdj3YBL-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for yangjianxin1\/Firefly.")
}).catch(err => {
const c = document.getElementById('repo-aW4QMnPyzCdj3YBL-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for yangjianxin1\/Firefly.")
})
&lt;/script>
&lt;p>&lt;strong>Firefly&lt;/strong> 是一个开源的大模型训练项目，支持对主流的大模型进行预训练、指令微调和 DPO，包括但不限于 Qwen2、Yi-1.5、Llama3、Gemma、Qwen1.5、MiniCPM、Llama、InternLM、Baichuan、ChatGLM、Yi、Deepseek、Qwen、Orion、Ziya、Xverse、Mistral、Mixtral-8x7B、Zephyr、Vicuna、Bloom 等。
本项目支持&lt;strong>全量参数训练、LoRA、QLoRA 高效训练&lt;/strong>，支持&lt;strong>预训练、SFT、DPO&lt;/strong>。 如果你的训练资源有限，我们极力推荐使用 QLoRA 进行指令微调，因为我们在 Open LLM Leaderboard 上验证了该方法的有效性，并且取得了非常不错的成绩。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>📗 支持预训练、指令微调、DPO，支持全量参数训练、LoRA、QLoRA 高效训练。通过配置文件的方式训练不同的模型，小白亦可快速上手训练模型。&lt;/li>
&lt;li>📗 支持使用&lt;a class="link" href="https://github.com/yangjianxin1/unsloth" target="_blank" rel="noopener" >Unsloth
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
加速训练，并且节省显存。&lt;/li>
&lt;li>📗 支持绝大部分主流的开源大模型，如 Llama3、Gemma、MiniCPM、Llama、InternLM、Baichuan、ChatGLM、Yi、Deepseek、Qwen、Orion、Ziya、Xverse、Mistral、Mixtral-8x7B、Zephyr、Vicuna、Bloom，训练时与各个官方的 chat 模型的 template 对齐。&lt;/li>
&lt;li>📗 整理并开源指令微调数据集：firefly-train-1.1M 、moss-003-sft-data、ultrachat、 WizardLM_evol_instruct_V2_143k、school_math_0.25M。&lt;/li>
&lt;li>📗 开源&lt;a class="link" href="https://huggingface.co/YeungNLP" target="_blank" rel="noopener" >Firefly 系列指令微调模型权重
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/li>
&lt;li>📗 在 Open LLM Leaderboard 上验证了 QLoRA 训练流程的有效性。&lt;/li>
&lt;/ul>
&lt;p>该项目的 README 中包含了详细的使用说明，包括如何安装、如何训练、如何微调、如何评估等。请访问 &lt;a class="link" href="https://github.com/yangjianxin1/Firefly" target="_blank" rel="noopener" >Firefly
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
项目主页。&lt;/p>
&lt;h3 id="xtuner">
&lt;a href="#xtuner" class="header-anchor">#&lt;/a>
XTuner
&lt;/h3>
&lt;a href="https://github.com/InternLM/xtuner" target="_blank" class="card-github fetch-waiting no-styling"
repo="InternLM/xtuner" id="repo-kHOEEsBkpUt9SLXl-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-kHOEEsBkpUt9SLXl-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">InternLM&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">xtuner&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-kHOEEsBkpUt9SLXl-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-kHOEEsBkpUt9SLXl-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-kHOEEsBkpUt9SLXl-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-kHOEEsBkpUt9SLXl-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-kHOEEsBkpUt9SLXl-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-kHOEEsBkpUt9SLXl-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/InternLM\/xtuner', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-kHOEEsBkpUt9SLXl-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-kHOEEsBkpUt9SLXl-language').innerText = data.language;
document.getElementById('repo-kHOEEsBkpUt9SLXl-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-kHOEEsBkpUt9SLXl-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-kHOEEsBkpUt9SLXl-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-kHOEEsBkpUt9SLXl-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-kHOEEsBkpUt9SLXl-license').classList.add = "no-license"
};
document.getElementById('repo-kHOEEsBkpUt9SLXl-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for InternLM\/xtuner.")
}).catch(err => {
const c = document.getElementById('repo-kHOEEsBkpUt9SLXl-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for InternLM\/xtuner.")
})
&lt;/script>
&lt;p>XTuner 是一个高效、灵活、全能的轻量化大模型微调工具库。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>高效&lt;/strong>
&lt;ul>
&lt;li>支持大语言模型 LLM、多模态图文模型 VLM 的预训练及轻量级微调。XTuner 支持在 8GB 显存下微调 7B 模型，同时也支持多节点跨设备微调更大尺度模型（70B+）。&lt;/li>
&lt;li>自动分发高性能算子（如 FlashAttention、Triton kernels 等）以加速训练吞吐。&lt;/li>
&lt;li>兼容 &lt;a class="link" href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener" >DeepSpeed
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
🚀，轻松应用各种 ZeRO 训练优化策略。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>灵活&lt;/strong>
&lt;ul>
&lt;li>支持多种大语言模型，包括但不限于 &lt;a class="link" href="https://huggingface.co/internlm" target="_blank" rel="noopener" >InternLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="https://huggingface.co/mistralai" target="_blank" rel="noopener" >Mixtral-8x7B
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="https://huggingface.co/meta-llama" target="_blank" rel="noopener" >Llama 2
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="https://huggingface.co/THUDM" target="_blank" rel="noopener" >ChatGLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="https://huggingface.co/Qwen" target="_blank" rel="noopener" >Qwen
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="https://huggingface.co/baichuan-inc" target="_blank" rel="noopener" >Baichuan
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/li>
&lt;li>支持多模态图文模型 LLaVA 的预训练与微调。利用 XTuner 训得模型 &lt;a class="link" href="https://huggingface.co/xtuner/llava-internlm2-20b" target="_blank" rel="noopener" >LLaVA-InternLM2-20B
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
表现优异。&lt;/li>
&lt;li>精心设计的数据管道，兼容任意数据格式，开源数据或自定义数据皆可快速上手。&lt;/li>
&lt;li>支持 &lt;a class="link" href="http://arxiv.org/abs/2305.14314" target="_blank" rel="noopener" >QLoRA
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="http://arxiv.org/abs/2106.09685" target="_blank" rel="noopener" >LoRA
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、全量参数微调等多种微调算法，支撑用户根据具体需求作出最优选择。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>全能&lt;/strong>
&lt;ul>
&lt;li>支持增量预训练、指令微调与 Agent 微调。&lt;/li>
&lt;li>预定义众多开源对话模版，支持与开源或训练所得模型进行对话。&lt;/li>
&lt;li>训练所得模型可无缝接入部署工具库 &lt;a class="link" href="https://github.com/InternLM/lmdeploy" target="_blank" rel="noopener" >LMDeploy
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、大规模评测工具库 &lt;a class="link" href="https://github.com/open-compass/opencompass" target="_blank" rel="noopener" >OpenCompass
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
及 &lt;a class="link" href="https://github.com/open-compass/VLMEvalKit" target="_blank" rel="noopener" >VLMEvalKit
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>快速上手：&lt;/strong>
&lt;details>
&lt;summary>安装&lt;/summary>
&lt;ul>
&lt;li>
&lt;p>推荐使用 conda 先构建一个 Python-3.10 的虚拟环境&lt;/p>
&lt;pre>&lt;code class="language-bash">conda create --name xtuner-env python=3.10 -y
conda activate xtuner-env
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>通过 pip 安装 XTuner：&lt;/p>
&lt;pre>&lt;code class="language-shell">pip install -U xtuner
&lt;/code>&lt;/pre>
&lt;p>亦可集成 DeepSpeed 安装：&lt;/p>
&lt;pre>&lt;code class="language-shell">pip install -U 'xtuner[deepspeed]'
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>从源码安装 XTuner：&lt;/p>
&lt;pre>&lt;code class="language-shell">git clone https://github.com/InternLM/xtuner.git
cd xtuner
pip install -e '.[all]'
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul>
&lt;/details>
&lt;/p>
&lt;details>
&lt;summary>微调&lt;/summary>
&lt;p>XTuner 支持微调大语言模型。数据集预处理指南请查阅&lt;a class="link" href="./docs/zh_cn/user_guides/dataset_prepare.md" >文档
&lt;/a>
。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>步骤 0&lt;/strong>，准备配置文件。XTuner 提供多个开箱即用的配置文件，用户可以通过下列命令查看：&lt;/p>
&lt;pre>&lt;code class="language-shell">xtuner list-cfg
&lt;/code>&lt;/pre>
&lt;p>或者，如果所提供的配置文件不能满足使用需求，请导出所提供的配置文件并进行相应更改：&lt;/p>
&lt;pre>&lt;code class="language-shell">xtuner copy-cfg ${CONFIG_NAME} ${SAVE_PATH}
vi ${SAVE_PATH}/${CONFIG_NAME}_copy.py
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>步骤 1&lt;/strong>，开始微调。&lt;/p>
&lt;pre>&lt;code class="language-shell">xtuner train ${CONFIG_NAME_OR_PATH}
&lt;/code>&lt;/pre>
&lt;p>例如，我们可以利用 QLoRA 算法在 oasst1 数据集上微调 InternLM2.5-Chat-7B：&lt;/p>
&lt;pre>&lt;code class="language-shell"># 单卡
xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2
# 多卡
(DIST) NPROC_PER_NODE=${GPU_NUM} xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2
(SLURM) srun ${SRUN_ARGS} xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --launcher slurm --deepspeed deepspeed_zero2
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>
&lt;p>&lt;code>--deepspeed&lt;/code> 表示使用 &lt;a class="link" href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener" >DeepSpeed
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
🚀 来优化训练过程。XTuner 内置了多种策略，包括 ZeRO-1、ZeRO-2、ZeRO-3 等。如果用户期望关闭此功能，请直接移除此参数。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>更多示例，请查阅&lt;a class="link" href="./docs/zh_cn/user_guides/finetune.md" >文档
&lt;/a>
。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>步骤 2&lt;/strong>，将保存的 PTH 模型（如果使用的 DeepSpeed，则将会是一个文件夹）转换为 HuggingFace 模型：&lt;/p>
&lt;pre>&lt;code class="language-shell">xtuner convert pth_to_hf ${CONFIG_NAME_OR_PATH} ${PTH} ${SAVE_PATH}
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul>
&lt;/details>
&lt;p>其它详细信息，请访问 &lt;a class="link" href="https://github.com/InternLM/xtuner" target="_blank" rel="noopener" >XTuner
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h2 id="模型量化">
&lt;a href="#%e6%a8%a1%e5%9e%8b%e9%87%8f%e5%8c%96" class="header-anchor">#&lt;/a>
模型量化
&lt;/h2>
&lt;p>LLM 通常体积庞大，对计算资源要求高。模型量化技术可以压缩模型大小，提高运行效率，使其更易于部署：&lt;/p>
&lt;h3 id="autogptq">
&lt;a href="#autogptq" class="header-anchor">#&lt;/a>
AutoGPTQ
&lt;/h3>
&lt;a href="https://github.com/PanQiWei/AutoGPTQ" target="_blank" class="card-github fetch-waiting no-styling"
repo="PanQiWei/AutoGPTQ" id="repo-tcRzvy8aNXcLxknJ-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-tcRzvy8aNXcLxknJ-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">PanQiWei&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">AutoGPTQ&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-tcRzvy8aNXcLxknJ-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-tcRzvy8aNXcLxknJ-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-tcRzvy8aNXcLxknJ-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-tcRzvy8aNXcLxknJ-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-tcRzvy8aNXcLxknJ-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-tcRzvy8aNXcLxknJ-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/PanQiWei\/AutoGPTQ', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-tcRzvy8aNXcLxknJ-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-tcRzvy8aNXcLxknJ-language').innerText = data.language;
document.getElementById('repo-tcRzvy8aNXcLxknJ-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-tcRzvy8aNXcLxknJ-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-tcRzvy8aNXcLxknJ-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-tcRzvy8aNXcLxknJ-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-tcRzvy8aNXcLxknJ-license').classList.add = "no-license"
};
document.getElementById('repo-tcRzvy8aNXcLxknJ-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for PanQiWei\/AutoGPTQ.")
}).catch(err => {
const c = document.getElementById('repo-tcRzvy8aNXcLxknJ-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for PanQiWei\/AutoGPTQ.")
})
&lt;/script>
&lt;p>AutoGPTQ 一个基于 GPTQ 算法，简单易用且拥有用户友好型接口的大语言模型量化工具包。&lt;/p>
&lt;p>&lt;strong>快速安装&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>对于 CUDA 11.7：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>对于 CUDA 11.8：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>对于 RoCm 5.4.2：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/rocm542/
&lt;/code>&lt;/pre>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/AutoGPTQ/AutoGPTQ/" target="_blank" rel="noopener" >AutoGPTQ
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="autoawq">
&lt;a href="#autoawq" class="header-anchor">#&lt;/a>
AutoAWQ
&lt;/h3>
&lt;a href="https://github.com/casper-hansen/AutoAWQ" target="_blank" class="card-github fetch-waiting no-styling"
repo="casper-hansen/AutoAWQ" id="repo-CKYYotkfqJSSazY9-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-CKYYotkfqJSSazY9-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">casper-hansen&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">AutoAWQ&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-CKYYotkfqJSSazY9-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-CKYYotkfqJSSazY9-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-CKYYotkfqJSSazY9-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-CKYYotkfqJSSazY9-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-CKYYotkfqJSSazY9-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-CKYYotkfqJSSazY9-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/casper-hansen\/AutoAWQ', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-CKYYotkfqJSSazY9-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-CKYYotkfqJSSazY9-language').innerText = data.language;
document.getElementById('repo-CKYYotkfqJSSazY9-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-CKYYotkfqJSSazY9-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-CKYYotkfqJSSazY9-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-CKYYotkfqJSSazY9-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-CKYYotkfqJSSazY9-license').classList.add = "no-license"
};
document.getElementById('repo-CKYYotkfqJSSazY9-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for casper-hansen\/AutoAWQ.")
}).catch(err => {
const c = document.getElementById('repo-CKYYotkfqJSSazY9-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for casper-hansen\/AutoAWQ.")
})
&lt;/script>
&lt;p>AutoAWQ 是另一款自动化模型量化工具，支持多种量化精度，并提供灵活的配置选项，可以根据不同的硬件平台和性能需求进行调整。&lt;/p>
&lt;p>AutoAWQ 是一个易于使用的 4 位量化模型软件包。与 FP16 相比，AutoAWQ 可将模型速度提高 3 倍，内存需求减少 3 倍。AutoAWQ 实现了用于量化 LLMs 的激活感知权重量化（AWQ）算法。AutoAWQ 是在 MIT 的原始工作 &lt;a class="link" href="https://github.com/mit-han-lab/llm-awq" target="_blank" rel="noopener" >AWQ
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
基础上创建和改进的。&lt;/p>
&lt;p>&lt;strong>安装方法：&lt;/strong>&lt;/p>
&lt;p>安装前，确保安装了 CUDA &amp;gt;= 12.1（注意：以下只是最快捷的安装方法）&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install autoawq
&lt;/code>&lt;/pre>
&lt;p>更多详细信息和示例，请访问 &lt;a class="link" href="https://github.com/casper-hansen/AutoAWQ" target="_blank" rel="noopener" >AutoAWQ
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="neural-compressor">
&lt;a href="#neural-compressor" class="header-anchor">#&lt;/a>
Neural Compressor
&lt;/h3>
&lt;a href="https://github.com/intel/neural-compressor" target="_blank" class="card-github fetch-waiting no-styling"
repo="intel/neural-compressor" id="repo-jRYAZbAZx2TuFIRg-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-jRYAZbAZx2TuFIRg-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">intel&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">neural-compressor&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-jRYAZbAZx2TuFIRg-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-jRYAZbAZx2TuFIRg-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-jRYAZbAZx2TuFIRg-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-jRYAZbAZx2TuFIRg-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-jRYAZbAZx2TuFIRg-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-jRYAZbAZx2TuFIRg-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/intel\/neural-compressor', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-jRYAZbAZx2TuFIRg-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-jRYAZbAZx2TuFIRg-language').innerText = data.language;
document.getElementById('repo-jRYAZbAZx2TuFIRg-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-jRYAZbAZx2TuFIRg-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-jRYAZbAZx2TuFIRg-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-jRYAZbAZx2TuFIRg-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-jRYAZbAZx2TuFIRg-license').classList.add = "no-license"
};
document.getElementById('repo-jRYAZbAZx2TuFIRg-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for intel\/neural-compressor.")
}).catch(err => {
const c = document.getElementById('repo-jRYAZbAZx2TuFIRg-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for intel\/neural-compressor.")
})
&lt;/script>
&lt;p>Neural Compressor 是英特尔开发的模型压缩工具包，支持所有主流深度学习框架（TensorFlow、PyTorch、ONNX Runtime 和 MXNet）上流行的模型压缩技术。&lt;/p>
&lt;p>&lt;strong>安装方法：&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install &amp;quot;neural-compressor&amp;gt;=2.3&amp;quot; &amp;quot;transformers&amp;gt;=4.34.0&amp;quot; torch torchvision
&lt;/code>&lt;/pre>
&lt;p>更多详细信息和示例，请访问 &lt;a class="link" href="https://github.com/intel/neural-compressor" target="_blank" rel="noopener" >Neural Compressor
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h2 id="模型部署">
&lt;a href="#%e6%a8%a1%e5%9e%8b%e9%83%a8%e7%bd%b2" class="header-anchor">#&lt;/a>
模型部署
&lt;/h2>
&lt;p>将训练好的 LLM 部署到生产环境至关重要。以下是一些常用的 LLM 部署工具：&lt;/p>
&lt;h3 id="vllm">
&lt;a href="#vllm" class="header-anchor">#&lt;/a>
vLLM
&lt;/h3>
&lt;a href="https://github.com/vllm-project/vllm" target="_blank" class="card-github fetch-waiting no-styling"
repo="vllm-project/vllm" id="repo-92lcKYzpSMXoKFZg-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-92lcKYzpSMXoKFZg-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">vllm-project&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">vllm&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-92lcKYzpSMXoKFZg-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-92lcKYzpSMXoKFZg-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-92lcKYzpSMXoKFZg-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-92lcKYzpSMXoKFZg-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-92lcKYzpSMXoKFZg-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-92lcKYzpSMXoKFZg-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/vllm-project\/vllm', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-92lcKYzpSMXoKFZg-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-92lcKYzpSMXoKFZg-language').innerText = data.language;
document.getElementById('repo-92lcKYzpSMXoKFZg-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-92lcKYzpSMXoKFZg-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-92lcKYzpSMXoKFZg-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-92lcKYzpSMXoKFZg-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-92lcKYzpSMXoKFZg-license').classList.add = "no-license"
};
document.getElementById('repo-92lcKYzpSMXoKFZg-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for vllm-project\/vllm.")
}).catch(err => {
const c = document.getElementById('repo-92lcKYzpSMXoKFZg-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for vllm-project\/vllm.")
})
&lt;/script>
&lt;p>vLLM 是一个快速、易用的 LLM 推理服务库。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>快速&lt;/strong>
&lt;ul>
&lt;li>SOTA 服务吞吐量&lt;/li>
&lt;li>利用 PagedAttention 高效管理注意力键值内存&lt;/li>
&lt;li>持续批量处理收到的请求&lt;/li>
&lt;li>利用 CUDA/HIP 图进行加速&lt;/li>
&lt;li>量化：支持 GPTQ、AWQ、SqueezeLLM、FP8 KV 高速缓存&lt;/li>
&lt;li>优化的 CUDA 内核&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>灵活&lt;/strong>
&lt;ul>
&lt;li>与流行的 Hugging Face 模型无缝集成&lt;/li>
&lt;li>利用各种解码算法（包括并行采样、波束搜索等）提供高吞吐量服务&lt;/li>
&lt;li>为分布式推理提供张量并行支持&lt;/li>
&lt;li>流输出&lt;/li>
&lt;li>兼容 OpenAI 的应用程序接口服务器&lt;/li>
&lt;li>支持 NVIDIA GPU、AMD GPU、Intel CPU 和 GPU
-（实验性）支持前缀缓存
-（试验性）支持多种语言&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>无缝支持&lt;/strong>
&lt;ul>
&lt;li>基于 Transformer 的模型，例如 Llama&lt;/li>
&lt;li>基于 MoE 的模型，例如 Mixtral&lt;/li>
&lt;li>多模态模型，例如 LLaVA&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>快速安装：&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install vllm
&lt;/code>&lt;/pre>
&lt;p>更多详细信息，请查看 &lt;a class="link" href="https://vllm.readthedocs.io/en/latest/" target="_blank" rel="noopener" >vLLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
官方文档。&lt;/p>
&lt;h3 id="sgl">
&lt;a href="#sgl" class="header-anchor">#&lt;/a>
SGL
&lt;/h3>
&lt;a href="https://github.com/sgl-project/sglang" target="_blank" class="card-github fetch-waiting no-styling"
repo="sgl-project/sglang" id="repo-47mybtYOfS8CdH1v-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-47mybtYOfS8CdH1v-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">sgl-project&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">sglang&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-47mybtYOfS8CdH1v-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-47mybtYOfS8CdH1v-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-47mybtYOfS8CdH1v-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-47mybtYOfS8CdH1v-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-47mybtYOfS8CdH1v-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-47mybtYOfS8CdH1v-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/sgl-project\/sglang', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-47mybtYOfS8CdH1v-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-47mybtYOfS8CdH1v-language').innerText = data.language;
document.getElementById('repo-47mybtYOfS8CdH1v-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-47mybtYOfS8CdH1v-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-47mybtYOfS8CdH1v-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-47mybtYOfS8CdH1v-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-47mybtYOfS8CdH1v-license').classList.add = "no-license"
};
document.getElementById('repo-47mybtYOfS8CdH1v-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for sgl-project\/sglang.")
}).catch(err => {
const c = document.getElementById('repo-47mybtYOfS8CdH1v-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for sgl-project\/sglang.")
})
&lt;/script>
&lt;p>SGLang 是一种结构化生成语言，专为大型语言模型（LLMs）而设计。它通过共同设计前端语言和运行系统，使你与 LLMs 的交互更快、更可控。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>灵活的前端语言&lt;/strong>：通过链式生成调用、高级提示、控制流、多种模式、并行性和外部交互，可轻松编写 LLM 应用程序。&lt;/li>
&lt;li>&lt;strong>高性能后端运行时&lt;/strong>：具有 RadixAttention 功能，可通过在多次调用中重复使用 KV 缓存来加速复杂的 LLM 程序。它还可以作为独立的推理引擎，实现所有常用技术（如连续批处理和张量并行）。&lt;/li>
&lt;/ul>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/sgl-project/" target="_blank" rel="noopener" >SGL
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="skypilot">
&lt;a href="#skypilot" class="header-anchor">#&lt;/a>
SkyPilot
&lt;/h3>
&lt;a href="https://github.com/skypilot-org/skypilot" target="_blank" class="card-github fetch-waiting no-styling"
repo="skypilot-org/skypilot" id="repo-Xydx7JBQdpgIkQKN-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-Xydx7JBQdpgIkQKN-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">skypilot-org&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">skypilot&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-Xydx7JBQdpgIkQKN-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-Xydx7JBQdpgIkQKN-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-Xydx7JBQdpgIkQKN-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-Xydx7JBQdpgIkQKN-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-Xydx7JBQdpgIkQKN-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-Xydx7JBQdpgIkQKN-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/skypilot-org\/skypilot', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-Xydx7JBQdpgIkQKN-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-Xydx7JBQdpgIkQKN-language').innerText = data.language;
document.getElementById('repo-Xydx7JBQdpgIkQKN-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-Xydx7JBQdpgIkQKN-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-Xydx7JBQdpgIkQKN-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-Xydx7JBQdpgIkQKN-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-Xydx7JBQdpgIkQKN-license').classList.add = "no-license"
};
document.getElementById('repo-Xydx7JBQdpgIkQKN-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for skypilot-org\/skypilot.")
}).catch(err => {
const c = document.getElementById('repo-Xydx7JBQdpgIkQKN-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for skypilot-org\/skypilot.")
})
&lt;/script>
&lt;p>SkyPilot 是 UC Berkeley RISELab 推出的灵活的云端 LLM 部署工具，支持多种云平台和硬件加速器，可以自动选择最优的部署方案，并提供成本优化功能。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>多云支持:&lt;/strong> 支持 AWS, GCP, Azure 等多种云平台，方便用户选择合适的部署环境。&lt;/li>
&lt;li>&lt;strong>轻松扩展&lt;/strong>：排队和运行多个作业，自动管理&lt;/li>
&lt;li>&lt;strong>轻松接入对象存储&lt;/strong>：轻松访问对象存储（S3、GCS、R2）&lt;/li>
&lt;/ul>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/skypilot-org/skypilot" target="_blank" rel="noopener" >SkyPilot
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="tensorrt-llm">
&lt;a href="#tensorrt-llm" class="header-anchor">#&lt;/a>
TensorRT-LLM
&lt;/h3>
&lt;a href="https://github.com/NVIDIA/TensorRT-LLM" target="_blank" class="card-github fetch-waiting no-styling"
repo="NVIDIA/TensorRT-LLM" id="repo-4RtAJXcgX8BpFPOW-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-4RtAJXcgX8BpFPOW-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">NVIDIA&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">TensorRT-LLM&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-4RtAJXcgX8BpFPOW-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-4RtAJXcgX8BpFPOW-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-4RtAJXcgX8BpFPOW-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-4RtAJXcgX8BpFPOW-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-4RtAJXcgX8BpFPOW-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-4RtAJXcgX8BpFPOW-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/NVIDIA\/TensorRT-LLM', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-4RtAJXcgX8BpFPOW-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-4RtAJXcgX8BpFPOW-language').innerText = data.language;
document.getElementById('repo-4RtAJXcgX8BpFPOW-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-4RtAJXcgX8BpFPOW-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-4RtAJXcgX8BpFPOW-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-4RtAJXcgX8BpFPOW-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-4RtAJXcgX8BpFPOW-license').classList.add = "no-license"
};
document.getElementById('repo-4RtAJXcgX8BpFPOW-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for NVIDIA\/TensorRT-LLM.")
}).catch(err => {
const c = document.getElementById('repo-4RtAJXcgX8BpFPOW-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for NVIDIA\/TensorRT-LLM.")
})
&lt;/script>
&lt;p>TensorRT-LLM 是 NVIDIA 推出的高性能 LLM 推理引擎，能够充分利用 GPU 加速计算，并针对 Transformer 模型结构进行了优化，大幅提升推理速度。&lt;/p>
&lt;p>TensorRT-LLM 为用户提供了易于使用的 Python API，用于定义大型语言模型 (LLMs) 和构建 TensorRT 引擎，这些引擎包含最先进的优化技术，可在英伟达™（NVIDIA®）图形处理器上高效执行推理。TensorRT-LLM 还包含用于创建执行这些 TensorRT 引擎的 Python 和 C++ 运行时的组件。&lt;/p>
&lt;p>详细信息，请访问 &lt;a class="link" href="https://github.com/NVIDIA/TensorRT-LLM" target="_blank" rel="noopener" >TensorRT-LLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="openvino">
&lt;a href="#openvino" class="header-anchor">#&lt;/a>
OpenVino
&lt;/h3>
&lt;a href="https://github.com/openvinotoolkit/openvino" target="_blank" class="card-github fetch-waiting no-styling"
repo="openvinotoolkit/openvino" id="repo-EXitZ0hsYgpH9tjG-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-EXitZ0hsYgpH9tjG-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">openvinotoolkit&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">openvino&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-EXitZ0hsYgpH9tjG-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-EXitZ0hsYgpH9tjG-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-EXitZ0hsYgpH9tjG-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-EXitZ0hsYgpH9tjG-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-EXitZ0hsYgpH9tjG-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-EXitZ0hsYgpH9tjG-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/openvinotoolkit\/openvino', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-EXitZ0hsYgpH9tjG-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-EXitZ0hsYgpH9tjG-language').innerText = data.language;
document.getElementById('repo-EXitZ0hsYgpH9tjG-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-EXitZ0hsYgpH9tjG-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-EXitZ0hsYgpH9tjG-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-EXitZ0hsYgpH9tjG-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-EXitZ0hsYgpH9tjG-license').classList.add = "no-license"
};
document.getElementById('repo-EXitZ0hsYgpH9tjG-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for openvinotoolkit\/openvino.")
}).catch(err => {
const c = document.getElementById('repo-EXitZ0hsYgpH9tjG-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for openvinotoolkit\/openvino.")
})
&lt;/script>
&lt;p>OpenVINO™ 是用于优化和部署人工智能推理的开源工具包。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>推理优化&lt;/strong>：提升深度学习在计算机视觉、自动语音识别、生成式人工智能、使用大型和小型语言模型的自然语言处理以及许多其他常见任务中的性能。&lt;/li>
&lt;li>&lt;strong>灵活的模型支持&lt;/strong>：使用 TensorFlow、PyTorch、ONNX、Keras 和 PaddlePaddle 等流行框架训练的模型。无需原始框架即可转换和部署模型。&lt;/li>
&lt;li>&lt;strong>广泛的平台兼容性&lt;/strong>：减少资源需求，在从边缘到云的一系列平台上高效部署。OpenVINO™ 支持在 CPU（x86、ARM）、GPU（支持 OpenCL 的集成和独立 GPU）和 AI 加速器（英特尔 NPU）上进行推理。&lt;/li>
&lt;li>&lt;strong>社区和生态系统&lt;/strong>：加入一个活跃的社区，为提高各个领域的深度学习性能做出贡献。&lt;/li>
&lt;/ul>
&lt;p>详细信息，请访问 &lt;a class="link" href="https://github.com/openvinotoolkit/openvino" target="_blank" rel="noopener" >OpenVino
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="tgi">
&lt;a href="#tgi" class="header-anchor">#&lt;/a>
TGI
&lt;/h3>
&lt;a href="https://github.com/huggingface/text-generation-inference" target="_blank" class="card-github fetch-waiting no-styling"
repo="huggingface/text-generation-inference" id="repo-Y55Ypg2PBeCn70Bf-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-Y55Ypg2PBeCn70Bf-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">huggingface&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">text-generation-inference&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-Y55Ypg2PBeCn70Bf-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-Y55Ypg2PBeCn70Bf-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-Y55Ypg2PBeCn70Bf-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-Y55Ypg2PBeCn70Bf-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-Y55Ypg2PBeCn70Bf-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-Y55Ypg2PBeCn70Bf-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/huggingface\/text-generation-inference', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-Y55Ypg2PBeCn70Bf-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-Y55Ypg2PBeCn70Bf-language').innerText = data.language;
document.getElementById('repo-Y55Ypg2PBeCn70Bf-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-Y55Ypg2PBeCn70Bf-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-Y55Ypg2PBeCn70Bf-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-Y55Ypg2PBeCn70Bf-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-Y55Ypg2PBeCn70Bf-license').classList.add = "no-license"
};
document.getElementById('repo-Y55Ypg2PBeCn70Bf-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for huggingface\/text-generation-inference.")
}).catch(err => {
const c = document.getElementById('repo-Y55Ypg2PBeCn70Bf-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for huggingface\/text-generation-inference.")
})
&lt;/script>
&lt;p>文本生成推理（TGI）是一个用于部署和服务大型语言模型（LLMs）的工具包。TGI 可为最流行的开源 LLMs 实现高性能文本生成，包括 Llama、Falcon、StarCoder、BLOOM、GPT-NeoX 等。&lt;/p>
&lt;p>TGI 实现了许多功能，可以在 &lt;a class="link" href="https://github.com/huggingface/text-generation-inference" target="_blank" rel="noopener" >TGI
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页上找到详细信息。&lt;/p>
&lt;h2 id="本地运行">
&lt;a href="#%e6%9c%ac%e5%9c%b0%e8%bf%90%e8%a1%8c" class="header-anchor">#&lt;/a>
本地运行
&lt;/h2>
&lt;p>得益于模型压缩和优化技术，我们也可以在个人设备上运行 LLM：&lt;/p>
&lt;h3 id="mlx">
&lt;a href="#mlx" class="header-anchor">#&lt;/a>
MLX
&lt;/h3>
&lt;a href="https://github.com/ml-explore/mlx" target="_blank" class="card-github fetch-waiting no-styling"
repo="ml-explore/mlx" id="repo-ST8mDa4UaJutrXeu-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-ST8mDa4UaJutrXeu-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">ml-explore&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">mlx&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-ST8mDa4UaJutrXeu-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-ST8mDa4UaJutrXeu-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-ST8mDa4UaJutrXeu-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-ST8mDa4UaJutrXeu-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-ST8mDa4UaJutrXeu-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-ST8mDa4UaJutrXeu-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/ml-explore\/mlx', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-ST8mDa4UaJutrXeu-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-ST8mDa4UaJutrXeu-language').innerText = data.language;
document.getElementById('repo-ST8mDa4UaJutrXeu-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-ST8mDa4UaJutrXeu-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-ST8mDa4UaJutrXeu-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-ST8mDa4UaJutrXeu-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-ST8mDa4UaJutrXeu-license').classList.add = "no-license"
};
document.getElementById('repo-ST8mDa4UaJutrXeu-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for ml-explore\/mlx.")
}).catch(err => {
const c = document.getElementById('repo-ST8mDa4UaJutrXeu-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for ml-explore\/mlx.")
})
&lt;/script>
&lt;p>MLX 是一个专门支持在 Apple 设备上运行 LLM 的框架，充分利用 Metal 加速计算，并提供简单易用的 API，方便开发者将 LLM 集成到 iOS 应用中.&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>相似的应用程序接口&lt;/strong>：MLX 的 Python API 与 NumPy 非常相似。MLX 还拥有功能齐全的 C++、C 和 Swift API，这些 API 与 Python API 非常相似。MLX 拥有更高级别的软件包，如 &lt;code>mlx.nn&lt;/code> 和 &lt;code>mlx.optimizers&lt;/code> ，其 API 与 PyTorch 非常接近，可简化更复杂模型的构建。&lt;/li>
&lt;li>&lt;strong>可组合函数变换&lt;/strong>：MLX 支持用于自动微分、自动矢量化和计算图优化的可组合函数变换。&lt;/li>
&lt;li>&lt;strong>懒计算&lt;/strong>：MLX 中的计算只有在需要时才将数组实体化。&lt;/li>
&lt;li>&lt;strong>动态图构建&lt;/strong>：MLX 中的计算图形是动态构建的。改变函数参数的形状不会导致编译速度变慢，而且调试简单直观。&lt;/li>
&lt;li>&lt;strong>多设备&lt;/strong>：操作可在任何支持的设备（目前是 CPU 和 GPU）上运行。&lt;/li>
&lt;li>&lt;strong>统一内存&lt;/strong>：统一内存模型是 MLX 与其他框架的一个显著区别。MLX 中的阵列位于共享内存中。对 MLX 数组的操作可在任何支持的设备类型上执行，而无需传输数据。&lt;/li>
&lt;/ul>
&lt;p>MLX 是机器学习研究人员为机器学习研究人员设计的。该框架旨在方便用户使用，但仍能高效地训练和部署模型。框架本身的设计概念也很简单。我们的目标是让研究人员能够轻松扩展和改进 MLX，从而快速探索新思路。更多详细信息，请访问 &lt;a class="link" href="https://github.com/ml-explore/mlx" target="_blank" rel="noopener" >MLX
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="llamacpp">
&lt;a href="#llamacpp" class="header-anchor">#&lt;/a>
Llama.cpp
&lt;/h3>
&lt;p>Llama.cpp 是使用 C++ 实现的 Llama 模型推理引擎，可以在 CPU 上高效运行，并支持多种操作系统和硬件平台，方便开发者在资源受限的设备上运行 LLM。&lt;/p>
&lt;a href="https://github.com/ggerganov/llama.cpp" target="_blank" class="card-github fetch-waiting no-styling"
repo="ggerganov/llama.cpp" id="repo-zxGiHlkW50TJvhef-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-zxGiHlkW50TJvhef-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">ggerganov&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">llama.cpp&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-zxGiHlkW50TJvhef-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-zxGiHlkW50TJvhef-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-zxGiHlkW50TJvhef-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-zxGiHlkW50TJvhef-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-zxGiHlkW50TJvhef-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-zxGiHlkW50TJvhef-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/ggerganov\/llama.cpp', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-zxGiHlkW50TJvhef-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-zxGiHlkW50TJvhef-language').innerText = data.language;
document.getElementById('repo-zxGiHlkW50TJvhef-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-zxGiHlkW50TJvhef-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-zxGiHlkW50TJvhef-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-zxGiHlkW50TJvhef-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-zxGiHlkW50TJvhef-license').classList.add = "no-license"
};
document.getElementById('repo-zxGiHlkW50TJvhef-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for ggerganov\/llama.cpp.")
}).catch(err => {
const c = document.getElementById('repo-zxGiHlkW50TJvhef-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for ggerganov\/llama.cpp.")
})
&lt;/script>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>CPU 推理:&lt;/strong> 针对 CPU 平台进行优化，可以在没有 GPU 的设备上运行 LLM。&lt;/li>
&lt;li>&lt;strong>跨平台支持:&lt;/strong> 支持 Linux, macOS, Windows 等多种操作系统，方便用户在不同平台上使用。&lt;/li>
&lt;li>&lt;strong>轻量级部署:&lt;/strong> 编译后的二进制文件体积小，方便用户部署和使用.&lt;/li>
&lt;/ul>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener" >Llama.cpp
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="ollama">
&lt;a href="#ollama" class="header-anchor">#&lt;/a>
Ollama
&lt;/h3>
&lt;a href="https://github.com/ollama/ollama" target="_blank" class="card-github fetch-waiting no-styling"
repo="ollama/ollama" id="repo-cclOlJB8uWVzQHNC-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-cclOlJB8uWVzQHNC-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">ollama&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">ollama&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-cclOlJB8uWVzQHNC-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-cclOlJB8uWVzQHNC-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-cclOlJB8uWVzQHNC-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-cclOlJB8uWVzQHNC-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-cclOlJB8uWVzQHNC-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-cclOlJB8uWVzQHNC-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/ollama\/ollama', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-cclOlJB8uWVzQHNC-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-cclOlJB8uWVzQHNC-language').innerText = data.language;
document.getElementById('repo-cclOlJB8uWVzQHNC-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-cclOlJB8uWVzQHNC-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-cclOlJB8uWVzQHNC-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-cclOlJB8uWVzQHNC-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-cclOlJB8uWVzQHNC-license').classList.add = "no-license"
};
document.getElementById('repo-cclOlJB8uWVzQHNC-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for ollama\/ollama.")
}).catch(err => {
const c = document.getElementById('repo-cclOlJB8uWVzQHNC-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for ollama\/ollama.")
})
&lt;/script>
&lt;p>在 &lt;a class="link" href="https://cuterwrite.top/p/ollama/" >【Ollama：从入门到进阶】
&lt;/a>
一文中介绍过，Ollama 是一个用于构建大型语言模型应用的工具，它提供了一个简洁易用的命令行界面和服务器，让你能够轻松下载、运行和管理各种开源 LLM。与需要复杂配置和强大硬件的传统 LLM 不同，Ollama 让你能够方便地像使用手机 App 一样体验 LLM 的强大功能。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>简单易用&lt;/strong>：Ollama 提供了一个简洁易用的命令行工具，方便用户下载、运行和管理 LLM。&lt;/li>
&lt;li>&lt;strong>多种模型&lt;/strong>：Ollama 支持多种开源 LLM，包括 Qwen2、Llama3、Mistral 等。&lt;/li>
&lt;li>&lt;strong>兼容 OpenAI 接口&lt;/strong>：Ollama 支持 OpenAI API 接口，便于切换原有应用到 Ollama 上。&lt;/li>
&lt;/ul>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/ollama/ollama" target="_blank" rel="noopener" >Ollama
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h2 id="agent-及-rag-框架">
&lt;a href="#agent-%e5%8f%8a-rag-%e6%a1%86%e6%9e%b6" class="header-anchor">#&lt;/a>
Agent 及 RAG 框架
&lt;/h2>
&lt;p>将 LLM 与外部数据和工具结合，可以构建更强大的应用。以下是一些常用的 Agent 及 RAG 框架：&lt;/p>
&lt;h3 id="llamaindex">
&lt;a href="#llamaindex" class="header-anchor">#&lt;/a>
LlamaIndex
&lt;/h3>
&lt;a href="https://github.com/run-llama/llama_index" target="_blank" class="card-github fetch-waiting no-styling"
repo="run-llama/llama_index" id="repo-cN4wm7iVyzfsXX81-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-cN4wm7iVyzfsXX81-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">run-llama&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">llama_index&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-cN4wm7iVyzfsXX81-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-cN4wm7iVyzfsXX81-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-cN4wm7iVyzfsXX81-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-cN4wm7iVyzfsXX81-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-cN4wm7iVyzfsXX81-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-cN4wm7iVyzfsXX81-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/run-llama\/llama_index', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-cN4wm7iVyzfsXX81-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-cN4wm7iVyzfsXX81-language').innerText = data.language;
document.getElementById('repo-cN4wm7iVyzfsXX81-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-cN4wm7iVyzfsXX81-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-cN4wm7iVyzfsXX81-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-cN4wm7iVyzfsXX81-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-cN4wm7iVyzfsXX81-license').classList.add = "no-license"
};
document.getElementById('repo-cN4wm7iVyzfsXX81-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for run-llama\/llama_index.")
}).catch(err => {
const c = document.getElementById('repo-cN4wm7iVyzfsXX81-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for run-llama\/llama_index.")
})
&lt;/script>
&lt;p>LlamaIndex（GPT 索引）是用于 LLM 应用程序的数据框架。使用 LlamaIndex 构建应用程序通常需要使用 LlamaIndex 核心和一组选定的集成（或插件）。在 Python 中使用 LlamaIndex 构建应用程序有两种方法：&lt;/p>
&lt;ul>
&lt;li>启动器： &lt;code>llama-index&lt;/code> ( &lt;a class="link" href="https://pypi.org/project/llama-index/" target="_blank" rel="noopener" >https://pypi.org/project/llama-index/
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
)。Python 入门包，包括核心 LlamaIndex 以及部分集成。&lt;/li>
&lt;li>定制化：&lt;code>llama-index-core&lt;/code> ( &lt;a class="link" href="https://pypi.org/project/llama-index-core/" target="_blank" rel="noopener" >https://pypi.org/project/llama-index-core/
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
)。安装核心 LlamaIndex，并在 LlamaHub 上添加应用程序所需的 LlamaIndex 集成包。目前有 300 多个 LlamaIndex 集成包可与核心无缝协作，让你可以使用自己喜欢的 LLM、嵌入和向量存储数据库进行构建&lt;/li>
&lt;/ul>
&lt;p>LlamaIndex Python 库是以名字命名的，因此包含 &lt;code>core&lt;/code> 的导入语句意味着使用的是核心包。相反，那些不含 &lt;code>core&lt;/code> 的语句则意味着使用的是集成包。&lt;/p>
&lt;pre>&lt;code class="language-python"># typical pattern
from llama_index.core.xxx import ClassABC # core submodule xxx
from llama_index.xxx.yyy import (
SubclassABC,
) # integration yyy for submodule xxx
# concrete example
from llama_index.core.llms import LLM
from llama_index.llms.openai import OpenAI
&lt;/code>&lt;/pre>
&lt;h3 id="crewai">
&lt;a href="#crewai" class="header-anchor">#&lt;/a>
CrewAI
&lt;/h3>
&lt;a href="https://github.com/joaomdmoura/crewAI" target="_blank" class="card-github fetch-waiting no-styling"
repo="joaomdmoura/crewAI" id="repo-R6roA8oQ6t325WbT-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-R6roA8oQ6t325WbT-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">joaomdmoura&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">crewAI&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-R6roA8oQ6t325WbT-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-R6roA8oQ6t325WbT-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-R6roA8oQ6t325WbT-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-R6roA8oQ6t325WbT-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-R6roA8oQ6t325WbT-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-R6roA8oQ6t325WbT-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/joaomdmoura\/crewAI', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-R6roA8oQ6t325WbT-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-R6roA8oQ6t325WbT-language').innerText = data.language;
document.getElementById('repo-R6roA8oQ6t325WbT-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-R6roA8oQ6t325WbT-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-R6roA8oQ6t325WbT-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-R6roA8oQ6t325WbT-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-R6roA8oQ6t325WbT-license').classList.add = "no-license"
};
document.getElementById('repo-R6roA8oQ6t325WbT-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for joaomdmoura\/crewAI.")
}).catch(err => {
const c = document.getElementById('repo-R6roA8oQ6t325WbT-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for joaomdmoura\/crewAI.")
})
&lt;/script>
&lt;p>CrewAI 是一个构建 AI Agent 的框架，可以将 LLM 与其他工具和 API 集成，实现更复杂的任务，例如自动执行网页操作、生成代码等。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>基于角色的智能体设计&lt;/strong>：你可以使用特定的角色、目标和工具来自定义智能体。&lt;/li>
&lt;li>&lt;strong>自主智能体间委托&lt;/strong>：智能体可以自主地将任务委托给其他智能体，并相互查询信息，从而提高解决问题的效率。&lt;/li>
&lt;li>&lt;strong>灵活的任务管理&lt;/strong>：可以使用可定制的工具来定义任务，并动态地将任务分配给智能体。&lt;/li>
&lt;li>&lt;strong>流程驱动&lt;/strong>：该系统以流程为中心，目前支持按顺序执行任务和分层流程。未来还会支持更复杂的流程，例如协商和自主流程。&lt;/li>
&lt;li>&lt;strong>保存输出为文件&lt;/strong>：可以将单个任务的输出保存为文件，以便以后使用。&lt;/li>
&lt;li>&lt;strong>将输出解析为 Pydantic 或 Json&lt;/strong>：可以将单个任务的输出解析为 Pydantic 模型或 Json 格式，以便于后续处理和分析。&lt;/li>
&lt;li>&lt;strong>支持开源模型&lt;/strong>：可以使用 OpenAI 或其他开源模型来运行您的智能体团队。更多关于配置智能体与模型连接的信息，包括如何连接到本地运行的模型，请参阅&lt;a class="link" href="https://docs.crewai.com/how-to/LLM-Connections/" target="_blank" rel="noopener" >将 crewAI 连接到大型语言模型
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/li>
&lt;/ul>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/joaomdmoura/crewAI" target="_blank" rel="noopener" >CrewAI
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="opendevin">
&lt;a href="#opendevin" class="header-anchor">#&lt;/a>
OpenDevin
&lt;/h3>
&lt;a href="https://github.com/opendevin/opendevin" target="_blank" class="card-github fetch-waiting no-styling"
repo="opendevin/opendevin" id="repo-5gdPYSAmWz9Vg7pa-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-5gdPYSAmWz9Vg7pa-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">opendevin&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">opendevin&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-5gdPYSAmWz9Vg7pa-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-5gdPYSAmWz9Vg7pa-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-5gdPYSAmWz9Vg7pa-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-5gdPYSAmWz9Vg7pa-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-5gdPYSAmWz9Vg7pa-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-5gdPYSAmWz9Vg7pa-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/opendevin\/opendevin', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-5gdPYSAmWz9Vg7pa-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-5gdPYSAmWz9Vg7pa-language').innerText = data.language;
document.getElementById('repo-5gdPYSAmWz9Vg7pa-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-5gdPYSAmWz9Vg7pa-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-5gdPYSAmWz9Vg7pa-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-5gdPYSAmWz9Vg7pa-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-5gdPYSAmWz9Vg7pa-license').classList.add = "no-license"
};
document.getElementById('repo-5gdPYSAmWz9Vg7pa-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for opendevin\/opendevin.")
}).catch(err => {
const c = document.getElementById('repo-5gdPYSAmWz9Vg7pa-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for opendevin\/opendevin.")
})
&lt;/script>
&lt;p>OpenDevin 是一个由人工智能和 LLMs 驱动的自主软件工程师平台。&lt;/p>
&lt;p>OpenDevin 智能体与人类开发人员合作编写代码、修复错误和发布功能。&lt;/p>
&lt;p>详细信息，请访问 &lt;a class="link" href="https://github.com/opendevin/opendevin" target="_blank" rel="noopener" >OpenDevin
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h2 id="模型评测">
&lt;a href="#%e6%a8%a1%e5%9e%8b%e8%af%84%e6%b5%8b" class="header-anchor">#&lt;/a>
模型评测
&lt;/h2>
&lt;p>为了选择合适的 LLM 并评估其性能，我们需要进行模型评测：&lt;/p>
&lt;h3 id="lmsys">
&lt;a href="#lmsys" class="header-anchor">#&lt;/a>
LMSys
&lt;/h3>
&lt;p>LMSys Org 是由加州大学伯克利分校的学生和教师与加州大学圣地亚哥分校以及卡内基梅隆大学合作成立的开放式研究组织。&lt;/p>
&lt;p>目标是通过共同开发开放模型、数据集、系统和评估工具，使大型模型对每个人都可访问。训练大型语言模型并广泛提供它们的应用，同时也在开发分布式系统以加速它们的训练和推理过程。&lt;/p>
&lt;p>目前，LMSys Chatbot Area 是最被认可的大模型排行榜之一，受多家公司和研究机构的认可。&lt;/p>
&lt;p>排行榜地址：&lt;a class="link" href="https://arena.lmsys.org/" target="_blank" rel="noopener" >https://arena.lmsys.org/
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h3 id="opencompass">
&lt;a href="#opencompass" class="header-anchor">#&lt;/a>
OpenCompass
&lt;/h3>
&lt;p>OpenCompass 是一个 LLM 评估平台，支持 100 多个数据集上的各种模型（Llama3、Mistral、InternLM2、GPT-4、LLaMa2、Qwen、GLM、Claude 等）。&lt;/p>
&lt;a href="https://github.com/open-compass/opencompass" target="_blank" class="card-github fetch-waiting no-styling"
repo="open-compass/opencompass" id="repo-Z3zdJtNI2paRcU6j-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-Z3zdJtNI2paRcU6j-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">open-compass&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">opencompass&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-Z3zdJtNI2paRcU6j-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-Z3zdJtNI2paRcU6j-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-Z3zdJtNI2paRcU6j-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-Z3zdJtNI2paRcU6j-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-Z3zdJtNI2paRcU6j-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-Z3zdJtNI2paRcU6j-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/open-compass\/opencompass', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-Z3zdJtNI2paRcU6j-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-Z3zdJtNI2paRcU6j-language').innerText = data.language;
document.getElementById('repo-Z3zdJtNI2paRcU6j-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-Z3zdJtNI2paRcU6j-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-Z3zdJtNI2paRcU6j-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-Z3zdJtNI2paRcU6j-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-Z3zdJtNI2paRcU6j-license').classList.add = "no-license"
};
document.getElementById('repo-Z3zdJtNI2paRcU6j-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for open-compass\/opencompass.")
}).catch(err => {
const c = document.getElementById('repo-Z3zdJtNI2paRcU6j-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for open-compass\/opencompass.")
})
&lt;/script>
&lt;h3 id="open-llm-leaderboard">
&lt;a href="#open-llm-leaderboard" class="header-anchor">#&lt;/a>
Open LLM Leaderboard
&lt;/h3>
&lt;p>Open LLM Leaderboard 是一个持续更新的 LLM 排行榜，根据多个评测指标对不同模型进行排名，方便开发者了解最新的模型性能和发展趋势。&lt;/p>
&lt;p>排行榜地址：&lt;a class="link" href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard" target="_blank" rel="noopener" >https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h2 id="总结">
&lt;a href="#%e6%80%bb%e7%bb%93" class="header-anchor">#&lt;/a>
总结
&lt;/h2>
&lt;p>LLM 生态正在蓬勃发展，涵盖了从模型训练到应用落地的各个环节。相信随着技术的不断进步，LLM 将会在更多领域发挥重要作用，为我们带来更加智能的应用体验。&lt;/p></description></item><item><title>Ollama：从入门到进阶</title><link>https://cuterwrite.top/p/ollama/</link><pubDate>Sat, 15 Jun 2024 23:10:00 +0000</pubDate><guid>https://cuterwrite.top/p/ollama/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116903387_p0_master1200.webp" alt="Featured image of post Ollama：从入门到进阶" />&lt;p>近年来，大型语言模型（LLM）以其强大的文本生成和理解能力，成为了人工智能领域的中坚力量。商业 LLM 的价格通常高昂且代码封闭，限制了研究者和开发者的探索空间。幸运的是，开源社区提供了像 Ollama 这样优秀的替代方案，让每个人都能够轻松体验 LLM 的魅力，并能结合 HPC 和 IDE 插件，打造更强大的个人助手。&lt;/p>
&lt;h2 id="什么是-ollama">
&lt;a href="#%e4%bb%80%e4%b9%88%e6%98%af-ollama" class="header-anchor">#&lt;/a>
什么是 Ollama？
&lt;/h2>
&lt;p>Ollama 是一个用于构建大型语言模型应用的工具，它提供了一个简洁易用的命令行界面和服务器，让你能够轻松下载、运行和管理各种开源 LLM。与需要复杂配置和强大硬件的传统 LLM 不同，Ollama 让你能够方便地像使用手机 App 一样体验 LLM 的强大功能。&lt;/p>
&lt;h2 id="ollama-的优势">
&lt;a href="#ollama-%e7%9a%84%e4%bc%98%e5%8a%bf" class="header-anchor">#&lt;/a>
Ollama 的优势
&lt;/h2>
&lt;p>Ollama 拥有以下显著优势：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>开源免费&lt;/strong>： Ollama 及其支持的模型完全开源免费，任何人都可以自由使用、修改和分发。&lt;/li>
&lt;li>&lt;strong>简单易用&lt;/strong>： 无需复杂的配置和安装过程，只需几条命令即可启动和运行 Ollama。&lt;/li>
&lt;li>&lt;strong>模型丰富&lt;/strong>： Ollama 支持 Llama 3、Mistral、Qwen2 等众多热门开源 LLM，并提供一键下载和切换功能。&lt;/li>
&lt;li>&lt;strong>资源占用低&lt;/strong>： 相比于商业 LLM，Ollama 对硬件要求更低，即使在普通笔记本电脑上也能流畅运行。&lt;/li>
&lt;li>&lt;strong>社区活跃&lt;/strong>： Ollama 拥有庞大且活跃的社区，用户可以轻松获取帮助、分享经验和参与模型开发。&lt;/li>
&lt;/ul>
&lt;h2 id="如何使用-ollama">
&lt;a href="#%e5%a6%82%e4%bd%95%e4%bd%bf%e7%94%a8-ollama" class="header-anchor">#&lt;/a>
如何使用 Ollama？
&lt;/h2>
&lt;p>使用 Ollama 非常简单，只需要按照以下步骤：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>安装 Ollama&lt;/strong>： 根据你的操作系统，从 &lt;a class="link" href="https://ollama.com/" target="_blank" rel="noopener" >Ollama 官网
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
下载并安装最新版本。&lt;/li>
&lt;li>&lt;strong>启动 Ollama&lt;/strong>： 打开终端或命令行，输入 &lt;code>ollama serve&lt;/code> 命令启动 Ollama 服务器。&lt;/li>
&lt;li>&lt;strong>下载模型&lt;/strong>： 在&lt;a class="link" href="https://ollama.com/library" target="_blank" rel="noopener" >模型仓库
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
找到想要的模型，然后使用 &lt;code>ollama pull&lt;/code> 命令下载，例如 &lt;code>ollama pull llama3:70b&lt;/code> 。&lt;/li>
&lt;li>&lt;strong>运行模型&lt;/strong>： 使用 &lt;code>ollama run&lt;/code> 命令启动模型，例如 &lt;code>ollama run llama3:70b&lt;/code> 。&lt;/li>
&lt;li>&lt;strong>开始聊天&lt;/strong>： 在终端中输入你的问题或指令，Ollama 会根据模型生成相应的回复。&lt;/li>
&lt;/ol>
&lt;h3 id="安装-ollama">
&lt;a href="#%e5%ae%89%e8%a3%85-ollama" class="header-anchor">#&lt;/a>
安装 Ollama
&lt;/h3>
&lt;h4 id="macos">
&lt;a href="#macos" class="header-anchor">#&lt;/a>
macOS
&lt;/h4>
&lt;p>&lt;a class="link" href="https://ollama.com/download/Ollama-darwin.zip" target="_blank" rel="noopener" >下载 Ollama for macOS
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h4 id="windows">
&lt;a href="#windows" class="header-anchor">#&lt;/a>
Windows
&lt;/h4>
&lt;p>&lt;a class="link" href="https://ollama.com/download/OllamaSetup.exe" target="_blank" rel="noopener" >下载 Ollama for Windows
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h4 id="linux">
&lt;a href="#linux" class="header-anchor">#&lt;/a>
Linux
&lt;/h4>
&lt;pre>&lt;code class="language-bash">curl -fsSL https://ollama.com/install.sh | sh
&lt;/code>&lt;/pre>
&lt;h4 id="docker">
&lt;a href="#docker" class="header-anchor">#&lt;/a>
Docker
&lt;/h4>
&lt;h5 id="cpu-版本">
&lt;a href="#cpu-%e7%89%88%e6%9c%ac" class="header-anchor">#&lt;/a>
CPU 版本
&lt;/h5>
&lt;pre>&lt;code class="language-bash">docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
&lt;/code>&lt;/pre>
&lt;h5 id="gpu-版本">
&lt;a href="#gpu-%e7%89%88%e6%9c%ac" class="header-anchor">#&lt;/a>
GPU 版本
&lt;/h5>
&lt;ol>
&lt;li>安装 &lt;a class="link" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation" target="_blank" rel="noopener" >Nvidia container toolkit
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;li>在 Docker 容器中运行 Ollama&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
&lt;/code>&lt;/pre>
&lt;h3 id="启动-ollama">
&lt;a href="#%e5%90%af%e5%8a%a8-ollama" class="header-anchor">#&lt;/a>
启动 Ollama
&lt;/h3>
&lt;pre>&lt;code class="language-bash">ollama serve
&lt;/code>&lt;/pre>
&lt;p>输出以下信息表示 Ollama 服务器已成功启动（V100 机器）：&lt;/p>
&lt;pre>&lt;code class="language-bash">$ ollama serve
### 省略的日志输出 ###
Listening on [::]:11434 (version 0.1.42)
&lt;/code>&lt;/pre>
&lt;h3 id="下载模型">
&lt;a href="#%e4%b8%8b%e8%bd%bd%e6%a8%a1%e5%9e%8b" class="header-anchor">#&lt;/a>
下载模型
&lt;/h3>
&lt;pre>&lt;code class="language-bash">ollama pull qwen2:72b
&lt;/code>&lt;/pre>
&lt;h3 id="运行模型">
&lt;a href="#%e8%bf%90%e8%a1%8c%e6%a8%a1%e5%9e%8b" class="header-anchor">#&lt;/a>
运行模型
&lt;/h3>
&lt;pre>&lt;code class="language-bash">ollama run qwen2:72b
&lt;/code>&lt;/pre>
&lt;p>例如，运行如下命令后：&lt;/p>
&lt;pre>&lt;code class="language-bash">$ ollama run qwen2:72b
&amp;gt;&amp;gt;&amp;gt; Who are you?
I am Qwen, a pre-trained language model developed by Alibaba Cloud. My purpose is to assist users in generating various types of text, such as articles, stories, poems, and answering
questions by using the natural language processing techniques. How may I assist you today?
&amp;gt;&amp;gt;&amp;gt; Send a message(/? for help)
&lt;/code>&lt;/pre>
&lt;h4 id="docker-容器中运行模型">
&lt;a href="#docker-%e5%ae%b9%e5%99%a8%e4%b8%ad%e8%bf%90%e8%a1%8c%e6%a8%a1%e5%9e%8b" class="header-anchor">#&lt;/a>
Docker 容器中运行模型
&lt;/h4>
&lt;pre>&lt;code class="language-bash">docker exec -it ollama ollama run qwen2:72b
&lt;/code>&lt;/pre>
&lt;h3 id="配置-ollama">
&lt;a href="#%e9%85%8d%e7%bd%ae-ollama" class="header-anchor">#&lt;/a>
配置 Ollama
&lt;/h3>
&lt;p>Ollama 提供了多种环境变量以供配置：&lt;/p>
&lt;ul>
&lt;li>&lt;code>OLLAMA_DEBUG&lt;/code>：是否开启调试模式，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_FLASH_ATTENTION&lt;/code>：是否闪烁注意力，默认为 &lt;code>true&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_HOST&lt;/code>：Ollama 服务器的主机地址，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_KEEP_ALIVE&lt;/code>：保持连接的时间，默认为 &lt;code>5m&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_LLM_LIBRARY&lt;/code>：LLM 库，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_LOADED_MODELS&lt;/code>：最大加载模型数，默认为 &lt;code>1&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_QUEUE&lt;/code>：最大队列数，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_VRAM&lt;/code>：最大虚拟内存，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MODELS&lt;/code>：模型目录，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_NOHISTORY&lt;/code>：是否保存历史记录，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_NOPRUNE&lt;/code>：是否启用剪枝，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_NUM_PARALLEL&lt;/code>：并行数，默认为 &lt;code>1&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_ORIGINS&lt;/code>：允许的来源，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_RUNNERS_DIR&lt;/code>：运行器目录，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_SCHED_SPREAD&lt;/code>：调度分布，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_TMPDIR&lt;/code>：临时文件目录，默认为空。Here is the optimized list in the desired format:&lt;/li>
&lt;li>&lt;code>OLLAMA_DEBUG&lt;/code>：是否开启调试模式，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_FLASH_ATTENTION&lt;/code>：是否闪烁注意力，默认为 &lt;code>true&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_HOST&lt;/code>：Ollama 服务器的主机地址，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_KEEP_ALIVE&lt;/code>：保持连接的时间，默认为 &lt;code>5m&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_LLM_LIBRARY&lt;/code>：LLM 库，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_LOADED_MODELS&lt;/code>：最大加载模型数，默认为 &lt;code>1&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_QUEUE&lt;/code>：最大队列数，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_VRAM&lt;/code>：最大虚拟内存，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MODELS&lt;/code>：模型目录，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_NOHISTORY&lt;/code>：是否保存历史记录，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_NOPRUNE&lt;/code>：是否启用剪枝，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_NUM_PARALLEL&lt;/code>：并行数，默认为 &lt;code>1&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_ORIGINS&lt;/code>：允许的来源，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_RUNNERS_DIR&lt;/code>：运行器目录，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_SCHED_SPREAD&lt;/code>：调度分布，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_TMPDIR&lt;/code>：临时文件目录，默认为空。&lt;/li>
&lt;/ul>
&lt;h2 id="进阶用法hpc-集群上部署-ollama">
&lt;a href="#%e8%bf%9b%e9%98%b6%e7%94%a8%e6%b3%95hpc-%e9%9b%86%e7%be%a4%e4%b8%8a%e9%83%a8%e7%bd%b2-ollama" class="header-anchor">#&lt;/a>
进阶用法：HPC 集群上部署 Ollama
&lt;/h2>
&lt;p>对于大型模型或需要更高性能的情况，可以利用 HPC 集群的强大算力来运行 Ollama。结合 Slurm 进行任务管理，并使用端口映射将服务暴露到本地，即可方便地进行远程访问和使用：&lt;/p>
&lt;ol>
&lt;li>在登录节点配置 Ollama 环境： 安装 Ollama，并下载需要的模型。&lt;/li>
&lt;li>&lt;strong>编写 slurm 脚本&lt;/strong>： 指定资源需求（CPU、内存、GPU 等），并使用 &lt;code>ollama serve&lt;/code> 命令启动模型服务，并绑定到特定端口。&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">#!/bin/bash
#SBATCH --job-name=ollama
#SBATCH -N 1
#SBATCH -p GPU
#SBATCH --gres=gpu:1
module load CUDA
ollama serve
&lt;/code>&lt;/pre>
&lt;ol start="3">
&lt;li>&lt;strong>提交 slurm 任务&lt;/strong>: 使用 &lt;code>sbatch&lt;/code> 命令提交脚本，Slurm 会将任务分配到计算节点运行。&lt;/li>
&lt;li>&lt;strong>本地端口映射&lt;/strong>： 使用 ssh -L 命令将计算节点的端口映射到本地，例如:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">ssh -t -t 用户名@登录节点 ip -L 11434:localhost:11434 -i 登录节点私钥 ssh 计算节点 IP -L 11434:127.0.0.1:11434
&lt;/code>&lt;/pre>
&lt;ol start="5">
&lt;li>&lt;strong>本地访问&lt;/strong>： 在浏览器或应用程序中访问 http://localhost:11434 即可使用 Ollama 服务。&lt;/li>
&lt;/ol>
&lt;blockquote class="alert-blockquote alert-important">
&lt;p class="alert-heading">
&lt;svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16">
&lt;path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v9.5A1.75 1.75 0 0 1 14.25 13H8.06l-2.573 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25Zm7 2.25v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 9a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z">&lt;/path>
&lt;/svg>
&lt;span>重要&lt;/span>
&lt;/p>
&lt;p>注意：由于计算节点不联网，需要提前在登录节点使用 &lt;code>ollama pull&lt;/code> 下载所需模型。此外，需要设置 &lt;code>OLLAMA_ORIGINS&lt;/code> 为 &lt;code>*&lt;/code>，设置 &lt;code>OLLAMA_HOST&lt;/code> 为 &lt;code>0.0.0.0&lt;/code>，以允许所有来源访问服务。&lt;/p>
&lt;/blockquote>
&lt;h2 id="进阶用法本地代码补全助手">
&lt;a href="#%e8%bf%9b%e9%98%b6%e7%94%a8%e6%b3%95%e6%9c%ac%e5%9c%b0%e4%bb%a3%e7%a0%81%e8%a1%a5%e5%85%a8%e5%8a%a9%e6%89%8b" class="header-anchor">#&lt;/a>
进阶用法：本地代码补全助手
&lt;/h2>
&lt;p>Ollama 不仅可以用于聊天和文本创作，还可以结合代码生成模型和 IDE 插件，打造强大的代码补全助手。例如，使用 Codeqwen 7B 模型和 VS Code 插件 &lt;a class="link" href="https://continue.dev/" target="_blank" rel="noopener" >Continue
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
，可以实现高效便捷的代码补全功能。&lt;/p>
&lt;p>首先介绍一下 Continue :
&lt;blockquote>
&lt;p>&lt;p>&lt;a class="link" href="https://continue.dev/" target="_blank" rel="noopener" >Continue
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
enables you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs. All this can run entirely on your own laptop or have Ollama deployed on a server to remotely power code completion and chat experiences based on your needs.&lt;/p>
&lt;p>Continue 使您能够轻松地在 Visual Studio Code 和 JetBrains 中创建自己的代码助手，利用开源 LLM。这一切都可以完全在您的笔记本电脑上运行，或者在服务器上部署 Ollama，远程根据您的需求提供代码补全和聊天体验。&lt;/p>
&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Continue&lt;/span>&lt;cite>&lt;/cite>&lt;/span>&lt;/blockquote>&lt;/p>
&lt;p>在开始之前，你需要安装如下工具：&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://docs.continue.dev/quickstart" target="_blank" rel="noopener" >Continue
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
：&lt;a class="link" href="https://marketplace.visualstudio.com/items?itemName=Continue.continue" target="_blank" rel="noopener" >VS Code 版本
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
或 &lt;a class="link" href="https://plugins.jetbrains.com/plugin/22707-continue" target="_blank" rel="noopener" >JetBrains 版本
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;li>&lt;a class="link" href="https://ollama.com/" target="_blank" rel="noopener" >Ollama
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;/ul>
&lt;p>接下来，我们以 VS Code 为例，介绍如何使用 Ollama + Continue 实现代码补全功能：&lt;/p>
&lt;h3 id="codestral-22b-模型">
&lt;a href="#codestral-22b-%e6%a8%a1%e5%9e%8b" class="header-anchor">#&lt;/a>
Codestral 22B 模型
&lt;/h3>
&lt;p>Codestral 既能完成代码自动补全，也支持聊天功能。但鉴于其拥有 220 亿参数且不具备生产许可，它对显存要求颇高，仅限于研究和测试使用，因此可能并不适合日常本地应用。&lt;/p>
&lt;h4 id="下载并运行-codestral-模型">
&lt;a href="#%e4%b8%8b%e8%bd%bd%e5%b9%b6%e8%bf%90%e8%a1%8c-codestral-%e6%a8%a1%e5%9e%8b" class="header-anchor">#&lt;/a>
下载并运行 Codestral 模型
&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run codestral
&lt;/code>&lt;/pre>
&lt;h4 id="配置-configjson">
&lt;a href="#%e9%85%8d%e7%bd%ae-configjson" class="header-anchor">#&lt;/a>
配置 config.json
&lt;/h4>
&lt;ul>
&lt;li>在 VS Code 侧边栏点击 Continue 插件图标，然后在面板右下角点击 “齿轮” 图标，打开 &lt;code>config.json&lt;/code> 文件。然后复制以下配置到 &lt;code>config.json&lt;/code> 文件中：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-json">{
&amp;quot;models&amp;quot;: [
{
&amp;quot;title&amp;quot;: &amp;quot;Codestral&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;codestral&amp;quot;
}
],
&amp;quot;tabAutocompleteModel&amp;quot;: {
&amp;quot;title&amp;quot;: &amp;quot;Codestral&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;codestral&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="deepseek-coder-67b-模型--llama-3-8b-模型">
&lt;a href="#deepseek-coder-67b-%e6%a8%a1%e5%9e%8b--llama-3-8b-%e6%a8%a1%e5%9e%8b" class="header-anchor">#&lt;/a>
DeepSeek Coder 6.7B 模型 + Llama 3 8B 模型
&lt;/h3>
&lt;p>根据机器的显存大小，可以利用 Ollama 同时运行多个模型并处理多个并发请求的能力，使用 &lt;code>DeepSeek Coder 6.7B&lt;/code> 进行自动补全，&lt;code>Llama 3 8B&lt;/code> 进行聊天。如果你的机器无法同时运行两者，那么可以分别尝试，决定你更偏好本地自动补全还是本地聊天体验。&lt;/p>
&lt;h4 id="下载并运行-deepseek-coder-模型">
&lt;a href="#%e4%b8%8b%e8%bd%bd%e5%b9%b6%e8%bf%90%e8%a1%8c-deepseek-coder-%e6%a8%a1%e5%9e%8b" class="header-anchor">#&lt;/a>
下载并运行 DeepSeek Coder 模型
&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run deepseek-coder:6.7b-base
&lt;/code>&lt;/pre>
&lt;h4 id="下载并运行-llama-3-模型">
&lt;a href="#%e4%b8%8b%e8%bd%bd%e5%b9%b6%e8%bf%90%e8%a1%8c-llama-3-%e6%a8%a1%e5%9e%8b" class="header-anchor">#&lt;/a>
下载并运行 Llama 3 模型
&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run llama3:8b
&lt;/code>&lt;/pre>
&lt;h4 id="配置-configjson-1">
&lt;a href="#%e9%85%8d%e7%bd%ae-configjson-1" class="header-anchor">#&lt;/a>
配置 config.json
&lt;/h4>
&lt;pre>&lt;code class="language-json">{
&amp;quot;models&amp;quot;: [
{
&amp;quot;title&amp;quot;: &amp;quot;Llama 3 8B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;llama3:8b&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
],
&amp;quot;tabAutocompleteModel&amp;quot;: {
&amp;quot;title&amp;quot;: &amp;quot;DeepSeek Coder 6.7B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;deepseek-coder:6.7b-base&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="codeqwen-7b-模型--qwen2-7b-模型">
&lt;a href="#codeqwen-7b-%e6%a8%a1%e5%9e%8b--qwen2-7b-%e6%a8%a1%e5%9e%8b" class="header-anchor">#&lt;/a>
Codeqwen 7B 模型 + Qwen2 7B 模型
&lt;/h3>
&lt;p>Codeqwen 7B 模型是一个专门用于代码补全的模型，而 Qwen2 7B 模型则是一个通用的聊天模型。这两个模型可以很好地结合在一起，实现代码补全和聊天功能。&lt;/p>
&lt;h4 id="下载并运行-codeqwen-模型">
&lt;a href="#%e4%b8%8b%e8%bd%bd%e5%b9%b6%e8%bf%90%e8%a1%8c-codeqwen-%e6%a8%a1%e5%9e%8b" class="header-anchor">#&lt;/a>
下载并运行 Codeqwen 模型
&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run codeqwen
&lt;/code>&lt;/pre>
&lt;h4 id="下载并运行-qwen2-模型">
&lt;a href="#%e4%b8%8b%e8%bd%bd%e5%b9%b6%e8%bf%90%e8%a1%8c-qwen2-%e6%a8%a1%e5%9e%8b" class="header-anchor">#&lt;/a>
下载并运行 Qwen2 模型
&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run qwen2:7b
&lt;/code>&lt;/pre>
&lt;h4 id="配置-configjson-2">
&lt;a href="#%e9%85%8d%e7%bd%ae-configjson-2" class="header-anchor">#&lt;/a>
配置 config.json
&lt;/h4>
&lt;pre>&lt;code class="language-json">{
&amp;quot;models&amp;quot;: [
{
&amp;quot;title&amp;quot;: &amp;quot;Codeqwen 7B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;codeqwen&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
],
&amp;quot;tabAutocompleteModel&amp;quot;: {
&amp;quot;title&amp;quot;: &amp;quot;Qwen2 7B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;qwen2:7b&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="利用-rag-向量检索优化聊天">
&lt;a href="#%e5%88%a9%e7%94%a8-rag-%e5%90%91%e9%87%8f%e6%a3%80%e7%b4%a2%e4%bc%98%e5%8c%96%e8%81%8a%e5%a4%a9" class="header-anchor">#&lt;/a>
利用 RAG 向量检索优化聊天
&lt;/h3>
&lt;p>Continue 内置了 &lt;a class="link" href="https://docs.continue.dev/customization/context-providers#codebase-retrieval" target="_blank" rel="noopener" >@codebase
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
上下文提供器，能自动从代码库中检索到最相关的代码片段。假如你已经设置好了聊天模型（例如 Codestral、Llama 3），那么借助 Ollama 和 &lt;a class="link" href="https://blog.lancedb.com/lancedb-x-continue/" target="_blank" rel="noopener" >LanceDB
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的向量化技术，可以实现更高效的代码检索和聊天体验。&lt;/p>
&lt;p>这里，我们使用 &lt;code>nomic-embed-text&lt;/code> 模型作为向量检索模型：&lt;/p>
&lt;h4 id="下载并运行-nomic-embed-text-模型">
&lt;a href="#%e4%b8%8b%e8%bd%bd%e5%b9%b6%e8%bf%90%e8%a1%8c-nomic-embed-text-%e6%a8%a1%e5%9e%8b" class="header-anchor">#&lt;/a>
下载并运行 Nomic Embed Text 模型
&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run nomic-embed-text
&lt;/code>&lt;/pre>
&lt;h4 id="配置-configjson-3">
&lt;a href="#%e9%85%8d%e7%bd%ae-configjson-3" class="header-anchor">#&lt;/a>
配置 config.json
&lt;/h4>
&lt;ul>
&lt;li>在文件中添加以下内容：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-json">{
&amp;quot;embeddingsProvider&amp;quot;: {
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;nomic-embed-text&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="代码补全效果">
&lt;a href="#%e4%bb%a3%e7%a0%81%e8%a1%a5%e5%85%a8%e6%95%88%e6%9e%9c" class="header-anchor">#&lt;/a>
代码补全效果
&lt;/h3>
&lt;ul>
&lt;li>&lt;code>Ctrl + I&lt;/code>: 根据指令生成代码片段。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_1-2024-06-17.webp"
alt="codeqwen_1-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_2-2024-06-17.webp"
alt="codeqwen_2-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_3-2024-06-17.webp"
alt="codeqwen_3-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>光标悬停自动补全代码&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_4-2024-06-17.webp"
alt="codeqwen_4-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;h3 id="与-ollama-聊天">
&lt;a href="#%e4%b8%8e-ollama-%e8%81%8a%e5%a4%a9" class="header-anchor">#&lt;/a>
与 Ollama 聊天
&lt;/h3>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_5-2024-06-17.webp"
alt="codeqwen_5-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;h3 id="代码自动注释">
&lt;a href="#%e4%bb%a3%e7%a0%81%e8%87%aa%e5%8a%a8%e6%b3%a8%e9%87%8a" class="header-anchor">#&lt;/a>
代码自动注释
&lt;/h3>
&lt;ul>
&lt;li>选中代码打开右键菜单&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_6-2024-06-17.webp"
alt="codeqwen_6-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_7-2024-06-17.webp"
alt="codeqwen_7-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;h2 id="总结">
&lt;a href="#%e6%80%bb%e7%bb%93" class="header-anchor">#&lt;/a>
总结
&lt;/h2>
&lt;p>Ollama 为我们打开了通往开源 LLM 世界的大门，让每个人都能轻松体验 LLM 的强大功能，并可以根据自身需求进行定制化应用。无论是进行研究、开发，还是日常使用，Ollama 都能为你提供探索 LLM 无限可能的平台。相信随着 Ollama 的不断发展，它将为我们带来更多惊喜，推动 LLM 技术在各个领域的应用和发展。&lt;/p></description></item></channel></rss>