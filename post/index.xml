<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Cuterwrite's Blog</title><link>https://cuterwrite.top/post/</link><description>Recent content in Posts on Cuterwrite's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>cuterwrite</copyright><lastBuildDate>Fri, 05 Jul 2024 22:46:00 +0000</lastBuildDate><atom:link href="https://cuterwrite.top/post/index.xml" rel="self" type="application/rss+xml"/><item><title>LLM 生态介绍：从模型微调到应用落地</title><link>https://cuterwrite.top/p/llm-ecosystem/</link><pubDate>Fri, 05 Jul 2024 22:46:00 +0000</pubDate><guid>https://cuterwrite.top/p/llm-ecosystem/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-29_119269220_p0_master1200.webp" alt="Featured image of post LLM 生态介绍：从模型微调到应用落地" />&lt;h1 id="llm-生态介绍从模型微调到应用落地">LLM 生态介绍：从模型微调到应用落地&lt;/h1>
&lt;h2 id="模型微调">模型微调&lt;/h2>
&lt;p>预训练的 LLM 通常具备广泛的知识，但要使其在特定任务上表现出色，微调是必不可少的。以下是一些常用的 LLM 微调工具：&lt;/p>
&lt;h3 id="axolotl">Axolotl&lt;/h3>
&lt;a href="https://github.com/OpenAccess-AI-Collective/axolotl" target="_blank" class="card-github fetch-waiting no-styling"
repo="OpenAccess-AI-Collective/axolotl" id="repo-WJPKStibSD7ClNJn-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-WJPKStibSD7ClNJn-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">OpenAccess-AI-Collective&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">axolotl&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-WJPKStibSD7ClNJn-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-WJPKStibSD7ClNJn-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-WJPKStibSD7ClNJn-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-WJPKStibSD7ClNJn-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-WJPKStibSD7ClNJn-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-WJPKStibSD7ClNJn-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/OpenAccess-AI-Collective\/axolotl', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-WJPKStibSD7ClNJn-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-WJPKStibSD7ClNJn-language').innerText = data.language;
document.getElementById('repo-WJPKStibSD7ClNJn-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-WJPKStibSD7ClNJn-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-WJPKStibSD7ClNJn-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-WJPKStibSD7ClNJn-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-WJPKStibSD7ClNJn-license').classList.add = "no-license"
};
document.getElementById('repo-WJPKStibSD7ClNJn-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for OpenAccess-AI-Collective\/axolotl.")
}).catch(err => {
const c = document.getElementById('repo-WJPKStibSD7ClNJn-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for OpenAccess-AI-Collective\/axolotl.")
})
&lt;/script>
&lt;p>Axolotl 是一款旨在简化各种人工智能模型微调的工具，支持多种配置和架构。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>训练各种 Huggingface 模型，如 llama、pythia、falcon、mpt&lt;/li>
&lt;li>支持 fullfinetune、lora、qlora、relora 和 gptq&lt;/li>
&lt;li>使用简单的 yaml 文件或 CLI 重写功能自定义配置&lt;/li>
&lt;li>加载不同的数据集格式，使用自定义格式，或自带标记化数据集&lt;/li>
&lt;li>与 xformer、闪存关注、绳索缩放和多重包装集成&lt;/li>
&lt;li>可通过 FSDP 或 Deepspeed 与单 GPU 或多 GPU 协同工作&lt;/li>
&lt;li>使用 Docker 在本地或云端轻松运行&lt;/li>
&lt;li>将结果和可选的检查点记录到 wandb 或 mlflow 中&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>快速入门：&lt;/strong>
要求： Python &amp;gt;=3.10 和 Pytorch &amp;gt;=2.1.1&lt;/p>
&lt;pre>&lt;code class="language-bash">git clone https://github.com/OpenAccess-AI-Collective/axolotl
cd axolotl
pip3 install packaging ninja
pip3 install -e '.[flash-attn,deepspeed]'
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>使用方法：&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash"># preprocess datasets - optional but recommended
CUDA_VISIBLE_DEVICES=&amp;quot;&amp;quot; python -m axolotl.cli.preprocess examples/openllama-3b/lora.yml
# finetune lora
accelerate launch -m axolotl.cli.train examples/openllama-3b/lora.yml
# inference
accelerate launch -m axolotl.cli.inference examples/openllama-3b/lora.yml \
--lora_model_dir=&amp;quot;./outputs/lora-out&amp;quot;
# gradio
accelerate launch -m axolotl.cli.inference examples/openllama-3b/lora.yml \
--lora_model_dir=&amp;quot;./outputs/lora-out&amp;quot; --gradio
# remote yaml files - the yaml config can be hosted on a public URL
# Note: the yaml config must directly link to the **raw** yaml
accelerate launch -m axolotl.cli.train https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/examples/openllama-3b/lora.yml
&lt;/code>&lt;/pre>
&lt;p>其它详细信息，请访问 &lt;a class="link" href="https://github.com/OpenAccess-AI-Collective/axolotl" target="_blank" rel="noopener" >Axolotl
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
项目主页。&lt;/p>
&lt;h3 id="llama-factory">Llama-Factory&lt;/h3>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-07-07_llama-factory-logo.webp"
alt="Llama Factory Logo" width="80%" loading="lazy">
&lt;/figure>
&lt;a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" class="card-github fetch-waiting no-styling"
repo="hiyouga/LLaMA-Factory" id="repo-7926ED3ZWAjUn27A-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-7926ED3ZWAjUn27A-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">hiyouga&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">LLaMA-Factory&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-7926ED3ZWAjUn27A-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-7926ED3ZWAjUn27A-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-7926ED3ZWAjUn27A-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-7926ED3ZWAjUn27A-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-7926ED3ZWAjUn27A-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-7926ED3ZWAjUn27A-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/hiyouga\/LLaMA-Factory', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-7926ED3ZWAjUn27A-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-7926ED3ZWAjUn27A-language').innerText = data.language;
document.getElementById('repo-7926ED3ZWAjUn27A-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-7926ED3ZWAjUn27A-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-7926ED3ZWAjUn27A-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-7926ED3ZWAjUn27A-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-7926ED3ZWAjUn27A-license').classList.add = "no-license"
};
document.getElementById('repo-7926ED3ZWAjUn27A-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for hiyouga\/LLaMA-Factory.")
}).catch(err => {
const c = document.getElementById('repo-7926ED3ZWAjUn27A-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for hiyouga\/LLaMA-Factory.")
})
&lt;/script>
&lt;p>Llama-Factory 是 Meta 推出的，专注于 Llama 模型微调的框架。它构建于 PyTorch 生态之上，并提供高效的训练和评估工具。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>多种模型&lt;/strong>：LLaMA、LLaVA、Mistral、Mixtral-MoE、Qwen、Yi、Gemma、Baichuan、ChatGLM、Phi 等等。&lt;/li>
&lt;li>&lt;strong>集成方法&lt;/strong>：（增量）预训练、（多模态）指令监督微调、奖励模型训练、PPO 训练、DPO 训练、KTO 训练、ORPO 训练等等。&lt;/li>
&lt;li>&lt;strong>多种精度&lt;/strong>：16 比特全参数微调、冻结微调、LoRA 微调和基于 AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ 的 2/3/4/5/6/8 比特 QLoRA 微调。&lt;/li>
&lt;li>&lt;strong>先进算法&lt;/strong>：GaLore、BAdam、DoRA、LongLoRA、LLaMA Pro、Mixture-of-Depths、LoRA+、LoftQ、PiSSA 和 Agent 微调。&lt;/li>
&lt;li>&lt;strong>实用技巧&lt;/strong>：FlashAttention-2、Unsloth、RoPE scaling、NEFTune 和 rsLoRA。&lt;/li>
&lt;li>&lt;strong>实验监控&lt;/strong>：LlamaBoard、TensorBoard、Wandb、MLflow 等等。&lt;/li>
&lt;li>&lt;strong>极速推理&lt;/strong>：基于 vLLM 的 OpenAI 风格 API、浏览器界面和命令行接口。&lt;/li>
&lt;/ul>
&lt;div class="video-wrapper">
&lt;video
controls
src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-07-07_309488092-ec36a9dd-37f4-4f72-81bd-d76c6d0a6594.mp4"
>
&lt;p>
Your browser doesn't support HTML5 video. Here is a
&lt;a href="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-07-07_309488092-ec36a9dd-37f4-4f72-81bd-d76c6d0a6594.mp4">link to the video&lt;/a> instead.
&lt;/p>
&lt;/video>
&lt;/div>
&lt;p>&lt;strong>性能指标&lt;/strong>&lt;/p>
&lt;p>与 ChatGLM 官方的 &lt;a class="link" href="https://github.com/THUDM/ChatGLM2-6B/tree/main/ptuning" target="_blank" rel="noopener" >P-Tuning
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
微调相比，LLaMA Factory 的 LoRA 微调提供了 &lt;strong>3.7 倍&lt;/strong>的加速比，同时在广告文案生成任务上取得了更高的 Rouge 分数。结合 4 比特量化技术，LLaMA Factory 的 QLoRA 微调进一步降低了 GPU 显存消耗。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-07-07_benchmark.webp"
alt="Performance" width="auto" loading="lazy">
&lt;/figure>
&lt;details>
&lt;summary>变量定义&lt;/summary>
&lt;ul>
&lt;li>&lt;strong>Training Speed&lt;/strong>: 训练阶段每秒处理的样本数量。（批处理大小=4，截断长度=1024）&lt;/li>
&lt;li>&lt;strong>Rouge Score&lt;/strong>: &lt;a class="link" href="https://aclanthology.org/D19-1321.pdf" target="_blank" rel="noopener" >广告文案生成
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
任务验证集上的 Rouge-2 分数。（批处理大小=4，截断长度=1024）&lt;/li>
&lt;li>&lt;strong>GPU Memory&lt;/strong>: 4 比特量化训练的 GPU 显存峰值。（批处理大小=1，截断长度=1024）&lt;/li>
&lt;li>我们在 ChatGLM 的 P-Tuning 中采用 &lt;code>pre_seq_len=128&lt;/code>，在 LLaMA Factory 的 LoRA 微调中采用 &lt;code>lora_rank=32&lt;/code>。&lt;/li>
&lt;/ul>
&lt;/details>
&lt;p>&lt;strong>快速入门&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash">git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e &amp;quot;.[torch,metrics]&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>可选的额外依赖项：torch、torch-npu、metrics、deepspeed、bitsandbytes、hqq、eetq、gptq、awq、aqlm、vllm、galore、badam、qwen、modelscope、quality&lt;/p>
&lt;div class="notice notice-tip" >
&lt;div class="notice-title">&lt;svg t="1705945832245" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="19409" width="200" height="200">&lt;path d="M512 74.666667C270.933333 74.666667 74.666667 270.933333 74.666667 512S270.933333 949.333333 512 949.333333 949.333333 753.066667 949.333333 512 753.066667 74.666667 512 74.666667z m238.933333 349.866666l-2.133333 2.133334-277.333333 277.333333c-10.666667 10.666667-29.866667 12.8-42.666667 2.133333L426.666667 704l-149.333334-149.333333c-12.8-12.8-12.8-32 0-44.8 10.666667-10.666667 29.866667-12.8 42.666667-2.133334l2.133333 2.133334 125.866667 125.866666 253.866667-253.866666c10.666667-10.666667 29.866667-12.8 42.666666-2.133334l2.133334 2.133334c12.8 12.8 12.8 32 4.266666 42.666666z" fill="#ffffff" p-id="19410">&lt;/path>&lt;/svg>&lt;/div>&lt;p>遇到包冲突时，可使用 pip install &amp;ndash;no-deps -e . 解决。&lt;/p>&lt;/div>
&lt;details>
&lt;summary>Windows 用户指南&lt;/summary>
&lt;p>如果要在 Windows 平台上开启量化 LoRA（QLoRA），需要安装预编译的 &lt;code>bitsandbytes&lt;/code> 库, 支持 CUDA 11.1 到 12.2, 请根据您的 CUDA 版本情况选择适合的&lt;a class="link" href="https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels" target="_blank" rel="noopener" >发布版本
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl
&lt;/code>&lt;/pre>
&lt;p>如果要在 Windows 平台上开启 FlashAttention-2，需要安装预编译的 &lt;code>flash-attn&lt;/code> 库，支持 CUDA 12.1 到 12.2，请根据需求到 &lt;a class="link" href="https://github.com/bdashore3/flash-attention/releases" target="_blank" rel="noopener" >flash-attention
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
下载对应版本安装。&lt;/p>
&lt;/details>
&lt;details>
&lt;summary>昇腾 NPU 用户指南&lt;/summary>
&lt;p>在昇腾 NPU 设备上安装 LLaMA Factory 时，需要指定额外依赖项，使用 &lt;code>pip install -e &amp;quot;.[torch-npu,metrics]&amp;quot;&lt;/code> 命令安装。此外，还需要安装 &lt;strong>&lt;a class="link" href="https://www.hiascend.com/developer/download/community/result?module=cann" target="_blank" rel="noopener" >Ascend CANN Toolkit 与 Kernels
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/strong>，安装方法请参考&lt;a class="link" href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/80RC2alpha002/quickstart/quickstart/quickstart_18_0004.html" target="_blank" rel="noopener" >安装教程
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
或使用以下命令：&lt;/p>
&lt;pre>&lt;code class="language-bash"># 请替换 URL 为 CANN 版本和设备型号对应的 URL
# 安装 CANN Toolkit
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-toolkit_8.0.RC1.alpha001_linux-&amp;quot;$(uname -i)&amp;quot;.run
bash Ascend-cann-toolkit_8.0.RC1.alpha001_linux-&amp;quot;$(uname -i)&amp;quot;.run --install
# 安装 CANN Kernels
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run
bash Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run --install
# 设置环境变量
source /usr/local/Ascend/ascend-toolkit/set_env.sh
&lt;/code>&lt;/pre>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>依赖项&lt;/th>
&lt;th>至少&lt;/th>
&lt;th>推荐&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>CANN&lt;/td>
&lt;td>8.0.RC1&lt;/td>
&lt;td>8.0.RC1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>torch&lt;/td>
&lt;td>2.1.0&lt;/td>
&lt;td>2.1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>torch-npu&lt;/td>
&lt;td>2.1.0&lt;/td>
&lt;td>2.1.0.post3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>deepspeed&lt;/td>
&lt;td>0.13.2&lt;/td>
&lt;td>0.13.2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>请使用 &lt;code>ASCEND_RT_VISIBLE_DEVICES&lt;/code> 而非 &lt;code>CUDA_VISIBLE_DEVICES&lt;/code> 来指定运算设备。&lt;/p>
&lt;p>如果遇到无法正常推理的情况，请尝试设置 &lt;code>do_sample: false&lt;/code>。&lt;/p>
&lt;p>下载预构建 Docker 镜像：&lt;a class="link" href="http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html" target="_blank" rel="noopener" >32GB
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
| &lt;a class="link" href="http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html" target="_blank" rel="noopener" >64GB
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;/details>
&lt;p>下面三行命令分别对 Llama3-8B-Instruct 模型进行 LoRA 微调、推理和合并。&lt;/p>
&lt;pre>&lt;code class="language-bash">llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml
&lt;/code>&lt;/pre>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" rel="noopener" >Llama-Factory
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
项目主页。&lt;/p>
&lt;h3 id="firefly">Firefly&lt;/h3>
&lt;a href="https://github.com/yangjianxin1/Firefly" target="_blank" class="card-github fetch-waiting no-styling"
repo="yangjianxin1/Firefly" id="repo-aUMUYE2obEfWoMo9-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-aUMUYE2obEfWoMo9-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">yangjianxin1&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">Firefly&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-aUMUYE2obEfWoMo9-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-aUMUYE2obEfWoMo9-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-aUMUYE2obEfWoMo9-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-aUMUYE2obEfWoMo9-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-aUMUYE2obEfWoMo9-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-aUMUYE2obEfWoMo9-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/yangjianxin1\/Firefly', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-aUMUYE2obEfWoMo9-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-aUMUYE2obEfWoMo9-language').innerText = data.language;
document.getElementById('repo-aUMUYE2obEfWoMo9-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-aUMUYE2obEfWoMo9-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-aUMUYE2obEfWoMo9-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-aUMUYE2obEfWoMo9-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-aUMUYE2obEfWoMo9-license').classList.add = "no-license"
};
document.getElementById('repo-aUMUYE2obEfWoMo9-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for yangjianxin1\/Firefly.")
}).catch(err => {
const c = document.getElementById('repo-aUMUYE2obEfWoMo9-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for yangjianxin1\/Firefly.")
})
&lt;/script>
&lt;p>&lt;strong>Firefly&lt;/strong> 是一个开源的大模型训练项目，支持对主流的大模型进行预训练、指令微调和 DPO，包括但不限于 Qwen2、Yi-1.5、Llama3、Gemma、Qwen1.5、MiniCPM、Llama、InternLM、Baichuan、ChatGLM、Yi、Deepseek、Qwen、Orion、Ziya、Xverse、Mistral、Mixtral-8x7B、Zephyr、Vicuna、Bloom 等。
本项目支持&lt;strong>全量参数训练、LoRA、QLoRA 高效训练&lt;/strong>，支持&lt;strong>预训练、SFT、DPO&lt;/strong>。 如果你的训练资源有限，我们极力推荐使用 QLoRA 进行指令微调，因为我们在 Open LLM Leaderboard 上验证了该方法的有效性，并且取得了非常不错的成绩。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>📗 支持预训练、指令微调、DPO，支持全量参数训练、LoRA、QLoRA 高效训练。通过配置文件的方式训练不同的模型，小白亦可快速上手训练模型。&lt;/li>
&lt;li>📗 支持使用&lt;a class="link" href="https://github.com/yangjianxin1/unsloth" target="_blank" rel="noopener" >Unsloth
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
加速训练，并且节省显存。&lt;/li>
&lt;li>📗 支持绝大部分主流的开源大模型，如 Llama3、Gemma、MiniCPM、Llama、InternLM、Baichuan、ChatGLM、Yi、Deepseek、Qwen、Orion、Ziya、Xverse、Mistral、Mixtral-8x7B、Zephyr、Vicuna、Bloom，训练时与各个官方的 chat 模型的 template 对齐。&lt;/li>
&lt;li>📗 整理并开源指令微调数据集：firefly-train-1.1M 、moss-003-sft-data、ultrachat、 WizardLM_evol_instruct_V2_143k、school_math_0.25M。&lt;/li>
&lt;li>📗 开源&lt;a class="link" href="https://huggingface.co/YeungNLP" target="_blank" rel="noopener" >Firefly 系列指令微调模型权重
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/li>
&lt;li>📗 在 Open LLM Leaderboard 上验证了 QLoRA 训练流程的有效性。&lt;/li>
&lt;/ul>
&lt;p>该项目的 README 中包含了详细的使用说明，包括如何安装、如何训练、如何微调、如何评估等。请访问 &lt;a class="link" href="https://github.com/yangjianxin1/Firefly" target="_blank" rel="noopener" >Firefly
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
项目主页。&lt;/p>
&lt;h3 id="xtuner">XTuner&lt;/h3>
&lt;a href="https://github.com/InternLM/xtuner" target="_blank" class="card-github fetch-waiting no-styling"
repo="InternLM/xtuner" id="repo-LIqjizpI44pmBvf4-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-LIqjizpI44pmBvf4-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">InternLM&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">xtuner&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-LIqjizpI44pmBvf4-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-LIqjizpI44pmBvf4-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-LIqjizpI44pmBvf4-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-LIqjizpI44pmBvf4-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-LIqjizpI44pmBvf4-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-LIqjizpI44pmBvf4-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/InternLM\/xtuner', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-LIqjizpI44pmBvf4-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-LIqjizpI44pmBvf4-language').innerText = data.language;
document.getElementById('repo-LIqjizpI44pmBvf4-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-LIqjizpI44pmBvf4-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-LIqjizpI44pmBvf4-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-LIqjizpI44pmBvf4-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-LIqjizpI44pmBvf4-license').classList.add = "no-license"
};
document.getElementById('repo-LIqjizpI44pmBvf4-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for InternLM\/xtuner.")
}).catch(err => {
const c = document.getElementById('repo-LIqjizpI44pmBvf4-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for InternLM\/xtuner.")
})
&lt;/script>
&lt;p>XTuner 是一个高效、灵活、全能的轻量化大模型微调工具库。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>高效&lt;/strong>
&lt;ul>
&lt;li>支持大语言模型 LLM、多模态图文模型 VLM 的预训练及轻量级微调。XTuner 支持在 8GB 显存下微调 7B 模型，同时也支持多节点跨设备微调更大尺度模型（70B+）。&lt;/li>
&lt;li>自动分发高性能算子（如 FlashAttention、Triton kernels 等）以加速训练吞吐。&lt;/li>
&lt;li>兼容 &lt;a class="link" href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener" >DeepSpeed
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
🚀，轻松应用各种 ZeRO 训练优化策略。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>灵活&lt;/strong>
&lt;ul>
&lt;li>支持多种大语言模型，包括但不限于 &lt;a class="link" href="https://huggingface.co/internlm" target="_blank" rel="noopener" >InternLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="https://huggingface.co/mistralai" target="_blank" rel="noopener" >Mixtral-8x7B
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="https://huggingface.co/meta-llama" target="_blank" rel="noopener" >Llama 2
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="https://huggingface.co/THUDM" target="_blank" rel="noopener" >ChatGLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="https://huggingface.co/Qwen" target="_blank" rel="noopener" >Qwen
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="https://huggingface.co/baichuan-inc" target="_blank" rel="noopener" >Baichuan
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/li>
&lt;li>支持多模态图文模型 LLaVA 的预训练与微调。利用 XTuner 训得模型 &lt;a class="link" href="https://huggingface.co/xtuner/llava-internlm2-20b" target="_blank" rel="noopener" >LLaVA-InternLM2-20B
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
表现优异。&lt;/li>
&lt;li>精心设计的数据管道，兼容任意数据格式，开源数据或自定义数据皆可快速上手。&lt;/li>
&lt;li>支持 &lt;a class="link" href="http://arxiv.org/abs/2305.14314" target="_blank" rel="noopener" >QLoRA
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、&lt;a class="link" href="http://arxiv.org/abs/2106.09685" target="_blank" rel="noopener" >LoRA
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、全量参数微调等多种微调算法，支撑用户根据具体需求作出最优选择。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>全能&lt;/strong>
&lt;ul>
&lt;li>支持增量预训练、指令微调与 Agent 微调。&lt;/li>
&lt;li>预定义众多开源对话模版，支持与开源或训练所得模型进行对话。&lt;/li>
&lt;li>训练所得模型可无缝接入部署工具库 &lt;a class="link" href="https://github.com/InternLM/lmdeploy" target="_blank" rel="noopener" >LMDeploy
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
、大规模评测工具库 &lt;a class="link" href="https://github.com/open-compass/opencompass" target="_blank" rel="noopener" >OpenCompass
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
及 &lt;a class="link" href="https://github.com/open-compass/VLMEvalKit" target="_blank" rel="noopener" >VLMEvalKit
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>快速上手：&lt;/strong>
&lt;details>
&lt;summary>安装&lt;/summary>
&lt;ul>
&lt;li>
&lt;p>推荐使用 conda 先构建一个 Python-3.10 的虚拟环境&lt;/p>
&lt;pre>&lt;code class="language-bash">conda create --name xtuner-env python=3.10 -y
conda activate xtuner-env
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>通过 pip 安装 XTuner：&lt;/p>
&lt;pre>&lt;code class="language-shell">pip install -U xtuner
&lt;/code>&lt;/pre>
&lt;p>亦可集成 DeepSpeed 安装：&lt;/p>
&lt;pre>&lt;code class="language-shell">pip install -U 'xtuner[deepspeed]'
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>从源码安装 XTuner：&lt;/p>
&lt;pre>&lt;code class="language-shell">git clone https://github.com/InternLM/xtuner.git
cd xtuner
pip install -e '.[all]'
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul>
&lt;/details>
&lt;/p>
&lt;details>
&lt;summary>微调&lt;/summary>
&lt;p>XTuner 支持微调大语言模型。数据集预处理指南请查阅&lt;a class="link" href="./docs/zh_cn/user_guides/dataset_prepare.md" >文档
&lt;/a>
。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>步骤 0&lt;/strong>，准备配置文件。XTuner 提供多个开箱即用的配置文件，用户可以通过下列命令查看：&lt;/p>
&lt;pre>&lt;code class="language-shell">xtuner list-cfg
&lt;/code>&lt;/pre>
&lt;p>或者，如果所提供的配置文件不能满足使用需求，请导出所提供的配置文件并进行相应更改：&lt;/p>
&lt;pre>&lt;code class="language-shell">xtuner copy-cfg ${CONFIG_NAME} ${SAVE_PATH}
vi ${SAVE_PATH}/${CONFIG_NAME}_copy.py
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>步骤 1&lt;/strong>，开始微调。&lt;/p>
&lt;pre>&lt;code class="language-shell">xtuner train ${CONFIG_NAME_OR_PATH}
&lt;/code>&lt;/pre>
&lt;p>例如，我们可以利用 QLoRA 算法在 oasst1 数据集上微调 InternLM2.5-Chat-7B：&lt;/p>
&lt;pre>&lt;code class="language-shell"># 单卡
xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2
# 多卡
(DIST) NPROC_PER_NODE=${GPU_NUM} xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2
(SLURM) srun ${SRUN_ARGS} xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --launcher slurm --deepspeed deepspeed_zero2
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>
&lt;p>&lt;code>--deepspeed&lt;/code> 表示使用 &lt;a class="link" href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener" >DeepSpeed
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
🚀 来优化训练过程。XTuner 内置了多种策略，包括 ZeRO-1、ZeRO-2、ZeRO-3 等。如果用户期望关闭此功能，请直接移除此参数。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>更多示例，请查阅&lt;a class="link" href="./docs/zh_cn/user_guides/finetune.md" >文档
&lt;/a>
。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>步骤 2&lt;/strong>，将保存的 PTH 模型（如果使用的 DeepSpeed，则将会是一个文件夹）转换为 HuggingFace 模型：&lt;/p>
&lt;pre>&lt;code class="language-shell">xtuner convert pth_to_hf ${CONFIG_NAME_OR_PATH} ${PTH} ${SAVE_PATH}
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul>
&lt;/details>
&lt;p>其它详细信息，请访问 &lt;a class="link" href="https://github.com/InternLM/xtuner" target="_blank" rel="noopener" >XTuner
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h2 id="模型量化">模型量化&lt;/h2>
&lt;p>LLM 通常体积庞大，对计算资源要求高。模型量化技术可以压缩模型大小，提高运行效率，使其更易于部署：&lt;/p>
&lt;h3 id="autogptq">AutoGPTQ&lt;/h3>
&lt;a href="https://github.com/PanQiWei/AutoGPTQ" target="_blank" class="card-github fetch-waiting no-styling"
repo="PanQiWei/AutoGPTQ" id="repo-pji7hZUYgldD5B1s-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-pji7hZUYgldD5B1s-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">PanQiWei&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">AutoGPTQ&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-pji7hZUYgldD5B1s-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-pji7hZUYgldD5B1s-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-pji7hZUYgldD5B1s-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-pji7hZUYgldD5B1s-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-pji7hZUYgldD5B1s-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-pji7hZUYgldD5B1s-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/PanQiWei\/AutoGPTQ', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-pji7hZUYgldD5B1s-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-pji7hZUYgldD5B1s-language').innerText = data.language;
document.getElementById('repo-pji7hZUYgldD5B1s-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-pji7hZUYgldD5B1s-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-pji7hZUYgldD5B1s-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-pji7hZUYgldD5B1s-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-pji7hZUYgldD5B1s-license').classList.add = "no-license"
};
document.getElementById('repo-pji7hZUYgldD5B1s-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for PanQiWei\/AutoGPTQ.")
}).catch(err => {
const c = document.getElementById('repo-pji7hZUYgldD5B1s-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for PanQiWei\/AutoGPTQ.")
})
&lt;/script>
&lt;p>AutoGPTQ 一个基于 GPTQ 算法，简单易用且拥有用户友好型接口的大语言模型量化工具包。&lt;/p>
&lt;p>&lt;strong>快速安装&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>对于 CUDA 11.7：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>对于 CUDA 11.8：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>对于 RoCm 5.4.2：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/rocm542/
&lt;/code>&lt;/pre>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/AutoGPTQ/AutoGPTQ/" target="_blank" rel="noopener" >AutoGPTQ
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="autoawq">AutoAWQ&lt;/h3>
&lt;a href="https://github.com/casper-hansen/AutoAWQ" target="_blank" class="card-github fetch-waiting no-styling"
repo="casper-hansen/AutoAWQ" id="repo-oLz7sQQbl7bQLvYX-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-oLz7sQQbl7bQLvYX-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">casper-hansen&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">AutoAWQ&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-oLz7sQQbl7bQLvYX-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-oLz7sQQbl7bQLvYX-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-oLz7sQQbl7bQLvYX-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-oLz7sQQbl7bQLvYX-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-oLz7sQQbl7bQLvYX-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-oLz7sQQbl7bQLvYX-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/casper-hansen\/AutoAWQ', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-oLz7sQQbl7bQLvYX-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-oLz7sQQbl7bQLvYX-language').innerText = data.language;
document.getElementById('repo-oLz7sQQbl7bQLvYX-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-oLz7sQQbl7bQLvYX-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-oLz7sQQbl7bQLvYX-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-oLz7sQQbl7bQLvYX-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-oLz7sQQbl7bQLvYX-license').classList.add = "no-license"
};
document.getElementById('repo-oLz7sQQbl7bQLvYX-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for casper-hansen\/AutoAWQ.")
}).catch(err => {
const c = document.getElementById('repo-oLz7sQQbl7bQLvYX-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for casper-hansen\/AutoAWQ.")
})
&lt;/script>
&lt;p>AutoAWQ 是另一款自动化模型量化工具，支持多种量化精度，并提供灵活的配置选项，可以根据不同的硬件平台和性能需求进行调整。&lt;/p>
&lt;p>AutoAWQ 是一个易于使用的 4 位量化模型软件包。与 FP16 相比，AutoAWQ 可将模型速度提高 3 倍，内存需求减少 3 倍。AutoAWQ 实现了用于量化 LLMs 的激活感知权重量化（AWQ）算法。AutoAWQ 是在 MIT 的原始工作 &lt;a class="link" href="https://github.com/mit-han-lab/llm-awq" target="_blank" rel="noopener" >AWQ
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
基础上创建和改进的。&lt;/p>
&lt;p>&lt;strong>安装方法：&lt;/strong>&lt;/p>
&lt;p>安装前，确保安装了 CUDA &amp;gt;= 12.1（注意：以下只是最快捷的安装方法）&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install autoawq
&lt;/code>&lt;/pre>
&lt;p>更多详细信息和示例，请访问 &lt;a class="link" href="https://github.com/casper-hansen/AutoAWQ" target="_blank" rel="noopener" >AutoAWQ
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="neural-compressor">Neural Compressor&lt;/h3>
&lt;a href="https://github.com/intel/neural-compressor" target="_blank" class="card-github fetch-waiting no-styling"
repo="intel/neural-compressor" id="repo-KAarREa1SQjMq9gx-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-KAarREa1SQjMq9gx-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">intel&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">neural-compressor&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-KAarREa1SQjMq9gx-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-KAarREa1SQjMq9gx-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-KAarREa1SQjMq9gx-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-KAarREa1SQjMq9gx-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-KAarREa1SQjMq9gx-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-KAarREa1SQjMq9gx-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/intel\/neural-compressor', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-KAarREa1SQjMq9gx-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-KAarREa1SQjMq9gx-language').innerText = data.language;
document.getElementById('repo-KAarREa1SQjMq9gx-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-KAarREa1SQjMq9gx-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-KAarREa1SQjMq9gx-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-KAarREa1SQjMq9gx-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-KAarREa1SQjMq9gx-license').classList.add = "no-license"
};
document.getElementById('repo-KAarREa1SQjMq9gx-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for intel\/neural-compressor.")
}).catch(err => {
const c = document.getElementById('repo-KAarREa1SQjMq9gx-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for intel\/neural-compressor.")
})
&lt;/script>
&lt;p>Neural Compressor 是英特尔开发的模型压缩工具包，支持所有主流深度学习框架（TensorFlow、PyTorch、ONNX Runtime 和 MXNet）上流行的模型压缩技术。&lt;/p>
&lt;p>&lt;strong>安装方法：&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install &amp;quot;neural-compressor&amp;gt;=2.3&amp;quot; &amp;quot;transformers&amp;gt;=4.34.0&amp;quot; torch torchvision
&lt;/code>&lt;/pre>
&lt;p>更多详细信息和示例，请访问 &lt;a class="link" href="https://github.com/intel/neural-compressor" target="_blank" rel="noopener" >Neural Compressor
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h2 id="模型部署">模型部署&lt;/h2>
&lt;p>将训练好的 LLM 部署到生产环境至关重要。以下是一些常用的 LLM 部署工具：&lt;/p>
&lt;h3 id="vllm">vLLM&lt;/h3>
&lt;a href="https://github.com/vllm-project/vllm" target="_blank" class="card-github fetch-waiting no-styling"
repo="vllm-project/vllm" id="repo-nJTPt7skFc6Vwdho-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-nJTPt7skFc6Vwdho-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">vllm-project&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">vllm&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-nJTPt7skFc6Vwdho-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-nJTPt7skFc6Vwdho-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-nJTPt7skFc6Vwdho-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-nJTPt7skFc6Vwdho-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-nJTPt7skFc6Vwdho-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-nJTPt7skFc6Vwdho-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/vllm-project\/vllm', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-nJTPt7skFc6Vwdho-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-nJTPt7skFc6Vwdho-language').innerText = data.language;
document.getElementById('repo-nJTPt7skFc6Vwdho-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-nJTPt7skFc6Vwdho-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-nJTPt7skFc6Vwdho-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-nJTPt7skFc6Vwdho-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-nJTPt7skFc6Vwdho-license').classList.add = "no-license"
};
document.getElementById('repo-nJTPt7skFc6Vwdho-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for vllm-project\/vllm.")
}).catch(err => {
const c = document.getElementById('repo-nJTPt7skFc6Vwdho-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for vllm-project\/vllm.")
})
&lt;/script>
&lt;p>vLLM 是一个快速、易用的 LLM 推理服务库。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>快速&lt;/strong>
&lt;ul>
&lt;li>SOTA 服务吞吐量&lt;/li>
&lt;li>利用 PagedAttention 高效管理注意力键值内存&lt;/li>
&lt;li>持续批量处理收到的请求&lt;/li>
&lt;li>利用 CUDA/HIP 图进行加速&lt;/li>
&lt;li>量化：支持 GPTQ、AWQ、SqueezeLLM、FP8 KV 高速缓存&lt;/li>
&lt;li>优化的 CUDA 内核&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>灵活&lt;/strong>
&lt;ul>
&lt;li>与流行的 Hugging Face 模型无缝集成&lt;/li>
&lt;li>利用各种解码算法（包括并行采样、波束搜索等）提供高吞吐量服务&lt;/li>
&lt;li>为分布式推理提供张量并行支持&lt;/li>
&lt;li>流输出&lt;/li>
&lt;li>兼容 OpenAI 的应用程序接口服务器&lt;/li>
&lt;li>支持 NVIDIA GPU、AMD GPU、Intel CPU 和 GPU
-（实验性）支持前缀缓存
-（试验性）支持多种语言&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>无缝支持&lt;/strong>
&lt;ul>
&lt;li>基于 Transformer 的模型，例如 Llama&lt;/li>
&lt;li>基于 MoE 的模型，例如 Mixtral&lt;/li>
&lt;li>多模态模型，例如 LLaVA&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>快速安装：&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash">pip install vllm
&lt;/code>&lt;/pre>
&lt;p>更多详细信息，请查看 &lt;a class="link" href="https://vllm.readthedocs.io/en/latest/" target="_blank" rel="noopener" >vLLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
官方文档。&lt;/p>
&lt;h3 id="sgl">SGL&lt;/h3>
&lt;a href="https://github.com/sgl-project/sglang" target="_blank" class="card-github fetch-waiting no-styling"
repo="sgl-project/sglang" id="repo-XiS46NyNt1nHwhro-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-XiS46NyNt1nHwhro-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">sgl-project&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">sglang&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-XiS46NyNt1nHwhro-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-XiS46NyNt1nHwhro-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-XiS46NyNt1nHwhro-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-XiS46NyNt1nHwhro-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-XiS46NyNt1nHwhro-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-XiS46NyNt1nHwhro-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/sgl-project\/sglang', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-XiS46NyNt1nHwhro-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-XiS46NyNt1nHwhro-language').innerText = data.language;
document.getElementById('repo-XiS46NyNt1nHwhro-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-XiS46NyNt1nHwhro-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-XiS46NyNt1nHwhro-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-XiS46NyNt1nHwhro-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-XiS46NyNt1nHwhro-license').classList.add = "no-license"
};
document.getElementById('repo-XiS46NyNt1nHwhro-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for sgl-project\/sglang.")
}).catch(err => {
const c = document.getElementById('repo-XiS46NyNt1nHwhro-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for sgl-project\/sglang.")
})
&lt;/script>
&lt;p>SGLang 是一种结构化生成语言，专为大型语言模型（LLMs）而设计。它通过共同设计前端语言和运行系统，使你与 LLMs 的交互更快、更可控。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>灵活的前端语言&lt;/strong>：通过链式生成调用、高级提示、控制流、多种模式、并行性和外部交互，可轻松编写 LLM 应用程序。&lt;/li>
&lt;li>&lt;strong>高性能后端运行时&lt;/strong>：具有 RadixAttention 功能，可通过在多次调用中重复使用 KV 缓存来加速复杂的 LLM 程序。它还可以作为独立的推理引擎，实现所有常用技术（如连续批处理和张量并行）。&lt;/li>
&lt;/ul>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/sgl-project/" target="_blank" rel="noopener" >SGL
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="skypilot">SkyPilot&lt;/h3>
&lt;a href="https://github.com/skypilot-org/skypilot" target="_blank" class="card-github fetch-waiting no-styling"
repo="skypilot-org/skypilot" id="repo-ryTS0x49ntzTY4yW-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-ryTS0x49ntzTY4yW-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">skypilot-org&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">skypilot&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-ryTS0x49ntzTY4yW-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-ryTS0x49ntzTY4yW-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-ryTS0x49ntzTY4yW-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-ryTS0x49ntzTY4yW-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-ryTS0x49ntzTY4yW-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-ryTS0x49ntzTY4yW-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/skypilot-org\/skypilot', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-ryTS0x49ntzTY4yW-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-ryTS0x49ntzTY4yW-language').innerText = data.language;
document.getElementById('repo-ryTS0x49ntzTY4yW-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-ryTS0x49ntzTY4yW-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-ryTS0x49ntzTY4yW-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-ryTS0x49ntzTY4yW-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-ryTS0x49ntzTY4yW-license').classList.add = "no-license"
};
document.getElementById('repo-ryTS0x49ntzTY4yW-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for skypilot-org\/skypilot.")
}).catch(err => {
const c = document.getElementById('repo-ryTS0x49ntzTY4yW-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for skypilot-org\/skypilot.")
})
&lt;/script>
&lt;p>SkyPilot 是 UC Berkeley RISELab 推出的灵活的云端 LLM 部署工具，支持多种云平台和硬件加速器，可以自动选择最优的部署方案，并提供成本优化功能。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>多云支持:&lt;/strong> 支持 AWS, GCP, Azure 等多种云平台，方便用户选择合适的部署环境。&lt;/li>
&lt;li>&lt;strong>轻松扩展&lt;/strong>：排队和运行多个作业，自动管理&lt;/li>
&lt;li>&lt;strong>轻松接入对象存储&lt;/strong>：轻松访问对象存储（S3、GCS、R2）&lt;/li>
&lt;/ul>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/skypilot-org/skypilot" target="_blank" rel="noopener" >SkyPilot
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="tensorrt-llm">TensorRT-LLM&lt;/h3>
&lt;a href="https://github.com/NVIDIA/TensorRT-LLM" target="_blank" class="card-github fetch-waiting no-styling"
repo="NVIDIA/TensorRT-LLM" id="repo-qjwxbVbVPsELdWkR-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-qjwxbVbVPsELdWkR-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">NVIDIA&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">TensorRT-LLM&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-qjwxbVbVPsELdWkR-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-qjwxbVbVPsELdWkR-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-qjwxbVbVPsELdWkR-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-qjwxbVbVPsELdWkR-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-qjwxbVbVPsELdWkR-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-qjwxbVbVPsELdWkR-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/NVIDIA\/TensorRT-LLM', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-qjwxbVbVPsELdWkR-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-qjwxbVbVPsELdWkR-language').innerText = data.language;
document.getElementById('repo-qjwxbVbVPsELdWkR-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-qjwxbVbVPsELdWkR-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-qjwxbVbVPsELdWkR-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-qjwxbVbVPsELdWkR-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-qjwxbVbVPsELdWkR-license').classList.add = "no-license"
};
document.getElementById('repo-qjwxbVbVPsELdWkR-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for NVIDIA\/TensorRT-LLM.")
}).catch(err => {
const c = document.getElementById('repo-qjwxbVbVPsELdWkR-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for NVIDIA\/TensorRT-LLM.")
})
&lt;/script>
&lt;p>TensorRT-LLM 是 NVIDIA 推出的高性能 LLM 推理引擎，能够充分利用 GPU 加速计算，并针对 Transformer 模型结构进行了优化，大幅提升推理速度。&lt;/p>
&lt;p>TensorRT-LLM 为用户提供了易于使用的 Python API，用于定义大型语言模型 (LLMs) 和构建 TensorRT 引擎，这些引擎包含最先进的优化技术，可在英伟达™（NVIDIA®）图形处理器上高效执行推理。TensorRT-LLM 还包含用于创建执行这些 TensorRT 引擎的 Python 和 C++ 运行时的组件。&lt;/p>
&lt;p>详细信息，请访问 &lt;a class="link" href="https://github.com/NVIDIA/TensorRT-LLM" target="_blank" rel="noopener" >TensorRT-LLM
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="openvino">OpenVino&lt;/h3>
&lt;a href="https://github.com/openvinotoolkit/openvino" target="_blank" class="card-github fetch-waiting no-styling"
repo="openvinotoolkit/openvino" id="repo-dguIcuUXdtdTe3tk-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-dguIcuUXdtdTe3tk-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">openvinotoolkit&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">openvino&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-dguIcuUXdtdTe3tk-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-dguIcuUXdtdTe3tk-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-dguIcuUXdtdTe3tk-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-dguIcuUXdtdTe3tk-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-dguIcuUXdtdTe3tk-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-dguIcuUXdtdTe3tk-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/openvinotoolkit\/openvino', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-dguIcuUXdtdTe3tk-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-dguIcuUXdtdTe3tk-language').innerText = data.language;
document.getElementById('repo-dguIcuUXdtdTe3tk-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-dguIcuUXdtdTe3tk-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-dguIcuUXdtdTe3tk-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-dguIcuUXdtdTe3tk-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-dguIcuUXdtdTe3tk-license').classList.add = "no-license"
};
document.getElementById('repo-dguIcuUXdtdTe3tk-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for openvinotoolkit\/openvino.")
}).catch(err => {
const c = document.getElementById('repo-dguIcuUXdtdTe3tk-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for openvinotoolkit\/openvino.")
})
&lt;/script>
&lt;p>OpenVINO™ 是用于优化和部署人工智能推理的开源工具包。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>推理优化&lt;/strong>：提升深度学习在计算机视觉、自动语音识别、生成式人工智能、使用大型和小型语言模型的自然语言处理以及许多其他常见任务中的性能。&lt;/li>
&lt;li>&lt;strong>灵活的模型支持&lt;/strong>：使用 TensorFlow、PyTorch、ONNX、Keras 和 PaddlePaddle 等流行框架训练的模型。无需原始框架即可转换和部署模型。&lt;/li>
&lt;li>&lt;strong>广泛的平台兼容性&lt;/strong>：减少资源需求，在从边缘到云的一系列平台上高效部署。OpenVINO™ 支持在 CPU（x86、ARM）、GPU（支持 OpenCL 的集成和独立 GPU）和 AI 加速器（英特尔 NPU）上进行推理。&lt;/li>
&lt;li>&lt;strong>社区和生态系统&lt;/strong>：加入一个活跃的社区，为提高各个领域的深度学习性能做出贡献。&lt;/li>
&lt;/ul>
&lt;p>详细信息，请访问 &lt;a class="link" href="https://github.com/openvinotoolkit/openvino" target="_blank" rel="noopener" >OpenVino
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="tgi">TGI&lt;/h3>
&lt;a href="https://github.com/huggingface/text-generation-inference" target="_blank" class="card-github fetch-waiting no-styling"
repo="huggingface/text-generation-inference" id="repo-g31CtAAjF8I3WHBi-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-g31CtAAjF8I3WHBi-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">huggingface&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">text-generation-inference&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-g31CtAAjF8I3WHBi-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-g31CtAAjF8I3WHBi-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-g31CtAAjF8I3WHBi-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-g31CtAAjF8I3WHBi-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-g31CtAAjF8I3WHBi-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-g31CtAAjF8I3WHBi-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/huggingface\/text-generation-inference', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-g31CtAAjF8I3WHBi-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-g31CtAAjF8I3WHBi-language').innerText = data.language;
document.getElementById('repo-g31CtAAjF8I3WHBi-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-g31CtAAjF8I3WHBi-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-g31CtAAjF8I3WHBi-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-g31CtAAjF8I3WHBi-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-g31CtAAjF8I3WHBi-license').classList.add = "no-license"
};
document.getElementById('repo-g31CtAAjF8I3WHBi-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for huggingface\/text-generation-inference.")
}).catch(err => {
const c = document.getElementById('repo-g31CtAAjF8I3WHBi-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for huggingface\/text-generation-inference.")
})
&lt;/script>
&lt;p>文本生成推理（TGI）是一个用于部署和服务大型语言模型（LLMs）的工具包。TGI 可为最流行的开源 LLMs 实现高性能文本生成，包括 Llama、Falcon、StarCoder、BLOOM、GPT-NeoX 等。&lt;/p>
&lt;p>TGI 实现了许多功能，可以在 &lt;a class="link" href="https://github.com/huggingface/text-generation-inference" target="_blank" rel="noopener" >TGI
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页上找到详细信息。&lt;/p>
&lt;h2 id="本地运行">本地运行&lt;/h2>
&lt;p>得益于模型压缩和优化技术，我们也可以在个人设备上运行 LLM：&lt;/p>
&lt;h3 id="mlx">MLX&lt;/h3>
&lt;a href="https://github.com/ml-explore/mlx" target="_blank" class="card-github fetch-waiting no-styling"
repo="ml-explore/mlx" id="repo-aBdOtvs36A8UEMUP-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-aBdOtvs36A8UEMUP-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">ml-explore&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">mlx&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-aBdOtvs36A8UEMUP-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-aBdOtvs36A8UEMUP-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-aBdOtvs36A8UEMUP-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-aBdOtvs36A8UEMUP-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-aBdOtvs36A8UEMUP-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-aBdOtvs36A8UEMUP-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/ml-explore\/mlx', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-aBdOtvs36A8UEMUP-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-aBdOtvs36A8UEMUP-language').innerText = data.language;
document.getElementById('repo-aBdOtvs36A8UEMUP-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-aBdOtvs36A8UEMUP-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-aBdOtvs36A8UEMUP-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-aBdOtvs36A8UEMUP-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-aBdOtvs36A8UEMUP-license').classList.add = "no-license"
};
document.getElementById('repo-aBdOtvs36A8UEMUP-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for ml-explore\/mlx.")
}).catch(err => {
const c = document.getElementById('repo-aBdOtvs36A8UEMUP-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for ml-explore\/mlx.")
})
&lt;/script>
&lt;p>MLX 是一个专门支持在 Apple 设备上运行 LLM 的框架，充分利用 Metal 加速计算，并提供简单易用的 API，方便开发者将 LLM 集成到 iOS 应用中.&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>相似的应用程序接口&lt;/strong>：MLX 的 Python API 与 NumPy 非常相似。MLX 还拥有功能齐全的 C++、C 和 Swift API，这些 API 与 Python API 非常相似。MLX 拥有更高级别的软件包，如 &lt;code>mlx.nn&lt;/code> 和 &lt;code>mlx.optimizers&lt;/code> ，其 API 与 PyTorch 非常接近，可简化更复杂模型的构建。&lt;/li>
&lt;li>&lt;strong>可组合函数变换&lt;/strong>：MLX 支持用于自动微分、自动矢量化和计算图优化的可组合函数变换。&lt;/li>
&lt;li>&lt;strong>懒计算&lt;/strong>：MLX 中的计算只有在需要时才将数组实体化。&lt;/li>
&lt;li>&lt;strong>动态图构建&lt;/strong>：MLX 中的计算图形是动态构建的。改变函数参数的形状不会导致编译速度变慢，而且调试简单直观。&lt;/li>
&lt;li>&lt;strong>多设备&lt;/strong>：操作可在任何支持的设备（目前是 CPU 和 GPU）上运行。&lt;/li>
&lt;li>&lt;strong>统一内存&lt;/strong>：统一内存模型是 MLX 与其他框架的一个显著区别。MLX 中的阵列位于共享内存中。对 MLX 数组的操作可在任何支持的设备类型上执行，而无需传输数据。&lt;/li>
&lt;/ul>
&lt;p>MLX 是机器学习研究人员为机器学习研究人员设计的。该框架旨在方便用户使用，但仍能高效地训练和部署模型。框架本身的设计概念也很简单。我们的目标是让研究人员能够轻松扩展和改进 MLX，从而快速探索新思路。更多详细信息，请访问 &lt;a class="link" href="https://github.com/ml-explore/mlx" target="_blank" rel="noopener" >MLX
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="llamacpp">Llama.cpp&lt;/h3>
&lt;p>Llama.cpp 是使用 C++ 实现的 Llama 模型推理引擎，可以在 CPU 上高效运行，并支持多种操作系统和硬件平台，方便开发者在资源受限的设备上运行 LLM。&lt;/p>
&lt;a href="https://github.com/ggerganov/llama.cpp" target="_blank" class="card-github fetch-waiting no-styling"
repo="ggerganov/llama.cpp" id="repo-nxuCVpVeb4Nv0H4c-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-nxuCVpVeb4Nv0H4c-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">ggerganov&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">llama.cpp&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-nxuCVpVeb4Nv0H4c-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-nxuCVpVeb4Nv0H4c-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-nxuCVpVeb4Nv0H4c-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-nxuCVpVeb4Nv0H4c-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-nxuCVpVeb4Nv0H4c-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-nxuCVpVeb4Nv0H4c-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/ggerganov\/llama.cpp', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-nxuCVpVeb4Nv0H4c-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-nxuCVpVeb4Nv0H4c-language').innerText = data.language;
document.getElementById('repo-nxuCVpVeb4Nv0H4c-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-nxuCVpVeb4Nv0H4c-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-nxuCVpVeb4Nv0H4c-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-nxuCVpVeb4Nv0H4c-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-nxuCVpVeb4Nv0H4c-license').classList.add = "no-license"
};
document.getElementById('repo-nxuCVpVeb4Nv0H4c-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for ggerganov\/llama.cpp.")
}).catch(err => {
const c = document.getElementById('repo-nxuCVpVeb4Nv0H4c-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for ggerganov\/llama.cpp.")
})
&lt;/script>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>CPU 推理:&lt;/strong> 针对 CPU 平台进行优化，可以在没有 GPU 的设备上运行 LLM。&lt;/li>
&lt;li>&lt;strong>跨平台支持:&lt;/strong> 支持 Linux, macOS, Windows 等多种操作系统，方便用户在不同平台上使用。&lt;/li>
&lt;li>&lt;strong>轻量级部署:&lt;/strong> 编译后的二进制文件体积小，方便用户部署和使用.&lt;/li>
&lt;/ul>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener" >Llama.cpp
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="ollama">Ollama&lt;/h3>
&lt;a href="https://github.com/ollama/ollama" target="_blank" class="card-github fetch-waiting no-styling"
repo="ollama/ollama" id="repo-ENWJWceuuaRFLRyT-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-ENWJWceuuaRFLRyT-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">ollama&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">ollama&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-ENWJWceuuaRFLRyT-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-ENWJWceuuaRFLRyT-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-ENWJWceuuaRFLRyT-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-ENWJWceuuaRFLRyT-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-ENWJWceuuaRFLRyT-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-ENWJWceuuaRFLRyT-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/ollama\/ollama', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-ENWJWceuuaRFLRyT-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-ENWJWceuuaRFLRyT-language').innerText = data.language;
document.getElementById('repo-ENWJWceuuaRFLRyT-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-ENWJWceuuaRFLRyT-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-ENWJWceuuaRFLRyT-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-ENWJWceuuaRFLRyT-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-ENWJWceuuaRFLRyT-license').classList.add = "no-license"
};
document.getElementById('repo-ENWJWceuuaRFLRyT-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for ollama\/ollama.")
}).catch(err => {
const c = document.getElementById('repo-ENWJWceuuaRFLRyT-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for ollama\/ollama.")
})
&lt;/script>
&lt;p>在 &lt;a class="link" href="https://cuterwrite.top/p/ollama/" >【Ollama：从入门到进阶】
&lt;/a>
一文中介绍过，Ollama 是一个用于构建大型语言模型应用的工具，它提供了一个简洁易用的命令行界面和服务器，让你能够轻松下载、运行和管理各种开源 LLM。与需要复杂配置和强大硬件的传统 LLM 不同，Ollama 让你能够方便地像使用手机 App 一样体验 LLM 的强大功能。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>简单易用&lt;/strong>：Ollama 提供了一个简洁易用的命令行工具，方便用户下载、运行和管理 LLM。&lt;/li>
&lt;li>&lt;strong>多种模型&lt;/strong>：Ollama 支持多种开源 LLM，包括 Qwen2、Llama3、Mistral 等。&lt;/li>
&lt;li>&lt;strong>兼容 OpenAI 接口&lt;/strong>：Ollama 支持 OpenAI API 接口，便于切换原有应用到 Ollama 上。&lt;/li>
&lt;/ul>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/ollama/ollama" target="_blank" rel="noopener" >Ollama
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h2 id="agent-及-rag-框架">Agent 及 RAG 框架&lt;/h2>
&lt;p>将 LLM 与外部数据和工具结合，可以构建更强大的应用。以下是一些常用的 Agent 及 RAG 框架：&lt;/p>
&lt;h3 id="llamaindex">LlamaIndex&lt;/h3>
&lt;a href="https://github.com/run-llama/llama_index" target="_blank" class="card-github fetch-waiting no-styling"
repo="run-llama/llama_index" id="repo-PC9mgYiikqIPmX4n-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-PC9mgYiikqIPmX4n-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">run-llama&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">llama_index&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-PC9mgYiikqIPmX4n-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-PC9mgYiikqIPmX4n-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-PC9mgYiikqIPmX4n-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-PC9mgYiikqIPmX4n-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-PC9mgYiikqIPmX4n-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-PC9mgYiikqIPmX4n-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/run-llama\/llama_index', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-PC9mgYiikqIPmX4n-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-PC9mgYiikqIPmX4n-language').innerText = data.language;
document.getElementById('repo-PC9mgYiikqIPmX4n-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-PC9mgYiikqIPmX4n-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-PC9mgYiikqIPmX4n-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-PC9mgYiikqIPmX4n-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-PC9mgYiikqIPmX4n-license').classList.add = "no-license"
};
document.getElementById('repo-PC9mgYiikqIPmX4n-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for run-llama\/llama_index.")
}).catch(err => {
const c = document.getElementById('repo-PC9mgYiikqIPmX4n-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for run-llama\/llama_index.")
})
&lt;/script>
&lt;p>LlamaIndex（GPT 索引）是用于 LLM 应用程序的数据框架。使用 LlamaIndex 构建应用程序通常需要使用 LlamaIndex 核心和一组选定的集成（或插件）。在 Python 中使用 LlamaIndex 构建应用程序有两种方法：&lt;/p>
&lt;ul>
&lt;li>启动器： &lt;code>llama-index&lt;/code> ( &lt;a class="link" href="https://pypi.org/project/llama-index/" target="_blank" rel="noopener" >https://pypi.org/project/llama-index/
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
)。Python 入门包，包括核心 LlamaIndex 以及部分集成。&lt;/li>
&lt;li>定制化：&lt;code>llama-index-core&lt;/code> ( &lt;a class="link" href="https://pypi.org/project/llama-index-core/" target="_blank" rel="noopener" >https://pypi.org/project/llama-index-core/
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
)。安装核心 LlamaIndex，并在 LlamaHub 上添加应用程序所需的 LlamaIndex 集成包。目前有 300 多个 LlamaIndex 集成包可与核心无缝协作，让你可以使用自己喜欢的 LLM、嵌入和向量存储数据库进行构建&lt;/li>
&lt;/ul>
&lt;p>LlamaIndex Python 库是以名字命名的，因此包含 &lt;code>core&lt;/code> 的导入语句意味着使用的是核心包。相反，那些不含 &lt;code>core&lt;/code> 的语句则意味着使用的是集成包。&lt;/p>
&lt;pre>&lt;code class="language-python"># typical pattern
from llama_index.core.xxx import ClassABC # core submodule xxx
from llama_index.xxx.yyy import (
SubclassABC,
) # integration yyy for submodule xxx
# concrete example
from llama_index.core.llms import LLM
from llama_index.llms.openai import OpenAI
&lt;/code>&lt;/pre>
&lt;h3 id="crewai">CrewAI&lt;/h3>
&lt;a href="https://github.com/joaomdmoura/crewAI" target="_blank" class="card-github fetch-waiting no-styling"
repo="joaomdmoura/crewAI" id="repo-trtBEg2PjAxKCIPc-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-trtBEg2PjAxKCIPc-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">joaomdmoura&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">crewAI&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-trtBEg2PjAxKCIPc-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-trtBEg2PjAxKCIPc-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-trtBEg2PjAxKCIPc-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-trtBEg2PjAxKCIPc-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-trtBEg2PjAxKCIPc-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-trtBEg2PjAxKCIPc-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/joaomdmoura\/crewAI', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-trtBEg2PjAxKCIPc-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-trtBEg2PjAxKCIPc-language').innerText = data.language;
document.getElementById('repo-trtBEg2PjAxKCIPc-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-trtBEg2PjAxKCIPc-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-trtBEg2PjAxKCIPc-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-trtBEg2PjAxKCIPc-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-trtBEg2PjAxKCIPc-license').classList.add = "no-license"
};
document.getElementById('repo-trtBEg2PjAxKCIPc-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for joaomdmoura\/crewAI.")
}).catch(err => {
const c = document.getElementById('repo-trtBEg2PjAxKCIPc-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for joaomdmoura\/crewAI.")
})
&lt;/script>
&lt;p>CrewAI 是一个构建 AI Agent 的框架，可以将 LLM 与其他工具和 API 集成，实现更复杂的任务，例如自动执行网页操作、生成代码等。&lt;/p>
&lt;p>&lt;strong>主要特点：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>基于角色的智能体设计&lt;/strong>：你可以使用特定的角色、目标和工具来自定义智能体。&lt;/li>
&lt;li>&lt;strong>自主智能体间委托&lt;/strong>：智能体可以自主地将任务委托给其他智能体，并相互查询信息，从而提高解决问题的效率。&lt;/li>
&lt;li>&lt;strong>灵活的任务管理&lt;/strong>：可以使用可定制的工具来定义任务，并动态地将任务分配给智能体。&lt;/li>
&lt;li>&lt;strong>流程驱动&lt;/strong>：该系统以流程为中心，目前支持按顺序执行任务和分层流程。未来还会支持更复杂的流程，例如协商和自主流程。&lt;/li>
&lt;li>&lt;strong>保存输出为文件&lt;/strong>：可以将单个任务的输出保存为文件，以便以后使用。&lt;/li>
&lt;li>&lt;strong>将输出解析为 Pydantic 或 Json&lt;/strong>：可以将单个任务的输出解析为 Pydantic 模型或 Json 格式，以便于后续处理和分析。&lt;/li>
&lt;li>&lt;strong>支持开源模型&lt;/strong>：可以使用 OpenAI 或其他开源模型来运行您的智能体团队。更多关于配置智能体与模型连接的信息，包括如何连接到本地运行的模型，请参阅&lt;a class="link" href="https://docs.crewai.com/how-to/LLM-Connections/" target="_blank" rel="noopener" >将 crewAI 连接到大型语言模型
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
。&lt;/li>
&lt;/ul>
&lt;p>更多详细信息，请访问 &lt;a class="link" href="https://github.com/joaomdmoura/crewAI" target="_blank" rel="noopener" >CrewAI
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h3 id="opendevin">OpenDevin&lt;/h3>
&lt;a href="https://github.com/opendevin/opendevin" target="_blank" class="card-github fetch-waiting no-styling"
repo="opendevin/opendevin" id="repo-5GGGYdG5RLhGtGsB-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-5GGGYdG5RLhGtGsB-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">opendevin&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">opendevin&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-5GGGYdG5RLhGtGsB-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-5GGGYdG5RLhGtGsB-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-5GGGYdG5RLhGtGsB-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-5GGGYdG5RLhGtGsB-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-5GGGYdG5RLhGtGsB-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-5GGGYdG5RLhGtGsB-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/opendevin\/opendevin', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-5GGGYdG5RLhGtGsB-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-5GGGYdG5RLhGtGsB-language').innerText = data.language;
document.getElementById('repo-5GGGYdG5RLhGtGsB-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-5GGGYdG5RLhGtGsB-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-5GGGYdG5RLhGtGsB-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-5GGGYdG5RLhGtGsB-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-5GGGYdG5RLhGtGsB-license').classList.add = "no-license"
};
document.getElementById('repo-5GGGYdG5RLhGtGsB-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for opendevin\/opendevin.")
}).catch(err => {
const c = document.getElementById('repo-5GGGYdG5RLhGtGsB-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for opendevin\/opendevin.")
})
&lt;/script>
&lt;p>OpenDevin 是一个由人工智能和 LLMs 驱动的自主软件工程师平台。&lt;/p>
&lt;p>OpenDevin 智能体与人类开发人员合作编写代码、修复错误和发布功能。&lt;/p>
&lt;p>详细信息，请访问 &lt;a class="link" href="https://github.com/opendevin/opendevin" target="_blank" rel="noopener" >OpenDevin
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的项目主页。&lt;/p>
&lt;h2 id="模型评测">模型评测&lt;/h2>
&lt;p>为了选择合适的 LLM 并评估其性能，我们需要进行模型评测：&lt;/p>
&lt;h3 id="lmsys">LMSys&lt;/h3>
&lt;p>LMSys Org 是由加州大学伯克利分校的学生和教师与加州大学圣地亚哥分校以及卡内基梅隆大学合作成立的开放式研究组织。&lt;/p>
&lt;p>目标是通过共同开发开放模型、数据集、系统和评估工具，使大型模型对每个人都可访问。训练大型语言模型并广泛提供它们的应用，同时也在开发分布式系统以加速它们的训练和推理过程。&lt;/p>
&lt;p>目前，LMSys Chatbot Area 是最被认可的大模型排行榜之一，受多家公司和研究机构的认可。&lt;/p>
&lt;p>排行榜地址：&lt;a class="link" href="https://arena.lmsys.org/" target="_blank" rel="noopener" >https://arena.lmsys.org/
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h3 id="opencompass">OpenCompass&lt;/h3>
&lt;p>OpenCompass 是一个 LLM 评估平台，支持 100 多个数据集上的各种模型（Llama3、Mistral、InternLM2、GPT-4、LLaMa2、Qwen、GLM、Claude 等）。&lt;/p>
&lt;a href="https://github.com/open-compass/opencompass" target="_blank" class="card-github fetch-waiting no-styling"
repo="open-compass/opencompass" id="repo-FWR4G6AugDRExSh1-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-FWR4G6AugDRExSh1-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">open-compass&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">opencompass&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-FWR4G6AugDRExSh1-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-FWR4G6AugDRExSh1-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-FWR4G6AugDRExSh1-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-FWR4G6AugDRExSh1-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-FWR4G6AugDRExSh1-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-FWR4G6AugDRExSh1-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/open-compass\/opencompass', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-FWR4G6AugDRExSh1-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-FWR4G6AugDRExSh1-language').innerText = data.language;
document.getElementById('repo-FWR4G6AugDRExSh1-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-FWR4G6AugDRExSh1-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-FWR4G6AugDRExSh1-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-FWR4G6AugDRExSh1-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-FWR4G6AugDRExSh1-license').classList.add = "no-license"
};
document.getElementById('repo-FWR4G6AugDRExSh1-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for open-compass\/opencompass.")
}).catch(err => {
const c = document.getElementById('repo-FWR4G6AugDRExSh1-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for open-compass\/opencompass.")
})
&lt;/script>
&lt;h3 id="open-llm-leaderboard">Open LLM Leaderboard&lt;/h3>
&lt;p>Open LLM Leaderboard 是一个持续更新的 LLM 排行榜，根据多个评测指标对不同模型进行排名，方便开发者了解最新的模型性能和发展趋势。&lt;/p>
&lt;p>排行榜地址：&lt;a class="link" href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard" target="_blank" rel="noopener" >https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>LLM 生态正在蓬勃发展，涵盖了从模型训练到应用落地的各个环节。相信随着技术的不断进步，LLM 将会在更多领域发挥重要作用，为我们带来更加智能的应用体验。&lt;/p></description></item><item><title>RDMA 之 Memory Window</title><link>https://cuterwrite.top/p/rdma-memory-window/</link><pubDate>Wed, 26 Jun 2024 23:55:00 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-memory-window/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116373724_p0_master1200.webp" alt="Featured image of post RDMA 之 Memory Window" />&lt;h1 id="rdma-之-memory-window">RDMA 之 Memory Window&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/353590347">&lt;cite>知乎专栏：14. RDMA 之 Memory Window&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>&lt;strong>本文是“RDMA 杂谈”专栏文章的第 14 篇，欢迎转载，转载请注明出处&lt;/strong>&lt;/p>
&lt;p>我们在 &lt;a class="link" href="https://cuterwrite.top/p/rdma-mr/" >【RDMA 之 Memory Region】
&lt;/a>
一文中介绍过 Memory Region，它是一片由用户注册的特殊的内存区域：一方面其中的内容不会被换页到硬盘中，另一方面 RDMA 网卡中记录了它的地址转换关系，使得硬件拿到用户指定在 WR 中的虚拟地址之后找到对应的物理地址。&lt;/p>
&lt;p>本文我们来讲解 Memory Window 的概念，它是一种基于 Memory Region 的、更灵活的内存管理单元。除了 MW 的概念之外，本文也会更详细的介绍一些 RDMA 领域的内存相关概念，比如 L_Key/R_Key 等。本文配合 &lt;a class="link" href="https://cuterwrite.top/p/rdma-mr/" >【RDMA 之 Memory Region】
&lt;/a>
阅读效果更佳，建议先读者温习一下。&lt;/p>
&lt;h2 id="memory-window-是什么">Memory Window 是什么&lt;/h2>
&lt;p>Memory Window 简称 MW，中文就翻译成内存窗口吧。是一种由用户申请的，用于让远端节点访问本端内存区域的 RDMA 资源。每个 MW 都会绑定（称为 bind）在一个已经注册的 MR 上，但是它相比于 MR 可以提供更灵活的权限控制。MW 可以粗略理解为是 MR 的子集，一个 MR 上可以划分出很多 MW，每个 MW 都可以设置自己的权限。MW 和 MR 的关系如下图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_1.webp"
alt="2024-06-28_12_1" width="30%" loading="lazy">&lt;figcaption>
&lt;h4>MR 与 MW 的关系&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h2 id="内存的访问权限控制">内存的访问权限控制&lt;/h2>
&lt;p>为了后文说明为何设计 MW，我们先来把 MR 和 MW 都涉及的权限控制讲解一下。&lt;/p>
&lt;h3 id="mrmw-的权限配置">MR/MW 的权限配置&lt;/h3>
&lt;p>这里的权限，指的是本端/对端节点，对于本端内存的读/写权限，它们两两组合形成了四种权限：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>本端&lt;/th>
&lt;th>对端&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>读&lt;/td>
&lt;td>Local Read&lt;/td>
&lt;td>Remote Read&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>写&lt;/td>
&lt;td>Local Write&lt;/td>
&lt;td>Remote Write&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>除了这四种权限之外，还有 Atomic 权限等，不在本文讨论范围内。&lt;/p>
&lt;p>上表中这四种权限中最低的是本地读（Local Read），是用户必须赋予 MR/MW 的权限，因为如果一块内存本地的用户都无法访问的话，那就失去意义了；另外还有个限制，如果某个 MR 需要配置远端写（Remote Write）或者还没介绍的远端原子操作权限（Remote Atomic），那么也一定要配置本地写（Local Write）权限。在此约束之下，每个 MR 或者 MW 都可以按需配置权限，比如我们注册的一个 MR 需要允许远端节点写入数据，而不允许读，那么我们就打开 Remote Write 权限，关闭 Remote Read 权限。这样 HCA（网卡）收到对端发起的对这个 MR 范围内的某个地址的 WRITE 请求之后，就可以予以放行；而 HCA 收到对端对这个 MR 的 READ 操作时，就会拒绝这个请求，并返回错误信息给对端。&lt;/p>
&lt;h3 id="memory-key">Memory Key&lt;/h3>
&lt;p>上述的访问权限配置，并不能杜绝恶意用户对于本地或者远端内存的访问。比如某个节点给了一块内存区域的 Remote Write 权限，那么岂不是任意远端节点（进程）只要传入了合法的地址信息，都可以对这片区域进行写入了？因此，IB 规范设计了 Memory Key，简单理解它就是访问 MR 的钥匙机制，只有持有正确的钥匙，才能打开 MR/MW 的大门。&lt;/p>
&lt;p>Key 是一串数字，由两部分组成：24bit 的 Index 以及 8bit 的 Key：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_2.webp"
alt="2024-06-28_12_2" width="60%" loading="lazy">&lt;figcaption>
&lt;h4>L_Key/R_Key 的组成&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>其中，Index 用于 HCA 快速索引到本地的虚拟-物理地址转换表等 MR 相关的信息，而 Key 用于校验整个字段的合法性，以防止未授权的用户任意传递 Index。&lt;/p>
&lt;p>Memory Key 按照用途分为两种，Local Key 和 Remote Key：&lt;/p>
&lt;h4 id="l_key">L_Key&lt;/h4>
&lt;p>即 Local Key，关联到一个 MR 上，用于 HCA 访问本端内存。当本端的某个进程试图使用一个已经注册的 MR 的内存时，HCA 会校验其传递的 L_Key。并且利用 L_Key 中的索引查找地址转换表，把虚拟地址翻译成物理地址然后访问内存。&lt;/p>
&lt;p>我们在 &lt;a class="link" href="https://cuterwrite.top/p/rdma-shared-receive-queue/" >【RDMA 之 Shared Receive Queue】
&lt;/a>
一文中描述过 sge，sge 由起始地址、长度和秘钥组成。用户在填写 WR 时，如果需要 HCA 访问本端内存，那么就需要通过一个 sge 的链表（sgl）来描述内存块，这里 sge 的秘钥填的就是 L_Key，也就是下图中的 key1 和 key3，他们分别是 MR1 的 L_Key 和 MR2 的 L_Key。如果没有 L_Key，那么任何一个本地用户进程都可以指挥硬件访问其他本地用户注册的 MR 的内容，硬件也难以高效的将虚拟地址翻译成物理地址。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_3.webp"
alt="2024-06-28_12_3" width="50%" loading="lazy">&lt;figcaption>
&lt;h4>L_Key 的作用&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h4 id="r_key">R_Key&lt;/h4>
&lt;p>即 Remote Key，关联到一个 MR 或者 MW 上，用于远端节点访问本端内存。当远端节点试图访问本端的内存时，一方面本端的 HCA 会校验 R_Key 是否合法，另一方面会利用 R_Key 中的索引查地址转换表，把虚拟地址翻译成物理地址然后访问内存。&lt;/p>
&lt;p>凡是 RDMA 操作（即 Write/Read/Atomic），用户都要在 WR 中携带远端内存区域的 R_Key。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_4.webp"
alt="2024-06-28_12_4" width="70%" loading="lazy">&lt;figcaption>
&lt;h4>R_Key 的作用&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>IB 规范通过上述两种机制，来确保 MR 可以按照用户的期望被正确且安全的访问。我们用一个比喻来总结下 MR/MW 权限控制相关的内容：&lt;/p>
&lt;p>A 给自己的房间（MR）配了两把钥匙（Memory Key），一把留作自用（L_Key），另一把钥匙（R_Key）邮寄（可以是任何通信方式）给了 B。B 可以在 A 不在家的时候（本端 CPU 不感知远端节点对本地内存的 RDMA 操作），通过钥匙（R_Key）打开门。打开门之后，可能 B 只能隔着玻璃查看房间的摆设（A 只给了这个 MR 远程读权限），或者进入房间内发现漆黑一片什么也看不到，但是可以向房间里放物品（A 只给了这个 MR 远程写权限），当然也有可能没有玻璃也开了灯（同时给了远程读写权限）。&lt;/p>
&lt;h2 id="为什么要有-mw">为什么要有 MW&lt;/h2>
&lt;p>简而言之，设计 MW 的目的就是想更灵活的控制内存的远程访问权限。&lt;/p>
&lt;p>&lt;a class="link" href="https://cuterwrite.top/p/rdma-mr/" >【RDMA 之 Memory Region】
&lt;/a>
一文中我们介绍过用户注册 MR 的过程，需要从用户态陷入内核态，调用内核提供的函数 pin 住内存（防止换页），然后制作虚拟-物理地址映射表并下发给硬件。&lt;/p>
&lt;p>因为 MR 是由内核管理的，如果用户想修改一个已经存在的 MR 的信息，比如我想收回某个 MR 的远端写权限，只保留远端读权限；或者想要使一个之前已经授权给远端节点的 R_Key 失效，那么用户需要通过重注册 MR（Reregister MR）接口来进行修改，该接口等价于先取消注册 MR（Deregister MR），然后注册 MR（Register MR）。&lt;strong>上述流程需要陷入内核态来完成，而这个过程是耗时较长的&lt;/strong>。&lt;/p>
&lt;p>不同于需要通过控制路径修改权限的 MR，&lt;strong>MW 在创建好之后，可以通过数据路径（即通过用户态直接下发 WR 到硬件的方式）动态的绑定到一个已经注册的 MR 上，并同时设置或者更改其访问权限，这个过程的速度远远超过重新注册 MR 的过程&lt;/strong>。&lt;/p>
&lt;p>那么现在为了使一片内存能够被远端节点进行 RDMA WRITE/READ 操作，我们就拥有了注册 MR 以及注册 MW 然后绑定到一个已注册的 MR 两种方式，它们都会产生一个 R_Key 来提供给远端节点。前一种方式准备阶段的步骤简单，但是不够灵活，一旦注册之后修改起来会比较麻烦；后一种方式相比前一种多了注册 MW 和绑定 MW 到 MR 两种操作，但是可以方便迅速的控制远端访问权限。&lt;/p>
&lt;h2 id="mw-和-mr-权限的关系">MW 和 MR 权限的关系&lt;/h2>
&lt;p>也许有的读者会想到，MR 申请时配置了自己的权限，MW 绑定到 MR 时也会配置自己的权限，这两者的权限是什么关系呢？IB 规范在 10.6.7.2.2 节有专门介绍：&lt;/p>
&lt;blockquote>
&lt;p>When binding a Memory Window, a Consumer can request any combination of remote access rights for the Window. However, if the associated Region does not have local write access enabled and the Consumer requests remote write or remote atomic access for the Window, the Channel Interface must return an error either at bind time or access time.&lt;/p>
&lt;/blockquote>
&lt;p>总结来说，&lt;strong>如果想要给 MW 配置远程写或者远程原子操作（Atomic）权限，那么它绑定到的 MR 必须有本地写权限，其他情况下两者权限互不干扰&lt;/strong>：远端用户用 MW，就要遵循 MW 的权限配置；远端用户用 MR，就要遵循 MR 的权限配置。&lt;/p>
&lt;h2 id="用户接口">用户接口&lt;/h2>
&lt;p>老样子，用户接口时我们按照控制路径和数据路径来分类：&lt;/p>
&lt;h3 id="控制路径">控制路径&lt;/h3>
&lt;p>MW 支持增、删和查，不能直接修改：&lt;/p>
&lt;h4 id="创建allocate-mw">创建——Allocate MW&lt;/h4>
&lt;p>申请 MW，主要是创建 MW 相关的软件结构和让硬件做好准备，用户需要指定后文中介绍的 MW 的类型。这个接口会产生一个 Memory Window 的句柄，用户以后可以用这个句柄指代这个 MW。&lt;/p>
&lt;p>注意此时 MW 没有绑定到 MR 上，处于不可从远端访问的状态。&lt;/p>
&lt;h4 id="删除deallocate-mw">删除——Deallocate MW&lt;/h4>
&lt;p>取消注册 MW。很好理解，就是销毁相关资源。&lt;/p>
&lt;h4 id="查询query-mw">查询——Query MW&lt;/h4>
&lt;p>查询 MW 的信息，包括 R_Key 及其状态、MW 类型以及 PD 等。&lt;/p>
&lt;p>需要再次强调的是，虽然这个 Verbs 在 IB 规范中有描述，但是并没有在 RDMA 软件栈中实现相关的 API。类似情况的 Verbs 接口还有不少，RDMA 软件栈以实用为原则，没有用户需求的接口一般都没有实现。&lt;/p>
&lt;h3 id="数据路径">数据路径&lt;/h3>
&lt;p>MW 在数据路径有一套独特的接口，分为 Bind 和 Invalidate 两类：&lt;/p>
&lt;h4 id="绑定bind">绑定——Bind&lt;/h4>
&lt;p>Bind(ing)意为“绑定”，指的是将一个 MW“关联”到一个已经注册的 MR 的指定范围上，并配置一定的读写权限。绑定的结果会产生一个 R_key，用户可以把这个 R_Key 传递给远端节点用于远程访问。注意一个 MW 可以被多次绑定，一个 MR 上也可以绑定多个 MW。如果一个 MR 还有被绑定的 MW，那么这个 MR 是不能被取消注册的。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_5.webp"
alt="2024-06-28_12_5" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>Bind 的软硬件交互&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>Bind 有两种方式，一种是调用 Post Send 接口下发 Bind MW WR，一种是调用 Bind MW 接口。&lt;/p>
&lt;ul>
&lt;li>Post Send Bind MW WR&lt;/li>
&lt;/ul>
&lt;p>前文我们讲过，相比于 MR，MW 最大的优势就是可以从数据路径快速的配置权限。Post Send Bind MW WR 操作，指的就是用户通过 post send 接口（比如 ibv_post_send()）下发一个 WR 到 SQ 中，这个 WR 的操作类型（比如 SEND/RDMA WRITE/RDMA READ）被指定为 BIND MW，此外 WR 中还携带有权限和要绑定到的 MR 的范围信息。与其他 WR 不同，下发 Bind MW 的 WR 之后，硬件并不会发送任何数据包，而是将 MW 绑定到了指定 MR 上。&lt;/p>
&lt;p>这种方式仅适用于后文介绍的 Type 2 的 MW。&lt;/p>
&lt;ul>
&lt;li>Bind MW&lt;/li>
&lt;/ul>
&lt;p>虽然这是一个独立的接口，但是实际是在 Post Send Bind MW WR 外面又封装了一层。用户传入 MW 绑定的相关信息，包括权限及要绑定的 MR 的信息，驱动程序负责组装和下发 WR 到硬件中。该接口成功后，会将新生成的 R_Key 返回给用户。&lt;/p>
&lt;p>这种方式仅适用于后文介绍的 Type 1 的 MW。&lt;/p>
&lt;p>上述两种操作的关系是这样的：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_6.webp"
alt="2024-06-28_12_6" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>两种 Bind 操作的关系&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h4 id="无效化invalidate">无效化——Invalidate&lt;/h4>
&lt;p>Invalidate 意为无效化，指的是用户通过下发一个带有 Invalidate 操作码的 WR 到硬件而使一个 R_Key 无效的操作。&lt;/p>
&lt;p>&lt;strong>需要强调的是，Invalidate 操作的对象是 R_Key 而不是 MW 本身，即 Invalidate 之后的效果是：远端用户无法再使用这个 R_Key 访问对应的 MW，而 MW 资源仍然存在，以后仍然可以生成新的 R_Key 给远端使用。&lt;/strong>&lt;/p>
&lt;p>Invalidate 操作只能用于下文介绍的 Type 2 的 MW。&lt;/p>
&lt;p>按照 Invalidate 操作的发起方不同，又可以进一步分成两种：&lt;/p>
&lt;ul>
&lt;li>Local Invalidate&lt;/li>
&lt;/ul>
&lt;p>本地无效操作。上层用户如果想在不回收 MW 资源的情况下，收回某个远端的用户的 R_Key 的权限。那么就可以下发一个 Local Invalidate 操作到 SQ 中，硬件收到之后会对相应的 MR 的配置进行修改。成功执行之后，如果持有这个 R_Key 的远端用户想要对 MW 进行 RDMA 操作，将会被本地的硬件拒绝并返回错误。&lt;/p>
&lt;p>因为是本地操作，所以硬件收到这个 WR 之后也不会发送消息到链路上。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_7.webp"
alt="2024-06-28_12_7" width="60%" loading="lazy">&lt;figcaption>
&lt;h4>Local Invalidate 操作的软硬件交互&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>Remote Invalidate&lt;/li>
&lt;/ul>
&lt;p>远端无效操作。当一个远端用户不再使用一个 R_Key 之后，可以主动发送消息，让本端回收这个 R_Key。远端用户下发一个带有此操作码的 WR 到 SQ 中，其硬件收到后，将会组装一个报文并发送到本端。本端硬件收到远端的 Remote Invalidate 操作之后，将会把对应的 R_Key 置为不可用状态。同 Local Invalidate 一样，此后对端将无法使用这个 R_Key 对对应的 MW 进行 RDMA 操作。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_12_8.webp"
alt="2024-06-28_12_8" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>Remote Invalidate 操作的软硬件交互&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h2 id="mw-的类型">MW 的类型&lt;/h2>
&lt;p>根据实现和应用场景的不同，IB 规范对 MW 进行了分类：&lt;/p>
&lt;h3 id="type-1">Type 1&lt;/h3>
&lt;p>Type 1 的 MW 通过 PD 和一个 QP 关联，不会绑定到一个 QP 上，所以也不会影响销毁同一个 PD 下的 QP。&lt;/p>
&lt;p>Type 1 的 MW 的 R_Key 的 key 域段由驱动和硬件掌握，这里“掌握”的意思是，由驱动和硬件分配 key，而不是上层用户。这也是前文中说 Type 1 的 MW 不能被执行 Invalidate 操作的原因。如果 Type 1 MW 的用户想要使一个 R_Key 失效，那么重新通过 Bind MW 接口绑定一次这个 MW，硬件或者驱动就回自动分配一个新的 R_Key 的 key 域段，原有的 R_Key 也就失效了。&lt;/p>
&lt;p>此外，如果用户暂时想要使一个 MW 不再绑定到任何 MR，但是又想保留相关的资源而不是销毁这个 MW，那么可以通过调用 Bind MW 接口，并将 MW 长度设置为 0 来实现。&lt;/p>
&lt;p>IB 规范允许多个 Type 1 MW 绑定到同一个 MR 上，范围可以相互覆盖。&lt;/p>
&lt;h3 id="type-2">Type 2&lt;/h3>
&lt;p>Type 2 的 MW 赋予了用户更大的自由度，其 R_Key 的 key 域段由用户掌握，即用户想怎么分配就怎么分配。前文已经讲过，用户通过 Post Send Bind MW WR 操作来进行绑定，这个过程并不会返回 R_Key。用户必须记住 Allocate MW 时的 index，并且和其选择的 8 bit key 组成 R_Key 并发送给对端。&lt;/p>
&lt;p>用户可以通过前文介绍过的 Invalidate 操作来使一个 R_Key 无效，如果想要分配一个新的 R_Key 到 MW 上，必须先通过 Invalidate 操作无效之前的 R_Key。&lt;/p>
&lt;p>与 Type 1 不同，Type 2 的 MW 不支持 0 长度的绑定。&lt;/p>
&lt;p>IB 规范同样也允许多个 Type 2 绑定到同一个 MR 上，范围可以相互覆盖。&lt;/p>
&lt;p>此外，根据绑定关系不同，Type 2 还可以分为两种实现方式，它们的差异仅在于和 QP 的绑定关系上。&lt;/p>
&lt;h4 id="type-2a">Type 2A&lt;/h4>
&lt;p>通过 QPN 和一个 QP 关联，也就是说远端访问这个 MW 范围内的内存时候，除了 R_Key 之外，还必须指定正确的 QPN。如果一个 QP 上还有绑定的 Type 2A 的 MW，那么这个 QP 不可以被销毁。&lt;/p>
&lt;h4 id="type-2b">Type 2B&lt;/h4>
&lt;p>通过 QPN 和 PD 与一个 QP 关联，比 Type 2A 多了个 PD 的校验，即远端通过 RDMA 操作访问 MW 的内存时，除了 QPN 要正确之外，其指定的本端 QP 的 PD 要与绑定这个 MW 时的 PD 相同。另外，与 Type 2A 不同，QP 如果还有 Type 2B MW 绑定关系时是可以被销毁的。&lt;/p>
&lt;p>这里 IB 规范中原有的介绍就比较分散，我们来简单总结一下几种 MW 的异同：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Type 1&lt;/th>
&lt;th>Type 2A&lt;/th>
&lt;th>Type 2B&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>关联关系&lt;/td>
&lt;td>PD&lt;/td>
&lt;td>QP&lt;/td>
&lt;td>PD + QP&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>R_Key 的 key 域归属&lt;/td>
&lt;td>驱动+硬件&lt;/td>
&lt;td>用户&lt;/td>
&lt;td>用户&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>绑定方式&lt;/td>
&lt;td>Bind MW 绑定后之前的 R_Key 自动失效&lt;/td>
&lt;td>Post Send Bind MWWR 绑定前需要先使之前的 R_Key 无效化&lt;/td>
&lt;td>Post Send Bind MWWR 绑定前需要先使之前的 R_Key 无效化&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>是否支持零长度&lt;/td>
&lt;td>是&lt;/td>
&lt;td>否&lt;/td>
&lt;td>否&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>是否支持 Invalidate&lt;/td>
&lt;td>否&lt;/td>
&lt;td>是&lt;/td>
&lt;td>是&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>关联的 QP 是否可以被销毁&lt;/td>
&lt;td>-&lt;/td>
&lt;td>否&lt;/td>
&lt;td>是&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>此外，IB 规范中对上述几种类型还有如下描述：HCA 必须实现 Type 1 的 MW，另外可以仅选择实现 Type 2A 和 2B 中的一种。Type 1 和 Type 2 的 MW 可以同时关联到同一个 MR 上。因为我了解到的应用程序中使用 MW 的情况不多，所以具体在什么场景下应该使用哪种 MW 也说不出所以然来，如果读者有对这方面的了解欢迎一起交流。&lt;/p>
&lt;p>好了，MW 就讲到这里，到此为止 RDMA 技术中常见的资源就都介绍完了。&lt;/p>
&lt;p>鉴于一般支持 RDMA 的设备都比较昂贵，下一篇我将介绍如何通过软件模拟设备的方式——即 Soft-RoCE 进行一些编程实验。&lt;/p>
&lt;h2 id="ib-规范相关章节">IB 规范相关章节&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>3.5.3 Memory Keys 介绍&lt;/p>
&lt;/li>
&lt;li>
&lt;p>9.4.1.1 Invalidate 操作&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.6.7 权限管理&lt;/p>
&lt;/li>
&lt;li>
&lt;p>11.2.10.9~12 相关 Verbs 介绍&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="参考文档">参考文档&lt;/h2>
&lt;p>[1] IB Specification Vol 1-Release-1.4&lt;/p>
&lt;p>[2] Linux Kernel Networking - Implementation and Theory. Chapter 13&lt;/p></description></item><item><title>RDMA 之 Shared Receive Queue</title><link>https://cuterwrite.top/p/rdma-shared-receive-queue/</link><pubDate>Wed, 26 Jun 2024 23:34:00 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-shared-receive-queue/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116373922_p0_master1200.webp" alt="Featured image of post RDMA 之 Shared Receive Queue" />&lt;h1 id="rdma-之-shared-receive-queue">RDMA 之 Shared Receive Queue&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/279904125">&lt;cite>知乎专栏：11. RDMA 之 Shared Receive Queue&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>我们曾在 &lt;a class="link" href="https://cuterwrite.top/p/rdma-element/" >【3. RDMA 基本元素】
&lt;/a>
中简单介绍了 SRQ 的概念，本文将带大家了解更多关于 SRQ 的细节。&lt;/p>
&lt;h2 id="基本概念">基本概念&lt;/h2>
&lt;h3 id="什么是-srq">什么是 SRQ&lt;/h3>
&lt;p>全称为 Shared Receive Queue，直译为共享接收队列。我们知道，RDMA 通信的基本单位是 QP，每个 QP 都由一个发送队列 SQ 和接收队列 RQ 组成。&lt;/p>
&lt;p>SRQ 是 IB 协议为了给接收端节省资源而设计的。我们可以把一个 RQ 共享给所有关联的 QP 使用，这个公用的 RQ 就称为 SRQ。当与其关联的 QP 想要下发接收 WQE 时，都填写到这个 SRQ 中。然后每当硬件接收到数据后，就根据 SRQ 中的下一个 WQE 的内容把数据存放到指定位置。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_11_1.webp"
alt="2024-06-28_11_1" width="80%" loading="lazy">
&lt;/figure>
&lt;h3 id="为什么要用-srq">为什么要用 SRQ&lt;/h3>
&lt;p>通常情况下，我们向 SQ 中下任务的数量要远远超过向 RQ 中下发任务的数量。为什么呢？请先回忆一下哪些操作类型会用到 SQ，哪些又会用到 RQ。&lt;/p>
&lt;p>SEND/WRITE/READ 都需要通信发起方向 SQ 中下发一个 WR，而只有和 SEND 配合的 RECV 操作才需要通信响应方下发 WR 到 RQ 中（带立即数的 Write 操作也会消耗 Receive WR，我们还没讲到）。而我们又知道，SEND-RECV 这一对操作通常都是用于传递控制信息，WRITE 和 READ 才是进行大量远端内存读写操作时的主角，所以自然 SQ 的使用率是远远高于 RQ 的。&lt;/p>
&lt;p>每个队列都是有实体的，占用着内存以及网卡的片上存储空间。在商用场景下，QP 的数量是可能达到十万级甚至更高的，对内存容量提出了很高的要求，内存都是白花花的银子买的，SRQ 就是 IB 协议为了节省用户的内存而设计的一种机制。&lt;/p>
&lt;p>来看一下协议中对为什么要使用 SRQ 的官方解释（10.2.9.1 章节）：&lt;/p>
&lt;blockquote>
&lt;p>Without SRQ, an RC, UC or UD Consumer must post the number of receive WRs necessary to handle incoming receives on a given QP. If the Consumer cannot predict the incoming rate on a given QP, because, for example, the connection has a bursty nature, the Consumer must either: post a sufficient number of RQ WRs to handle the highest incoming rate for each connection, or, for RC, let message flow control cause the remote sender to back off until local Consumer posts more WRs.&lt;/p>
&lt;p>• Posting sufficient WRs on each QP to hold the possible incoming rate, wastes WQEs, and the associated Data Segments, when the Receive Queue is inactive. Furthermore, the HCA doesn’t provide a way of reclaiming these WQEs for use on other connections.&lt;/p>
&lt;p>• Letting the RC message flow control cause the remote sender to back off can add unnecessary latencies, specially if the local Consumer is unaware that the RQ is starving.&lt;/p>
&lt;/blockquote>
&lt;p>简单来说，就是没有 SRQ 的情况下，因为 RC/UC/UD 的接收方不知道对端什么时候会发送过来多少数据，所以必须做好最坏的打算，做好突发性收到大量数据的准备，也就是向 RQ 中下发足量的接收 WQE；另外 RC 服务类型可以利用流控机制来反压发送方，也就是告诉对端”我这边 RQ WQE 不够了“，这样发送端就会暂时放缓或停止发送数据。&lt;/p>
&lt;p>但是正如我们前文所说，第一种方法由于是为最坏情况准备的，大部分时候有大量的 RQ WQE 处于空闲状态未被使用，这对内存是一种极大地浪费；第二种方法虽然不用下发那么多 RQ WQE 了，但是流控是有代价的，即会增加通信时延。&lt;/p>
&lt;p>而 SRQ 通过允许很多 QP 共享接收 WQE（以及用于存放数据的内存空间）来解决了上面的问题。当任何一个 QP 收到消息后，硬件会从 SRQ 中取出一个 WQE，根据其内容存放接收到的数据，然后硬件通过 Completion Queue 来返回接收任务的完成信息给对应的上层用户。&lt;/p>
&lt;p>我们来看一下使用 SRQ 比使用普通的 RQ 可以节省多少内存&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>：&lt;/p>
&lt;p>假设接受数据的节点上有 N 对 QP，并且每个 QP 都可能在随机的时间收到连续的 M 个消息（每个消息都需要消耗一个 RQ 中的 WQE），&lt;/p>
&lt;ul>
&lt;li>如果不使用 SRQ 的话，用户一共需要下发 N * M 个 RQ WQE。&lt;/li>
&lt;li>如果使用 SRQ 的话，用户只需要下发 K * M 个 RQ WQE，而 K 远小于 N。&lt;/li>
&lt;/ul>
&lt;p>这个 K 是可以由用户根据业务来配置的，如果存在大量的并发接收的情况，那么就把 K 设置大一点，否则 K 设置成个位数就足够应付一般的情况了。&lt;/p>
&lt;p>我们一共节省了 (N - K) * M 个 RQ WQE，RQ WQE 本身其实不是很大，大约在几个 KB 的样子，看起来好像占不了多少内存。但是如前文所说，实际上节省的还有用于&lt;strong>存放数据的内存空间&lt;/strong>，这可是很大一块内存了，我们用图来说明：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_11_2.webp"
alt="2024-06-28_11_2" width="80%" loading="lazy">
&lt;/figure>
&lt;p>上图中的 SRQ 中有两个 RQ WQE，我们看一下 RQ WQE 的内容，它们是由数个 sge（Scatter/Gather Element）组成的，每个 sge 由一个内存地址，长度和秘钥组成。有了起始地址和长度，sge 就可以指向一块连续的内存区域，那么多个 sge 就可以表示多个彼此离散的连续内存块，我们称多个 sge 为 sgl（Scatter/Gather List）。sge 在 IB 软件协议栈中随处可见（其实在整个 Linux 都很常见），可以用非常少的空间表示非常大的内存区域，IB 的用户都使用 sge 来指定发送和接收区域的。&lt;/p>
&lt;p>可以简单估算下每个 sge 可以指向多大的内存区域，length 是一个 32bit 的无符号整型，可以表示 4GB 的空间。假设一个 RQ WQE 最大可以存放 256 个 sge，那么一个 RQ WQE 一共就是 1TB。当然实际上不可能这么大，这里只是想直观的告诉读者 RQ WQE 背后可能占用着多大的内存空间。&lt;/p>
&lt;h3 id="srqc">SRQC&lt;/h3>
&lt;p>即 SRQ Context。同 QPC 一样，SRQC 是用来告知硬件跟 SRQ 有关的属性的，包括深度、WQE 大小等信息，本文不再赘述了。&lt;/p>
&lt;h3 id="srqn">SRQN&lt;/h3>
&lt;p>即 SRQ Number。同 QP 一样，每个节点中可能存在多个 SRQ，为了标识和区分这些 SRQ，每个 SRQ 都有一个序号，称为 SRQN。&lt;/p>
&lt;h3 id="srq-的-pd">SRQ 的 PD&lt;/h3>
&lt;p>我们在 &lt;a class="link" href="https://cuterwrite.top/p/rdma-protection-domain/" >【7. RDMA 之 Protection Domain】
&lt;/a>
中介绍过 Protection Domain 的概念，它用来隔离不同的 RDMA 资源。每个 SRQ 都必须指定一个自己的 PD，可以跟自己关联的 QP 的 PD 相同，也可以不同；SRQ 之间也可以使用相同的 PD。&lt;/p>
&lt;p>如果在使用 SRQ 的时候，收到了数据包，那么只有在要访问的 MR 和 SRQ 处于同一个 PD 下，才会正常接收这个数据包，否则会产生立即错误。&lt;/p>
&lt;h2 id="异步事件">异步事件&lt;/h2>
&lt;p>我们在 &lt;a class="link" href="https://cuterwrite.top/p/rdma-completion-queue/" >【10. RDMA 之 Completion Queue】
&lt;/a>
一文中介绍过，IB 协议根据错误的上报方式将错误类型分为立即错误，完成错误和异步错误。其中的异步错误类似于中断/事件，所以我们有时候也称其为异步事件。每个 HCA 都会注册一个事件处理函数专门用来处理异步事件，收到异步事件后，驱动程序会对其进行必要的处理和进一步上报给用户。&lt;/p>
&lt;p>关于 SRQ 有一个特殊的异步事件，用来及时通知上层用户 SRQ 的状态，即 SRQ Limit Reached 事件。&lt;/p>
&lt;h3 id="srq-limit">SRQ Limit&lt;/h3>
&lt;p>SRQ 可以设置一个水线/阈值，当队列中剩余的 WQE 数量小于水线时，这个 SRQ 会就上报一个异步事件。提醒用户“队列中的 WQE 快用完了，请下发更多 WQE 以防没有地方接收新的数据”。这个水线/阈值就被称为 SRQ Limit，这个上报的事件就被称为 SRQ Limit Reached。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_11_3.webp"
alt="2024-06-28_11_3" width="30%" loading="lazy">
&lt;/figure>
&lt;p>因为 SRQ 是多个 QP 共享的，所以如果深度比较小的情况下，很有可能突然里面的 WQE 就用完了。所以协议设计了这种机制，来保证用户能够及时干预 WQE 不够的情况。&lt;/p>
&lt;p>上报异步事件之后，SRQ Limit 的值会被硬件重新设置为 0（应该是为了防止一直上报异步事件给上层）。当然用户可以不使用这个机制，只需要将 SRQ Limit 的值设为 0 即可。&lt;/p>
&lt;h2 id="用户接口">用户接口&lt;/h2>
&lt;h3 id="控制面">控制面&lt;/h3>
&lt;p>还是老四样——“增、删、改、查”：&lt;/p>
&lt;ul>
&lt;li>创建——Create SRQ&lt;/li>
&lt;/ul>
&lt;p>创建 SRQ 的时候，跟 QP 一样会申请所有 SRQ 相关的软硬件资源，比如驱动程序会申请 SRQN，申请 SRQC 的空间并向其中填写配置。创建 SRQ 时还必须指定每个 SRQ 的深度（能存放多少 WQE）以及每个 WQE 的最大 sge 数量。&lt;/p>
&lt;ul>
&lt;li>销毁——Destroy SRQ&lt;/li>
&lt;/ul>
&lt;p>销毁 SRQ 的所有相关软硬件资源。&lt;/p>
&lt;ul>
&lt;li>修改——Modify SRQ&lt;/li>
&lt;/ul>
&lt;p>除了 SRQ 深度等属性外，SRQ Limit 的值也是通过这个接口设置的。因为每次产生 SRQ Limit Reached 事件之后，水线的值都会被清零，所以每次都需要用户调用 Modify SRQ 重新设置水线。&lt;/p>
&lt;ul>
&lt;li>查询——Query SRQ&lt;/li>
&lt;/ul>
&lt;p>通常是用来查询水线的配置的。&lt;/p>
&lt;h3 id="数据面">数据面&lt;/h3>
&lt;h3 id="post-srq-receive">Post SRQ Receive&lt;/h3>
&lt;p>跟 Post Receive 一样，就是向 SRQ 中下发接收 WQE，里面包含了作为接收缓冲区的内存块的信息。需要注意的是，&lt;strong>主语是 SRQ，与 QP 没有任何关系&lt;/strong>，现在用户是不关心这个 SRQ 被哪些 QP 关联的。&lt;/p>
&lt;h2 id="srq-和-rq-的区别">SRQ 和 RQ 的区别&lt;/h2>
&lt;p>从功能上来说，SRQ 和 RQ 一样都是用来储存接收任务书的，但是由于 SRQ 的共享性，所以其和 RQ 有一些差异。&lt;/p>
&lt;h3 id="状态机">状态机&lt;/h3>
&lt;p>我们在 &lt;a class="link" href="https://cuterwrite.top/p/rdma-queue-pair/" >【9. RDMA 之 Queue Pair】
&lt;/a>
中介绍过，QP 有着复杂的状态机，不同的状态下 QP 的收发能力存在差异。而 SRQ 只有非错误和错误两种状态：&lt;/p>
&lt;p>无论是哪种状态下，用户都可以向 SRQ 中下发 WQE，但是在错误状态下，相关联的 QP 不能从这个 SRQ 中获得收到的数据。另外在错误状态下，用户也无法查询和修改 SRQ 的属性。&lt;/p>
&lt;p>QP 处于错误状态时，可以通过 Modify QP 来使其回到 RESET 状态，但是对 SRQ 来说，只能通过销毁它来退出错误状态。&lt;/p>
&lt;h3 id="接收流程">接收流程&lt;/h3>
&lt;p>对于一个 QP 来说，RQ 和 SRQ 不能同时使用，两者需选其一，如果对一个已经关联 SRQ 的 QP 的 RQ 下发 WQE，那么会返回一个立即错误。&lt;/p>
&lt;p>下面我们来对比看一下 SRQ 和 RQ 的接收流程。本小结的内容是本文的重点，相信读者看过之后，就对 SRQ 的机制有比较完整的了解了。&lt;/p>
&lt;h3 id="rq-的接收流程">RQ 的接收流程&lt;/h3>
&lt;p>首先，我们重温一下普通 RQ 的接收流程（结合发送端的完整流程请阅读 &lt;a class="link" href="https://cuterwrite.top/p/rdma-op/" >【4. RDMA 操作类型】
&lt;/a>
）一文）：&lt;/p>
&lt;ol start="0">
&lt;li>
&lt;p>创建 QP。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>通过 Post Recv 接口，用户分别向 QP2 和 QP3 的 RQ 下发接收 WQE，WQE 中包含接收到数据后放到哪块内存区域的信息。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>硬件收到数据。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>硬件发现是发给 QP3 的，那么从 QP3 的 RQ 中取出 WQE1，将接收到的数据放到 WQE1 指定的内存区域。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>硬件完成数据存放后，向 QP3 的 RQ 关联的 CQ3 产生一个 CQE，上报任务完成信息。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>用户从 CQ3 中取出 WC（CQE），然后从指定内存区域取走数据。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>硬件收到数据。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>硬件发现是发送给 QP2 的，那么从 QP2 的 RQ 中取出 WQE1，将接收到的数据放到 WQE1 指定的内存区域。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>硬件完成数据存放后，向 QP2 的 RQ 关联的 CQ2 产生一个 CQE，上报任务完成信息。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>用户从 CQ2 中取出 WC（CQE），然后从指定内存区域取走数据。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_11_4.webp"
alt="2024-06-28_11_4" width="90%" loading="lazy">
&lt;/figure>
&lt;h3 id="srq-的接收流程">SRQ 的接收流程&lt;/h3>
&lt;p>而 SRQ 的接收流程有一些区别：&lt;/p>
&lt;ol start="0">
&lt;li>
&lt;p>创建 SRQ1，并创建 QP2 和 QP3，都关联到 SRQ1 上。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>通过 Post SRQ Recv 接口，用户向 SRQ1 中下发两个接收 WQE，WQE 中包含接收到数据后放到哪块内存区域的信息。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>硬件收到数据。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>硬件发现是发给 QP3 的，从 SRQ1 中取出第一个 WQE（现在是 WQE1），根据 WQE 内容存放收到的数据。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>SRQ 中的每个 WQE 是“无主的“，不关联到任何一个 QP，硬件按队列顺序依次取出 WQE 就把数据放到里面了。&lt;/p>
&lt;/blockquote>
&lt;ol start="4">
&lt;li>
&lt;p>硬件发现 QP3 的 RQ 关联的 CQ 是 CQ3，所以向其中产生一个 CQE。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>用户从 CQ3 中取出 CQE，从指定内存区域取走数据。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>细心地读者可能会问，用户下发 WR 时，每个 WR 都指定了一些未来用来存放数据的内存区域。但是 SRQ 是一个池子，里面每个 WQE 都指向了不同的若干段内存区域。用户收到某个 QP 对应的 CQ 中的 WC 后如何知道接收到的数据存放到哪里了呢？&lt;/p>
&lt;p>WC 中其实有 wr_id 信息，告知用户数据放到哪个 WR（WQE）指定的内存区域了，既然 WR 是用户下发的，用户自然知道其指向的具体位置。&lt;/p>
&lt;/blockquote>
&lt;ol start="6">
&lt;li>
&lt;p>硬件收到数据&lt;/p>
&lt;/li>
&lt;li>
&lt;p>硬件发现是发给 QP2 的，从 SRQ1 中取出第一个 WQE（现在是 WQE2），根据 WQE 内容存放收到的数据。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>硬件发现 QP2 的 RQ 关联的 CQ 是 CQ2，所以向其中产生一个 CQE。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>用户从 CQ2 中取出 CQE，从指定内存区域取走数据。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-28_11_5.webp"
alt="2024-06-28_11_5" width="90%" loading="lazy">
&lt;/figure>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>本文首先介绍了 SRQ 的基本概念，然后是其设计初衷、相关机制和用户接口，最后对比 RQ 描述了 SRQ 的接收流程。在实际业务中，SRQ 的使用率还是蛮高的，希望读者能够深入理解。&lt;/p>
&lt;p>就写到这里吧，感谢阅读。下一篇我将给大家介绍下 Memeory Window。&lt;/p>
&lt;h2 id="协议相关章节">协议相关章节&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>10.2.9 SRQ 的设计思想以及相关操作&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.2.3 SRQ 和 QP 的 PD&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.8.2 关联 SRQ 的 QP 和不使用 SRQ 的 QP 的关系&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.8.5 SRQ 相关的返回 WC&lt;/p>
&lt;/li>
&lt;li>
&lt;p>11.5.2.4 异步事件&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="其他参考资料">其他参考资料&lt;/h2>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>Linux Kernel Networking - Implement and Theory. Chapter 13. Shared Receive Queue&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>RDMA 之 Completion Queue</title><link>https://cuterwrite.top/p/rdma-completion-queue/</link><pubDate>Wed, 26 Jun 2024 23:11:00 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-completion-queue/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116903369_p0_master1200.webp" alt="Featured image of post RDMA 之 Completion Queue" />&lt;h1 id="rdma-之-completion-queue">RDMA 之 Completion Queue&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/259650980">&lt;cite>知乎专栏：10. RDMA 之 Completion Queue&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>我们曾经在前面的文章中简单介绍过 CQ，本文将更深入的讲解关于它的一些细节。阅读本文前，读者可以先温习一下这篇文章： &lt;a class="link" href="https://cuterwrite.top/p/rdma-element/" >【“3. RDMA 基本元素”】
&lt;/a>
。&lt;/p>
&lt;h2 id="基本概念">基本概念&lt;/h2>
&lt;p>我们先回顾下 CQ 的作用。CQ 意为完成队列，它的作用和 WQ（SQ 和 RQ）相反，硬件通过 CQ 中的 CQE/WC 来告诉软件某个 WQE/WR 的完成情况。再次提醒读者，对于上层用户来说一般用 WC，对于驱动程序来说，一般称为 CQE，本文不对两者进行区分。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_1.webp"
alt="2024-06-27_10_1" width="80%" loading="lazy">
&lt;/figure>
&lt;p>CQE 可以看作一份“报告”，其中写明了某个任务的执行情况，其中包括：&lt;/p>
&lt;ul>
&lt;li>本次完成了哪个 QP 的哪一个 WQE 指定的任务（QP Number 和 WR ID）&lt;/li>
&lt;li>本次任务执行了什么操作（Opcode 操作类型）&lt;/li>
&lt;li>本次任务执行成功/失败，失败原因是 XXX（Status 状态和错误码）&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>每当硬件处理完一个 WQE 之后，都会产生一个 CQE 放在 CQ 队列中。如果一个 WQE 对应的 CQE 没有产生，那么这个 WQE 就会一直被认为还未处理完，这意味着什么呢？&lt;/p>
&lt;ul>
&lt;li>涉及从内存中取数据的操作（SEND 和 WRITE）&lt;/li>
&lt;/ul>
&lt;p>在产生 CQE 之前，硬件可能还未发送消息，可能正在发送消息，可能对端有接收到正确的消息。由于内存区域是在发送前申请好的，所以上层软件收到对应的 CQE 之前，其必须认为这片内存区域仍在使用中，不能将所有相关的内存资源进行释放。&lt;/p>
&lt;ul>
&lt;li>涉及向内存中存放数据的操作（RECV 和 READ）&lt;/li>
&lt;/ul>
&lt;p>在产生 CQE 之前，有可能硬件还没有开始写入数据，有可能数据才写了一半，也有可能数据校验出错。所以上层软件在获得 CQE 之前，这段用于存放接收数据的内存区域中的内容是不可信的。&lt;/p>
&lt;p>总之，用户必须获取到 CQE 并确认其内容之后才能认为消息收发任务已经完成。&lt;/p>
&lt;h3 id="何时产生">何时产生&lt;/h3>
&lt;p>我们将按照服务类型（本篇只讲 RC 和 UD）和操作类型来分别说明，因为不同的情况产生 CQE 的时机和含义都不同，建议读者回顾第 4 篇&lt;a class="link" href="https://cuterwrite.top/p/rdma-op/" >【“4. RDMA 基本操作”】
&lt;/a>
和第 5 篇&lt;a class="link" href="https://cuterwrite.top/p/rdma-service-types/" >【“5. RDMA 基本服务类型”】
&lt;/a>
。&lt;/p>
&lt;ul>
&lt;li>可靠服务类型（RC）&lt;/li>
&lt;/ul>
&lt;p>前面的文章说过，&lt;strong>可靠意味着本端关心发出的消息能够被对端准确的接收&lt;/strong>，这是通过 ACK、校验和重传等机制保证的。&lt;/p>
&lt;ul>
&lt;li>SEND&lt;/li>
&lt;/ul>
&lt;p>SEND 操作需要硬件从内存中获取数据，然后组装成数据包通过物理链路发送到对端。对 SEND 来说，Client 端产生 CQE 表示&lt;strong>对端已准确无误的收到数据&lt;/strong>，对端硬件收到数据并校验之后，会回复 ACK 包给发送方。发送方收到这 ACK 之后才会产生 CQE，从而告诉用户这个任务成功执行了。如图所示，左侧 Client 端在红点的位置产生了本次任务的 CQE。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_2.webp"
alt="2024-06-27_10_2" width="50%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>RECV&lt;/li>
&lt;/ul>
&lt;p>RECV 操作需要硬件将收到的数据放到用户 WQE 中指定的内存区域，完成校验和数据存放动作后，硬件就会产生 CQE。如上图右侧 Server 端所示。&lt;/p>
&lt;ul>
&lt;li>WRITE&lt;/li>
&lt;/ul>
&lt;p>对于 Client 端来说，WRITE 操作和 SEND 操作是一样的，硬件会从内存中取出数据，并等待对端回复 ACK 后，才会产生 CQE。差别在于，因为 WRITE 是 RDMA 操作，对端 CPU 不感知，自然用户也不感知，所以上面的图变成了这样：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_3.webp"
alt="2024-06-27_10_3" width="50%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>READ&lt;/li>
&lt;/ul>
&lt;p>READ 和 RECV 有点像，Client 端发起 READ 操作后，对端会回复我们想读取的数据，然后本端校验没问题后，会把数据放到 WQE 中指定的位置。完成上述动作后，本端会产生 CQE。READ 同样是 RDMA 操作，对端用户不感知，自然也没有 CQE 产生。这种情况上图变成了这样：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_4.webp"
alt="2024-06-27_10_4" width="50%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>不可靠服务类型（UD）&lt;/li>
&lt;/ul>
&lt;p>因为不可靠的服务类型没有重传和确认机制，所以产生 CQE 表示硬件&lt;strong>已经将对应 WQE 指定的数据发送出去了&lt;/strong>。以前说过 UD 只支持 SEND-RECV 操作，不支持 RDMA 操作。所以对于 UD 服务的两端，CQE 产生时机如下图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_5.webp"
alt="2024-06-27_10_5" width="50%" loading="lazy">
&lt;/figure>
&lt;h3 id="wq-和-cq-的对应关系">WQ 和 CQ 的对应关系&lt;/h3>
&lt;p>&lt;strong>每个 WQ 都必须关联一个 CQ，而每个 CQ 可以关联多个 SQ 和 RQ。&lt;/strong>&lt;/p>
&lt;p>这里的所谓“关联”，指的是一个 WQ 的所有 WQE 对应的 CQE，都会被硬件放到绑定的 CQ 中，需要注意同属于一个 QP 的 SQ 和 RQ 可以各自关联不同的 CQ。如下图所示，QP1 的 SQ 和 RQ 都关联了 CQ1，QP2 的 RQ 关联到了 CQ1、SQ 关联到了 CQ2。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_6.webp"
alt="2024-06-27_10_6" width="auto" loading="lazy">
&lt;/figure>
&lt;p>因为每个 WQ 必须关联一个 CQ，所以用户创建 QP 前需要提前创建好 CQ，然后分别指定 SQ 和 RQ 将会使用的 CQ。&lt;/p>
&lt;p>&lt;strong>同一个 WQ 中的 WQE，其对应的 CQE 间是保序的&lt;/strong>&lt;/p>
&lt;p>硬件是按照“先进先出”的 FIFO 顺序从某一个 WQ（SQ 或者 RQ）中取出 WQE 并进行处理的，而向 WR 关联的 CQ 中存放 CQE 时，也是遵从这些 WQE 被放到 WQ 中的顺序的。简单来说，就是谁先被放到队列里，谁就先被完成。该过程如下图所示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_7.webp"
alt="2024-06-27_10_7" width="auto" loading="lazy">
&lt;/figure>
&lt;p>需要注意的是，使用 SRQ 的情况以及 RD 服务类型的 RQ 这两种情况是不保序的，本文中不展开讨论。&lt;/p>
&lt;p>&lt;strong>不同 WQ 中的 WQE，其对应的 CQE 间是不保序的&lt;/strong>&lt;/p>
&lt;p>前文中我们说过，一个 CQ 可能会被多个 WQ 共享。这种情况下，是不能保证这些 WQE 对应的 CQE 的产生顺序的。如下图所示（WQE 编号表示下发的次序，即 1 最先被下发，6 最后被下发）：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_8.webp"
alt="2024-06-27_10_8" width="auto" loading="lazy">
&lt;/figure>
&lt;p>上面的描述其实还包含了“同一个 QP 的 SQ 和 RQ 中的 WQE，其对应的 CQE 间是不保序的”的情况，这一点其实比较容易理解，SQ 和 RQ，一个负责主动发起的任务，一个负责被动接收的任务，它们本来就可以是认为是两条不同方向的通道，自然不应该相互影响。假设用户对同一个 QP 先下发了一个 Receive WQE，又下发一个 Send WQE，总不能对端不给本端发送消息，本端就不能发送消息给对端了吧？&lt;/p>
&lt;p>既然这种情况下 CQE 产生的顺序和获取 WQE 的顺序是不相关的，那么上层应用和驱动是如何知道收到的 CQE 关联的是哪个 WQE 呢？其实很简单，&lt;strong>CQE 中指明它所对应的 WQE 的编号&lt;/strong>就可以了。&lt;/p>
&lt;p>另外需要注意的是，即使在多个 WQ 共用一个 CQ 的情况下，“同一个 WQ 中的 WQE，其对应的 CQE 间是保序的”这一点也是一定能够保证的，即上图中的属于 WQ1 的 WQE 1、3、4 对应的 CQE 一定是按照顺序产生的，对于属于 WQ2 的 WQE 2、5、6 也是如此。&lt;/p>
&lt;h3 id="cqc">CQC&lt;/h3>
&lt;p>同 QP 一样，CQ 只是一段存放 CQE 的队列内存空间。硬件除了知道首地址以外，对于这片区域可以说是一无所知。所以需要提前跟软件约定好格式，然后驱动将申请内存，并按照格式把 CQ 的基本信息填写到这片内存中供硬件读取，这片内存就是 CQC。CQC 中包含了 CQ 的容量大小，当前处理的 CQE 的序号等等信息。所以把 QPC 的图稍微修改一下，就能表示出 CQC 和 CQ 的关系：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_9.webp"
alt="2024-06-27_10_9" width="auto" loading="lazy">
&lt;/figure>
&lt;h3 id="cqn">CQN&lt;/h3>
&lt;p>CQ Number，就是 CQ 的编号，用来区别不同的 CQ。CQ 没有像 QP0 和 QP1 一样的特殊保留编号，本文中不再赘述了。&lt;/p>
&lt;h2 id="完成错误">完成错误&lt;/h2>
&lt;p>IB 协议中有三种错误类型，立即错误（immediate error）、完成错误（Completion Error）以及异步错误（Asynchronous Errors)。&lt;/p>
&lt;p>立即错误的是“立即停止当前操作，并返回错误给上层用户”；完成错误指的是“通过 CQE 将错误信息返回给上层用户”；而异步错误指的是“通过中断事件的方式上报给上层用户”。可能还是有点抽象，我们来举个例子说明这两种错误都会在什么情况下产生：&lt;/p>
&lt;ul>
&lt;li>用户在 Post Send 时传入了非法的操作码，比如想在 UD 的时候使用 RDMA WRITE 操作。&lt;/li>
&lt;/ul>
&lt;p>结果：产生立即错误（有的厂商在这种情况会产生完成错误）&lt;/p>
&lt;p>一般这种情况下，驱动程序会直接退出 post send 流程，并返回错误码给上层用户。注意此时 WQE 还没有下发到硬件就返回了。&lt;/p>
&lt;ul>
&lt;li>用户下发了一个 WQE，操作类型为 SEND，但是长时间没有受到对方的 ACK。&lt;/li>
&lt;/ul>
&lt;p>结果：产生完成错误&lt;/p>
&lt;p>因为 WQE 已经到达了硬件，所以硬件会产生对应的 CQE，CQE 中包含超时未响应的错误详情。&lt;/p>
&lt;ul>
&lt;li>用户态下发了多个 WQE，所以硬件会产生多个 CQE，但是软件一直没有从 CQ 中取走 CQE，导致 CQ 溢出。
结果：产生异步错误&lt;/li>
&lt;/ul>
&lt;p>因为软件一直没取 CQE，所以自然不会从 CQE 中得到信息。此时 IB 框架会调用软件注册的事件处理函数，来通知用户处理当前的错误。&lt;/p>
&lt;p>由此可见，它们都是底层向上层用户报告错误的方式，只是产生的时机不一样而已。IB 协议中对不同情况的错误应该以哪种方式上报做了规定，比如下图中，对于 Modify QP 过程中修改非法的参数，应该返回立即错误。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_10.webp"
alt="2024-06-27_10_10" width="auto" loading="lazy">
&lt;/figure>
&lt;p>本文的重点在于 CQ，所以介绍完错误类型之后，我们着重来看一下完成错误。完成错误是硬件通过在 CQE 中填写错误码来实现上报的，一次通信过程需要发起端（Requester）和响应端（Responder）参与，具体的错误原因也分为本端和对端。我们先来看一下错误检测是在什么阶段进行的（下图对 IB 协议中 Figure 118 进行了重画）：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-27_10_11.webp"
alt="2024-06-27_10_11" width="auto" loading="lazy">
&lt;/figure>
&lt;p>Requester 的错误检测点有两个：&lt;/p>
&lt;ol>
&lt;li>本地错误检测&lt;/li>
&lt;/ol>
&lt;p>即对 SQ 中的 WQE 进行检查，如果检测到错误，就从本地错误检查模块直接产生 CQE 到 CQ，不会发送数据到响应端了；如果没有错误，则发送数据到对端。&lt;/p>
&lt;ol start="2">
&lt;li>远端错误检测&lt;/li>
&lt;/ol>
&lt;p>即检测响应端的 ACK 是否异常，ACK/NAK 是由对端的本地错误检测模块检测后产生的，里面包含了响应端是否有错误，以及具体的错误类型。无论远端错误检测的结果是否有问题，都会产生 CQE 到 CQ 中。&lt;/p>
&lt;p>Responder 的错误检测点只有一个：&lt;/p>
&lt;ol>
&lt;li>本地错误检测&lt;/li>
&lt;/ol>
&lt;p>实际上检测的是对端报文是否有问题，IB 协议也将其称为“本地”错误检测。如果检测到错误，则会体现在 ACK/NAK 报文中回复给对端，以及在本地产生一个 CQE。&lt;/p>
&lt;p>需要注意的是，上述的产生 ACK 和远端错误检测只对面向连接的服务类型有效，无连接的服务类型。比如 UD 类型并不关心对端是否收到，接收端也不会产生 ACK，所以在 Requester 的本地错误检测之后就一定会产生 CQE，无论是否有远端错误。&lt;/p>
&lt;p>然后我们简单介绍下几种常见的完成错误：&lt;/p>
&lt;ul>
&lt;li>RC 服务类型的 SQ 完成错误&lt;/li>
&lt;li>Local Protection Error
&lt;ul>
&lt;li>本地保护域错误。本地 WQE 中指定的数据内存地址的 MR 不合法，即用户试图使用一片未注册的内存中的数据。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Remote Access Error
&lt;ul>
&lt;li>远端权限错误。本端没有权限读/写指定的对端内存地址。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Transport Retry Counter Exceeded Error
&lt;ul>
&lt;li>重传超次错误。对端一直未回复正确的 ACK，导致本端多次重传，超过了预设的次数。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>RC 服务类型的 RQ 完成错误&lt;/li>
&lt;li>Local Access Error
&lt;ul>
&lt;li>本地访问错误。说明对端试图写入其没有权限写入的内存区域。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Local Length Error
&lt;ul>
&lt;li>本地长度错误。本地 RQ 没有足够的空间来接收对端发送的数据。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>完整的完成错误类型列表请参考 IB 协议的 10.10.3 节。&lt;/p>
&lt;h2 id="用户接口">用户接口&lt;/h2>
&lt;p>同 QP 一样，我们依然从通信准备阶段（控制面）和通信进行阶段（数据面）来介绍 IB 协议对上层提供的关于 CQ 的接口。&lt;/p>
&lt;h3 id="控制面">控制面&lt;/h3>
&lt;p>同 QP 一样，还是“增删改查”四种，但是可能因为对于 CQ 来说，上层用户是资源使用者而不是管理者，只能从 CQ 中读数据而不能写数据，所以对用户开放的可配的参数就只有“CQ 规格”一种。&lt;/p>
&lt;ul>
&lt;li>创建——Create CQ&lt;/li>
&lt;/ul>
&lt;p>创建的时候用户必须指定 CQ 的规格，即能够储存多少个 CQE，另外用户还可以填写一个 CQE 产生后的回调函数指针（下文会涉及）。内核态驱动会将其他相关的参数配置好，填写到跟硬件约定好的 CQC 中告知硬件。&lt;/p>
&lt;ul>
&lt;li>销毁——Destroy CQ&lt;/li>
&lt;/ul>
&lt;p>释放一个 CQ 软硬件资源，包含 CQ 本身及 CQC，另外 CQN 自然也将失效。&lt;/p>
&lt;ul>
&lt;li>修改——Resize CQ&lt;/li>
&lt;/ul>
&lt;p>这里名字稍微有点区别，因为 CQ 只允许用户修改规格大小，所以就用的 Resize 而不是 Modify。&lt;/p>
&lt;ul>
&lt;li>查询——Query CQ&lt;/li>
&lt;/ul>
&lt;p>查询 CQ 的当前规格，以及用于通知的回调函数指针。&lt;/p>
&lt;blockquote>
&lt;p>通过对比 RDMA 规范和软件协议栈，可以发现很多 verbs 接口并不是按照规范实现的。所以读者如果发现软件 API 和协议有差异时也无须感到疑惑，RDMA 技术本身一直还在演进，软件框架也处于活跃更新的状态。如果更关心编程实现，那么请以软件协议栈的 API 文档为准；如果更关心学术上的研究，那么请以 RDMA 规范为准。&lt;/p>
&lt;/blockquote>
&lt;h3 id="数据面">数据面&lt;/h3>
&lt;p>CQE 是硬件将信息传递给软件的媒介，虽然软件知道在什么情况下会产生 CQE，但是软件并不知道具体什么时候硬件会把 CQE 放到 CQ 中。在通信和计算机领域，我们把这种接收方不知道发送方什么时候发送的模式称为“异步”。我们先来举一个网卡的例子，再来说明用户如何通过数据面接口获取 CQE（WC）。&lt;/p>
&lt;p>网卡收到数据包后如何让 CPU 知道这件事，并进行数据包处理，有两种常见的模式：&lt;/p>
&lt;ul>
&lt;li>中断模式&lt;/li>
&lt;/ul>
&lt;p>当数据量较少，或者说偶发的数据交换较多时，适合采用中断模式——即 CPU 平常在做其他事情，当网卡收到数据包时，会上报中断打断 CPU 当前的任务，CPU 转而来处理数据包（比如 TCP/IP 协议栈的各层解析）。处理完数据之后，CPU 跳回到中断前的任务继续执行。&lt;/p>
&lt;p>每次中断都需要保护现场，也就是把当前各个寄存器的值、局部变量的值等等保存到栈中，回来之后再恢复现场（出栈），这本身是有开销的。如果业务负载较重，网卡一直都在接收数据包，那么 CPU 就会一直收到中断，CPU 将一直忙于中断切换，导致其他任务得不到调度。&lt;/p>
&lt;ul>
&lt;li>轮询模式&lt;/li>
&lt;/ul>
&lt;p>所以除了中断模式之外，网卡还有一种轮询模式，即收到数据包后都先放到缓冲区里，CPU 每隔一段时间会去检查网卡是否受到数据。如果有数据，就把缓冲区里的数据一波带走进行处理，没有的话就接着处理别的任务。&lt;/p>
&lt;p>通过对比中断模式我们可以发现，轮询模式虽然每隔一段时间需要 CPU 检查一次，带来了一定的开销，但是当业务繁忙的时候采用轮询模式能够极大的减少中断上下文的切换次数，反而减轻了 CPU 的负担。&lt;/p>
&lt;p>现在的网卡，一般都是中断+轮询的方式，也就是根据业务负载动态切换。&lt;/p>
&lt;p>在 RDMA 协议中，CQE 就相当于是网卡收到的数据包，RDMA 硬件把它传递给 CPU 去处理。RDMA 框架定义了两种对上层的接口，分别是 poll 和 notify，对应着轮询和中断模式。&lt;/p>
&lt;h3 id="poll-completion-queue">Poll completion queue&lt;/h3>
&lt;p>很直白，poll 就是轮询的意思。用户调用这个接口之后，CPU 就会定期去检查 CQ 里面是否有新鲜的 CQE，如果有的话，就取出这个 CQE（注意取出之后 CQE 就被“消耗”掉了），解析其中的信息并返回给上层用户。&lt;/p>
&lt;h3 id="request-completion-notification">Request completion notification&lt;/h3>
&lt;p>直译过来是请求完成通知，用户调用这个接口之后，相当于向系统注册了一个中断。这样当硬件将 CQE 放到 CQ 中后，会立即触发一个中断给 CPU，CPU 进而就会停止手上的工作取出 CQE，处理后返回给用户。&lt;/p>
&lt;p>同样的，这两种接口使用哪种，取决于用户对于实时性的要求，以及实际业务的繁忙程度。&lt;/p>
&lt;p>感谢阅读，CQ 就介绍到这里，下篇打算详细讲讲 SRQ。&lt;/p>
&lt;h2 id="协议相关章节">协议相关章节&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>9.9 CQ 错误检测和恢复&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.2.6 CQ 和 WQ 的关系&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.10 错误类型及其处理&lt;/p>
&lt;/li>
&lt;li>
&lt;p>11.2.8 CQ 相关控制面接口&lt;/p>
&lt;/li>
&lt;li>
&lt;p>11.4.2 CQ 相关数据面接口&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="其他参考资料">其他参考资料&lt;/h2>
&lt;p>[1] Linux Kernel Networking - Implement and Theory. Chapter 13. Completion Queue&lt;/p></description></item><item><title>RDMA 之 Queue Pair</title><link>https://cuterwrite.top/p/rdma-queue-pair/</link><pubDate>Tue, 25 Jun 2024 02:21:00 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-queue-pair/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116342820_p0_master1200.webp" alt="Featured image of post RDMA 之 Queue Pair" />&lt;h1 id="rdma-之-queue-pair">RDMA 之 Queue Pair&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/195757767">&lt;cite>知乎专栏：9. RDMA 基本服务类型&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;h2 id="queue-pair">Queue Pair&lt;/h2>
&lt;p>我们曾经在 &lt;a class="link" href="https://cuterwrite.top/p/rdma-element/" >【“3. RDMA 基本元素”】
&lt;/a>
一文中简单的介绍了 QP 的概念，本文将更深入的讲解一些关于 QP 的细节。&lt;/p>
&lt;h2 id="基本概念回顾">基本概念回顾&lt;/h2>
&lt;p>首先我们来简单回顾下关于 QP 的基础知识：&lt;/p>
&lt;p>根据 IB 协议中的描述，QP 是硬件和软件之间的一个虚拟接口。QP 是队列结构，按顺序存储着软件给硬件下发的任务（WQE），WQE 中包含从哪里取出多长的数据，并且发送给哪个目的地等等信息。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-26_9_1.webp"
alt="2024-06-26_9_1" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>QP 的概念&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>每个 QP 间都是独立的，彼此通过 PD 隔离，因此一个 QP 可以被视为某个用户独占的一种资源，一个用户也可以同时使用多个 QP。&lt;/p>
&lt;p>QP 有很多种服务类型，包括 RC、UD、RD 和 UC 等，所有的源 QP 和目的 QP 必须为同一种类型才能进行数据交互。&lt;/p>
&lt;p>虽然 IB 协议将 QP 称为“虚拟接口”，但是它是有实体的：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>硬件上，QP 是一段包含着若干个 WQE 的存储空间，IB 网卡会从这段空间中读取 WQE 的内容，并按照用户的期望去内存中存取数据。至于这个存储空间是内存空间还是 IB 网卡的片内存储空间，IB 协议并未做出限制，每个厂商有各自的实现&lt;/p>
&lt;/li>
&lt;li>
&lt;p>软件上，QP 是一个由 IB 网卡的驱动程序所维护的数据结构，其中包含 QP 的地址指针以及一些相关的软件属性。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="qpc">QPC&lt;/h3>
&lt;p>&lt;a class="link" href="https://cuterwrite.top/p/rdma-service-types/" >【“5. RDMA 基本服务类型”】
&lt;/a>
一文中，我们曾经提到过 QPC 全称是 Queue Pair Context，用于存储 QP 相关属性。驱动程序里面是有储存 QP 的软件属性的，既然我们可以在软件里储存 QP 的属性，为什么还要用使用 QPC 呢？&lt;/p>
&lt;p>这是因为&lt;strong>QPC 主要是给硬件看的，也会用来在软硬件之间同步 QP 的信息。&lt;/strong>&lt;/p>
&lt;p>我们说过 QP 在硬件上的实体只是一段存储空间而已，硬件除了知道这段空间的起始地址和大小之外一无所知，甚至连这个 QP 服务类型都不知道。还有很多其他的重要信息，比如某个 QP 中包含了若干个 WQE，硬件怎么知道有多少个，当前应该处理第几个呢？&lt;/p>
&lt;p>所有上述的这些信息，软件是可以设计一定的数据结构并为其申请内存空间的，但是软件看到的都是虚拟地址，这些内存空间在物理上是离散的，硬件并不知道这些数据存放到了哪里。所以就需要软件通过操作系统提前申请好一大片连续的空间，即 QPC 来承载这些信息给硬件看。网卡及其配套的驱动程序提前约定好了 QPC 中都有哪些内容，这些内容分别占据多少空间，按照什么顺序存放。这样驱动和硬件就可以通过通过 QPC 这段空间来读写 QP 的状态等等信息。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-26_9_2_QPC.webp"
alt="2024-06-26_9_2" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>QPC 的概念&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>如上图所示，硬件其实只需要知道 QPC 的地址 0x12350000 就可以了，因为它可以解析 QPC 的内容，从而得知 QP 的位置，QP 序号，QP 大小等等信息。进而就能找到 QP，知道应该取第几个 WQE 去处理。不同的厂商可能实现有些差异，但是大致的原理就是这样。&lt;/p>
&lt;p>IB 软件栈中还有很多 Context 的概念，除了 QPC 之外，还有 Device Context，SRQC，CQC，EQC（Event Queue Context，事件队列上下文）等，它们的作用与 QPC 类似，都是用来在记录和同步某种资源的相关属性。&lt;/p>
&lt;h3 id="qp-number">QP Number&lt;/h3>
&lt;p>简称为 QPN，就是每个 QP 的编号。IB 协议中规定用 $2^{24}$ 个 bit 来表示 QPN，即每个节点最大可以同时使用 $2^{24}$ 个 QP，这已经是一个很大的数量了，几乎不可能用完。每个节点都各自维护着 QPN 的集合，相互之间是独立的，即不同的节点上可以存在编号相同的 QP。&lt;/p>
&lt;p>QPN 的概念本身非常简单，但是有两个特殊的保留编号需要额外注意一下：&lt;/p>
&lt;h4 id="qp0">QP0&lt;/h4>
&lt;p>编号为 0 的 QP 用于子网管理接口 SMI（Subnet Management Interface），用于管理子网中的全部节点，说实话我也还没搞清楚这个接口的作用，暂且按下不表。&lt;/p>
&lt;h4 id="qp1">QP1&lt;/h4>
&lt;p>编号为 1 的 QP 用于通用服务接口 GSI（General Service Interface），GSI 是一组管理服务，其中最出名的就是 CM（Communication Management），是一种在通信双方节点正式建立连接之前用来交换必须信息的一种方式。其细节将在后面的文章中专门展开介绍。&lt;/p>
&lt;p>这也就是我们之前的文章画的关于 QP 的图中，没有出现过 QP0 和 QP1 的原因了。这两个 QP 之外的其他 QP 就都是普通 QP 了。用户在创建 QP 的时候，驱动或者硬件会给这个新 QP 分配一个 QPN，一般的 QPN 都是 2、3、4 这样按顺序分配的。当 QP 被销毁之后，它的 QPN 也会被重新回收，并在合适的时候分配给其他新创建的 QP。&lt;/p>
&lt;h2 id="用户接口">用户接口&lt;/h2>
&lt;p>我们从控制层面和数据层面来分类介绍用户接口，控制面即用户对某种资源进行某种设置，一般都是在正式收发数据之前进行；而数据面自然就是真正的数据收发过程中进行的操作。&lt;/p>
&lt;h3 id="控制面">控制面&lt;/h3>
&lt;p>接触过算法的读者应该都了解，链表的节点涉及到“增、删、改、查”四个操作，链表的节点是一片内存区域，是一种软件资源。&lt;/p>
&lt;p>“增”即向操作系统申请一片内存用来存放数据，系统将在内存中划分一块空间，并将其标记为“已被进程 XX 使用”，其他没有权限的进程将无法覆盖甚至读取这片内存空间。&lt;/p>
&lt;p>“删”即通知操作系统，这片空间我不使用了，可以标记成“未使用”并给其它进程使用了。&lt;/p>
&lt;p>“改”就是写，即修改这片内存区域的内容。&lt;/p>
&lt;p>&amp;ldquo;查&amp;quot;就是读，即获取这片内存区域的内容。&lt;/p>
&lt;p>QP 作为 RDMA 技术中最重要的一种资源，在生命周期上与链表并无二致：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>操作&lt;/th>
&lt;th>链表节点&lt;/th>
&lt;th>QP&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>增&lt;/td>
&lt;td>struct ListNode *node = malloc(sizeof(struct ListNode *));&lt;/td>
&lt;td>Create QP&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>删&lt;/td>
&lt;td>free(node);&lt;/td>
&lt;td>Destroy QP&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>改&lt;/td>
&lt;td>node-&amp;gt;val = xxx;&lt;/td>
&lt;td>Modify QP&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>查&lt;/td>
&lt;td>xxx = node-&amp;gt;val;&lt;/td>
&lt;td>Query QP&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>这四种操作，其实就是 Verbs（RDMA 对上层应用的 API）在控制面上对上层用户提供给用户的几个接口：&lt;/p>
&lt;h4 id="create-qp">Create QP&lt;/h4>
&lt;p>创建一个 QP 的软硬件资源，包含 QP 本身以及 QPC。用户创建时会写传入一系列的初始化属性，包含该 QP 的服务类型，可以储存的 WQE 数量等信息&lt;/p>
&lt;h4 id="destroy-qp">Destroy QP&lt;/h4>
&lt;p>释放一个 QP 的全部软硬件资源，包含 QP 本身及 QPC。销毁 QP 后，用户将无法通过 QPN 索引到这个 QP。&lt;/p>
&lt;h4 id="modify-qp">Modify QP&lt;/h4>
&lt;p>修改一个 QP 的某些属性，比如 QP 的状态，路径的 MTU 等等。这个修改过程既包括软件数据结构的修改，也包括对 QPC 的修改。&lt;/p>
&lt;h4 id="query-qp">Query QP&lt;/h4>
&lt;p>查询一个 QP 当前的状态和一些属性，查询到的数据来源于驱动以及 QPC 的内容。&lt;/p>
&lt;p>这四种操作都有配套的 Verbs 接口，类似于 &lt;code>ibv_create_qp()&lt;/code> 这种形式，我们编写 APP 时直接调用就可以了。更多关于对上层的 API 的细节，我们将在后面专门进行介绍。&lt;/p>
&lt;h2 id="数据面">数据面&lt;/h2>
&lt;p>数据面上，一个 QP 对上层的接口其实只有两种，分别用于向 QP 中填写发送和接收请求。&lt;strong>这里的“发送”和“接收”并不是指的发送和接收数据，而是指的是一次通信过程的“发起方”（Requestor）和“接收方”（Responser）&lt;/strong>。&lt;/p>
&lt;p>在行为上都是软件向 QP 中填写一个 WQE（对应用层来说叫 WR），请求硬件执行一个动作。所以这两种行为都叫做“Post XXX Request”的形式，即下发 XXX 请求。&lt;/p>
&lt;h3 id="post-send-request">Post Send Request&lt;/h3>
&lt;p>再强调一下，Post Send 本身不是指这个 WQE 的操作类型是 Send，而是表示这个 WQE 属于通信发起方。这个流程中填写到 QP 中的 WQE/WR 可以是 Send 操作，RDMA Write 操作以及 RDMA Read 操作等。&lt;/p>
&lt;p>用户需要提前准备好数据缓冲区、目的地址等信息，然后调用接口将 WR 传给驱动，驱动再把 WQE 填写到 QP 中。&lt;/p>
&lt;h3 id="post-receive-request">Post Receive Request&lt;/h3>
&lt;p>Post Recv 的使用场景就相对比较少了，一般只在 Send-Recv 操作的接收端执行，接收端需要提前准备好接收数据的缓冲区，并将缓冲区地址等信息以 WQE 的形式告知硬件。&lt;/p>
&lt;h2 id="qp-状态机">QP 状态机&lt;/h2>
&lt;p>说到 QP 的状态，就不得不祭出下面这张图（取自 IB 协议 10.3.1 节）：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-26_9_3.webp"
alt="2024-06-26_9_3" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>QP 状态机&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>所谓状态机，就是描述一个对象的不同状态，以及触发状态间跳转的条件。为一个对象设计状态机可以使这个对象的生命周期变得非常明确，实现上也会使得逻辑更加清晰。&lt;/p>
&lt;p>对于 QP 来说，IB 规范也为其设计了几种状态，处于不同状态的 QP 的功能是有差异的，比如只有进入到 Ready to Send 状态之后，QP 才能够进行 Post Send 数据操作。正常状态（绿色的）之间的状态转换都是由用户通过上文介绍的 Modify QP 的用户接口来主动触发的；而错误状态（红色的）往往是出错之后自动跳转的，当一个 QP 处于错误状态之后就无法执行正常的业务了，就需要上层通过 Modify QP 将其重新配置到正常状态上。&lt;/p>
&lt;p>上图中我们只关注 QP 的部分，EE（End-to-End Context）是专门给 RD 服务类型使用的一个概念，我们暂不涉及。我们通过 Create QP 接口来进入这个状态图，通过 Destroy QP 接口来离开这个状态图。&lt;/p>
&lt;p>QP 有以下几种状态，我们仅介绍一下比较重要的点：&lt;/p>
&lt;h3 id="rstreset">RST（Reset）&lt;/h3>
&lt;p>复位状态。当一个 QP 通过 Create QP 创建好之后就处于这个状态，相关的资源都已经申请好了，但是这个 QP 目前什么都做不了，其无法接收用户下发的 WQE，也无法接受对端某个 QP 的消息。&lt;/p>
&lt;h3 id="initinitialized">INIT（Initialized）&lt;/h3>
&lt;p>已初始化状态。这个状态下，用户可以通过 Post Receive 给这个 QP 下发 Receive WR，但是接收到的消息并不会被处理，会被静默丢弃；如果用户下发了一个 Post Send 的 WR，则会报错。&lt;/p>
&lt;h3 id="rtrready-to-receive">RTR（Ready to Receive）&lt;/h3>
&lt;p>准备接收状态。在 INIT 状态的基础上，RQ 可以正常工作，即对于接收到的消息，可以按照其中 WQE 的指示搬移数据到指定内存位置。此状态下 SQ 仍然不能工作。&lt;/p>
&lt;h3 id="rtsready-to-send">RTS（Ready to Send）&lt;/h3>
&lt;p>准备发送状态。在 RTR 基础上，SQ 可以正常工作，即用户可以进行 Post Send，并且硬件也会根据 SQ 的内容将数据发送出去。进入该状态前，QP 必须已于对端建立好链接。&lt;/p>
&lt;h3 id="sqdsend-queue-drain">SQD（Send Queue Drain）&lt;/h3>
&lt;p>SQ 排空状态。顾名思义，该状态会将 SQ 队列中现存的未处理的 WQE 全部处理掉，这个时候用户还可以下发新的 WQE 下来，但是这些 WQE 要等到旧的 WQE 全处理之后才会被处理。&lt;/p>
&lt;h3 id="sqersend-queue-error">SQEr（Send Queue Error）&lt;/h3>
&lt;p>SQ 错误状态。当某个 Send WR 发生完成错误（即硬件通过 CQE 告知驱动发生的错误）时，会导致 QP 进入此状态。&lt;/p>
&lt;h3 id="errerror">ERR（Error）&lt;/h3>
&lt;p>即错误状态。其他状态如果发生了错误，都可能进入该状态。Error 状态时，QP 会停止处理 WQE，已经处理到一半的 WQE 也会停止。上层需要在修复错误后再将 QP 重新切换到 RST 的初始状态。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>本文先回顾了 QP 的一些重要基本概念，然后讲解了 QPC、QPN 等 QP 强相关的概念，最后介绍了用户操作 QP 常用的接口以及 QP 状态机，相信本文过后读者一定对 QP 有了更深的了解。&lt;/p>
&lt;p>其实作为 RDMA 的核心概念，QP 的内容很多，本文难以全部囊括。我将在后面的文章中逐渐把相关的内容补全，比如 QKey 的概念将在后续专门介绍各种 Key 的文章中讲解。&lt;/p>
&lt;p>好了，本文就到这了，感谢阅读。预告下一篇文章将详细讲解 CQ。&lt;/p>
&lt;h2 id="协议相关章节">协议相关章节&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>3.5.1 10.2.4 QP 的基本概念&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.3 QP 状态机&lt;/p>
&lt;/li>
&lt;li>
&lt;p>10.2.5 QP 相关的软件接口&lt;/p>
&lt;/li>
&lt;li>
&lt;p>11.4 Post Send Post Recv&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>基于 Workbox 实现 Hugo 渐进式 Web 应用</title><link>https://cuterwrite.top/p/hugo-pwa/</link><pubDate>Tue, 18 Jun 2024 22:28:00 +0000</pubDate><guid>https://cuterwrite.top/p/hugo-pwa/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116406967_p0_master1200.webp" alt="Featured image of post 基于 Workbox 实现 Hugo 渐进式 Web 应用" />&lt;h1 id="基于-workbox-实现-hugo-pwa">基于 Workbox 实现 Hugo PWA&lt;/h1>
&lt;p>最近给基于 &lt;a class="link" href="https://gohugo.io/" target="_blank" rel="noopener" >Hugo
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
搭建的博客添加了 PWA 功能，显著提升了加载速度和用户体验，甚至实现了离线访问。至于如何实现，那么你需要了解 &lt;strong>Progressive Web Apps (PWA)&lt;/strong>。&lt;/p>
&lt;h2 id="什么是-pwa">什么是 PWA&lt;/h2>
&lt;p>渐进式 Web 应用（Progressive Web Apps，简称 PWA）利用现代 Web API 和传统的渐进式增强策略，打造出跨平台的 Web 应用程序。这些应用无处不在，功能丰富，为用户带来媲美原生应用的体验。&lt;/p>
&lt;p>&lt;strong>PWA 的优势：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>⚡️ &lt;strong>更快的加载速度&lt;/strong>: PWA 可以缓存重要资源，即使网络状况不佳也能快速加载。&lt;/li>
&lt;li>✈️ &lt;strong>离线访问&lt;/strong>: PWA 可以缓存内容，让用户即使离线也能访问内容。&lt;/li>
&lt;li>🔔 &lt;strong>推送通知&lt;/strong>: 像原生应用一样，PWA 可以向用户发送推送通知，提高用户参与度。&lt;/li>
&lt;li>📱 &lt;strong>安装到主屏幕&lt;/strong>: 用户可以将你的应用添加到电脑或手机桌面，像原生应用一样浏览你的 Web 应用。&lt;/li>
&lt;/ul>
&lt;p>PWA 的实现原理是 &lt;strong>Service Worker&lt;/strong>。&lt;strong>Service Worker 是一种特殊的 JavaScript 资源，在浏览器后台独立运行，充当着网络浏览器和 Web 服务器之间的代理。它可以拦截和处理网络请求、缓存资源以及推送通知&lt;/strong>。&lt;/p>
&lt;p>主流的前端框架 Vue、React、Angular 都提供了相应的 PWA 插件。而对于 Hugo 这样的静态网站生成器，我们可以通过手动添加 &lt;a class="link" href="https://developer.chrome.com/docs/workbox" target="_blank" rel="noopener" >Workbox
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
来实现 PWA 功能。&lt;/p>
&lt;h2 id="workbox">Workbox&lt;/h2>
&lt;p>&lt;a class="link" href="https://developer.chrome.com/docs/workbox" target="_blank" rel="noopener" >Workbox
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
是由 Google Chrome 团队开发的一套模块，旨在简化常见的 Service Worker 路由和缓存操作。每个模块都针对 Service Worker 开发的特定方面进行了优化。Workbox 的目标是尽可能简化 Service Worker 的使用，同时在需要时灵活地满足复杂应用的需求。&lt;/p>
&lt;p>如果没有 Workbox，我们需要手动编写 Service Worker 来监听 fetch 事件、缓存资源并实现离线访问等功能。而 Workbox 提供了一套工具，可以帮助我们自动生成 Service Worker，并且内置了一些常用的缓存策略，使我们能够更加专注于业务逻辑。&lt;/p>
&lt;h2 id="配置-pwa">配置 PWA&lt;/h2>
&lt;p>在上一节中，我们了解了 PWA 的概念和优势，以及 Workbox 如何简化 Service Worker 的开发。接下来将一步步地给 Hugo 博客配置 PWA 功能。&lt;/p>
&lt;h3 id="注册-service-worker">注册 Service Worker&lt;/h3>
&lt;p>首先，我们需要在页面中注册 Service Worker。将以下代码段添加到你的 Hugo 主题的 &lt;code>layouts/partials/footer/custom.html&lt;/code> 文件中（其他主题可能需要根据文件结构进行调整）：&lt;/p>
&lt;pre>&lt;code class="language-javascript">&amp;lt;script&amp;gt;
// Check that service workers are registered
if ('serviceWorker' in navigator) {
// Use the window load event to keep the page load performant
window.addEventListener('load', () =&amp;gt; {
navigator.serviceWorker.register('/sw.js').then(reg =&amp;gt; {
console.log('Service worker registered with scope: ', reg.scope);
}, err =&amp;gt; {
console.log('Service worker registration failed: ', err);
});
});
}
&amp;lt;/script&amp;gt;
&lt;/code>&lt;/pre>
&lt;div class="notice notice-tip" >
&lt;div class="notice-title">&lt;svg t="1705945832245" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="19409" width="200" height="200">&lt;path d="M512 74.666667C270.933333 74.666667 74.666667 270.933333 74.666667 512S270.933333 949.333333 512 949.333333 949.333333 753.066667 949.333333 512 753.066667 74.666667 512 74.666667z m238.933333 349.866666l-2.133333 2.133334-277.333333 277.333333c-10.666667 10.666667-29.866667 12.8-42.666667 2.133333L426.666667 704l-149.333334-149.333333c-12.8-12.8-12.8-32 0-44.8 10.666667-10.666667 29.866667-12.8 42.666667-2.133334l2.133333 2.133334 125.866667 125.866666 253.866667-253.866666c10.666667-10.666667 29.866667-12.8 42.666666-2.133334l2.133334 2.133334c12.8 12.8 12.8 32 4.266666 42.666666z" fill="#ffffff" p-id="19410">&lt;/path>&lt;/svg>&lt;/div>&lt;p>注意： 在注册 Service Worker 之前，你需要先创建 &lt;code>sw.js&lt;/code> 文件，我们将在下一小节中完成这一步骤。&lt;/p>&lt;/div>
&lt;p>完成注册后，你可以在浏览器的开发者工具 (F12) 中的 &lt;strong>&amp;ldquo;Application&amp;rdquo; -&amp;gt; &amp;ldquo;Service Workers&amp;rdquo;&lt;/strong> 面板中查看 Service Worker 的注册状态。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-19_service-worker.webp"
alt="Service Worker" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>Service Worker&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="导入-workbox">导入 Workbox&lt;/h3>
&lt;p>在你的 Hugo 网站根目录下的 &lt;code>static&lt;/code> 文件夹中创建 &lt;code>sw.js&lt;/code> 文件。然后，在 &lt;code>sw.js&lt;/code> 文件中添加以下代码，使用 CDN 导入 Workbox：&lt;/p>
&lt;pre>&lt;code class="language-javascript">importScripts('https://cdn.bootcdn.net/ajax/libs/workbox-sw/7.1.0/workbox-sw.js');
&lt;/code>&lt;/pre>
&lt;h3 id="缓存策略">缓存策略&lt;/h3>
&lt;p>Workbox 提供了一些常用的缓存策略，如 &lt;code>CacheFirst&lt;/code>、&lt;code>NetworkFirst&lt;/code>、&lt;code>StaleWhileRevalidate&lt;/code> 等。这里先介绍几种常用的策略。&lt;/p>
&lt;h4 id="cacheonly-仅缓存">CacheOnly 仅缓存&lt;/h4>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-19_cache-6850d07d742bf_1440.webp"
alt="CacheOnly" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>CacheOnly&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>强制响应来自缓存。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp(regex),
new workbox.strategies.CacheOnly()
);
&lt;/code>&lt;/pre>
&lt;h4 id="networkonly-仅网络">NetworkOnly 仅网络&lt;/h4>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-19_network-48f46158a5575_1440.webp"
alt="NetworkOnly" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>NetworkOnly&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>这种缓存策略强制要求所有请求都从网络获取最新数据，完全绕过缓存。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp(regex),
new workbox.strategies.NetworkOnly()
);
&lt;/code>&lt;/pre>
&lt;h4 id="cachefirst-优先缓存">CacheFirst 优先缓存&lt;/h4>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-19_cache-falling-to-networ-f4c1aa5570621_1440.webp"
alt="CacheFirst" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>CacheFirst&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>这种缓存策略以速度为优先，会首先尝试从缓存中获取响应，以尽快向用户显示内容。如果缓存中没有所需数据，它才会向网络发起请求获取数据。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp(regex),
new workbox.strategies.CacheFirst()
);
&lt;/code>&lt;/pre>
&lt;h4 id="networkfirst-优先网络">NetworkFirst 优先网络&lt;/h4>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-19_network-falling-to-cache-39d267a044b35_1440.webp"
alt="NetworkFirst" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>NetworkFirst&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>这种缓存策略优先使用最新数据，因此会首先尝试从网络获取响应。如果网络请求失败，例如用户离线或网络连接不稳定，它会回退使用缓存中的数据，确保用户仍然可以访问内容。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp(regex),
new workbox.strategies.NetworkFirst()
);
&lt;/code>&lt;/pre>
&lt;h4 id="stalewhilerevalidate-读取缓存同时发起网络请求">StaleWhileRevalidate 读取缓存，同时发起网络请求&lt;/h4>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-19_cache-network-873b1ec5f25cc_1440.webp"
alt="StaleWhileRevalidate" width="90%" loading="lazy">&lt;figcaption>
&lt;h4>StaleWhileRevalidate&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>这种缓存策略优先返回缓存内容（如果有）。即使缓存内容有效，它也会在后台发起网络请求以获取最新数据，保证用户最终能看到最新内容。虽然这种策略能确保用户定期更新缓存，但也意味着每次请求都会产生网络流量，即使数据没有变化，也比较浪费带宽。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp(regex),
new workbox.strategies.StaleWhileRevalidate()
);
&lt;/code>&lt;/pre>
&lt;h4 id="策略配置">策略配置&lt;/h4>
&lt;p>Workbox 不仅提供上述策略，还允许通过 cacheName、plugins 和 expiration 等配置项进行自定义。你可以通过定义要使用的插件来自定义路由行为。例如，你可以配置缓存名称、缓存有效期以及可缓存的响应状态码，如下所示：&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp(regex),
new workbox.strategies.CacheFirst({
cacheName: 'my-cache',
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: 60,
maxAgeSeconds: 30 * 24 * 60 * 60, // 30 Days
}),
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
&lt;/code>&lt;/pre>
&lt;h3 id="本站配置">本站配置&lt;/h3>
&lt;h4 id="全局配置">全局配置&lt;/h4>
&lt;p>以下是全局缓存配置：&lt;/p>
&lt;pre>&lt;code class="language-javascript">// 缓存版本号
let cacheVersion = '-240619';
// 最大条目数
const maxEntries = 100;
&lt;/code>&lt;/pre>
&lt;h4 id="twitto-配置">Twitto 配置&lt;/h4>
&lt;p>为了确保用户即使在离线状态下也能查看评论，Twitto 评论 API 采用了 &lt;code>NetworkFirst&lt;/code> 缓存策略。这意味着浏览器会优先尝试从网络获取最新数据，如果网络不可用，则使用缓存中的数据。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('^https://comment\.cuterwrite\.top'),
new workbox.strategies.NetworkFirst({
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
&lt;/code>&lt;/pre>
&lt;h4 id="rss-与-sitemap-配置">RSS 与 Sitemap 配置&lt;/h4>
&lt;p>为了确保用户始终获取最新的 RSS 和 Sitemap 数据，这些页面配置为仅使用网络策略 (&lt;code>NetworkOnly&lt;/code>)，不进行缓存。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('^https://cuterwrite\.top/(index|sitemap)\.xml'),
new workbox.strategies.NetworkOnly()
);
&lt;/code>&lt;/pre>
&lt;h4 id="html-配置">HTML 配置&lt;/h4>
&lt;p>为了在保证用户快速加载页面的同时，也能获取到最新内容，网站对 HTML 页面采用了 &lt;code>StaleWhileRevalidate&lt;/code> 缓存策略。这意味着浏览器会优先使用缓存中的页面进行展示，同时在后台向服务器发起请求，获取最新版本，并在下次请求时使用。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('.*\.html'),
new workbox.strategies.StaleWhileRevalidate({
cacheName: 'html-cache' + cacheVersion,
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
&lt;/code>&lt;/pre>
&lt;h4 id="google-fonts-配置">Google Fonts 配置&lt;/h4>
&lt;p>为了在保证字体文件更新的同时，也能利用缓存加速页面加载速度，网站对 Google Fonts 资源采用了 &lt;code>CacheFirst&lt;/code> 缓存策略，并设置了较长的缓存过期时间。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('.*\.(?:woff|woff2|ttf|otf|eot)'),
new workbox.strategies.StaleWhileRevalidate({
cacheName: 'google-fonts' + cacheVersion,
plugins: [
// 使用 expiration 插件实现缓存条目数目和时间控制
new workbox.expiration.ExpirationPlugin({
// 最大缓存条目数
maxEntries: maxEntries,
// 最长缓存时间 30 天
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
// 使用 cacheableResponse 插件缓存状态码为 0 的请求
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
&lt;/code>&lt;/pre>
&lt;h4 id="cdn-配置">CDN 配置&lt;/h4>
&lt;p>为了最大程度地利用缓存加速页面加载速度，网站对来自常用 CDN 的资源采用了 &lt;code>CacheFirst&lt;/code> 缓存策略，并设置了较长的缓存过期时间。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('^https://(?:cdn\.bootcdn\.net|unpkg\.com|cdn\.jsdelivr\.net)'),
new workbox.strategies.CacheFirst({
cacheName: 'cdn' + cacheVersion,
fetchOptions: {
mode: 'cors',
credentials: 'omit',
},
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
],
})
);
&lt;/code>&lt;/pre>
&lt;h4 id="umani-网站统计配置">Umani 网站统计配置&lt;/h4>
&lt;p>为了确保网站统计数据的准确性，网站对 Umani 网站统计请求采用了 &lt;code>NetworkOnly&lt;/code> 策略，并使用 &lt;code>BackgroundSyncPlugin&lt;/code> 插件来实现即使在网络离线的情况下也能保证数据最终发送成功。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('^https://analytics\.cuterwrite\.top/uma'),
new workbox.strategies.NetworkOnly({
plugins: [
// 使用 background sync 插件实现后台同步
new workbox.backgroundSync.BackgroundSyncPlugin('Optical_Collect', {
maxRetentionTime: 12 * 60,
}),
],
})
);
&lt;/code>&lt;/pre>
&lt;h4 id="图片配置">图片配置&lt;/h4>
&lt;p>为了加速图片加载速度，并减少网络请求次数，网站对图片资源采用了 &lt;code>CacheFirst&lt;/code> 缓存策略，并设置了较长的缓存过期时间。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('^(https://cuterwrite-1302252842\.file\.myqcloud\.com|https://s2\.loli\.net)'),
new workbox.strategies.CacheFirst({
cacheName: 'image-cache' + cacheVersion,
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
&lt;/code>&lt;/pre>
&lt;h4 id="后缀匹配配置">后缀匹配配置&lt;/h4>
&lt;p>为了兼顾加载速度和内容更新，网站对未被域名匹配到的静态文件（例如图片、CSS 和 JavaScript 文件）采用了 &lt;code>StaleWhileRevalidate&lt;/code> 缓存策略。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.registerRoute(
new RegExp('.*\.(?:png|jpg|jpeg|svg|gif|webp|ico)'),
new workbox.strategies.StaleWhileRevalidate()
);
workbox.routing.registerRoute(
new RegExp('.*\.(css|js)'),
new workbox.strategies.StaleWhileRevalidate()
);
&lt;/code>&lt;/pre>
&lt;h4 id="默认行为配置">默认行为配置&lt;/h4>
&lt;p>为了处理未被任何自定义路由规则匹配到的请求，网站配置了默认缓存行为，使用 &lt;code>NetworkFirst&lt;/code> 策略并设置了网络超时时间，以兼顾资源获取速度和离线可用性。&lt;/p>
&lt;pre>&lt;code class="language-javascript">workbox.routing.setDefaultHandler(
// 优先使用缓存，缓存没有则使用网络请求
new workbox.strategies.NetworkFirst({
networkTimeoutSeconds: 3,
})
);
&lt;/code>&lt;/pre>
&lt;h3 id="完整配置">完整配置&lt;/h3>
&lt;details>
&lt;summary>sw.js&lt;/summary>
&lt;pre>&lt;code class="language-javascript">importScripts('https://cdn.bootcdn.net/ajax/libs/workbox-sw/7.1.0/workbox-sw.js');
// 缓存版本号
let cacheVersion = '-240619';
// 最大条目数
const maxEntries = 100;
if (workbox) {
console.log(`Yay! Workbox is loaded 🎉`);
// 评论缓存
workbox.routing.registerRoute(
new RegExp('^https://comment\.cuterwrite\.top'),
new workbox.strategies.NetworkFirst({
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
// rss 、sitemap 不缓存
workbox.routing.registerRoute(
new RegExp('^https://cuterwrite\.top/(index|sitemap)\.xml'),
new workbox.strategies.NetworkOnly()
);
// 缓存 HTML
workbox.routing.registerRoute(
new RegExp('.*\.html'),
new workbox.strategies.StaleWhileRevalidate({
cacheName: 'html-cache' + cacheVersion,
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
// 缓存 Google Fonts
workbox.routing.registerRoute(
new RegExp('.*\.(?:woff|woff2|ttf|otf|eot)'),
new workbox.strategies.StaleWhileRevalidate({
cacheName: 'google-fonts' + cacheVersion,
plugins: [
// 使用 expiration 插件实现缓存条目数目和时间控制
new workbox.expiration.ExpirationPlugin({
// 最大缓存条目数
maxEntries: maxEntries,
// 最长缓存时间 30 天
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
// 使用 cacheableResponse 插件缓存状态码为 0 的请求
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
// 缓存 bootcdn、unpkg、jsdelivr 等公共库，用正则匹配
workbox.routing.registerRoute(
new RegExp('^https://(?:cdn\.bootcdn\.net|unpkg\.com|cdn\.jsdelivr\.net)'),
new workbox.strategies.CacheFirst({
cacheName: 'cdn' + cacheVersion,
fetchOptions: {
mode: 'cors',
credentials: 'omit',
},
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
],
})
);
// 自建 UMA 统计脚本: https://analytics.cuterwrite.top/uma
workbox.routing.registerRoute(
new RegExp('^https://analytics\.cuterwrite\.top/uma'),
new workbox.strategies.NetworkOnly({
plugins: [
// 使用 background sync 插件实现后台同步
new workbox.backgroundSync.BackgroundSyncPlugin('Optical_Collect', {
maxRetentionTime: 12 * 60,
}),
],
})
);
// 缓存存储桶图片 https://cuterwrite-1302252842.file.myqcloud.com/
workbox.routing.registerRoute(
new RegExp('^(https://cuterwrite-1302252842\.file\.myqcloud\.com|https://s2\.loli\.net)'),
new workbox.strategies.CacheFirst({
cacheName: 'image-cache' + cacheVersion,
plugins: [
new workbox.expiration.ExpirationPlugin({
maxEntries: maxEntries,
maxAgeSeconds: 30 * 24 * 60 * 60,
}),
new workbox.cacheableResponse.CacheableResponsePlugin({
statuses: [0, 200],
}),
],
})
);
// 后缀匹配，针对其余没有被域名匹配到的静态文件
workbox.routing.registerRoute(
new RegExp('.*\.(?:png|jpg|jpeg|svg|gif|webp|ico)'),
new workbox.strategies.StaleWhileRevalidate()
);
workbox.routing.registerRoute(
new RegExp('.*\.(css|js)'),
new workbox.strategies.StaleWhileRevalidate()
);
// 默认匹配剩下的请求
workbox.routing.setDefaultHandler(
// 优先使用缓存，缓存没有则使用网络请求
new workbox.strategies.NetworkFirst({
networkTimeoutSeconds: 3,
})
);
} else {
console.log(`Boo! Workbox didn't load 😬`);
}
&lt;/code>&lt;/pre>
&lt;/details>
&lt;h3 id="manifestjson">manifest.json&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>创建 manifest.json 文件&lt;/strong>&lt;/li>
&lt;/ol>
&lt;p>在你的 Hugo 博客的根目录 &lt;code>static&lt;/code> 文件夹下创建 &lt;code>manifest.json&lt;/code> 文件，该文件包含了关于你的博客的元数据，例如名称、图标和显示选项。&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;name&amp;quot;: &amp;quot;你的博客名称&amp;quot;,
&amp;quot;short_name&amp;quot;: &amp;quot;博客简称&amp;quot;,
&amp;quot;start_url&amp;quot;: &amp;quot;/&amp;quot;,
&amp;quot;display&amp;quot;: &amp;quot;standalone&amp;quot;,
&amp;quot;background_color&amp;quot;: &amp;quot;#ffffff&amp;quot;,
&amp;quot;theme_color&amp;quot;: &amp;quot;#000000&amp;quot;,
&amp;quot;icons&amp;quot;: [{
&amp;quot;src&amp;quot;: &amp;quot;/icon-192x192.png&amp;quot;,
&amp;quot;sizes&amp;quot;: &amp;quot;192x192&amp;quot;,
&amp;quot;type&amp;quot;: &amp;quot;image/png&amp;quot;
},
{
&amp;quot;src&amp;quot;: &amp;quot;/icon-512x512.png&amp;quot;,
&amp;quot;sizes&amp;quot;: &amp;quot;512x512&amp;quot;,
&amp;quot;type&amp;quot;: &amp;quot;image/png&amp;quot;
}
]
}
&lt;/code>&lt;/pre>
&lt;div class="notice notice-tip" >
&lt;div class="notice-title">&lt;svg t="1705945832245" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="19409" width="200" height="200">&lt;path d="M512 74.666667C270.933333 74.666667 74.666667 270.933333 74.666667 512S270.933333 949.333333 512 949.333333 949.333333 753.066667 949.333333 512 753.066667 74.666667 512 74.666667z m238.933333 349.866666l-2.133333 2.133334-277.333333 277.333333c-10.666667 10.666667-29.866667 12.8-42.666667 2.133333L426.666667 704l-149.333334-149.333333c-12.8-12.8-12.8-32 0-44.8 10.666667-10.666667 29.866667-12.8 42.666667-2.133334l2.133333 2.133334 125.866667 125.866666 253.866667-253.866666c10.666667-10.666667 29.866667-12.8 42.666666-2.133334l2.133334 2.133334c12.8 12.8 12.8 32 4.266666 42.666666z" fill="#ffffff" p-id="19410">&lt;/path>&lt;/svg>&lt;/div>&lt;p>注意：将 icon-192x192.png 和 icon-512x512.png 替换为你自己的图标文件名。并确保将这两个图标文件放置在 Hugo 博客的 &lt;code>static&lt;/code> 文件夹中。如果你想修改主题颜色和背景颜色，可以修改 theme_color 和 background_color 字段。&lt;/p>&lt;/div>
&lt;ol start="2">
&lt;li>&lt;strong>链接 manifest.json 文件&lt;/strong>&lt;/li>
&lt;/ol>
&lt;p>在你的 Hugo 博客的 &lt;code>layouts/partials/head/custom.html&lt;/code> 文件中添加以下代码，将 &lt;code>manifest.json&lt;/code> 文件链接到你的网站：&lt;/p>
&lt;pre>&lt;code class="language-html">&amp;lt;link rel=&amp;quot;manifest&amp;quot; href=&amp;quot;/manifest.json&amp;quot;&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>完成以上步骤后，你的 Hugo 博客就具备了 PWA 功能，用户可以像使用原生应用程序一样访问你的网站。&lt;/p>
&lt;h2 id="参考资料">参考资料&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;a class="link" href="https://web.dev/articles/offline-cookbook?hl=zh-cn" target="_blank" rel="noopener" >离线实战宝典
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a class="link" href="https://developers.google.com/web/tools/workbox" target="_blank" rel="noopener" >Workbox
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a class="link" href="https://github.com/GoogleChrome/workbox" target="_blank" rel="noopener" >Workbox Github
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>Ollama：从入门到进阶</title><link>https://cuterwrite.top/p/ollama/</link><pubDate>Sat, 15 Jun 2024 23:10:00 +0000</pubDate><guid>https://cuterwrite.top/p/ollama/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116903387_p0_master1200.webp" alt="Featured image of post Ollama：从入门到进阶" />&lt;p>近年来，大型语言模型（LLM）以其强大的文本生成和理解能力，成为了人工智能领域的中坚力量。商业 LLM 的价格通常高昂且代码封闭，限制了研究者和开发者的探索空间。幸运的是，开源社区提供了像 Ollama 这样优秀的替代方案，让每个人都能够轻松体验 LLM 的魅力，并能结合 HPC 和 IDE 插件，打造更强大的个人助手。&lt;/p>
&lt;h2 id="什么是-ollama">什么是 Ollama？&lt;/h2>
&lt;p>Ollama 是一个用于构建大型语言模型应用的工具，它提供了一个简洁易用的命令行界面和服务器，让你能够轻松下载、运行和管理各种开源 LLM。与需要复杂配置和强大硬件的传统 LLM 不同，Ollama 让你能够方便地像使用手机 App 一样体验 LLM 的强大功能。&lt;/p>
&lt;h2 id="ollama-的优势">Ollama 的优势&lt;/h2>
&lt;p>Ollama 拥有以下显著优势：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>开源免费&lt;/strong>： Ollama 及其支持的模型完全开源免费，任何人都可以自由使用、修改和分发。&lt;/li>
&lt;li>&lt;strong>简单易用&lt;/strong>： 无需复杂的配置和安装过程，只需几条命令即可启动和运行 Ollama。&lt;/li>
&lt;li>&lt;strong>模型丰富&lt;/strong>： Ollama 支持 Llama 3、Mistral、Qwen2 等众多热门开源 LLM，并提供一键下载和切换功能。&lt;/li>
&lt;li>&lt;strong>资源占用低&lt;/strong>： 相比于商业 LLM，Ollama 对硬件要求更低，即使在普通笔记本电脑上也能流畅运行。&lt;/li>
&lt;li>&lt;strong>社区活跃&lt;/strong>： Ollama 拥有庞大且活跃的社区，用户可以轻松获取帮助、分享经验和参与模型开发。&lt;/li>
&lt;/ul>
&lt;h2 id="如何使用-ollama">如何使用 Ollama？&lt;/h2>
&lt;p>使用 Ollama 非常简单，只需要按照以下步骤：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>安装 Ollama&lt;/strong>： 根据你的操作系统，从 &lt;a class="link" href="https://ollama.com/" target="_blank" rel="noopener" >Ollama 官网
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
下载并安装最新版本。&lt;/li>
&lt;li>&lt;strong>启动 Ollama&lt;/strong>： 打开终端或命令行，输入 &lt;code>ollama serve&lt;/code> 命令启动 Ollama 服务器。&lt;/li>
&lt;li>&lt;strong>下载模型&lt;/strong>： 在&lt;a class="link" href="https://ollama.com/library" target="_blank" rel="noopener" >模型仓库
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
找到想要的模型，然后使用 &lt;code>ollama pull&lt;/code> 命令下载，例如 &lt;code>ollama pull llama3:70b&lt;/code> 。&lt;/li>
&lt;li>&lt;strong>运行模型&lt;/strong>： 使用 &lt;code>ollama run&lt;/code> 命令启动模型，例如 &lt;code>ollama run llama3:70b&lt;/code> 。&lt;/li>
&lt;li>&lt;strong>开始聊天&lt;/strong>： 在终端中输入你的问题或指令，Ollama 会根据模型生成相应的回复。&lt;/li>
&lt;/ol>
&lt;h3 id="安装-ollama">安装 Ollama&lt;/h3>
&lt;h4 id="macos">macOS&lt;/h4>
&lt;p>&lt;a class="link" href="https://ollama.com/download/Ollama-darwin.zip" target="_blank" rel="noopener" >下载 Ollama for macOS
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h4 id="windows">Windows&lt;/h4>
&lt;p>&lt;a class="link" href="https://ollama.com/download/OllamaSetup.exe" target="_blank" rel="noopener" >下载 Ollama for Windows
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/p>
&lt;h4 id="linux">Linux&lt;/h4>
&lt;pre>&lt;code class="language-bash">curl -fsSL https://ollama.com/install.sh | sh
&lt;/code>&lt;/pre>
&lt;h4 id="docker">Docker&lt;/h4>
&lt;h5 id="cpu-版本">CPU 版本&lt;/h5>
&lt;pre>&lt;code class="language-bash">docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
&lt;/code>&lt;/pre>
&lt;h5 id="gpu-版本">GPU 版本&lt;/h5>
&lt;ol>
&lt;li>安装 &lt;a class="link" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation" target="_blank" rel="noopener" >Nvidia container toolkit
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;li>在 Docker 容器中运行 Ollama&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
&lt;/code>&lt;/pre>
&lt;h3 id="启动-ollama">启动 Ollama&lt;/h3>
&lt;pre>&lt;code class="language-bash">ollama serve
&lt;/code>&lt;/pre>
&lt;p>输出以下信息表示 Ollama 服务器已成功启动（V100 机器）：&lt;/p>
&lt;pre>&lt;code class="language-bash">$ ollama serve
### 省略的日志输出 ###
Listening on [::]:11434 (version 0.1.42)
&lt;/code>&lt;/pre>
&lt;h3 id="下载模型">下载模型&lt;/h3>
&lt;pre>&lt;code class="language-bash">ollama pull qwen2:72b
&lt;/code>&lt;/pre>
&lt;h3 id="运行模型">运行模型&lt;/h3>
&lt;pre>&lt;code class="language-bash">ollama run qwen2:72b
&lt;/code>&lt;/pre>
&lt;p>例如，运行如下命令后：&lt;/p>
&lt;pre>&lt;code class="language-bash">$ ollama run qwen2:72b
&amp;gt;&amp;gt;&amp;gt; Who are you?
I am Qwen, a pre-trained language model developed by Alibaba Cloud. My purpose is to assist users in generating various types of text, such as articles, stories, poems, and answering
questions by using the natural language processing techniques. How may I assist you today?
&amp;gt;&amp;gt;&amp;gt; Send a message(/? for help)
&lt;/code>&lt;/pre>
&lt;h4 id="docker-容器中运行模型">Docker 容器中运行模型&lt;/h4>
&lt;pre>&lt;code class="language-bash">docker exec -it ollama ollama run qwen2:72b
&lt;/code>&lt;/pre>
&lt;h3 id="配置-ollama">配置 Ollama&lt;/h3>
&lt;p>Ollama 提供了多种环境变量以供配置：&lt;/p>
&lt;ul>
&lt;li>&lt;code>OLLAMA_DEBUG&lt;/code>：是否开启调试模式，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_FLASH_ATTENTION&lt;/code>：是否闪烁注意力，默认为 &lt;code>true&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_HOST&lt;/code>：Ollama 服务器的主机地址，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_KEEP_ALIVE&lt;/code>：保持连接的时间，默认为 &lt;code>5m&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_LLM_LIBRARY&lt;/code>：LLM 库，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_LOADED_MODELS&lt;/code>：最大加载模型数，默认为 &lt;code>1&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_QUEUE&lt;/code>：最大队列数，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_VRAM&lt;/code>：最大虚拟内存，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MODELS&lt;/code>：模型目录，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_NOHISTORY&lt;/code>：是否保存历史记录，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_NOPRUNE&lt;/code>：是否启用剪枝，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_NUM_PARALLEL&lt;/code>：并行数，默认为 &lt;code>1&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_ORIGINS&lt;/code>：允许的来源，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_RUNNERS_DIR&lt;/code>：运行器目录，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_SCHED_SPREAD&lt;/code>：调度分布，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_TMPDIR&lt;/code>：临时文件目录，默认为空。Here is the optimized list in the desired format:&lt;/li>
&lt;li>&lt;code>OLLAMA_DEBUG&lt;/code>：是否开启调试模式，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_FLASH_ATTENTION&lt;/code>：是否闪烁注意力，默认为 &lt;code>true&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_HOST&lt;/code>：Ollama 服务器的主机地址，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_KEEP_ALIVE&lt;/code>：保持连接的时间，默认为 &lt;code>5m&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_LLM_LIBRARY&lt;/code>：LLM 库，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_LOADED_MODELS&lt;/code>：最大加载模型数，默认为 &lt;code>1&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_QUEUE&lt;/code>：最大队列数，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MAX_VRAM&lt;/code>：最大虚拟内存，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_MODELS&lt;/code>：模型目录，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_NOHISTORY&lt;/code>：是否保存历史记录，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_NOPRUNE&lt;/code>：是否启用剪枝，默认为 &lt;code>false&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_NUM_PARALLEL&lt;/code>：并行数，默认为 &lt;code>1&lt;/code>。&lt;/li>
&lt;li>&lt;code>OLLAMA_ORIGINS&lt;/code>：允许的来源，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_RUNNERS_DIR&lt;/code>：运行器目录，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_SCHED_SPREAD&lt;/code>：调度分布，默认为空。&lt;/li>
&lt;li>&lt;code>OLLAMA_TMPDIR&lt;/code>：临时文件目录，默认为空。&lt;/li>
&lt;/ul>
&lt;h2 id="进阶用法hpc-集群上部署-ollama">进阶用法：HPC 集群上部署 Ollama&lt;/h2>
&lt;p>对于大型模型或需要更高性能的情况，可以利用 HPC 集群的强大算力来运行 Ollama。结合 Slurm 进行任务管理，并使用端口映射将服务暴露到本地，即可方便地进行远程访问和使用：&lt;/p>
&lt;ol>
&lt;li>在登录节点配置 Ollama 环境： 安装 Ollama，并下载需要的模型。&lt;/li>
&lt;li>&lt;strong>编写 slurm 脚本&lt;/strong>： 指定资源需求（CPU、内存、GPU 等），并使用 &lt;code>ollama serve&lt;/code> 命令启动模型服务，并绑定到特定端口。&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">#!/bin/bash
#SBATCH --job-name=ollama
#SBATCH -N 1
#SBATCH -p GPU
#SBATCH --gres=gpu:1
module load CUDA
ollama serve
&lt;/code>&lt;/pre>
&lt;ol start="3">
&lt;li>&lt;strong>提交 slurm 任务&lt;/strong>: 使用 &lt;code>sbatch&lt;/code> 命令提交脚本，Slurm 会将任务分配到计算节点运行。&lt;/li>
&lt;li>&lt;strong>本地端口映射&lt;/strong>： 使用 ssh -L 命令将计算节点的端口映射到本地，例如:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">ssh -t -t 用户名@登录节点 ip -L 11434:localhost:11434 -i 登录节点私钥 ssh 计算节点 IP -L 11434:127.0.0.1:11434
&lt;/code>&lt;/pre>
&lt;ol start="5">
&lt;li>&lt;strong>本地访问&lt;/strong>： 在浏览器或应用程序中访问 http://localhost:11434 即可使用 Ollama 服务。&lt;/li>
&lt;/ol>
&lt;div class="notice notice-tip" >
&lt;div class="notice-title">&lt;svg t="1705945832245" class="icon notice-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="19409" width="200" height="200">&lt;path d="M512 74.666667C270.933333 74.666667 74.666667 270.933333 74.666667 512S270.933333 949.333333 512 949.333333 949.333333 753.066667 949.333333 512 753.066667 74.666667 512 74.666667z m238.933333 349.866666l-2.133333 2.133334-277.333333 277.333333c-10.666667 10.666667-29.866667 12.8-42.666667 2.133333L426.666667 704l-149.333334-149.333333c-12.8-12.8-12.8-32 0-44.8 10.666667-10.666667 29.866667-12.8 42.666667-2.133334l2.133333 2.133334 125.866667 125.866666 253.866667-253.866666c10.666667-10.666667 29.866667-12.8 42.666666-2.133334l2.133334 2.133334c12.8 12.8 12.8 32 4.266666 42.666666z" fill="#ffffff" p-id="19410">&lt;/path>&lt;/svg>&lt;/div>&lt;p>注意：由于计算节点不联网，需要提前在登录节点使用 &lt;code>ollama pull&lt;/code> 下载所需模型。此外，需要设置 &lt;code>OLLAMA_ORIGINS&lt;/code> 为 &lt;code>*&lt;/code>，设置 &lt;code>OLLAMA_HOST&lt;/code> 为 &lt;code>0.0.0.0&lt;/code>，以允许所有来源访问服务。&lt;/p>&lt;/div>
&lt;h2 id="进阶用法本地代码补全助手">进阶用法：本地代码补全助手&lt;/h2>
&lt;p>Ollama 不仅可以用于聊天和文本创作，还可以结合代码生成模型和 IDE 插件，打造强大的代码补全助手。例如，使用 Codeqwen 7B 模型和 VS Code 插件 &lt;a class="link" href="https://continue.dev/" target="_blank" rel="noopener" >Continue
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
，可以实现高效便捷的代码补全功能。&lt;/p>
&lt;p>首先介绍一下 Continue :
&lt;blockquote>
&lt;p>&lt;p>&lt;a class="link" href="https://continue.dev/" target="_blank" rel="noopener" >Continue
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
enables you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs. All this can run entirely on your own laptop or have Ollama deployed on a server to remotely power code completion and chat experiences based on your needs.&lt;/p>
&lt;p>Continue 使您能够轻松地在 Visual Studio Code 和 JetBrains 中创建自己的代码助手，利用开源 LLM。这一切都可以完全在您的笔记本电脑上运行，或者在服务器上部署 Ollama，远程根据您的需求提供代码补全和聊天体验。&lt;/p>
&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Continue&lt;/span>&lt;cite>&lt;/cite>&lt;/span>&lt;/blockquote>&lt;/p>
&lt;p>在开始之前，你需要安装如下工具：&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://docs.continue.dev/quickstart" target="_blank" rel="noopener" >Continue
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
：&lt;a class="link" href="https://marketplace.visualstudio.com/items?itemName=Continue.continue" target="_blank" rel="noopener" >VS Code 版本
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
或 &lt;a class="link" href="https://plugins.jetbrains.com/plugin/22707-continue" target="_blank" rel="noopener" >JetBrains 版本
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;li>&lt;a class="link" href="https://ollama.com/" target="_blank" rel="noopener" >Ollama
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;/ul>
&lt;p>接下来，我们以 VS Code 为例，介绍如何使用 Ollama + Continue 实现代码补全功能：&lt;/p>
&lt;h3 id="codestral-22b-模型">Codestral 22B 模型&lt;/h3>
&lt;p>Codestral 既能完成代码自动补全，也支持聊天功能。但鉴于其拥有 220 亿参数且不具备生产许可，它对显存要求颇高，仅限于研究和测试使用，因此可能并不适合日常本地应用。&lt;/p>
&lt;h4 id="下载并运行-codestral-模型">下载并运行 Codestral 模型&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run codestral
&lt;/code>&lt;/pre>
&lt;h4 id="配置-configjson">配置 config.json&lt;/h4>
&lt;ul>
&lt;li>在 VS Code 侧边栏点击 Continue 插件图标，然后在面板右下角点击 “齿轮” 图标，打开 &lt;code>config.json&lt;/code> 文件。然后复制以下配置到 &lt;code>config.json&lt;/code> 文件中：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-json">{
&amp;quot;models&amp;quot;: [
{
&amp;quot;title&amp;quot;: &amp;quot;Codestral&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;codestral&amp;quot;
}
],
&amp;quot;tabAutocompleteModel&amp;quot;: {
&amp;quot;title&amp;quot;: &amp;quot;Codestral&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;codestral&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="deepseek-coder-67b-模型--llama-3-8b-模型">DeepSeek Coder 6.7B 模型 + Llama 3 8B 模型&lt;/h3>
&lt;p>根据机器的显存大小，可以利用 Ollama 同时运行多个模型并处理多个并发请求的能力，使用 &lt;code>DeepSeek Coder 6.7B&lt;/code> 进行自动补全，&lt;code>Llama 3 8B&lt;/code> 进行聊天。如果你的机器无法同时运行两者，那么可以分别尝试，决定你更偏好本地自动补全还是本地聊天体验。&lt;/p>
&lt;h4 id="下载并运行-deepseek-coder-模型">下载并运行 DeepSeek Coder 模型&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run deepseek-coder:6.7b-base
&lt;/code>&lt;/pre>
&lt;h4 id="下载并运行-llama-3-模型">下载并运行 Llama 3 模型&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run llama3:8b
&lt;/code>&lt;/pre>
&lt;h4 id="配置-configjson-1">配置 config.json&lt;/h4>
&lt;pre>&lt;code class="language-json">{
&amp;quot;models&amp;quot;: [
{
&amp;quot;title&amp;quot;: &amp;quot;Llama 3 8B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;llama3:8b&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
],
&amp;quot;tabAutocompleteModel&amp;quot;: {
&amp;quot;title&amp;quot;: &amp;quot;DeepSeek Coder 6.7B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;deepseek-coder:6.7b-base&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="codeqwen-7b-模型--qwen2-7b-模型">Codeqwen 7B 模型 + Qwen2 7B 模型&lt;/h3>
&lt;p>Codeqwen 7B 模型是一个专门用于代码补全的模型，而 Qwen2 7B 模型则是一个通用的聊天模型。这两个模型可以很好地结合在一起，实现代码补全和聊天功能。&lt;/p>
&lt;h4 id="下载并运行-codeqwen-模型">下载并运行 Codeqwen 模型&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run codeqwen
&lt;/code>&lt;/pre>
&lt;h4 id="下载并运行-qwen2-模型">下载并运行 Qwen2 模型&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run qwen2:7b
&lt;/code>&lt;/pre>
&lt;h4 id="配置-configjson-2">配置 config.json&lt;/h4>
&lt;pre>&lt;code class="language-json">{
&amp;quot;models&amp;quot;: [
{
&amp;quot;title&amp;quot;: &amp;quot;Codeqwen 7B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;codeqwen&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
],
&amp;quot;tabAutocompleteModel&amp;quot;: {
&amp;quot;title&amp;quot;: &amp;quot;Qwen2 7B&amp;quot;,
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;qwen2:7b&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="利用-rag-向量检索优化聊天">利用 RAG 向量检索优化聊天&lt;/h3>
&lt;p>Continue 内置了 &lt;a class="link" href="https://docs.continue.dev/customization/context-providers#codebase-retrieval" target="_blank" rel="noopener" >@codebase
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
上下文提供器，能自动从代码库中检索到最相关的代码片段。假如你已经设置好了聊天模型（例如 Codestral、Llama 3），那么借助 Ollama 和 &lt;a class="link" href="https://blog.lancedb.com/lancedb-x-continue/" target="_blank" rel="noopener" >LanceDB
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
的向量化技术，可以实现更高效的代码检索和聊天体验。&lt;/p>
&lt;p>这里，我们使用 &lt;code>nomic-embed-text&lt;/code> 模型作为向量检索模型：&lt;/p>
&lt;h4 id="下载并运行-nomic-embed-text-模型">下载并运行 Nomic Embed Text 模型&lt;/h4>
&lt;pre>&lt;code class="language-bash">ollama run nomic-embed-text
&lt;/code>&lt;/pre>
&lt;h4 id="配置-configjson-3">配置 config.json&lt;/h4>
&lt;ul>
&lt;li>在文件中添加以下内容：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-json">{
&amp;quot;embeddingsProvider&amp;quot;: {
&amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;,
&amp;quot;model&amp;quot;: &amp;quot;nomic-embed-text&amp;quot;,
&amp;quot;apiBase&amp;quot;: &amp;quot;http://127.0.0.1:11434&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="代码补全效果">代码补全效果&lt;/h3>
&lt;ul>
&lt;li>&lt;code>Ctrl + I&lt;/code>: 根据指令生成代码片段。&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_1-2024-06-17.png"
alt="codeqwen_1-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_2-2024-06-17.png"
alt="codeqwen_2-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_3-2024-06-17.png"
alt="codeqwen_3-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>光标悬停自动补全代码&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_4-2024-06-17.png"
alt="codeqwen_4-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;h3 id="与-ollama-聊天">与 Ollama 聊天&lt;/h3>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_5-2024-06-17.png"
alt="codeqwen_5-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;h3 id="代码自动注释">代码自动注释&lt;/h3>
&lt;ul>
&lt;li>选中代码打开右键菜单&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_6-2024-06-17.png"
alt="codeqwen_6-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/codeqwen_7-2024-06-17.png"
alt="codeqwen_7-2024-06-17" width="90%" loading="lazy">
&lt;/figure>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>Ollama 为我们打开了通往开源 LLM 世界的大门，让每个人都能轻松体验 LLM 的强大功能，并可以根据自身需求进行定制化应用。无论是进行研究、开发，还是日常使用，Ollama 都能为你提供探索 LLM 无限可能的平台。相信随着 Ollama 的不断发展，它将为我们带来更多惊喜，推动 LLM 技术在各个领域的应用和发展。&lt;/p></description></item><item><title>RDMA 之 Address Handle</title><link>https://cuterwrite.top/p/rdma-address-handle/</link><pubDate>Sat, 15 Jun 2024 01:00:00 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-address-handle/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_116373922_p9_master1200.webp" alt="Featured image of post RDMA 之 Address Handle" />&lt;h1 id="rdma-之-address-handle">RDMA 之 Address Handle&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/163552044">&lt;cite>知乎专栏：8. RDMA 之 Address Handle&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>前面已经介绍过，RDMA 通信的基本单元是 QP。我们来思考一个问题，假设 A 节点的某个 QP 要跟 B 节点的某个 QP 交换信息，除了要知道 B 节点的 QP 序号——QPN 之外，还需要什么信息？要知道，QPN 是每个节点独立维护的序号，不是整个网络中唯一的。比如 A 的 QP 3 要跟 B 的 QP 5 通信，网络中可不止一个 QP5，可能有很多个节点都有自己的 QP 5。所以我们自然可以想到，还需要找到让每个节点都有一个独立的标识。&lt;/p>
&lt;p>在传统 TCP-IP 协议栈中，使用了家喻户晓的 IP 地址来标识网络层的每个节点。而 IB 协议中的这个标识被称为&lt;strong>GID（Global Identifier，全局 ID）&lt;/strong>，是一个 128 bits 的序列。关于 GID 本篇不展开讨论，将在后面介绍。&lt;/p>
&lt;h2 id="ah-是什么">AH 是什么&lt;/h2>
&lt;p>AH 全称为 Address Handle，没有想到特别合适的中文翻译，就先直译为“地址句柄”吧。这里的地址，指的是一组用于找到某个远端节点的信息的集合，在 IB 协议中，地址指的是 GID、端口号等等信息；而所谓句柄，我们可以理解为一个指向某个对象的指针。&lt;/p>
&lt;p>大家是否还记得 IB 协议中有四种基本服务类型——RC、UD、RD 和 UC，其中最常用的是 RC 和 UD。RC 的特点是两个节点的 QP 之间会建立可靠的连接，一旦建立连接关系便不容易改变，对端的信息是创建 QP 的时候储存在 QP Context 中的；&lt;/p>
&lt;p>而对于 UD 来说，QP 间没有连接关系，用户想发给谁，就在 WQE 中填好对端的地址信息就可以了。&lt;strong>用户不是直接把对端的地址信息填到 WQE 中的，而是提前准备了一个“地址薄”，每次通过一个索引来指定对端节点的地址信息，而这个索引就是 AH。&lt;/strong>&lt;/p>
&lt;p>AH 的概念大致可以用下图表示：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_8_1.webp"
alt="2024-06-16_8_1" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>Address Handle 功能示意图&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>对于每一个目的节点，本端都会创建一个对应的 AH，而同一个 AH 可以被多个 QP 共同使用。&lt;/p>
&lt;h2 id="ah-的作用">AH 的作用&lt;/h2>
&lt;p>每次进行 UD 服务类型的通信之前，用户都需要先通过 IB 框架提供的接口，来&lt;strong>为每一个可能的对端节点创建一个 AH&lt;/strong>，然后这些 AH 会被驱动放到一个“安全”的区域，并返回一个索引（指针/句柄）给用户。用户真正下发 WR（Work Request）时，就把这个索引传递进来就可以了。&lt;/p>
&lt;p>上述过程如下图所示，A 节点收到用户的这样一个任务——使用本端的 QP4 与 B 节点（通过 AH 指定）的 QP3 进行数据交换：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_8_2.webp"
alt="2024-06-16_8_2" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>UD 服务类型使用 AH 指定对端节点&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>IB 协议中并没有对为什么使用 AH 做出解释，我认为定义 AH 的概念的原因有以下三种：&lt;/p>
&lt;ol>
&lt;li>保证目的地址可用，提高效率&lt;/li>
&lt;/ol>
&lt;p>因为 UD 无连接的特点，用户可以在用户态直接通过 WR 来指定目的地。而如果让用户随意填写地址信息，然后硬件就根据这些信息进行组包的话，是会带来问题的。比如有这样一种场景：用户通过 WR 告诉硬件请给 GID 为 X，MAC 地址为 Y 的节点的端口 Z 发送数据。然而 X，Y，Z 可能不是一个合法的组合，或者 GID 为 X 的节点压根都不存在于网络中，而硬件是无力校验这些内容的，只能乖乖的组包、发送数据，这个目的地无效的数据包就白白发送出去了。&lt;/p>
&lt;p>而提前准备好地址信息，则可以避免上述情况。用户在创建 AH 时会陷入内核态，如果用户传递的参数有效，内核会把这些目的节点信息储存起来，生成一个指针返回给用户；如果用户传递的参数无效，AH 将创建失败。这一过程可以保证地址信息是有效的。用户通过指针就可以快速指定目的节点，加快数据交互流程。&lt;/p>
&lt;p>可能有人会问，既然内核是可信的，为什么不能在发送数据时陷入内核态去校验用户传递的地址信息呢？请别忘了 RDMA 技术的一大优势在哪里——数据流程可以直接从用户空间到硬件，完全绕过内核，这样可以避免系统调用和拷贝的开销。如果每次发送都要检验地址合法性的话，必然会降低通信速率。&lt;/p>
&lt;ol start="2">
&lt;li>向用户隐藏底层地址细节&lt;/li>
&lt;/ol>
&lt;p>用户创建 AH 时，只需要传递 gid、端口号、静态速率等信息，而其他通信所需的地址信息（主要是 MAC 地址）是内核驱动通过查询系统邻居表等方式解析到的，底层没有必要暴露这些额外的信息给用户层。&lt;/p>
&lt;ol start="3">
&lt;li>可以使用 PD 对目的地址进行管理&lt;/li>
&lt;/ol>
&lt;p>前文我们介绍保护域时曾经提过，除了 QP、MR 之外，AH 也由 PD 来进行资源划分。当定义了 AH 这个软件实体之后，我们就可以对所有的 QP 可达的目的地进行相互隔离和管理。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-16_8_3.webp"
alt="2024-06-16_8_3" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>使用 PD 隔离 AH&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>​比如上图中，AH1~3 只能被同一个 PD 下的 QP3 和 QP9 使用，而 AH4 只能被 QP5 使用。&lt;/p>
&lt;h2 id="协议相关章节">协议相关章节&lt;/h2>
&lt;p>协议中关于 AH 的篇幅并不多，甚至没有独立介绍其概念的章节：&lt;/p>
&lt;p>[1] 9.8.3 UD 服务类型中的目的地址由哪些部分组成：包括 AH、 QPN 和 Q_key&lt;/p>
&lt;p>[2] 10.2.2.2 目的地址的相关注意事项&lt;/p>
&lt;p>[3] 11.2.2.1 AH 相关的 Verbs 接口&lt;/p>
&lt;p>AH 就介绍到这里，感谢阅读。下一篇打算向大家描述更多关于 QP 的细节。&lt;/p></description></item><item><title>Docker Hub 镜像下架解决方案</title><link>https://cuterwrite.top/p/dockerhub-takedown/</link><pubDate>Wed, 12 Jun 2024 19:00:00 +0000</pubDate><guid>https://cuterwrite.top/p/dockerhub-takedown/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-05-28_crop_68935d3d33a3abc75c797b67ad25160f195413.webp" alt="Featured image of post Docker Hub 镜像下架解决方案" />&lt;h1 id="docker-hub-镜像下架解决方案">Docker Hub 镜像下架解决方案&lt;/h1>
&lt;p>Docker Hub 作为 Docker 官方的镜像仓库，拥有着丰富的镜像资源， 极大地方便了开发者获取和使用各种软件和服务。然而，从 2024 年 6 月 6 日开始，国内各大镜像站点陆续出现了 Docker Hub 镜像下架的情况，包括阿里云、科大、南大、上交等全部挂掉，导致很多开发者无法正常拉取镜像。在执行 &lt;code>docker pull&lt;/code> 命令拉取镜像 docker 镜像时无反应，会一直循环尝试。&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-13_takedown-dockerhub.webp"
alt="2024-06-13_takedown-dockerhub" width="auto" loading="lazy">
&lt;/figure>
&lt;p>Docker Hub 镜像下架的可能原因主要为一些镜像包含违规内容，导致上面信息监管部门出了最新要求，要求各大镜像站点下架相关镜像。&lt;/p>
&lt;h2 id="解决方案">解决方案&lt;/h2>
&lt;p>面对 Docker Hub 镜像下架问题，目前我们可以通过以下几种方式解决：&lt;/p>
&lt;h3 id="1-使用-atomhub-镜像站点">1. 使用 AtomHub 镜像站点&lt;/h3>
&lt;p>AtomHub 是由开放原子开源基金会发起，遵循 OCI 标准，旨在为开发者提供开放中立、安全可信、高效便捷的新一代开源容器镜像中心。其具有官方背书，是当前唯一正常的 Docker Hub 镜像站点。&lt;/p>
&lt;p>不过，AtomHub 的问题是镜像数量较少，目前只有几百个镜像文件；以及，部分软件的版本较旧。&lt;/p>
&lt;h4 id="配置-atomhub-镜像站点">配置 AtomHub 镜像站点&lt;/h4>
&lt;ul>
&lt;li>修改 /etc/docker/daemon.json 文件，添加以下内容：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-json">{
&amp;quot;registry-mirrors&amp;quot;: [&amp;quot;https://atomhub.openatom.cn&amp;quot;]
}
&lt;/code>&lt;/pre>
&lt;h4 id="重启-docker-服务">重启 Docker 服务：&lt;/h4>
&lt;pre>&lt;code class="language-bash">sudo systemctl daemon-reload
sudo systemctl restart docker
&lt;/code>&lt;/pre>
&lt;p>然后就可以正常拉取一些常用镜像了。但是，如果你需要的镜像不在 AtomHub 上，这个方法就不适用了。&lt;/p>
&lt;h3 id="2-配置-vpn-代理">2. 配置 VPN 代理&lt;/h3>
&lt;p>通过配置 VPN 代理，可以访问被屏蔽的 Docker Hub 源站点，从而拉取镜像。但是，这种方法需要自备 VPN 服务，且速度可能较慢且不稳定。&lt;/p>
&lt;h4 id="配置方法">配置方法&lt;/h4>
&lt;ul>
&lt;li>创建 &lt;code>/etc/systemd/system/docker.service.d/http-proxy.conf&lt;/code> 文件，并添加以下内容：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">[Service]
Environment=&amp;quot;HTTP_PROXY=http://your_proxy_server:port&amp;quot;
Environment=&amp;quot;HTTPS_PROXY=http://your_proxy_server:port&amp;quot;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>重启 Docker 服务：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">sudo systemctl daemon-reload
sudo systemctl restart docker
&lt;/code>&lt;/pre>
&lt;h3 id="3-使用-skopeo-拷贝镜像到私有镜像仓库">3. 使用 Skopeo 拷贝镜像到私有镜像仓库&lt;/h3>
&lt;p>Skopeo 是一个命令行工具，可以用于在不同的镜像仓库之间复制、检查和签名镜像。该方法需要一台海外云主机，且需要没有被墙。&lt;/p>
&lt;h4 id="具体步骤">具体步骤&lt;/h4>
&lt;p>这里以阿里云私有镜像仓库为例，将 Docker Hub 上的镜像复制到阿里云私有镜像仓库。&lt;/p>
&lt;ul>
&lt;li>首先，你需要启用&lt;a class="link" href="https://cr.console.aliyun.com/" target="_blank" rel="noopener" >阿里云容器镜像服务
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
，创建一个个人实例：&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-13_aliyun-container.webp"
alt="2024-06-13_aliyun-container" width="90%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>进入个人实例，创建一个命名空间：&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-13_create-namespace.webp"
alt="2024-06-13_create-namespace" width="90%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>创建一个镜像仓库（对应你想要复制的镜像）：&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-13_create-repository.webp"
alt="2024-06-13_create-repository" width="90%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>设置访问凭证：&lt;/li>
&lt;/ul>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/2024-06-13_set-pass.webp"
alt="2024-06-13_set-pass" width="90%" loading="lazy">
&lt;/figure>
&lt;ul>
&lt;li>然后，登录到你的海外云主机，先安装 Skopeo：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y skopeo
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>使用 Skopeo 拷贝镜像：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash">docker login --username=yourusername registry.cn-hangzhou.aliyuncs.com
skopeo copy docker://docker.io/library/image:tag docker://registry.cn-hangzhou.aliyuncs.com/yournamespace/yourimage:tag
&lt;/code>&lt;/pre>
&lt;h3 id="4-部署私有镜像仓库">4. 部署私有镜像仓库&lt;/h3>
&lt;p>除了以上方法，还可以通过最近一大佬开源的 &lt;a class="link" href="https://github.com/dqzboy/Docker-Proxy" target="_blank" rel="noopener" >Docker-Proxy
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
项目搭建自己的私有镜像仓库。&lt;/p>
&lt;a href="https://github.com/dqzboy/Docker-Proxy" target="_blank" class="card-github fetch-waiting no-styling"
repo="dqzboy/Docker-Proxy" id="repo-AoA94QzCUrFt4oCg-card">
&lt;div class="gc-titlebar">
&lt;div class="gc-titlebar-left">
&lt;div class="gc-owner">
&lt;div id="repo-AoA94QzCUrFt4oCg-avatar" class="gc-avatar">&lt;/div>
&lt;div class="gc-user">dqzboy&lt;/div>
&lt;/div>
&lt;div class="gc-divider">/&lt;/div>
&lt;div class="gc-repo">Docker-Proxy&lt;/div>
&lt;/div>
&lt;div class="github-logo">&lt;/div>
&lt;/div>
&lt;div id="repo-AoA94QzCUrFt4oCg-description" class="gc-description">Waiting for api.github.com...&lt;/div>
&lt;div class="gc-infobar">
&lt;div id="repo-AoA94QzCUrFt4oCg-stars" class="gc-stars">0&lt;/div>
&lt;div id="repo-AoA94QzCUrFt4oCg-forks" class="gc-forks">0&lt;/div>
&lt;div id="repo-AoA94QzCUrFt4oCg-license" class="gc-license">unkown&lt;/div>
&lt;div id="repo-AoA94QzCUrFt4oCg-language" class="gc-language">Waiting...&lt;/div>
&lt;/div>
&lt;/a>
&lt;script id="repo-AoA94QzCUrFt4oCg-script" type="text/javascript" defer>
fetch('https://api.cuterwrite.top/api/repos/dqzboy\/Docker-Proxy', {
referrerPolicy: "no-referrer"
})
.then(response => response.json())
.then(data => {
document.getElementById('repo-AoA94QzCUrFt4oCg-description').innerText = data.description.replace(
/:[a-zA-Z0-9_]+:/g, '');
document.getElementById('repo-AoA94QzCUrFt4oCg-language').innerText = data.language;
document.getElementById('repo-AoA94QzCUrFt4oCg-forks').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.forks).replaceAll("\u202f", '');
document.getElementById('repo-AoA94QzCUrFt4oCg-stars').innerText = Intl.NumberFormat('en-us', {
notation: "compact",
maximumFractionDigits: 1
}).format(data.stargazers_count).replaceAll("\u202f", '');
const avatarEl = document.getElementById('repo-AoA94QzCUrFt4oCg-avatar');
avatarEl.style.backgroundImage = 'url(' + data.owner.avatar_url + ')';
avatarEl.style.backgroundColor = 'transparent';
if (data.license?.spdx_id) {
document.getElementById('repo-AoA94QzCUrFt4oCg-license').innerText = data.license.spdx_id
} else {
document.getElementById('repo-AoA94QzCUrFt4oCg-license').classList.add = "no-license"
};
document.getElementById('repo-AoA94QzCUrFt4oCg-card').classList.remove("fetch-waiting");
console.log("[GITHUB-CARD] Loaded card for dqzboy\/Docker-Proxy.")
}).catch(err => {
const c = document.getElementById('repo-AoA94QzCUrFt4oCg-card');
c.classList.add("fetch-error");
console.warn("[GITHUB-CARD] (Error) Loading card for dqzboy\/Docker-Proxy.")
})
&lt;/script>
&lt;p>该方法也需要一台海外云主机，且需要没有被墙；以及一个域名（不需要备案）。&lt;/p>
&lt;h4 id="部署">部署&lt;/h4>
&lt;p>目前项目提供了三种部署方式，我采用的是第一种方式，使用项目脚本一键部署。&lt;/p>
&lt;ul>
&lt;li>使用项目脚本一键部署：&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-bash"># CentOS
yum -y install wget curl
# ubuntu
apt -y install wget curl
bash -c &amp;quot;$(curl -fsSL https://raw.githubusercontent.com/dqzboy/Docker-Proxy/main/install/DockerProxy_Install.sh)&amp;quot;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>
&lt;p>部署到 Render&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/dqzboy/Docker-Proxy/blob/main/Render/README.md" target="_blank" rel="noopener" >使用 Render 快速部署
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Docker Compose 部署&lt;/p>
&lt;ol>
&lt;li>下载 config 目录下对应的 yml 文件到你本地机器上&lt;/li>
&lt;li>下载 &lt;a class="link" href="https://github.com/dqzboy/Docker-Proxy/blob/main/docker-compose.yaml" target="_blank" rel="noopener" >docker-compose.yaml
&lt;span style="white-space: nowrap;">&lt;svg width=".8em" height=".8em" viewBox="0 0 21 21"
xmlns="http://www.w3.org/2000/svg">
&lt;path d="m13 3l3.293 3.293l-7 7l1.414 1.414l7-7L21 11V3z" fill="currentColor" />
&lt;path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"
fill="currentColor">
&lt;/svg>&lt;/span>
&lt;/a>
文件到你本地机器上，并且与配置文件同级目录下&lt;/li>
&lt;li>执行 &lt;code>docker compose&lt;/code> 命令启动容器服务&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash">docker compose up -d
# 查看容器日志
docker logs -f [容器 ID 或名称]
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul>
&lt;p>最后，需要登录域名解析方，将 &lt;code>hub&lt;/code>、&lt;code>gcr&lt;/code>、&lt;code>ghcr&lt;/code>、&lt;code>k8s-gcr&lt;/code>、&lt;code>ui&lt;/code>、&lt;code>quay&lt;/code> 等 A 记录解析到你的海外云主机 IPv4 地址，然后就可以将 &lt;code>/etc/docker/daemon.json&lt;/code> 中的 &lt;strong>registry-mirrors&lt;/strong> 设置为 &lt;code>https://hub.yourdomain&lt;/code> 进行镜像加速了。&lt;/p></description></item><item><title>RDMA 之 Protection Domain</title><link>https://cuterwrite.top/p/rdma-protection-domain/</link><pubDate>Thu, 18 Apr 2024 21:42:00 +0000</pubDate><guid>https://cuterwrite.top/p/rdma-protection-domain/</guid><description>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/d31a474af07682028ca085f871bc5d07195413-2024-04-19.webp" alt="Featured image of post RDMA 之 Protection Domain" />&lt;h1 id="rdma-之-protection-domain">RDMA 之 Protection Domain&lt;/h1>
&lt;p>&lt;strong>本文欢迎非商业转载，转载请注明出处。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>声明：仅用于收藏，便于阅读&lt;/p>&lt;span class="cite">&lt;span>― &lt;/span>&lt;span>Savir, &lt;/span>&lt;a href="https://zhuanlan.zhihu.com/p/159493100">&lt;cite>知乎专栏：7. RDMA 之 Protection Domain&lt;/cite>&lt;/a>&lt;/span>&lt;/blockquote>
&lt;p>前文我们简单介绍了 RDMA 中最常见的一些资源，包括各种 Queue，以及 MR 的概念等等。MR 用于控制和管理 HCA 对于本端和远端内存的访问权限，确保 HCA 只有拿到正确 Key 之后才能读写用户已经注册了的内存区域。为了更好的保障安全性，IB 协议又提出了 Protection Domain（PD）的概念，用于保证 RDMA 资源间的相互隔离，本文就介绍一下 PD 的概念。&lt;/p>
&lt;h2 id="pd-是什么">PD 是什么&lt;/h2>
&lt;p>PD 全称是 Protection Domain，意为&amp;quot;保护域&amp;quot;。域的概念我们经常见到，从数学上的“实数域”、“复数域”，到地理上的“空域”、“海域”等等，表示一个空间/范围。在 RDMA 中，PD 像是一个容纳了各种资源（QP、MR 等）的“容器”，将这些资源纳入自己的保护范围内，避免他们被未经授权的访问。一个节点中可以定义多个保护域，各个 PD 所容纳的资源彼此隔离，无法一起使用。&lt;/p>
&lt;p>概念还是有些抽象，下面我们来看一下 PD 有什么作用，具体解决了什么问题。&lt;/p>
&lt;h2 id="pd-的作用">PD 的作用&lt;/h2>
&lt;p>一个用户可能创建多个 QP 和多个 MR，每个 QP 可能和不同的远端 QP 建立了连接，比如下图这样（灰色箭头表示 QP 间的连接关系）：&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/7_1-2024-04-19.webp"
alt="7_1-2024-04-19" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>图 1：没有 PD 概念时的 RDMA 资源&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>由于 MR 和 QP 之间并没有绑定关系，这就意味着一旦某个远端的 QP 与本端的一个 QP 建立了连接，具备了通信的条件，那么理论上远端节点只要知道 VA 和 R_key（甚至可以靠不断的猜测直到得到一对有效的值），就可以访问本端节点某个 MR 的内容。&lt;/p>
&lt;p>其实一般情况下，MR 的虚拟地址 VA 和秘钥 R_Key 是很难猜到的，已经可以保证一定的安全性了。但是为了更好的保护内存中的数据，把各种资源的权限做进一步的隔离和划分，我们在又在每个节点中定义了 PD，如下图所示&lt;/p>
&lt;figure>&lt;img src="https://cuterwrite-1302252842.file.myqcloud.com/img/7_2-2024-04-19.webp"
alt="7_2-2024-04-19" width="auto" loading="lazy">&lt;figcaption>
&lt;h4>图 2：加入 PD 概念时的 RDMA 资源&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>图中 Node 0 上有两个 PD，将 3 个 QP 和 2 个 MR 分为了两组，此外 Node 1 和 Node 2 中各有一个 PD 包含了所有 QP 和 MR。Node 0 上的两个 PD 中的资源不可以一起使用，也就是说 QP3 和 QP9 不能访问 MR1 的数据，QP6 也不可以访问 MR0 的数据。如果我们在数据收发时，指定硬件使用 QP3 和 MR1，那么硬件校验他们不属于同一个 PD 后，会返回错误。&lt;/p>
&lt;p>对于远端节点来说，Node1 只能通过 QP8 相连的 QP3 来访问 Node0 的内存，但是因为 Node 0 的 QP3 被“圈”到了 PD0 这个保护域中，所以 Node 1 的 QP8 也只能访问 MR0 对应的内存，&lt;strong>无论如何都无法访问 MR1 中的数据&lt;/strong>，这是从两个方面限制的：&lt;/p>
&lt;ol>
&lt;li>Node 1 的 QP8 只跟 Node 0 的 QP3 有连接关系，无法通过 Node 0 的 QP6 进行内存访问。&lt;/li>
&lt;li>Node 0 的 MR1 和 QP3 属于不同的 PD，就算 Node 1 的 QP8 拿到了 MR1 的 VA 和 R_key，硬件也会因为 PD 不同而拒绝提供服务。&lt;/li>
&lt;/ol>
&lt;p>所以就如本文一开始所说的，PD 就像是一个容器，将一些 RDMA 资源保护起来，彼此隔离，以提高安全性。其实 RDMA 中不止有 QP、MR 这些资源，后文即将介绍的 Address Handle，Memory Window 等也是由 PD 进行隔离保护的。&lt;/p>
&lt;h2 id="如何使用-pd">如何使用 PD&lt;/h2>
&lt;p>还是看上面的图，我们注意到 Node 0 为了隔离资源，存在两个 PD；而 Node 1 和 Node 2 只有一个 PD 包含了所有资源。&lt;/p>
&lt;p>我之所以这样画，是为了说明一个节点上划分多少个 PD 完全是由用户决定的，&lt;strong>如果想提高安全性，那么对每个连接到远端节点的 QP 和供远端访问的 MR 都应该尽量通过划分 PD 做到隔离；如果不追求更高的安全性，那么创建一个 PD，囊括所有的资源也是可以的&lt;/strong>。&lt;/p>
&lt;p>IB 协议中规定：&lt;strong>每个节点都至少要有一个 PD，每个 QP 都必须属于一个 PD，每个 MR 也必须属于一个 PD&lt;/strong>。&lt;/p>
&lt;p>那么 PD 的包含关系在软件上是如何体现的呢？它本身是有一个软件实体的（结构体），记录了这个保护域的一些信息。用户在创建 QP 和 MR 等资源之前，必须先通过 IB 框架的接口创建一个 PD，拿到它的指针/句柄。接下来在创建 QP 和 MR 的时候，需要传入这个 PD 的指针/句柄，PD 信息就会包含在 QP 和 MR 中。硬件收发包时，会对 QP 和 MR 的 PD 进行校验。更多的软件协议栈的内容，我会在后面的文章中介绍。&lt;/p>
&lt;p>另外需要强调的是，&lt;strong>PD 是本地概念，仅存在于节点内部&lt;/strong>，对其他节点是不可见的；而 MR 是对本端和对端都可见的。&lt;/p>
&lt;p>为了方便大家查阅和学习，以后我会列出文章涉及的协议章节，前面的内容有时间的时候我也会补充一下。&lt;/p>
&lt;h2 id="pd-相关协议章节">PD 相关协议章节&lt;/h2>
&lt;ul>
&lt;li>3.5.5 PD 的基本概念和作用&lt;/li>
&lt;li>10.2.3 介绍了 PD 和其他一些 RDMA 资源的关系，以及 PD 相关的软件接口。&lt;/li>
&lt;li>10.6.3.5 再次强调 PD 和 MR 及 QP 的关系。&lt;/li>
&lt;li>11.2.1.5 详细介绍 PD 的 Verbs 接口，包括作用、入参、出参和返回值等。&lt;/li>
&lt;/ul>
&lt;p>好了，关于 PD 的介绍就到这里。下文我会介绍用于 UD 服务类型的 Address Handle 的概念。&lt;/p></description></item></channel></rss>